{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to the UPPMAX documentation  Get support Give anonymous feedback Apply for a project \ud83d\ude80 Getting started \ud83d\ude80 Software \ud83d\ude80 Reset your UPPMAX password \u26a0\ufe0f Migration to Dardel \ud83d\ude80 Go to the UPPMAX main page"},{"location":"backup/","title":"Backup","text":""},{"location":"contributors/","title":"Contributors","text":"","tags":["contributors","thanks","thank you"]},{"location":"contributors/#contributors","title":"Contributors","text":"<p>Thanks to all these for helping improve this documentation, with many being unmentioned:</p> <ul> <li>All direct contributors</li> <li>Fredrik Boulund</li> </ul>","tags":["contributors","thanks","thank you"]},{"location":"support/","title":"Support","text":"","tags":["support","help","contact","email","ask","talk"]},{"location":"support/#uppmax-support","title":"UPPMAX support","text":"<p>If you lost your UPPMAX password, see how to reset your UPPMAX password.</p> <p>If you need other help on using UPPMAX, you preferably contact us through the Support Form. If that does not work, use <code>support@uppmax.uu.se</code>.</p> <p>If you want to contribute, see how to contribute.</p> <p>If you need general Uppsala University IT support, use the Uppsala University IT Servicedesk.</p> <p>If you plan a course using UPPMAX resources, see Education and training projects</p>","tags":["support","help","contact","email","ask","talk"]},{"location":"cluster_guides/arrhenius/","title":"Arrhenius","text":""},{"location":"cluster_guides/arrhenius/#arrhenius","title":"Arrhenius","text":"<p>Arrhenius is a future NAISS cluster and likely the successor of Bianca and Rackham and Tetralith.</p> <p>Some of its features, as shown at the NAISS User Meeting of 2024-10-01 (and are likely to change):</p> <ul> <li>Around 40 PFlops</li> <li>Has CPUs and GPUs</li> <li>Allows for regular and sensitive data</li> <li>Allows for cloud services</li> <li>Allows for AI</li> <li>Storage about 27 PB for regular data, 10 PB for sensitive data</li> <li>65% will be owned by Sweden (the numbers above show the values for the   complete cluster)</li> </ul>"},{"location":"cluster_guides/backup/","title":"Backup","text":"","tags":["backup","back-up","back up"]},{"location":"cluster_guides/backup/#backup","title":"Backup","text":"<p>A backup allows one to restore his/her data after it has been (accidentally) lost.</p> <p>This page describes how UPPMAX does backups.</p> <p>All folders have a backup, except those in a folder called <code>nobackup</code>.</p> <p>While UPPMAX systems may have backup, these are not designed to act as the sole repository of primary data, e.g. raw data or originals.</p> <p>The PI is the main responsible person</p> <p>A PI and his/her academic institution are ultimately responsible for his/her data.</p> <p>We recommend PIs to maintain a primary copy of their data on a system they control, when possible.</p> <p>If not, ensure that collaborators can only use the data in a responsible way. See the best practices on an UPPMAX filesystem</p>","tags":["backup","back-up","back up"]},{"location":"cluster_guides/backup/#how-can-i-access-my-backups","title":"How can I access my backups?","text":"<p>Contact UPPMAX support and ask for help. Provide as much information as possible, especially directory and file names.</p>","tags":["backup","back-up","back up"]},{"location":"cluster_guides/backup/#what-is-the-uppmax-backup-procedure","title":"What is the UPPMAX backup procedure?","text":"<p>UPPMAX performs an incremental backup with 30 day retention.</p> <p>This means:</p> <ul> <li>After 30 days: your data is irretrievably gone</li> <li>Until 30 days: you can get your data back. If you've edited data,   there is change you may be able to retrieve the newest version</li> </ul> What determines if a newly-edited file gets a backup? <ul> <li>The duration of the change persisting.   For example, a file that is created and deleted   within a day is unlikely to get a backup.   The longer the change persisted,   the likelier it is to have   its latest version in the backup</li> <li>The workload of the backup service is low.   The lower the workload of the backup service,   the likelier it is you have more recent versions of your files   in a backup</li> </ul> <p>The backup service works best when it can keep up with the changes on files that have a backup.</p> <p>One important way to help work the backup service, is to put intermediate/temporary data in a directory with <code>nobackup</code> in its name.</p> <p>All folders have a backup, except those in a folder called <code>nobackup</code>, such as:</p> Folder Example Description Exceptions <code>/home/[username]</code> <code>/home/sven</code> Your home folder Folders named <code>nobackup</code> <code>/proj/sensYYYYXXX</code> <code>/proj/sens2016001</code> Sensitive data project Folders named <code>nobackup</code> <code>/proj/sllstoreYYYYXXX</code> <code>/proj/sllstore2017096</code> SciLifeLab Storage Folders named <code>nobackup</code> <code>/proj/uppoff20YYXXX</code> <code>/proj/uppoff2021003</code> UPPMAX offload storage Folders named <code>nobackup</code> <code>/proj/snicYYYY-X-ZZZZ</code> <code>/proj/snic2022-6-85</code> SNIC projects Folders named <code>nobackup</code> <p>Additionally, your home folder has snapshots taken, which take place more often and can be recovered yourself. See the UPPMAX documentation on snapshots.</p>","tags":["backup","back-up","back up"]},{"location":"cluster_guides/backup/#what-should-i-put-in-directories-with-backup","title":"What should I put in directories with backup?","text":"<p>Irreplaceable data that you are not actively working on.</p> What are examples of irreplaceable data? <p>Examples of irreplaceable data are:</p> <ul> <li>Raw/unprocessed measurements, which cannot be reproduced from   a script</li> <li>Scripts for your analysis</li> </ul> Why should I not work actively on my data in a regular folder? <p>The backup mechanisms cannot keep up with large amounts of files changing on a rapid basis.</p>","tags":["backup","back-up","back up"]},{"location":"cluster_guides/backup/#what-should-i-put-in-directories-without-backup","title":"What should I put in directories without backup?","text":"<p>Reproducible/intermediate data that you are actively working on.</p> <p>The backup mechanisms cannot keep up with large amounts of files changing on a rapid basis.</p>","tags":["backup","back-up","back up"]},{"location":"cluster_guides/backup/#how-robust-is-uppmax-storage","title":"How robust is UPPMAX storage?","text":"<p>The hardware setup of UPPMAX storage is robust and unlikely to be the cause of lost data.</p> How is the hardware set up? <p>All UPPMAX storage systems use RAID technology to make storage more robust through redundancy.</p> <p>This means that two or more disks must fail in the same 'RAID volume' before there is a risk of data loss, which has a rather low chance.</p> <p>Still, this does not protect against disasters, e.g. a fire in the computer hall.</p> <p>To take this into account, backups are sent off-site to either KTH or LiU, depending on the storage system.</p> <p>This setup, however, does not protect against user error (e.g. removing all files in your project directory).</p>","tags":["backup","back-up","back up"]},{"location":"cluster_guides/bianca/","title":"Bianca","text":"","tags":["Bianca","cluster","sensitive data"]},{"location":"cluster_guides/bianca/#bianca","title":"Bianca","text":"<p>Bianca is one of the UPPMAX clusters, suitable for working with sensitive data.</p> <p>In the near future, Bianca will be replaced by Maja.</p> <ul> <li>Bianca's name</li> <li>Bianca's design</li> <li>Bianca's hardware</li> <li>Log in</li> <li>Submitting jobs, using Slurm</li> <li>Starting an interactive session</li> <li>File transfer<ul> <li>File transfer using rsync (recommended)</li> <li>File transfer using FileZilla (easiest)</li> </ul> </li> <li>The module system</li> <li>IDEs<ul> <li>Jupyter</li> <li>RStudio</li> <li> VSCode</li> <li>VSCodium</li> </ul> </li> <li>Courses and workshops</li> <li>NAISS-sens</li> <li>Best practices<ul> <li>Git on Bianca</li> </ul> </li> <li>Bianca installation guides</li> </ul>","tags":["Bianca","cluster","sensitive data"]},{"location":"cluster_guides/bianca_fingerprints/","title":"Bianca fingerprints","text":"","tags":["Bianca","fingerprint","SHA256"]},{"location":"cluster_guides/bianca_fingerprints/#bianca-fingerprints","title":"Bianca fingerprints","text":"<p>Below are the valid fingerprints on Bianca. If you see these, you can assume you are actually connecting to Bianca.</p> <pre><code>2048 SHA256:WDimSDqd6+b9bgbcNP+zCMCUp3Cen6Js7lAXPVFscBw bianca (RSA)\n256 SHA256:Z9FeKcfgw9PicHfotfkCVZTzWTY0xPjy0qa9Ap/7Aws bianca (ECDSA)\n256 SHA256:ZUsJUznqix7DjFbwV90nhfKq5u/x3+GUSX7F6C9s3rA bianca (ED25519)\n</code></pre>","tags":["Bianca","fingerprint","SHA256"]},{"location":"cluster_guides/bianca_login_node/","title":"Bianca login node","text":"","tags":["Bianca","login node","login","node"]},{"location":"cluster_guides/bianca_login_node/#bianca-login-node","title":"Bianca login node","text":"<p>There are running up to 7 login nodes on a physical Bianca-node.</p>","tags":["Bianca","login node","login","node"]},{"location":"cluster_guides/bianca_modules/","title":"Working with environment modules on Bianca","text":"","tags":["modules","Bianca"]},{"location":"cluster_guides/bianca_modules/#working-with-environment-modules-on-bianca","title":"Working with environment modules on Bianca","text":"<p>Bianca is shared Linux computer with all the standard Linux tools installed, on which all users should be able to do their work independently and undisturbed.</p> <p>Because this is the same for nearly all UPPMAX clusters, there is a general page on the UPPMAX modules</p>","tags":["modules","Bianca"]},{"location":"cluster_guides/bianca_overview/","title":"Bianca overview","text":""},{"location":"cluster_guides/bianca_overview/#bianca-overview","title":"Bianca overview","text":"<p>Bianca is an HPC cluster designed to work on sensitive data named after a Tintin character, maintained by UPPMAX.</p> What is an HPC cluster? <p>See the UPPMAX page about HPC clusters.</p> What is the design of Bianca? <p>See the UPPMAX page about Bianca's design</p> What is UPPMAX? <p>See the UPPMAX page about itself.</p>"},{"location":"cluster_guides/biancas_design/","title":"Bianca's design","text":""},{"location":"cluster_guides/biancas_design/#biancas-design","title":"Bianca's design","text":"<p>Bianca is an high-performance computing (HPC) cluster for sensitive data.</p> What is an HPC cluster for sensitive data? <p>See the UPPMAX page about HPC clusters.</p> <p>Or: Bianca is a group of computers that can effectively run many calculations, as requested by multiple people, at the same time. As the data is sensitive, it is protected to remain only on Bianca.</p> <p>Bianca is designed to:</p> <ul> <li>make accidental data leaks difficult</li> <li>make correct data management as easy as possible</li> <li>emulate the HPC cluster environment that SNIC users were familiar with</li> <li>provide a maximum amount of resources</li> <li>satisfy regulations</li> </ul> The Bianca architecture <p></p> <p>Bianca's architecture. Red shows the university networks. Blue shows the whole cluster, with hundreds of nodes. Green shows virtual project clusters. Yellow shows where file transfer occurs.</p> <p>Bianca's architecture reflects that she is an HPC cluster for sensitive data: the whole Bianca cluster has hundreds of virtual project clusters, each of which is isolated from each other and the Internet. File transfer is only possible through the the so-called 'wharf', which is a special file area that is visible from the Internet.</p> <p>Bianca has no internet</p> <ul> <li>You can log in, but with extra steps</li> <li>You can transfer files, but with extra steps</li> <li>We recommend using the remote desktop login, see   the UPPMAX page on how to login to Bianca</li> </ul> <p>As Bianca is an HPC cluster that should be as easy to use as possible, there are two ways to interact with Bianca: one more visual, the other a command-line environment. Both environments are shown below.</p> <p>As Bianca has sensitive data, there are constraints on how to access Bianca.</p> <p>One such constraint in accessing Bianca, is that one has to be within the university networks, as described at get within the university networks.</p> <p>Another such constraint, is that data can be transferred to or from a virtual project cluster through the so-called 'wharf', which is a special file area that is visible from the Internet. See the UPPMAX page about file transfer on Bianca.</p> <p>Bianca runs the Linux operating system and all users need some basic Linux knowledge to use Bianca.</p> Using Linux <p>Using Linux (and especially the so-called command-line/terminal) is essential to use Bianca. See the UPPMAX page about Linux.</p>"},{"location":"cluster_guides/biancas_design/#overview-of-all-steps-possibleneeded-to-access-bianca","title":"Overview of all steps possible/needed to access Bianca","text":"<pre><code>flowchart TD\n\n    subgraph sub_outside[IP outside SUNET]\n      outside(Physically outside SUNET)\n    end\n\n    subgraph sub_inside[IP inside SUNET]\n      physically_inside(Physically inside SUNET)\n      inside_using_vpn(Inside SUNET using VPN)\n      inside_using_rackham(Inside SUNET using Rackham)\n\n      subgraph sub_bianca_shared_env[Bianca shared network]\n        bianca_shared_console[Bianca console environment login]\n        bianca_shared_remote_desktop[Bianca remote desktop login]\n        subgraph sub_bianca_private_env[The project's private virtual project cluster]\n          bianca_private_console[Bianca console environment]\n          bianca_private_remote_desktop[Bianca remote desktop]\n          bianca_private_terminal[Terminal]\n        end\n      end\n    end\n\n    %% Outside SUNET\n    outside--&gt;|Move physically|physically_inside\n    outside--&gt;|Use a VPN|inside_using_vpn\n    outside--&gt;|Login to Rackham|inside_using_rackham\n\n    %% Inside SUNET\n    physically_inside--&gt;|SSH|bianca_shared_console\n    physically_inside--&gt;|UPPMAX website|bianca_shared_remote_desktop\n    physically_inside-.-&gt;inside_using_rackham\n    physically_inside-.-&gt;inside_using_vpn\n    inside_using_vpn--&gt;|SSH|bianca_shared_console\n    inside_using_vpn--&gt;|UPPMAX website|bianca_shared_remote_desktop\n    inside_using_rackham--&gt;|SSH|bianca_shared_console\n\n    %% Shared Bianca\n    bianca_shared_console --&gt; |UPPMAX password|bianca_private_console\n    bianca_shared_remote_desktop--&gt;|UPPMAX password|bianca_private_remote_desktop\n\n    %% Private Bianca\n    bianca_private_console---|is a|bianca_private_terminal\n    bianca_private_remote_desktop--&gt;|must also use|bianca_private_terminal</code></pre> <p>This is an overview of all steps possible/needed to access Bianca.</p>"},{"location":"cluster_guides/biancas_name/","title":"Bianca's name","text":""},{"location":"cluster_guides/biancas_name/#biancas-name","title":"Bianca's name","text":"<p>Bianca, like most UPPMAX clusters between 2013 and 2025, is named after a Tintin character, in this case after Bianca Castafiore.</p> <p></p> What are the UPPMAX clusters? <p>See the UPPMAX page about HPC clusters.</p>"},{"location":"cluster_guides/cluster_guide_faq/","title":"UPPMAX clusters FAQ","text":"","tags":["FAQ","UPPMAX","cluster","clusters"]},{"location":"cluster_guides/cluster_guide_faq/#uppmax-clusters-faq","title":"UPPMAX clusters FAQ","text":"<ul> <li>How to access data from an expired NAISS project?</li> <li>How to extend the duration of a job that is running?</li> </ul>","tags":["FAQ","UPPMAX","cluster","clusters"]},{"location":"cluster_guides/cluster_speeds/","title":"Cluster speeds","text":"","tags":["UPPMAX","cluster","clusters","speed","fast","slow"]},{"location":"cluster_guides/cluster_speeds/#cluster-speeds","title":"Cluster speeds","text":"<p>Sometimes you feel a cluster is slow.</p> <p>Below are some benchmark results, so you can compare with what you are experiencing.</p> <p>Please contact support when you find out that your favorite cluster is slower than expected.</p> What could cause such a slowdown? <p>When things are slow it is usually due to latency when many processes are accessing the same files and physical hard drives</p> <p>Examples:</p> <ul> <li>2024-12-06: Castor is still holding some file systems for Bianca   or a user that is running a lot of very short lived Perl jobs on Bianca,   that are running too hard on Castor.</li> </ul>","tags":["UPPMAX","cluster","clusters","speed","fast","slow"]},{"location":"cluster_guides/cluster_speeds/#starting-an-interactive-session-with-two-cores-for-one-hour","title":"Starting an interactive session with two cores for one hour","text":"<p>In general:</p> <ul> <li>It takes seconds if a free compute node is available</li> <li>It takes minutes to start a new node</li> </ul> Date and time Cluster Command You waited for <code>x</code> seconds Complete time (secs) 2024-12-19 8:00 Bianca <code>interactive -A sens2023036 -n 2 -t 1:00:00</code> 518 (8:38) 548 (9:08) <p>You can expect longer times if:</p> <ul> <li>The Bianca system usage is at maximum   capacity (which it nearly never is)</li> <li>Your project has used up all of its   computational resources</li> </ul> How can I check that my project has used all of its resources? <p>Go to https://supr.naiss.se, click on your project and scroll down to 'Resources'.</p> <p>There you can see how much of the computational resources you've used.</p> <p>For example, below is an example project showing its resource usage:</p> <p></p> <p>We can see that it uses zero percent of the computation resources of the Dardel HPC cluster, yet 116.4% of Rackham's. Starting an interactive job on Rackham may hence take longer.</p>","tags":["UPPMAX","cluster","clusters","speed","fast","slow"]},{"location":"cluster_guides/cluster_speeds/#loading-the-r_packages431-module","title":"Loading the <code>R_packages/4.3.1</code> module","text":"<p>For a benchmark to solve a ticket, the following command was run in multiple settings:</p> <pre><code>time module load R_packages/4.3.1\n</code></pre> <p>From the three resulting times, the 'Real' time is used.</p> <p>Here are some expected timings:</p> Project Setting Real loading time Rackham SSH 0m0.758s Bianca SSH 0m8.984s <p>Here are some unexpected timings:</p> Project Setting Real loading time sens2023598 SSH 6m1.265s sens2023598 Website 6m20.234s sens2017625 SSH 6m4.584s sens2017625 Website, interactive session 7m41.433s sens2017625 SSH, interactive session 7m13.111s","tags":["UPPMAX","cluster","clusters","speed","fast","slow"]},{"location":"cluster_guides/cluster_speeds/#loading-the-rstudio2023121-402-module","title":"Loading the <code>RStudio/2023.12.1-402</code> module","text":"Project Setting Real loading time Bianca Website 2m3.184s","tags":["UPPMAX","cluster","clusters","speed","fast","slow"]},{"location":"cluster_guides/compress_fastQ/","title":"How should I compress FastQ-format files?","text":""},{"location":"cluster_guides/compress_fastQ/#how-should-i-compress-fastq-format-files","title":"How should I compress FastQ-format files?","text":"<p>Short answer: The best compression using a widely available format is provided by bzip2 and its parallel equivalent pbzip2. The best compression ratio for FastQ is provided by fqz_comp in the fqzcomp/4.6 module.  However, this tool is experimental and is not recommended for general, everyday use.</p>"},{"location":"cluster_guides/compress_fastQ/#long-answer","title":"Long answer","text":"<p>We conducted an informal examination of two specialty FastQ compression tools by recompressing an existing fastq.gz file. The first tool fqzcomp (available in the module fqzcomp/4.6) uses a compiled executable (fqz_comp) that works similar to e.g., gzip, while the second tool (LFQC in the module lfqc/1.1) uses separate ruby-language scripts for compression (lfqc.rb) and decompression (lfqcd.rb). It does not appear the LFQC scripts accept piping but the documentation is limited.</p> <p>Loading the needed modules:</p> <pre><code>module load bioinfo-tools\nmodule load fqzcomp/4.6\nmodule load lfqc/1.1\n</code></pre> <p>Both modules have 'module help' available for more info. The help for fqzcomp gives the location of their README which is very helpful in describing minor changes that might occur to the FastQ file during decompression (these do not affect the read name, sequence or quality data).</p> <p>One thing changed from the 'standard' implementation of LFQC was to make the scripts stand-alone with #! header lines, rather than requiring e.g., 'ruby lfqc.rb ...' as you see in their documentation.</p> <p>Since piping is not available with LFQC, it is preferable to avoid creating a large intermediate decompressed FastQ file. So, create a named pipe using mkfifo that is named like a fastq file.</p> <pre><code>mkfifo UME_081102_P05_WF03.se.fastq\nzcat UME_081102_P05_WF03.se.fastq.gz &gt; UME_081102_P05_WF03.se.fastq &amp;\nlfqc.rb UME_081102_P05_WF03.se.fastq\nrm UME_081102_P05_WF03.se.fastq\n</code></pre> <p>This took a long time, 310 wall seconds.</p> <p>Next,fqz_comp from fqzcomp/4.6. Since this works like gzip, just use it in a pipe.</p> <pre><code>zcat UME_081102_P05_WF03.se.fastq.gz | fqz_comp &gt; UME_081102_P05_WF03.se.fastq.fqz\n</code></pre> <p>This used a little multithreading (up to about 150% CPU) and was much faster than LFQC, just 2-3 seconds. There are other compression options (we tried -s1 and -s9+) but these did not outperform the default (equivalent to -s3). This is not necessarily a surprise; stronger compression means attempting to make better guesses and sometimes these guesses are not correct. No speedup/slowdown was noticed with other settings but the input file was relatively small.</p> <pre><code>-rw-rw-r-- 1 28635466 Mar 10 12:53 UME_081102_P05_WF03.se.fastq.fqz1\n-rw-rw-r-- 1 29271063 Mar 10 12:52 UME_081102_P05_WF03.se.fastq.fqz9+\n-rw-rw-r-- 1 46156932 Jun 6   2015 UME_081102_P05_WF03.se.fastq.gz\n-rw-rw-r-- 1 28015892 Mar 10 12:53 UME_081102_P05_WF03.se.fastq.fqz\n-rw-rw-r-- 1 24975360 Mar 10 12:45 UME_081102_P05_WF03.se.fastq.lfqc\n</code></pre> <p>We also compared against bzip2 and xz, which are general-use compressors. These both function like gzip (and thus like fqz_comp) and both outperform gzip, as expected. xz is becoming a more widely-used general compressor like bzip2, but for this file it required perhaps 20x as much time as bzip2 and did worse.</p> <pre><code>-rw-rw-r-- 1 35664555 Mar 10 13:10 UME_081102_P05_WF03.se.fastq.bz2\n-rw-rw-r-- 1 36315260 Mar 10 13:10 UME_081102_P05_WF03.se.fastq.xz\n</code></pre> <p>Neither of these improved general-use compressors did as well with FastQ as the specialty compressors. This makes sense given the specialty compressors can take advantages of the restrictions of the format.</p>"},{"location":"cluster_guides/compress_fastQ/#which-is-the-best-method-in-this-trial","title":"Which is the best method in this trial?","text":"<p>From the results of this trial, the tool that provides the best compression ratio in a reasonable amount of time is fqz_comp in the fqzcomp/4.6 module. It is as fast as bzip2 which is also much better than gzip but does a much better job of compressing FastQ.  However, fqz_comp is experimental so we do not recommend using fqz_comp for everyday use.  We recommend using bzip2 or its parallel equivalent, pbzip2.</p> <p>The fqz_comp executable could be used to decompress FastQ within a named pipe if FastQ is required for input:</p> <pre><code>... &lt;(fqz_comp -d &lt; file.fastq.fqz) ...\n</code></pre> <p>Note that fqz_comp is designed to compress FastQ files alone, and neither method here provides the blocked compression format suitable for random access that bgzip does; see which-compression-format-should-i-use-for-ngs-related-files for more on that subject.</p>"},{"location":"cluster_guides/compress_fastQ/#why-not-lfqc","title":"Why not LFQC?","text":"<p>Though LFQC has the best compression of FastQ, there are some strong disadvantages. First, it takes quite a long time, perhaps 50x longer than fqz_comp. Second, it apparently cannot be used easily within a pipe like many other compressors. Third, it contains multiple scripts with multiple auxiliary programs, rather than a single executable. Fourth, it is quite verbose during operation, which can be helpful but cannot be turned off. Finally, it was difficult to track down for installation; two different links were provided in the publications and neither worked. It was finally found in a github repository, the location of which is provided in the module help.</p>"},{"location":"cluster_guides/compress_format/","title":"Which compression format should I use for NGS-related files?","text":""},{"location":"cluster_guides/compress_format/#which-compression-format-should-i-use-for-ngs-related-files","title":"Which compression format should I use for NGS-related files?","text":"<p>How well things compress will vary a great deal with the input data. An additional consideration is how useful the compressed format will be to you later. Some tools can handle only one compressed format (almost always gzip) and some can handle two (almost always gzip and bzip2). The help information for the tool should be explicit about which formats it understands. You can also use named pipes or the bash &lt;() syntax to uncompress files 'on the fly' if the tool you are using cannot handle that compressed format.</p> <p>Another consideration for usefulness is the structure of the specific compressed format. By default gzip is not 'blocked'; the compression is applied continually across the entire file, and to uncompress something in the middle it is necessary to uncompress everything up to that point. Tools that understand compressed VCF and GFF files require these to be compressed with bgzip (available as part of the htslib module), which applies blocked gzip compression, so that it is possible to uncompress interior chunks of the files efficiently. This is useful when viewing compressed VCF/GFF files in a viewer such as IGV, for example. For viewing, such files also need an index created, which is accomplished using tabix (also part of the htslib module), which understands bgzip-compressed files. BAM files also use a type of gzip compression that is blocked. Files compressed with bgzip can be uncompressed with gzip.</p> <p>Bzip2 is inherently blocked. Bzip2 is a more efficient compression method than gzip, but takes perhaps twice as long or longer to compress the same file. Fortunately, another advantage of blocked compression is that multiple parts of the file can be compressed at once. Uppmax has pbzip2 available as a system tool, which can perform parallel compression and decompression of bzip2-format files using multiple threads. This is quite fast. Do 'pbzip2 -h' for help. An Uppmax user has provided a helpful SBATCH script.</p> <p>Another disadvantage of compression formats that are not blocked is that an error in a file generally screws up the remainder of the file. Files with blocked compression can recover all non-error-containing blocks.</p> <p>Other compressed formats are available, including 7z, available by loading the p7zip module, and xz, available as the system tool xz and by loading the liblzma module. For compressing FastQ files in particular, which have a very strict format, our small-scale comparison of tools showed that xz was slightly inferior to bzip2 and much slower during compression, and specialty tools for FastQ compression were superior in compression ratios to general-purpose compressors. See How Should I Compress FastQ-format Files? for more on FastQ compression.</p> <p>Most compression tools have options that allow you to trade off between speed of compression and size reduction of compression. The defaults are almost always sufficient.</p> <p>It makes little sense to compress an already-compressed file with a different format. It is much better to set up a pipe to uncompress the file and then recompress in the new format.</p> <p>Apart from compression dictated by file usage (see above), it is recommended that files that are being compressed for long-term storage (e.g., raw sequence data) are compressed using pbzip2. If the files are already compressed in long-term storage (e.g. Swestore) I don't think it is worthwhile to retrieve the files, decompress-recompress them, then reupload them.</p>"},{"location":"cluster_guides/compress_guide/","title":"Brief compression guide","text":""},{"location":"cluster_guides/compress_guide/#brief-compression-guide","title":"Brief compression guide","text":"<p>To avoid filling up the storage at UPPMAX, we all users to do their part and store their files in a good way. The best way to store files is to delete everything you don't need anymore, like temporary and intermediate files. For everything else you need to keep, here are some useful commands to know (section about biological data below).</p>"},{"location":"cluster_guides/compress_guide/#general-files","title":"General files","text":"<p>We have several compression programs installed and you are free to chose whichever you want (any better than none). Examples:</p>"},{"location":"cluster_guides/compress_guide/#gzip-fast-good-compression","title":"gzip (fast, good compression)","text":"<p>gzip also has a parallel version (pigz) that will let the program use multiple cores, making it much faster. If you want to run multithreaded you should make a reservation in the queue system, as the login nodes will throttle your programs if they use too much resources.</p> <pre><code># compress a file\n$ gzip file.txt            # single threaded\n$ pigz -p 4 file.txt       # using 4 threads\n# decompress a file\n$ gunzip file.txt.gz       # single threaded\n$ unpigz -p 4 file.txt     # using 4 threads (4 is max)\n</code></pre>"},{"location":"cluster_guides/compress_guide/#bzip2-slow-better-compression","title":"bzip2 (slow, better compression)","text":"<p>bzip2 also has a parallel version (pbzip2) that will let the program use multiple cores, making it much faster. If you want to run multithreaded you should make a reservation in the queue system, as the login nodes will throttle your programs if they use too much resources.</p> <pre><code># compress a file\n$ bzip2 file.txt            # single threaded\n$ pbzip2 -p4 file.txt       # using 4 threads\n# decompress a file\n$ bunzip2 file.txt.gz       # single threaded\n$ pbunzip2 -p4 file.txt.gz  # using 4 threads\n</code></pre>"},{"location":"cluster_guides/compress_guide/#zstd-fast-better-compression","title":"zstd (fast, better compression)","text":"<p>zstd has built in support for using multiple threads when compressing data only, making it much faster. If you want to run multithreaded you should make a reservation in the queue system, as the login nodes will throttle your programs if they use too much resources.</p> <pre><code># compress a file\n$ zstd --rm file.txt        # single threaded\n$ zstd --rm -T4 file.txt    # using 4 threads\n# decompress a file, only single threaded\n$ unzstd --rm file.txt.zst\n</code></pre>"},{"location":"cluster_guides/compress_guide/#compressing-lots-of-files","title":"Compressing lots of files","text":"<p>The commands above work on a single file at a time, and if you have 1000s of files it is quite boring to go through them manually. If you want to combine all the files into a single compressed archive, you can use a program named tar.</p> <pre><code># to compress a folder (folder/)\n# and all files/folder inside it,\n# creating a archive file named files.tar.gz\n$ tar -czvf files.tar.gz folder/\n# to decompress the archive later\n$ tar -xzvf files.tar.gz\n</code></pre> <p>If you don't want to combine them in a single file, and instead compress them one by one, you can use the command find.</p> <pre><code># to find all files with a name ending\n# with .fq and compress them\n$ find /path/to/search/in -iname *.fq -print -exec gzip \"{}\" \\;\n# example to compress all FastQ file in\n# the current directory and all its\n# subdirectories, using 4 threads\n$ find . \\( -iname '*.fq' -o -iname '*.fastq' \\) -print -exec pigz -p 4 \"{}\" \\;\n# same as above, but starting 4 single\n# threaded instances of gzip in parallel\n$ find . \\( -iname '*.fq' -o -iname '*.fastq' \\) -print0 | xargs -0 -P 4 gzip\n</code></pre>"},{"location":"cluster_guides/compress_guide/#biological-data","title":"Biological data","text":"<p>There are some compression algorithms that have become standard practice to use in the realm of biological data. Most programs can read the compressed versions of files as long as it's compressed with the correct program. Leaving out the decompression commands, mostly because they are already described in the General files section about, but also because there is little reason to ever decompress biological data.</p>"},{"location":"cluster_guides/compress_guide/#fastq-files","title":"fastq files","text":"<pre><code># compress sample.fq\n$ gzip sample.fq        # single threaded\n$ pigz -p 4 sample.fq   # using 4 threads\n</code></pre>"},{"location":"cluster_guides/compress_guide/#sam-files","title":"sam files","text":"<p>Loading the needed modules:</p> <pre><code># load samtools\n$ module load bioinfo-tools samtools\n</code></pre> <p>Then:</p> <pre><code># compress sample.sam, but remember to delete\n# sample.sam when finished, since samtools\n# will not do that automatically\n# single threaded\n$ samtools view -b -o sample.bam sample.sam\n# using 4 threads\n$ samtools view -@ 4 -b -o sample.bam sample.sam\n</code></pre>"},{"location":"cluster_guides/compress_guide/#vcf-gvcf-files","title":"vcf / g.vcf files","text":"<pre><code># load htslib\n$ module load bioinfo-tools htslib\n# compress sample.vcf / sample.g.vcf\n$ bgzip sample.vcf         # single threaded\n$ bgzip -@ 4 sample.vcf    # using 4 threads\n# index sample.vcf.gz / sample.g.vcf.gz\n$ tabix sample.vcf.gz\n</code></pre>"},{"location":"cluster_guides/compress_guide/#programs-that-dont-read-compressed-files","title":"Programs that don't read compressed files","text":"<p>There are clever ways to get around programs that don't support reading compressed files. Let's say you have a program that only reads plain text files. You can use something called process substitution (also known as anonymous named pipes) to be able to decompress the data on-the-fly while feeding it to the program.</p>"},{"location":"cluster_guides/compress_guide/#how-you-normally-would-run-the-program","title":"How you normally would run the program","text":"<pre><code># run the program with uncompressed file\n$ the_program uncompressed.txt\n# now, let's compress the file first and run\n# the program using process substitution\n# to decompress the file\n$ gzip uncompressed.txt\n# run the program using the compressed file\n# (zcat works like cat, but read gzipped files)\n$ the_program &lt;(zcat compressed.txt.gz)\n# same as above, but reading a\n# bzip2 compressed file\n$ the_program &lt;(bzcat compressed.txt.gz)\n# same as above, but reading a\n# zstd compressed file\n$ the_program &lt;(zstdcat compressed.txt.gz)\n</code></pre> <p>In this example we give the program not the name of a file to read, but instead we use process substitution to run zcat and feed the uncompressed data to the program, as if it was reading a file.</p>"},{"location":"cluster_guides/crex/","title":"Crex","text":""},{"location":"cluster_guides/crex/#crex","title":"Crex","text":"<p>Crex is an UPPMAX storage system.</p>"},{"location":"cluster_guides/cygnus/","title":"Cygnus","text":""},{"location":"cluster_guides/cygnus/#cygnus","title":"Cygnus","text":"<p>Cygnus is a DDN Lustre system.</p>"},{"location":"cluster_guides/dardel/","title":"Dardel (at PDC)","text":""},{"location":"cluster_guides/dardel/#dardel","title":"Dardel","text":"<p>Dardel is an HPC cluster at Stockholm maintained by PDC.</p> <p>If you are working on Rackham, consider moving to Dardel.</p> <ul> <li>Login to Dardel</li> <li>Create and upload SSH key for Dardel</li> <li>Migration to Dardel</li> </ul>"},{"location":"cluster_guides/dardel/#links","title":"Links","text":"<ul> <li>PDC page about Dardel</li> </ul>"},{"location":"cluster_guides/dardel_migration/","title":"Migrate to Dardel (at PDC)","text":"","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#dardel-migration-andor-rackham-recovery","title":"Dardel migration and/or Rackham recovery","text":"<p>This page describes how to transfer files to Dardel, the HPC cluster at PDC in Stockholm.</p> Visit the Rackham 2 Dardel Drop-in <p>Every Tuesday at 11:15 (except for the month of July) there is online Rackham 2 Dardel Drop-in at Zoom with meeting ID 64896912764</p> <p>Please join us if you need assistance logging in to Dardel or migrating your data.</p> Why do I need this? <p>The Rackham cluster will be decommissioned at the end of 2024, hence all project directories will be deleted. The plan from NAISS is that all Rackham users can move to the Dardel cluster at PDC, and we encourage you to do so right away.</p> <p>Researchers at Uppsala University, should they so desire, can choose to keep data at UPPMAX. Projects with UU affiliation that remain on Rackham at the end of this year can be transferred to a new local system.</p> <p>To facilitate this move, we have created a tool that makes the transfer easier.</p> <p>More details of Rackham end of life here.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#short-version","title":"Short version","text":"<p>The really short description is:</p> <ol> <li>Become a member of a project with resources at Dardel in SUPR.</li> <li>Create a passwordless SSH key.</li> <li>Add the key to the PDC login portal.</li> <li>Add <code>*.uppmax.uu.se</code> as allowed address for the key.</li> <li>Load module <code>darsync</code> and run <code>darsync check</code> on the folder you want to transfer.</li> <li>Create a Slurm script using <code>darsync gen</code> on the folder you want to transfer.</li> <li>Submit the created Slurm script.</li> </ol> <p>See the rest of this guide for more information about these steps.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#long-version","title":"Long version","text":"Prefer a video? <p>This procedure is also shown in this YouTube video.</p> <p>First, we are here to help. Please contact support if you run into problems when trying the guide below.</p> <p>This migration consists of a couple of steps summarized below. Press the links to get more detailed explanation of each step. Note that step 1 requires some hours of waiting and step 2 requires an overnight wait.</p> <pre><code>flowchart TD\n  get_supr_project[1 Access to a SUPR project with Dardel]\n  get_pdc_account[2 Access to a PDC account]\n  create_ssh_key[3 Create temporary SSH keys]\n  add_ssh_key[4 Add the SSH keys to the PDC Login Portal]\n  run_darsync[5 Run Darsync]\n  slurm[6 Submit the script created by Darsync]\n  check_logs[7 Check logs]\n  double_check_transfer[8 double-check the transfer]\n  delete_ssh_keys[9 Delete the temporary SSH keys]\n  delete_rackham_files[10 Delete the files on Rackham]\n\n  get_supr_project --&gt; |needed for| get_pdc_account\n\n  create_ssh_key --&gt; |needed for| add_ssh_key\n  get_pdc_account --&gt; |needed for| add_ssh_key\n  add_ssh_key --&gt; |needed for| run_darsync\n  run_darsync --&gt; |needed for| slurm\n  slurm --&gt; |needed for| check_logs\n  check_logs --&gt; |optional| double_check_transfer\n  double_check_transfer --&gt; delete_ssh_keys\n  check_logs --&gt; |needed for| delete_ssh_keys\n  delete_ssh_keys --&gt; |needed for| delete_rackham_files</code></pre> <p>Overview of the migration process. Note that step 1 requires some hours of waiting and step 2 requires an overnight wait.</p> <p>After those steps, the procedure will take around 20 minutes, as shown in the YouTube video that shows this procedure.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#1-get-access-to-a-supr-project-with-dardel","title":"1. Get access to a SUPR project with Dardel","text":"<p>First step is to get get access to a SUPR project with Dardel. This is described at PDC's page on getting access to Dardel (note that it is impossible to give more precise links, as the PDC documentation does not allow for it). You will get an email when you are added to a project, this can take some hours.</p> How do I know I have access to a Dardel project? <p>Login to https://supr.naiss.se/. If there is a PDC project, you may have access to a project with Dardel.</p> <p></p> <p>An example user that has access to a PDC project</p> <p>If you may a PDC project that does not use Dardel, click on the project to go the the project overview.</p> <p></p> <p>An example PDC project overview</p> <p>From there, scroll down to 'Resources'. If you see 'Dardel' among the compute resources, you have confirmed you have access to a Dardel project.</p> <p></p> <p>Resources from an example PDC project</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#2-get-a-pdc-account-via-supr","title":"2. Get a PDC account via SUPR","text":"<p>Get a PDC account via SUPR. This is described at the PDC page on how to apply for a SUPR account. You will get a PDC account overnight.</p> How do I know I have a PDC account? <p>Login to https://supr.naiss.se/. and click on 'Accounts' in the main menu bar at the left.</p> <p>If you see 'Dardel' among the resources, and status 'Enabled' in the same row, you have a PDC account!</p> <p></p> <p>Example of a user having an account at PDC's Dardel HPC cluster</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#3-create-ssh-key-pair","title":"3. Create SSH key pair","text":"<p>First we will create SSH keys to be able to connect to Dardel. We have made a small tool to create the keys for Darsync for you, so just run these commands on UPPMAX:</p> <p>Loading the needed module:</p> <pre><code>module load darsync\n</code></pre> <p>Then creating the key:</p> <pre><code>darsync sshkey\n</code></pre> How does that look like? <p>The screen output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ module load darsync\n[sven@rackham1 ~]$ darsync sshkey\n\n\n  ____ ____  _   _ _  _________   __\n / ___/ ___|| | | | |/ / ____\\ \\ / /\n \\___ \\___ \\| |_| | ' /|  _|  \\ V /\n  ___) |__) |  _  | . \\| |___  | |\n |____/____/|_| |_|_|\\_\\_____| |_|\n\nThe sshkey module of this script will generate a SSH key pair that you can use to login to Dardel.\nIt will create two files, one with the private key and one with the public key.\nThe private key should be kept secret and the public key should be added to your authorized_keys file on Dardel.\n\n\n\n\nCreated SSH key: /home/sven/id_ed25519_pdc and /home/sven/id_ed25519_pdc.pub\n\nContent of the public key:\n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAZkAoqlvm+YQrw26mCuH/4B/meG8O6aS8BB3kw1FDfl sven@rackham1.uppmax.uu.se\n\n\n\n\nYou will now have to add the public key above to the Dardel Login Portal, https://loginportal.pdc.kth.se\n\nSee the user guide for more info about this,\nhttps://docs.uppmax.uu.se/software/ssh_key_use_dardel/#2-how-to-add-an-ssh-key-to-the-pdc-login-portal\n</code></pre>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#4-add-the-public-key-to-the-pdc-login-portal","title":"4. Add the public key to the PDC Login Portal","text":"<p>See create and use an SSH key pair for Dardel, step 2, to see how to upload the public SSH key to the PDC Login Portal.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#5-run-the-migration-tool-darsync","title":"5. Run the migration tool Darsync","text":"","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#51-load-the-module","title":"5.1 Load the module","text":"<pre><code>module load darsync\n</code></pre>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#52-check-for-problems","title":"5.2 Check for problems","text":"<p>This step is optional, yet may help against possible problems.</p> <p>Running <code>darsync check</code> will make Darsync prompt for questions:</p> <pre><code>darsync check\n</code></pre> How does that look like? <p>Here is output similar to yours, for a user with username <code>sven</code> that wants to transfer his <code>Documents</code> folder:</p> <pre><code>[sven@rackham1 ~]$ darsync check\n\n\n   ____ _   _ _____ ____ _  __\n  / ___| | | | ____/ ___| |/ /\n | |   | |_| |  _|| |   | ' /\n | |___|  _  | |__| |___| . \\\n  \\____|_| |_|_____\\____|_|\\_\\\n\nThe check module of this script will recursively go through\nall the files in, and under, the folder you specify to see if there\nare any improvements you can to do save space and speed up the data transfer.\n\nIt will look for file formats that are uncompressed, like .fasta and .vcf files\n(most uncompressed file formats have compressed variants of them that only\ntake up 25% of the space of the uncompressed file).\n\nIf you have many small files, e.g. folders with 100 000 or more files,\nit will slow down the data transfer since there is an overhead cost per file\nyou want to transfer. Large folders like this can be archived/packed into\na single file to speed things up.\n\n\nSpecify which directory you want to copy.\nMake sure to use tab completion (press the tab key to complete directory names)\nto avoid spelling errors.\nEx.\n/proj/naiss2099-22-999/\nor\n/proj/naiss2099-22-999/raw_data_only\n\nSpecify local directory: Documents\n/domus/h1/sven/Documents/MATLAB\n\n\n  ____   ___  _   _ _____\n |  _ \\ / _ \\| \\ | | ____|\n | | | | | | |  \\| |  _|\n | |_| | |_| | |\\  | |___\n |____/ \\___/|_| \\_|_____|\n\nChecking completed. Unless you got any warning messages above you\nshould be good to go.\n\nGenerate a Slurm script file to do the transfer by running this script again,\nbut use the 'gen' option this time. See the help message for details,\nor continue reading the user guide for examples on how to run it.\n\ndarsync gen -h\n\nA file containing file ownership information,\n/domus/h1/sven/Documents/darsync_Documents.ownership.gz,\nhas been created. This file can be used to make sure that the\nfile ownership (user/group) will look the same on Dardel as it does here.\nSee ../cluster_guides/dardel_migration/#52-check-for-problems\nfor more info about this.\n</code></pre> Can I also give the arguments on the command line? <p>If you prefer to specify everything from the command-line, do:</p> <pre><code>darsync check --local-dir [foldername]\n</code></pre> <p>where <code>[foldername]</code> is the name to a folder, for example <code>darsync check --local-dir ~/my_folder</code>.</p> <p>There are some more optional arguments, see these by doing:</p> <pre><code>darsync check --help\n</code></pre> <p>If there are problems reported, contact support or try to fix them yourself.</p> What is the file <code>darsync_[dirname].ownership.gz</code>? <p>This is a file containing file ownership information. It is created in the root of the folder you told Darsync to transfer to Dardel.</p> <p>When a user transfer all the files in a project to a project at Dardel, all the files at Dardel will be owned by the user who did the transfer. By saving the ownership information of the files at UPPMAX, we can map the file ownership information to the corresponding users at Dardel.</p> Can I delete the file <code>darsync_[dirname].ownership.gz</code>? <p>No, keep it until you feel at home at Dardel and have worked in your new project a couple of months. By that time you should have encountered any problems with file permissions that you might have.</p> <p>If you discover that you get problems because of wrong owner of files (write permissions etc), this file contains the information needed to recreate the file ownerships as they were before you transfered the files, even if your UPPMAX project has already been deleted.</p> How to fix <code>WARNING: files with uncompressed file extensions above the threshold detected</code> <p>It looks for files with file endings matching common uncompressed file formats, like <code>.fq</code>, <code>.sam</code>, <code>.vcf</code>, <code>.txt</code>. If the combined file size of these files are above a threshold it will trigger the warning. Most programs that uses these formats can also read the compressed version of them.</p> <p>Examples of how to compress common formats:</p> <pre><code># fastq/fq/fasta/txt\ngzip file.fq\n\n# vcf\nbgzip file.vcf\n\n# sam\nsamtools view -b file.sam &gt; file.bam\n# when the above command is completed successfully:\n# rm file.sam\n</code></pre> <p>For examples on how to compress other file formats, use an internet search engine to look for</p> <pre><code>how to compress &lt;insert file format name&gt; file\n</code></pre> How to fix <code>WARNING: Total number of files, or number of files in a single directory</code> <p>If a project consists of many small files it will decrease the data transfer speed, as there is an overhead cost to starting and stopping each file transfer. A way around this is to pack all the small files into a single <code>tar</code> archive, so that it only has to start and stop a single time.</p> <p>Example of how to pack a folder and all files in it into a single <code>tar</code> archive.</p> <p>```bash</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#on-uppmax","title":"on uppmax","text":"","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#pack-it","title":"pack it","text":"<p>tar -czvf folder.tar.gz /path/to/folder</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#the-the-command-above-finished-without-error-messages-and-you-have-a-foldertargz-file-that-seems-about-right-in-size","title":"the the command above finished without error messages and you have a folder.tar.gz file that seems about right in size,","text":"<p>rm -r /path/to/folder</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#53-generate-script","title":"5.3 Generate script","text":"<p>In this third step, the Slurm script is created.</p> <p>A lot of questions</p> <p>The script will ask multiple questions. Below it is described how to get the answers :-)</p> <p>Running <code>darsync gen</code> will make Darsync prompt for questions:</p> <pre><code>darsync gen\n</code></pre> How does that look like? <p>Here is output similar to yours, for a fictional user called Sven Svensson, with the UPPMAX username of <code>sven</code> and the PCD username of <code>svensv</code>:</p> <pre><code>[sven@rackham1 ~]$ darsync gen\n\n\n   ____ _____ _   _\n  / ___| ____| \\ | |\n | |  _|  _| |  \\| |\n | |_| | |___| |\\  |\n  \\____|_____|_| \\_|\n\nThe gen module of this script will collect the information needed\nand generate a script that can be submitted to Slurm to preform the\ndata transfer.\n\nIt will require you to know\n\n    1) Which directory on UPPMAX you want to transfer (local directory).\n    2) Which UPPMAX project id the Slurm job should be run under.\n        ex. naiss2099-23-999\n    3) Which cluster the Slurm job should be run on.\n        ex. rackham, snowy\n    4) Which username you have at Dardel.\n    5) Where on Dardel it should transfer your data to.\n        ex. /cfs/klemming/projects/snic/naiss2099-23-999/from_uppmax\n    6) Which SSH key should be used when connecting to Dardel.\n        ex. /home/user/id_ed25519_pdc\n    7) Where you want to save the generated Slurm script.\n\n\n\nSpecify which directory you want to copy.\nMake sure to use tab completion (press the tab key to complete directory names)\nto avoid spelling errors.\nEx.\n/proj/naiss2099-22-999/\nor\n/proj/naiss2099-22-999/raw_data_only\n\nSpecify local directory: Documents\n\n\nSpecify which project id should be used to run the data transfer job in Slurm.\nEx.\nnaiss2099-23-999\n\nSpecify project id: naiss2099-23-999\n\n\nSpecify which cluster the Slurm job should be run on.\nChoose between rackham and snowy.\nDefault is rackham\n\nSpecify cluster: rackham\n\n\nSpecify the username that should be used to login at Dardel.\nIt is the username you have created at PDC and it is\nprobably not the same as your UPPMAX username.\n\nSpecify Dardel username: svensv\n\n\nSpecify the directory on Dardel you want to transfer your data to.\nEx.\n/cfs/klemming/projects/snic/naiss2099-23-999\n\nSpecify Dardel path: /cfs/klemming/projects/snic/naiss2099-23-999\n\n\nSpecify which SSH key should be used to login to Dardel.\nCreate one by running `dardel_ssh-keygen` if you have not done so yet.\nIf no path is given it will use the default key created by `dardel_ssh-keygen`,\n~/id_ed25519_pdc\n\nSpecify SSH key:\n\n\nSpecify where the Slurm script file should be saved.\nIf not given it will save it here: ~/darsync_Documents.slurm\n\nSpecify Slurm script path:\n\n\n  ____   ___  _   _ _____\n |  _ \\ / _ \\| \\ | | ____|\n | | | | | | |  \\| |  _|\n | |_| | |_| | |\\  | |___\n |____/ \\___/|_| \\_|_____|\n\n\nCreated Slurm script: /home/sven/darsync_Documents.slurm\n\ncontaining the following command:\n\nrsync -e \"ssh -i /home/sven/id_ed25519_pdc -o StrictHostKeyChecking=no\" -acPuv /domus/h1/sven/Documents/ svensv@dardel.pdc.kth.se:/cfs/klemming/projects/snic/naiss2099-23-999\n\n\nTo test if the generated file works, run\n\nbash /home/sven/darsync_Documents.slurm\n\nIf the transfer starts you know the script is working, and you can terminate\nit by pressing ctrl+c and submit the script as a Slurm job.\n\nRun this command to submit it as a job:\n\nsbatch /home/sven/darsync_Documents.slurm\n</code></pre> <p>After answering all the questions a new file will be created. By default it will be created in your home directory, named <code>darsync_foldername.sh</code>, where <code>foldername</code> is the name of the folder you told it to transfer, e.g. <code>~/darsync_nais2024-23-9999.sh</code></p> <p>In case of a typo, you can also modify the transfer script created by Darsync, which is a regular Slurm script.</p> Can I also give the arguments on the command line? <p>If you prefer to specify everything from the command-line, do:</p> <pre><code>darsync gen \\\n  --local-dir [foldername on UPPMAX] \\\n  --remote-dir [foldername on Dardel] \\\n  --slurm-account [slurm_account] \\\n  --cluster [slurm_cluster] \\\n  --username [pdc_username] \\\n  --ssh-key [private_ssh_key_path] \\\n  --outfile [output_filename]\n</code></pre> <p>where</p> <ul> <li><code>[foldername]</code> is the name to a folder, e.g. <code>~/my_folder</code></li> <li><code>[slurm_account]</code> is the UPPMAX project ID, e.g. <code>uppmax2023-2-25</code></li> <li><code>[slurm_cluster]</code> is the cluster on UPPMAX where the job will run, e.g. <code>rackham</code> or <code>snowy</code></li> <li><code>[pdc_username]</code> is your PDC username, e.g <code>svenan</code></li> <li><code>[private_ssh_key_path]</code> is the path the private SSH key, e.g. <code>~/id_ed25519_pdc</code></li> <li><code>[output_filename]</code> is the name of the Slurm output file, e.g. <code>~/dardel_naiss2024-23-9999.sh</code></li> </ul> <p>resulting in:</p> <pre><code>darsync gen \\\n  --local-dir ~/my_folder \\\n  --remote-dir /cfs/klemming/projects/nais2024-23-9999\n  --slurm-account uppmax2023-2-25 \\\n  --username svenan \\\n  --ssh-key ~/id_ed25519_pdc \\\n  --outfile ~/dardel_naiss2024-23-9999.sh\n</code></pre> <p>There are some more optional arguments, see these by doing:</p> <pre><code>darsync gen --help\n</code></pre> How to find out my UPPMAX project ID? <p>The UPPMAX project ID is used in your Slurm scripts, with the <code>-A</code> flag.</p> <p>Your UPPMAX project IDs can be found at https://supr.naiss.se/. UPPMAX projects for Rackham usually start with <code>NAISS</code> or <code>UPPMAX</code> and have '(UPPMAX)' after the project name.</p> <p>Here is how to convert the UPPMAX project name to UPPMAX project ID:</p> UPPMAX project name UPPMAX project ID NAISS 2024/22-49 <code>naiss2024-22-49</code> UPPMAX 2023/2-25 <code>uppmax2023-2-25</code> <p></p> <p>An example https://supr.naiss.se/ page. Eligible candidates seem 'NAISS 2024/22-49' and 'UPPMAX 2023/2-25'.</p> How to find out my PDC username? <p>Login to https://supr.naiss.se/. and click on 'Accounts' in the main menu bar at the left.</p> <p>If you see 'Dardel' among the resources, and status 'Enabled' in the same row, you have a PDC account. In the first column of such a row, you will see your username</p> <p></p> <p>An example of a user having an account at PDC's Dardel HPC cluster. In this case, the username is <code>svenbi</code></p> How to find out where on Dardel I will transfer your data to? <ul> <li>Your home folder: <code>/cfs/klemming/home/[first letter of username]/[username]</code>,   where <code>[first letter of username]</code> is the first letter of your PDC username, and <code>[username]</code> is your PDC username,   for example <code>/cfs/klemming/home/s/sven</code></li> <li>Your project folder: <code>/cfs/klemming/projects/[project_storage]</code>,   where <code>[project_storage]</code> is your PDC project storage folder,   for example <code>/cfs/klemming/projects/snic/naiss2023-22-1027</code></li> </ul> <p></p> <p>Composite image of a PDC project and its associated storage folder at the bottom. In this case, the full folder name is <code>/cfs/klemming/projects/snic/naiss2023-22-10271</code></p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#6-runsubmit-the-script-created-by-darsync","title":"6. Run/submit the script created by Darsync","text":"<p>You can then start the transfer script the same way you let <code>bash</code> run a script:</p> <pre><code>bash dardel_naiss2024-23-9999.sh\n</code></pre> <p>Replace <code>nais2024-23-9999</code> with the name of the folder you told Darsync to transfer.</p> <p>You terminal does needs to be running during the whole process. If you do need to log out, use <code>sbatch</code> as show below.</p> Shouldn't I use sbatch? <p>No.</p> <p>Indeed, usually (and until September 17th) we recommend to use <code>sbatch</code>.</p> <p>However, in this case, the login node has a bigger file transfer bandwidth compared to the compute nodes. Hence, now the advice is to run the script on the login node.</p> Wouldn't I get complaints? <p>No.</p> <p>Normally, when you run CPU intensive tasks on a login node, we will either contact you or make your program use less CPU power.</p> <p>In this case, however, the login node is the superior node for file transfer and we at UPPMAX agreed on allowing our users to run the transfer from it.</p> Will this run when I close the terminal? <p>No.</p> <p>Normally, when you run CPU intensive tasks on a login node, we will either contact you or make your program use less CPU power.</p> <p>In this case, however, the login node is the superior node for file transfer and we at UPPMAX agreed on allowing our users to run the transfer from it.</p> My transfer job stopped. Is progress lost? Can I restart it? <p>No progress is lost. Yes, you can restart it: <code>rsync</code> will continue transferring files that have not been transferred or have not been transferred completely.</p> <p>If you want to start the job by submitting it to the job queue, use the following command:</p> <pre><code>sbatch ~/dardel_naiss2024-23-9999.sh\n</code></pre> <p>Replace <code>nais2024-23-9999</code> with the name of the folder you told Darsync to transfer.</p> How does that look like? <p>Similar to this:</p> <pre><code>[sven@rackham1 ~]$ sbatch /home/sven/darsync_Documents.slurm\nSubmitted batch job 49021945 on cluster rackham\n</code></pre> I get an error 'sbatch: error: Batch job submission failed'. What do I do? <p>It means that the script created for you has a mistake.</p> <p>See Slurm troubleshooting for guidance on how to troubleshoot this.</p> How do I know this job has finished? <p>One way is to see if your job queue is empty:</p> <pre><code>[sven@rackham1 ~]$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n</code></pre> <p>Here, an empty job queue is shown. If the job is still running, you can find it in this list.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#7-check-logs","title":"7. Check logs","text":"<p>Once the submitted job has finished, have a look at the log file produced by the job and make sure it did not end in a error message. Replace <code>nais2024-23-9999</code> with the name of the folder you told Darsync to transfer.</p> <pre><code>tail ~/dardel_naiss2024-23-9999.out\ntail ~/dardel_naiss2024-23-9999.err\n</code></pre> How does that look like? <p>If the job finished successfully, the output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ tail darsync_Documents.out\nsending incremental file list\n[sven@rackham1 ~]$ tail darsync_Documents.err\n[sven@rackham1 ~]$\n</code></pre> I have the warning <code>rsync: [generator] failed to set times on \"...\": Operation not permitted (1)</code>. Did something go wrong? <p>No.</p> <p>Here is the full warning:</p> <pre><code>rsync: [generator] failed to set times on \"/cfs/klemming/projects/snic/my_project/.\": Operation not permitted (1)\n</code></pre> <p>This is a warning, indicating that the target folder on Dardel already exists. You can safely ignore it.</p> I have the warning <code>rsync error: some files/attrs were not transferred</code>. Did something go wrong? <p>No.</p> <p>Here is the full warning:</p> <pre><code>rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1179) [sender=3.1.2]\n</code></pre> <p>This is a warning, indicating that some file attributes were not transferred. An easy example is the file attribute for who is the file creator: this will differ between UPPMAX and PDC (the organisation that takes care of Dardel) because you have different usernames, for example <code>svesv</code> ('Sven Svensson') on UPPMAX and <code>svensv</code> on PDC. Hence, the file creator will differ between files.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#8-optional-confirm-all-files-are-transferred","title":"8. (optional) confirm all files are transferred","text":"<p>If your Slurm log looks like below, all file transfers are finished.</p> <pre><code>[sven@rackham3 ~]$ bash darsync_cedi.slurm\nsending incremental file list\nrsync: [generator] failed to set times on \"/cfs/klemming/projects/snic/cedi/.\": Operation not permitted (1)\n</code></pre> <p>The tool that <code>darsync</code> uses (called <code>rsync</code>) inherently cares about file integrity: you can reasonably assume your files have been transferred. See the box below for details.</p> How can I reasonably assume my files are transferred? <p><code>rsync</code> will only stop transferring data if all MD5 checksums between Rackham and Dardel match. An MD5 checksum is a way to sum up a file's content in one big number. If any bit in a file differs, this results in a different MD5 checksum. Hence, if the MD5 checksums match, you can reasonably assume the files are identical.</p> <p>One way to double-check, is to see if the total file sizes between Rackham and Dardel match.</p> <p>In https://supr.naiss.se, you can see the disk usage of your projects</p> How does that look like? <p>This looks like this, for an UPPMAX project:</p> <p></p> <p>A PDC project will look similar.</p> <p>You can also use a command line tool, <code>uquota</code>, to see your project's disk usage on Rackham.</p> How does that look like? <p>This looks like this, for an UPPMAX project:</p> <pre><code>[sven@rackham1 ~]$ uquota\nYour project       Your File Area           Unit        Usage  Quota Limit  Over Quota\n-----------------  -----------------------  -------  --------  -----------  ----------\nhome               /home/sven               GiB          17.6           32\nhome               /home/sven               files      112808       300000\nnaiss2024-22-1202  /proj/r-py-jl-m-rackham  GiB           6.1          128\nnaiss2024-22-1202  /proj/r-py-jl-m-rackham  files       52030       100000\nnaiss2024-22-1442  /proj/hpc-python-fall    GiB           0.0          128\nnaiss2024-22-1442  /proj/hpc-python-fall    files           4       100000\nnaiss2024-22-49    /proj/introtouppmax      GiB           5.1          128\nnaiss2024-22-49    /proj/introtouppmax      files       20290       100000\n</code></pre> <p>For PDC, read their documentation here: you will need to search for 'Klemming data management'.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#9-delete-the-ssh-key","title":"9. Delete the SSH key","text":"<p>After the migration, these temporary SSH keys can and should be deleted:</p> <pre><code>rm ~/id_ed25519_pdc*\n</code></pre> How does this look like? <p>You screen will show something similar to this:</p> <pre><code>[sven@rackham1 ~]$ rm ~/id_ed25519_pdc*\n[sven@rackham1 ~]$\n</code></pre>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#10-delete-the-files-on-rackham","title":"10. Delete the files on Rackham","text":"<p>Now that the files are transferred to Dardel, you can delete the files on Rackham that you've just transferred to Dardel.</p> How does that look like? <p>If you transferred one folder, for example, <code>Documents</code>, here is how to delete it and how that looks like:</p> <pre><code>[sven@rackham1 ~]$ rm -rf Documents/\n[sven@rackham1 ~]$\n</code></pre> <p>The <code>rm</code> command (<code>rm</code> is short for 'remove') cannot be undone. Luckily, your files are on Dardel already :-)</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#questions","title":"Questions","text":"How long does the transfer take? <p>Estimates range from 23 to 360 to gigabyte per hour. This excludes the extremes of 7 and 3600 gigabyte per hour.</p> <p>However, for large numbers of small files the metric files/seconds would be better, yet requires a benchmark.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t-troubleshooting","title":"T. Troubleshooting","text":"","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t1-ssh-connect-to-host-dardelpdckthse-port-22-no-route-to-host","title":"T1. <code>ssh: connect to host dardel.pdc.kth.se port 22: No route to host</code>","text":"","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t1-full-error-message","title":"T1. Full error message","text":"<pre><code>[sven@rackham1 ~]$ bash /domus/h1/sven/dardel_transfer_script.sh\nssh: connect to host dardel.pdc.kth.se port 22: No route to host\nrsync: connection unexpectedly closed (0 bytes received so far) [sender]\nrsync error: unexplained error (code 255) at io.c(226) [sender=3.1.2]\n</code></pre>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t1-likely-cause","title":"T1. Likely cause","text":"<p>This probably means that Dardel is down, likely due to maintenance.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t1-solution","title":"T1. Solution","text":"<p>You can do nothing, except wait until Dardel is up again.</p> <p>You may check the PDC news at https://www.pdc.kth.se/about/pdc-news to confirm that there is indeed a problem with Dardel.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t2-rsync-generator-failed-to-set-times-on-cfsklemmingprojectssnicnaiss2024-23-352-operation-not-permitted-1","title":"T2. <code>rsync: [generator] failed to set times on \"/cfs/klemming/projects/snic/naiss2024-23-352/.\": Operation not permitted (1)</code>","text":"","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t2-full-error-message","title":"T2. Full error message","text":"<pre><code>$ bash darsync_my_folder.slurm\nsending incremental file list\nrsync: [generator] failed to set times on \"/cfs/klemming/projects/snic/naiss2024-23-352/.\": Operation not permitted (1)\n</code></pre> <p>after which the script keeps running.</p> For UPPMAX staff <p>An example can be found at https://github.com/UPPMAX/ticket_296149.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t2-hypothesized-cause","title":"T2. Hypothesized cause","text":"<p>This darsync script is running for the second (or more) time, hence it has already created the target folders on Dardel. This hypothesis is backed by this Stack Overflow post where it is suggested to delete the folders; in this case: the target folders on Dardel.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t2-solution","title":"T2. Solution","text":"<p>On Dardel, delete the target folders that are already there and re-run the script.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t3-permission-denied-publickeygssapi-keyexgssapi-with-mic","title":"T3. <code>Permission denied (publickey,gssapi-keyex,gssapi-with-mic)</code>","text":"","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t3-full-error-message","title":"T3. Full error message","text":"<pre><code>[sven@rackham1 .ssh]$ bash /home/sven/darsync_my_script.slurm\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic).\n\nrsync: connection unexpectedly closed (0 bytes received so far) [sender]\n\nrsync error: unexplained error (code 255) at io.c(226) [sender=3.1.2]\n</code></pre> <p>Note that our fictional user runs the Slurm script via <code>bash</code>, instead of via <code>squeue</code>.</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t3-first-possible-fix","title":"T3. First possible fix","text":"<p>Run the script as such:</p> <pre><code>sbatch /home/sven/darsync_my_script.slurm\n</code></pre>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/dardel_migration/#t3-second-possible-fix","title":"T3. Second possible fix","text":"<p>Another possible fix comes from StackOverflow:</p> <p>Setting 700 to .ssh and 600 to authorized_keys solved the issue.</p> <pre><code>chmod 700 /root/.ssh\nchmod 600 /root/.ssh/authorized_keys\n</code></pre> <p>Hence, try:</p> <pre><code>chmod 700 ~/.ssh\nchmod 600 ~/.ssh/authorized_keys\n</code></pre> <p>Still does not work? Contact support</p>","tags":["Dardel","migration","Rackham","recovery"]},{"location":"cluster_guides/disk_quota_more/","title":"How can I display my disk quota?","text":""},{"location":"cluster_guides/disk_quota_more/#how-can-i-display-my-disk-quota","title":"How can I display my disk quota?","text":"<p>To limit the amount of disk space each user can allocate we use a disk quota system at UPPMAX. The default disk quota is 32 GByte in your home directory. Every NAISS-project also comes with a default 128 GByte backed-up project storage. If more data is needed you may apply for an UPPMAX Storage Project and get more quota. UPPMAX project have a default 512 GByte backed-up project storage and a and 512 GB nobackup space.</p> <p>You can display your current usage with the command uquota.</p> <p>When you exceed your quota, the system will not let you write any more data and you have to either remove some files or request more quota. The 'uquota' command will also show the date and to what limit your quota will change to, if you have been given a larger quota.</p> <p>Before contacting support, clean out unnecessary data and make an inventory of the data in your project (what type of data, how big, why it's needed). Here are two commands:</p> <pre><code>du -b $PWD | sort -rn | awk 'NR==1 {ALL=$1} {print int($1*100/ALL) \"% \" $0}'\n</code></pre> <p>This first command results in a list of subdirectories ordered by size and proportion of total size.</p> <pre><code>find $PWD -print0 -type f | xargs -0 stat -c \"%s %n\" | sort -rn\n</code></pre> <p>This second command produces a list of the files in the current directory that take up most space. These may take a long time to complete, use <code>CTRL + C</code> to cancel execution if you change your mind.</p> <p>After these two checks, to get more disk space, contact support and state how much, for how long time, and why you need it. See the storage project application page for more information on how we handle and prioritise storage requests.</p> <p>You should also read the Disk Storage Guide.</p>"},{"location":"cluster_guides/disk_storage_guide/","title":"Disk storage guide","text":"","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#disk-storage-guide","title":"Disk storage guide","text":"","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#quota","title":"Quota","text":"<p>Users have access to shared network storage on various cluster file systems. This mean that whenever you are logged in to a login server or if you are running on a compute node you will have the same view of the storage.</p> <p>There are several different classes of disk storage available with different policies for usage, limits and backup:</p> <ul> <li>The user home file system</li> <li>Local scratch file systems</li> <li>The network project and nobackup file system</li> <li>Temporary virtual filesystem</li> </ul> <p>Users have access to shared network storage on various cluster file systems, and backup home directories and some project storage to tape.</p>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#how-much-of-my-quota-do-i-use","title":"How much of my quota do I use?","text":"<p>Use uquotato check current disk usage and limits.</p> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham3 ~]$ uquota\nYour project       Your File Area           Unit        Usage  Quota Limit  Over Quota\n-----------------  -----------------------  -------  --------  -----------  ----------\nhome               /home/sven             GiB          16.6           32\nhome               /home/sven             files      104165       300000\nnaiss2024-22-1202  /proj/r-py-jl-m-rackham  GiB           0.0          128\nnaiss2024-22-1202  /proj/r-py-jl-m-rackham  files           4       100000\nnaiss2024-22-49    /proj/introtouppmax      GiB           5.1          128\nnaiss2024-22-49    /proj/introtouppmax      files       20290       100000\nstaff              /proj/staff              GiB       66064.8       102400\nstaff              /proj/staff              files    21325500     15000000           *\n</code></pre>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#i-use-more-quota-than-i-think-how-do-i-find-out-the-cause","title":"I use more quota than I think. How do I find out the cause?","text":"<p>To find out which folder uses most storage, run the following command to find the 20 folders that take up most storage space:</p> <pre><code>du -b $PWD | sort -rn | awk 'NR==1 {ALL=$1} {print int($1*100/ALL) \"% \" $0}' | head -n 20\n</code></pre> How does that look like? <p>Your output looks similar to this:</p> <pre><code>[sven@rackham3 ~]$ du -b $PWD | sort -rn | awk 'NR==1 {ALL=$1} {print int($1*100/ALL) \"% \" $0}' | head -n 20\n100% 17643266006 /home/sven\n50% 8984271436 /home/sven/.cache\n45% 8016778981 /home/sven/.cache/pip\n39% 6988369390 /home/sven/.cache/pip/http\n28% 4986824453 /home/sven/.local\n28% 4986117855 /home/sven/.local/lib\n27% 4816372185 /home/sven/.local/lib/python3.8\n27% 4816368089 /home/sven/.local/lib/python3.8/site-packages\n15% 2797022871 /home/sven/.local/lib/python3.8/site-packages/nvidia\n10% 1876238645 /home/sven/.cache/pip/http/3\n9% 1648194862 /home/sven/.local/lib/python3.8/site-packages/torch\n9% 1589833684 /home/sven/users\n8% 1569946463 /home/sven/users/fares\n8% 1553069908 /home/sven/.local/lib/python3.8/site-packages/torch/lib\n8% 1431151816 /home/sven/.cache/pip/http/0\n7% 1411093224 /home/sven/.cache/pip/http/3/c\n5% 1023338615 /home/sven/.local/lib/python3.8/site-packages/nvidia/cudnn\n5% 1022966263 /home/sven/.local/lib/python3.8/site-packages/nvidia/cudnn/lib\n5% 983932032 /home/sven/.cache/pip/http-v2\n5% 983390581 /home/sven/.cache/pip/http/9\n</code></pre> <p>To find out which files uses most storage, run the following command to find the 20 files that take up most storage space:</p> <pre><code>find $PWD -print0 -type f | xargs -0 stat -c \"%s %n\" | sort -rn\n</code></pre> How does that look like? <p>Your output looks similar to this:</p> <pre><code>[sven@rackham3 ~]$ find $PWD -print0 -type f | xargs -0 stat -c \"%s %n\" | sort -rn | head -n 20\n1546936200 /home/sven/users/anna/H10_Avian_1650_2000_HA_alignment.trees\n902414441 /home/sven/.local/lib/python3.8/site-packages/torch/lib/libtorch_cuda.so\n797076603 /home/sven/.cache/pip/http/0/c/d/a/3/0cda36001dc173401b525a7e434e8b7f1079d34f31141b688325244b\n755535721 /home/sven/.cache/pip/http/9/b/8/7/5/9b875d1148ce95ad551df724a540378d1dc8158fa59145beb2ec4125\n731727087 /home/sven/.cache/pip/http/3/c/e/f/9/3cef90e2f33f3b9a1b50e02cc0736e09cc97714cb8b1101d706d912d\n664753951 /home/sven/.cache/pip/http/3/c/8/2/7/3c827aae7500e30cec6930647f8971adb3eafb1cd65a44fcf02ba940\n589831274 /home/sven/.cache/pip/http-v2/9/4/c/e/7/94ce755eb45386ac0cd2115e71a8162388f908eac28abff6118b7e7a.body\n569645536 /home/sven/.local/lib/python3.8/site-packages/nvidia/cudnn/lib/libcudnn_engines_precompiled.so.9\n515090264 /home/sven/.local/lib/python3.8/site-packages/nvidia/cublas/lib/libcublasLt.so.12\n497648053 /home/sven/.cache/pip/http/0/e/3/7/9/0e379b2d265d90194ab62c0f7704318e349017777b755c72c955e025\n497624428 /home/sven/.cache/pip/http/4/b/7/9/b/4b79bbc6cc88163d2cba55b1492741f457013fc2c14b26bdd398a0a3\n495148366 /home/sven/.cache/pip/http/e/7/c/6/1/e7c618a0177b1a48a4599a6785fda5ffd4946442a77e875b970fdfee\n492151297 /home/sven/.local/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so\n468354983 /home/sven/.cache/huggingface/hub/models--zhihan1996--DNABERT-2-117M/blobs/7ff39ec77a484dd01070a41bfd6e95cdd7247bec80fe357ab43a4be33687aeba\n410595986 /home/sven/.cache/pip/http/3/9/a/e/6/39ae6aa825aebb75b0193714975cbc9defffa90203c5342f2214137e\n264876688 /home/sven/.local/lib/python3.8/site-packages/nvidia/cusparse/lib/libcusparse.so.12\n240706416 /home/sven/.local/lib/python3.8/site-packages/nvidia/cudnn/lib/libcudnn_adv.so.9\n232685936 /home/sven/.local/lib/python3.8/site-packages/nvidia/nccl/lib/libnccl.so.2\n209353474 /home/sven/.cache/pip/http/d/4/c/3/e/d4c3ec899ac2836a7f89ffad88e243bf35b92f56ff0b61ad0f5badf5\n195959494 /home/sven/.cache/pip/http/6/e/f/7/a/6ef7ae373253a3997ffc8ac7b70e67716f79d6365ffa5c28f40f349a\n</code></pre>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#if-you-need-more-quota","title":"If you need more quota","text":"<p>If more quota is needed, contact support for advice. We do not extend quotas for home directories or SNIC project directories, but it's possible to apply for storage projects.</p> <p>Before contacting support, clean out unnecessary data and make an inventory of the data in your project (what type of data, how big, why it's needed). Here are two commands:</p> <pre><code>du -b $PWD | sort -rn | awk 'NR==1 {ALL=$1} {print int($1*100/ALL) \"% \" $0}'\n</code></pre> <p>This first command results in a list of subdirectories ordered by size and proportion of total size.</p> <pre><code>find $PWD -print0 -type f | xargs -0 stat -c \"%s %n\" | sort -rn\n</code></pre> <p>This second command produces a list of the files in the current directory that take up most space. These may take a long time to complete, use <code>CTRL + C</code> to cancel execution if you change your mind.</p>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#if-you-need-even-more-quota-for-archiving","title":"If you need even more quota for archiving","text":"<p>Please contact support.</p> <p>We have a previously been able to provide users with a low-cost moderate performant storage solution for a cost of 500SEK/TB/year, for a commitment of four years and 50TB.</p>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#environmental-variables","title":"Environmental variables","text":"<p>We have defines several environment variables to help our users. They are:</p> <ul> <li><code>$HOME</code> (or <code>$SNIC_BACKUP</code>) is a traditional one, pointing to the users home directory.</li> <li><code>$TMPDIR</code> or (<code>$SNIC_TMP</code>) points to node-local storage, suitable for temporary files that can be deleted when the job finishes</li> <li><code>$SNIC_NOBACKUP</code> points to an UPPMAX-wide storage suitable for temporary files (not deleted when the job is finished)</li> </ul>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#types-of-storage","title":"Types of storage","text":"","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#user-home-directories","title":"User Home directories","text":"<p>Paths: $HOME or $SNIC_BACKUP</p> <p>Permanent storage of users files during the lifetime of the accounts. Shared access on all cluster nodes. Snapshots are normally enabled on this file system, and you can access the snapshots in every directory by 'ls .snapshot' or 'cd .snapshot'. The quota is 32GB per user. We provide backup of this volume, and we keep the files on tape up to 90 days after they are deleted from disk. If you have files you do not want to back up place them in a folder called 'nobackup'.</p> <p>We recommend you do not use your home directory for running jobs. Our general recommendation is to keep everything related to a specific research project in its project directory.</p>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#local-scratch","title":"Local Scratch","text":"<p>Paths: <code>$TMPDIR</code> or <code>$SNIC_TMP</code></p> <p>Each node has a <code>/scratch</code> volume for local access providing the most efficient disk storage for temporary files. Users have read/write access to this file system. Slurm defines the environment variable TMPDIR which you may use in job scripts. On clusters with Slurm you may use /scratch/$SLURM_JOB_ID. This area is for local access only, and is not directly reachable from other nodes or from the front node. There is no backup of the data and the lifetime of any file created is limited to the current user session or batch job. Files are automatically erased when space is needed by other users.</p>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#projects-global-network-storage","title":"Projects global (network) storage","text":"<p>Paths: /proj/[proj-id]</p> <p>The project global storage is permanent storage of project's files during the lifetime of the project. Disk quota on this volume is shared by all project members. Default quota allocation is determined by your project type.</p> <p>Note that the quota system on crex is built on group ownership for files/directories. This means that moving files between project directories does not directly affect quota. We have scripts and other tricks that tries to ensure the correct group is always used, but  in general this may lag quite some time - it takes a while to go through everything, especially since we don't want to affect performance. To make sure quota information is correct, you can change the group to the correct one after moving directories:</p> <p>chgrp -R PROJECT_YOU_MOVED_TO PATH_OF_THE_MOVED_DIRECTORY</p> <p>if you don't do this, it will still be fixed, but it may take a while.</p> <p>The files are backed up to tape and we keep the files for 30 days after they are deleted from disk. In the project folder you should keep all your raw data and important scripts.</p> <p>On Bianca and in SLLStore and UppStore projects, all temporary files, and files that can be regenerated (e.g.. data created from your computations), should be moved to the nobackup folder.</p> <p>More information about backup at UPPMAX.</p>","tags":["disk","storage","guide"]},{"location":"cluster_guides/disk_storage_guide/#temporary-virtual-filesystem","title":"Temporary virtual filesystem","text":"<p>Paths: <code>/dev/shm/[job-id]</code></p> <p>On all our clusters we have a temporary virtual filesystem implemented as a shared memory area. I.e. it uses primarily the RAM for storage (until it eventually might have to swap out to physical disk), and can be accessed via the path /dev/shm/[job-id].</p> <p>In some situations this \"disk\" area can be quicker to read/write to, but depending on the circumstances it can also be slower than local scratch disk. Also note that it is a shared resource among all running jobs on a specific node, so depending on the node and how much memory your job has been allocated, the amount of data you can write will vary.</p>","tags":["disk","storage","guide"]},{"location":"cluster_guides/extend_duration_of_running_job/","title":"How to extend the duration of a job that is running?","text":"","tags":["FAQ","job","longer","extend","duration","time"]},{"location":"cluster_guides/extend_duration_of_running_job/#how-to-extend-the-duration-of-a-job-that-is-running","title":"How to extend the duration of a job that is running?","text":"<p>There are many UPPMAX cluster frequently asked questions. This page describes how to extend a job that has been running for a long time and is near its duration limit.</p>","tags":["FAQ","job","longer","extend","duration","time"]},{"location":"cluster_guides/extend_duration_of_running_job/#casus","title":"Casus","text":"<p>You need to do a hard calculation. It is expected to take a log time, hence you set the time limit to the maximum time limit of 10 days.</p> <p>At the 8th day, you see that the job is completed for only 75%, hence you need a bit more extra days.</p> <p>How to extend the duration of your job?</p>","tags":["FAQ","job","longer","extend","duration","time"]},{"location":"cluster_guides/extend_duration_of_running_job/#solution","title":"Solution","text":"<p>Contact support</p>","tags":["FAQ","job","longer","extend","duration","time"]},{"location":"cluster_guides/file_transfer/","title":"File transfer","text":"","tags":["file","transfer"]},{"location":"cluster_guides/file_transfer/#file-transfer","title":"File transfer","text":"<p>File transfer is the process of getting files from one place to the other.</p> <ul> <li>File transfer to/from Bianca</li> <li>File transfer to/from Dardel</li> <li>File transfer to/from Rackham</li> <li>File transfer to/from Transit</li> </ul>","tags":["file","transfer"]},{"location":"cluster_guides/files/","title":"Files on UPPMAX","text":""},{"location":"cluster_guides/files/#files-on-uppmax","title":"Files on UPPMAX","text":""},{"location":"cluster_guides/files/#disk-storage-guide","title":"Disk storage guide","text":"<p>See the UPPMAX disk storage guide.</p>"},{"location":"cluster_guides/files/#where-are-my-files-or-what-are-the-different-file-systems","title":"Where are my files? (Or, what are the different file systems?)","text":"<p>You have access to the same home directory regardless of what cluster you have logged into. Here you store your private files.</p> <p>All projects also have a central storage area under the <code>/proj/[project id]/</code> directory path, i.e. when you first login to UPPMAX, you will see your home directory, so you will have to change to the project directory if you want to transfer project data files.</p> <p>Also note that UPPMAX uses different disk quotas on your home directory and other areas you have access to (like the project folder). Use uquota to see who much disk space you use.</p>"},{"location":"cluster_guides/files/#your-private-files","title":"Your private files","text":"<p>When you log in to UPPMAX for the first time you only have the following files created by the system:</p> <pre><code>$ ls -la\ntotal 68\ndrwxr-x---  7 user     uppmax 4096 Jun  2 23:11 .\ndrwxr-xr-x 19 root     root      0 Jun  9 13:16 ..\n-rw-r--r--  1 user     uppmax   24 Jan  9  2008 .bash_logout\n-rw-r--r--  1 user     uppmax  435 Apr 21  2008 .bash_profile\n-rw-r--r--  1 user     uppmax  446 Jan  9  2008 .bashrc\ndrwxr-xr-x  2 user     uppmax 4096 Jan  9  2008 bin\n-rw-r--r--  1 user     uppmax  385 Jan  9  2008 .cshrc\n-rw-r--r--  1 user     uppmax  237 Jan  9  2008 .emacs\ndrwxrwxrwx  1 user     uppmax   14 Jun  2 11:05 glob\n-rw-r--r--  1 user     uppmax  120 Jan  9  2008 .gtkrc\n-rw-r--r--  1 user     uppmax  279 Apr 21  2008 .login\ndrwx--S---  2 user     uppmax 4096 May  2  2008 private\n-rw-r--r--  1 user     uppmax  307 Apr 21  2008 .profile\n-rw-r--r--  1 user     uppmax  220 Jan  9  2008 .zshrc\n</code></pre> <p>The files starting with a <code>.</code> (i.e. a dot or period) are hidden files. These are commonly startup scripts or configuration files.</p> <p>The default permission of your home directory is <code>750</code>, i.e. you can do everything, people belonging to the same group can read and execute your files and other people can not do anything.</p> <p>Also note the <code>private</code> sub-folder: here you can put files that you want only you, and no one else, to be able to access. Each day we have a job that ensures that all users private folders still can't be accessed by anyone else, even if the permissions somehow accidentally would change.</p>"},{"location":"cluster_guides/files/#creating-and-editing-files","title":"Creating and editing files","text":"<p>Creating and editing files is taught:</p> <ul> <li>UPPMAX intro day 1: use the remote desktop environment</li> <li>UPPMAX intro day 1: use the terminal</li> </ul>"},{"location":"cluster_guides/get_data_from_an_expired_project/","title":"How to get data from an expired project?","text":"","tags":["FAQ","data","expired","project","NAISS","UPPMAX"]},{"location":"cluster_guides/get_data_from_an_expired_project/#how-to-get-data-from-an-expired-project","title":"How to get data from an expired project?","text":"<p>There are many UPPMAX cluster frequently asked questions. This page describes how to get the data from an expired project.</p>","tags":["FAQ","data","expired","project","NAISS","UPPMAX"]},{"location":"cluster_guides/get_data_from_an_expired_project/#casus","title":"Casus","text":"<p>You've been working on a project for some time, published your results, and the project expires.</p> <p>Later you realize that there is still some data you need from that project.</p> <p>How to get the data from an expired project?</p>","tags":["FAQ","data","expired","project","NAISS","UPPMAX"]},{"location":"cluster_guides/get_data_from_an_expired_project/#solution","title":"Solution","text":"<p>Contact support</p>","tags":["FAQ","data","expired","project","NAISS","UPPMAX"]},{"location":"cluster_guides/gorilla/","title":"Gorilla","text":"","tags":["Gorilla","storage system","UPPMAX"]},{"location":"cluster_guides/gorilla/#gorilla","title":"Gorilla","text":"<p>Gorilla is a future storage system, using the ceph file system.</p>","tags":["Gorilla","storage system","UPPMAX"]},{"location":"cluster_guides/grace_period/","title":"Grace period","text":"","tags":["grace","period"]},{"location":"cluster_guides/grace_period/#grace-period","title":"Grace period","text":"<p>The grace period is the period of time that one can access an HPC cluster without using a TOTP, for a cluster (e.g. a sensitive data cluster) that requires 2FA.</p> <p>It exists to allow our users to access our HPC clusters with tools that assume there is no 2FA needed, i.e. there is no dialog where you can fill in your TOTP.</p> <p>To start the grace period, log in on your favorite HPC cluster (again, the log in will require a TOTP). Now, you have around 5 minutes to connect your tools to that cluster.</p>","tags":["grace","period"]},{"location":"cluster_guides/interactive_more/","title":"How to run an interactive session?","text":""},{"location":"cluster_guides/interactive_more/#how-to-run-an-interactive-session","title":"How to run an interactive session?","text":"<p>You may want to run an interactive application on one or several compute nodes. You may want to use one or several compute nodes as a development workbench, interactively. How can this be arranged? The program <code>interactive</code> may be what you are looking for.</p> <p>The best way to use the command is usually to add as few parameters as possible, because the <code>interactive</code> command tries to find an optimal solution to give you a high queue priority and thus a quick job start. If you have a clear idea about what parameters you need, please specify them, otherwise it might be a good idea to first see what you get with fewer parameters.</p> <p>The one parameter you must always specify is the project name. Let's assume for this article that your project name is p2010099.</p> <p>To get one compute core with the proportional amount of RAM, we recommend you to use the most simple command on the login node for the cluster you want to use:</p> <pre><code>interactive -A p2010099\n</code></pre> <p>If you need more than one core, or special features on your node, you can specify that to the interactive command, e.g. on milou:</p> <pre><code>interactive -A p2010099 -n 16 -C fat\n</code></pre> <p>as if it was an sbatch command. Actually, interactive is implemented partly as an sbatch command and you can use most sbatch flags here. Please note that only a few nodes are fat, so you may have to wait for quite a long time to get your session started.</p> <p>There are three ways to get a priority boost, and the interactive command knows how to use them all:</p> <p>Internally using the sbatch flag \"--qos=interact\", that allows a single-node job with a timelimit of up to 12 hours. (Please note that you are not allowed to keep more than one \"--qos=interact\" jobs in the batch system simultaneously, and please note that you can not use this \"priority lane\" when you have oversubscribed your 30 days running core hour allocation.) Internally using the special devel partition, that allows the job to use 1-4 nodes, with a timelimit of up to one hour. (Please note that you are not allowed to keep more than one \"devel\" job in the batch system simultaneously, regardless if they are running or merely queued.) Internally using the sbatch flag \"--qos=short\", that allows the job to use 1-4 nodes, with a timelimit of up to 15 minutes. (Please note that your are not allowed to keep more than two \"short\" jobs in the batch system simultaneously.) If you do not specify any timelimit, the interactive command will give you the maximum timelimit allowed, according to the rules for priority boosts.</p> <p>In the last example (\"interactive -A p2010099 -n 16 -C fat\"), the interactive command can not use \"priority lane\" 1 above, because it uses more than one node (one node contains eight cores, two nodes contain a total of sixteen cores), and it can not use \"priority lane\" 2 above, because the special devel partition contains no fat nodes, so the interactive command tries to give you a high-priority 15-minute job.</p> <p>If you also want to run for 15 hours, you may say so, with the command</p> <pre><code>interactive -A p2010099 -n 16 -C fat -t 15:00:00\n</code></pre> <p>but no \"priority lane\" can be used, you get your normal queue priority, and you might have to wait for a very long time for your session to start. Please note that you need to keep watch over when the job starts, because you are accounted for all the time from job start even if you are sleeping, and because an allocated and unused node is a waste of expensive resources.</p> <p>NB. You can not launch an interactive job from an other cluster with the flag -M, which otherwise is a common flag to other Slurm commands. You must launch it from a login node to the cluster you want to use.</p>"},{"location":"cluster_guides/login_node/","title":"Login node","text":"","tags":["login node","node","login"]},{"location":"cluster_guides/login_node/#login-node","title":"Login node","text":"<p>A login node is the computer where you arrive after logging in to an UPPMAX HPC cluster.</p> How does that look like? <p>Here is how it looks like to be on a login node:</p> <p></p> <p>A user on a login node, in this case on a Rackham login node called <code>rackham4</code>. The user used an SSH client and is in a console environment.</p> <p></p> <p>A user on a login node, in this case on the Bianca login node of his/her virtual cluster. This user logged in to the Bianca remote desktop via the website</p> <p>A login node is a shared resource. With this diagram you can determine if you are alone on a login node:</p> <pre><code>flowchart TD\n  question[Are you alone on the login node?]\n  which_cluster[Which cluster?]\n  alone_in_project[Are you alone in this project?]\n  no[No: you share the login node with others]\n  yes[Yes: you have the login node for yourself]\n  question --&gt; which_cluster\n  which_cluster --&gt; |Rackham| no\n  which_cluster --&gt; |Bianca| alone_in_project\n  alone_in_project --&gt; |yes| yes\n  alone_in_project --&gt; |no| no</code></pre> <p>Decision tree to determine if you are alone on a login node</p> <p>Because usually you share a login node with others, this is the rule how to behave on a login node:</p> <p>Only do short and light things on the login node</p> <p>Examples of short and light things are:</p> <ul> <li>Editing files</li> <li>Copying, deleting, moving files</li> <li>Scheduling jobs</li> <li>Starting an interactive session</li> </ul> <p>Examples of heavy things are:</p> <ul> <li>Running code with big calculations,   use the job scheduler instead</li> <li>Develop code with big calculations line-by-line,   use an interactive session instead</li> </ul> <pre><code>flowchart TD\n    UPPMAX(What to run on which node?)\n    operation_type[What type of operation/calculation?]\n    interaction_type[What type of interaction?]\n    login_node(Work on login node)\n    interactive_session(Uses on calculation node, in an interactive session)\n    calculation_node(Schedule for calculation node)\n\n    UPPMAX--&gt;operation_type\n    operation_type--&gt;|light,short|login_node\n    operation_type--&gt;|heavy,long|interaction_type\n    interaction_type--&gt;|Direct|interactive_session\n    interaction_type--&gt;|Indirect|calculation_node</code></pre> <p>Decision tree to determine which type of node you should probably work on</p> I work alone on a Bianca project. Can I use the login node for heavy things? <p>Yes!</p> <p>Or, to be more precise: yes, if the login node is powerful enough for your calculations.</p> <p>For example, when using RStudio on Bianca it is recommended to use at least two cores (and login node has 2 cores only).</p> <p>So, if you can, use the login node. If you need more resources, either use the job scheduler or use an interactive session with more nodes than the login node has.</p>","tags":["login node","node","login"]},{"location":"cluster_guides/login_node_restrictions/","title":"Login node restrictions","text":""},{"location":"cluster_guides/login_transit/","title":"Log in to Transit","text":""},{"location":"cluster_guides/login_transit/#log-in-to-transit","title":"Log in to Transit","text":"<p>Below is a step-by-step procedure to login to Transit.</p> Enjoy a video? <p>See how to log in to Transit as a video.</p>"},{"location":"cluster_guides/login_transit/#1-get-within-sunet","title":"1. Get within SUNET","text":"<p>Get inside the university networks.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>"},{"location":"cluster_guides/login_transit/#2-use-ssh-to-login","title":"2. Use SSH to login","text":"<p>On your local computer, start a terminal and use <code>ssh</code> to login to Transit:</p> <pre><code>ssh [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>ssh sven@transit.uppmax.uu.se\n</code></pre> <p>If you haven't setup using SSH keys, you will be asked for your UPPMAX password.</p> <p>If this is your first time on Transit, you will be asked for adding it to your list of known hosts. Type <code>yes</code>.</p> How does that look like? <p>This is how it looks like when you are asked for adding Transit to your list of known hosts.</p> <p></p> <p>You are now logged in to Transit!</p> How does that look like? <pre><code>sven@sven-N141CU:~/GitHubs/UPPMAX-documentation/docs/cluster_guides$ ssh sven@transit.uppmax.uu.se\nsven@transit.uppmax.uu.se's password:\nLast login: Tue May 14 07:32:22 2024 from vpnpool188-185.anst.uu.se\n\nTransit server\n\nYou can mount bianca wharf with the command\n\nmount_wharf PROJECT [path]\n\nIf you do not give a path the mount will show up as PROJECT in your home\ndirectory.\n\nNote; any chagnes you do to your normal home directory will not persist.\n</code></pre>"},{"location":"cluster_guides/maja/","title":"Maja","text":"","tags":["Maja","cluster","sensitive data"]},{"location":"cluster_guides/maja/#maja","title":"Maja","text":"<p>Maja is an upcoming UPPMAX cluster for sensitive data, taking over from Bianca.</p>","tags":["Maja","cluster","sensitive data"]},{"location":"cluster_guides/maja/#features-of-maja-compared-to-bianca","title":"Features of Maja, compared to Bianca","text":"<p>Although we from UPPMAX cannot say exactly how Maja will look, we try to keep her similar to Bianca.</p>","tags":["Maja","cluster","sensitive data"]},{"location":"cluster_guides/maja/#migration-from-bianca-to-maja","title":"Migration from Bianca to Maja","text":"<p>As both clusters are UPPMAX clusters, we are probably able to transfer your data from Bianca to Maja.</p>","tags":["Maja","cluster","sensitive data"]},{"location":"cluster_guides/miarka/","title":"Miarka","text":""},{"location":"cluster_guides/miarka/#miarka","title":"Miarka","text":"<p>Miarka is a SciLifeLab cluster.</p>"},{"location":"cluster_guides/module_conflicts/","title":"How can I resolve problems with conflicting modules?","text":"","tags":["module","conflict","conflicts","solve","resolve","fix"]},{"location":"cluster_guides/module_conflicts/#how-can-i-resolve-problems-with-conflicting-modules","title":"How can I resolve problems with conflicting modules?","text":"<p>Sometimes you may experience conflicting modules. An example would be that your program finds an incorrect library. This can be caused by two or more modules providing libraries with the same name.</p> <p>Since there are a lot of different modules installed at UPPMAX, we have no possibility to test the compatibility of all the modules.</p> <p>If you get error messages that you think might be because of conflicting modules, you can do the following:</p> <p>Check what modules you have loaded:</p> <pre><code>module list\n</code></pre> <p>If you want to remove one module:</p> <pre><code>module unload modulename\n</code></pre> <p>If you want to remove ALL modules:</p> <pre><code>module purge\n</code></pre> <p>Then start to load the modules you need, one by one:</p> <pre><code>module load modulename\n</code></pre> <p>Until you can run your program without errors.</p> <p>UPPMAX recommends that you only load as many modules as you need for each program, to minimize the risk of having conflicting modules.</p>","tags":["module","conflict","conflicts","solve","resolve","fix"]},{"location":"cluster_guides/modules/","title":"Modules","text":"","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#software-modules","title":"Software modules","text":"<p>Here we show how to use the environment module system.</p> <p>After describing the background/reasoning why such a system is needed, we show how to work with the module system.</p> <p>There is a table of commonly used shorthand syntaxes, as well as links to almost all installed software and databases on UPPMAX.</p>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#background","title":"Background","text":"<p>The UPPPMAX clusters are shared Linux computers with all the standard Linux tools installed, on which all users should be able to do their work independently and undisturbed.</p> <p>To ensure this, users cannot modify, upgrade or uninstall software themselves and instead an environment module system (from now on: 'module system') is used. This allow users to independently use their favorite versions of their favorite software.</p> <p>Using a module system keeps installed software hidden by default, and users have to explicitly tell their terminal which version of which software they need.</p> <p>To have new software installed on an UPPMAX cluster, users must explicitly request a version of a piece of software. As of today, there are nearly 800+ programs and packages, with multiple versions available on all UPPMAX clusters. Using explicit versions of software is easy to do and improves the reproducibility of the scripts written.</p> <p>To preserve hard disk space, Bianca also has multiple big databases installed.</p>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#working-with-the-module-system","title":"Working with the module system","text":"<p>Info</p> Command Description <code>module spider</code> Search for a module <code>module spider [module]</code> Get info about a module, e.g. <code>module spider cowsay</code> <code>module avail</code> Search for a module that is available <code>module list</code> List all activated modules <code>module load [module]</code> Load a module, e.g. <code>module load cowsay</code> <code>module load [module]/[version]</code> Load a module of a specific versions, e.g. <code>module load cowsay/3.04</code> <code>module help</code> Show the help for a module <code>module unload [module]</code> Unload the module <code>[module]</code>, e.g. <code>module unload cowsay</code> What is the difference between <code>module spider</code> and <code>module avail</code>? <ul> <li><code>module spider</code>: search for a module,   also those that are not available (yet)</li> <li><code>module avail</code>: search for a module that is available</li> </ul> <p>As an example, use the <code>samtools</code> module, which will always be found by <code>module spider samtools</code>, but will only be found by <code>module avail</code> after a `module load bioinfo-tools\"</p> <p>Working with the module system means:</p> <ul> <li>searching for a module</li> <li>activating ('loading') a module</li> <li>deactivate ('unloading') a module</li> </ul> <p>This section describes these steps in more details.</p> <p>The <code>module</code> command is the basic interface to the module system.</p> <p>To search for a module, use <code>module spider [module]</code>, for example <code>module spider cowsay</code>.</p> Would you like to see a video instead? <p>See the YouTube video the use of modules on Bianca</p> What is <code>cowsay</code>? <p>See the UPPMAX page on <code>cowsay</code></p> What is <code>R</code>? <p><code>R</code> is the module for the R programming language. R is a free and open-source programming language, commonly used in data analysis and visualization.</p> How does the output of <code>module spider R</code> look like? <pre><code>$ module spider R\n\n-------------------------------------------\n  R:\n-------------------------------------------\n     Versions:\n        R/3.0.2\n        R/3.2.3\n        R/3.3.2\n        R/3.4.0\n        R/3.4.3\n        R/3.5.0\n        R/3.5.2\n        R/3.6.0\n        R/3.6.1\n        R/4.0.0\n        R/4.0.4\n        R/4.1.1\n        R/4.2.1\n     Other possible modules matches:\n        454-dataprocessing  ADMIXTURE  ANTLR  ARCS  ARC_assembler  ARPACK-NG  ART  AdapterRemoval  AlienTrimmer  Amber  AnchorWave  Arlequin  Armadillo  ArrowGrid  Bamsurgeon  BclConverter  BioBakery  BioBakery_data  ...\n\n-------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*R.*'\n\n-------------------------------------------\n  For detailed information about a specific \"R\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider R/4.2.1\n-------------------------------------------\n</code></pre> What is <code>samtools</code>? <p><code>samtools</code> is the module for SAMtools. From wikipedia:</p> <p>SAMtools is a set of utilities for interacting with and post-processing short DNA sequence read alignments in the SAM (Sequence Alignment/Map), BAM (Binary Alignment/Map) and CRAM formats</p> How does the output of <code>module spider samtools</code> look like? <pre><code>$ module spider samtools\n\n-------------------------------------------\n  samtools:\n-------------------------------------------\n     Versions:\n        samtools/0.1.12-10\n        samtools/0.1.19\n        samtools/1.1\n        samtools/1.2\n        samtools/1.3\n        samtools/1.4\n        samtools/1.5_debug\n        samtools/1.5\n        samtools/1.6\n        samtools/1.8\n        samtools/1.9\n        samtools/1.10\n        samtools/1.12\n        samtools/1.14\n        samtools/1.16\n        samtools/1.17\n     Other possible modules matches:\n        SAMtools\n\n-------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*samtools.*'\n\n-------------------------------------------\n  For detailed information about a specific \"samtools\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider samtools/1.17\n-------------------------------------------\n</code></pre> How does the output of <code>module spider samtools/1.17</code> look like? <pre><code>$ module spider samtools/1.17\n\n-------------------------------------------\n  samtools: samtools/1.17\n-------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"samtools/1.17\" module is available to load.\n\n      bioinfo-tools\n\n    Help:\n        samtools - use samtools 1.17\n\n        Version 1.17\n</code></pre> <p>This reminds us that we need to load the <code>bioinfo-tools</code> module to be able to load <code>samtools/1.17</code>. Again, this is required (just once) before loading bioinformatics software.</p> <p>If there is an exact match, that module is reported first. Of the module shown, also the different versions are reported.</p> What to do when you cannot find a module <p>Run <code>module load bioinfo-tools</code>.</p> <p>This will allow other modules to be found.</p> What to do when <code>module load</code> gives an 'Lmod has detected the following error:  These module(s) or extension(s) exist but cannot be loaded as requested' error? <p>Ouch, now it is time to try out many things.</p> <p>Do not hesitate to contact support so that you can spend time on your research and we figure this out :-)</p> <p>To load a module, use <code>module load [module]</code>, for example <code>module load cowsay</code>. This will load the default version of that module, which is almost always the latest version. Loading a module always results in a helpful message (such as that it worked fine), however, it is not general help for using the tool itself.</p> How can I see which modules I've loaded? <p>Use the command <code>module list</code>.</p> How does the output of <code>module list</code> look like? <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) uppmax   2) bioinfo-tools   3) samtools/1.17\n</code></pre> <p>In this example case, we can see that the modules <code>bioinfo-tools</code> and <code>samtools</code> version 1.17 are loaded.</p> <p>Getting help on a module</p> <p>Run <code>module help [module]</code>, e.g. <code>module help cowsay</code> to get the general help on a module</p> <p>For reproducible research, however, it is good practice to load a specific version. The information given by <code>module spider</code> contains the versions of the module. For example, to load the <code>samtools/1.17</code> module, do <code>module load samtools/1.17</code>.</p> How does the output of <code>module load GATK/4.3.0.0</code> look like? <pre><code>$ module load GATK/4.3.0.0\nNote that all versions of GATK starting with 4.0.8.0 use a different wrapper\nscript (gatk) than previous versions of GATK.  You might need to update your\njobs accordingly.\n\nThe complete GATK resource bundle is in /sw/data/GATK\n\nSee 'module help GATK/4.3.0.0' for information on activating the GATK Conda\nenvironment for using DetermineGermlineContigPloidy and similar other tools.\n</code></pre> <p>This message references the command <code>module help GATK/4.3.0.0</code> for additional help with this module.</p> Huh, <code>module load samtools/1.17</code> gives an error? <p>If you do <code>module load samtools/1.17</code> without doing <code>module load bioinfo-tools</code> first, you'll get the error:</p> <pre><code>$ module load samtools/1.17\nLmod has detected the following error:  These module(s) or\nextension(s) exist but cannot be loaded as requested: \"samtools/1.17\"\n   Try: \"module spider samtools/1.17\" to see how to load the module(s).\n</code></pre> <p>The solution is to do <code>module load bioinfo-tools</code> first.</p> <p>To see which modules are loaded, use <code>module list</code>.</p> How does the output of <code>module list</code> look like? <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) uppmax   2) bioinfo-tools   3) samtools/1.17   4) java/sun_jdk1.8.0_151   5) GATK/4.3.0.0\n</code></pre> <p>Modules can also be unloaded, which also unloads their prerequisites.</p> <p>To see a module-specific help, use <code>module help [module]</code> (e.g. <code>module help cowsay</code>).</p> How does the output of <code>module help GATK/4.3.0.0</code> look like? <pre><code>$ module help GATK/4.3.0.0\n\n-------------- Module Specific Help for \"GATK/4.3.0.0\" ---------------\nGATK - use GATK 4.3.0.0\nVersion 4.3.0.0\n\n**GATK 4.3.0.0**\n\nUsage:\n\n    gatk --help     for general options, including how to pass java options\n\n    gatk --list     to list available tools\n\n    gatk ToolName -OPTION1 value1 -OPTION2 value2 ...\n                  to run a specific tool, e.g., HaplotypeCaller, GenotypeGVCFs, ...\n\nFor more help getting started, see\n\n    https://software.broadinstitute.org/gatk/documentation/article.php?id=9881\n\n...\n</code></pre> <p>To unload a module, do <code>module unload [module]</code> (e.g. <code>module unload cowsay</code>). This will also unload module that depend on the unloaded one. For example, <code>module unload bioinfo-tools</code> will unload all bioinformatics tool.</p>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#using-modules-in-an-executable-script","title":"Using modules in an executable script","text":"<p>Using modules in an executable script is straightforward: just load the module needed before using the software in that module.</p> <p>For example, this is a valid bash script:</p> <pre><code>#!/bin/bash\nmodule load cowsay/3.04\ncowsay hello\n</code></pre> <p>When using a bioinformatics tool such as <code>samtools</code> version 1.17, one needs to first load <code>bioinfo-tools</code>:</p> <pre><code>#!/bin/bash\nmodule load bioinfo-tools\nmodule load samtools/1.17\n</code></pre>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#common-shorthand-syntaxes","title":"Common shorthand syntaxes","text":"Full command Shorthand syntax <code>module</code> - <code>module avail</code> <code>ml av</code> <code>module spider</code> <code>ml spider</code> <code>module load</code> <code>ml</code> <code>module list</code> <code>ml</code> <code>module unload [module]</code> <code>ml -[module]</code>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#links","title":"Links","text":"<ul> <li>Almost all installed software on UPPMAX</li> <li>Almost all installed databases on UPPMAX</li> <li>Wikipedia page on environment modules</li> <li>lmod homepage</li> </ul>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#extra-materials","title":"Extra materials","text":"","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#about-module-avail","title":"About <code>module avail</code>","text":"<p>Why here?</p> <p>As far as I can see, there is no use case for <code>module avail</code>.</p> <p><code>module avail</code> list all modules immediately available, or search for a specific available module:</p> <ul> <li><code>module avail</code></li> <li><code>module avail *tool*</code></li> </ul> <p>This command is not so smart, though, especially when searching for a specific tool, or a bioinformatics tool. It only reports modules that are immediately available.</p> <pre><code>module avail R\n</code></pre> <p>outputs everything that has an <code>r</code> in the name... not useful.</p> <pre><code>$ module avail samtools\nNo module(s) or extension(s) found!\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/modules/#conflicting-modules","title":"Conflicting modules","text":"<p>Sometimes some tools cannot be run together, that is working when another module is loaded. Read about this in the page:</p> <ul> <li>Conflicting modules</li> </ul>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/mount_wharf/","title":"Mount a Bianca project","text":""},{"location":"cluster_guides/mount_wharf/#mount-a-bianca-project","title":"Mount a Bianca project","text":"<p>On transit, mount the wharf of your Bianca project:</p> <pre><code>mount_wharf [project_id]\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> </ul> What about the <code>[path]</code> argument? <p>Well spotted!</p> <p>Indeed, the Transit server gives these arguments:</p> <pre><code>mount_wharf [project_id] [path]\n</code></pre> <p>However, the <code>[path]</code> argument is optional: if not given, a default will be used.</p> <p>To simplify matters, here we use the default.</p> <p>for example:</p> <pre><code>mount_wharf sens2016001\n</code></pre> <p>The password is your normal UPPMAX password directly followed by the six digits from the the <code>UPPMAX</code> 2-factor authentication. For example, if your password is <code>VerySecret</code> and the second factor code is <code>123456</code> you would type <code>VerySecret123456</code> as the password in this step.</p> <p>Now a folder called <code>sens2016001</code> is created.</p>"},{"location":"cluster_guides/nodes_own_disk/","title":"How to use the nodes' own disk","text":"","tags":["node","disk","disc","SNIC_TMP","scratch"]},{"location":"cluster_guides/nodes_own_disk/#how-to-use-the-nodes-own-disk","title":"How to use the nodes' own disk","text":"","tags":["node","disk","disc","SNIC_TMP","scratch"]},{"location":"cluster_guides/nodes_own_disk/#short-version","title":"Short version","text":"<p>When possible, copy the files you want to use in the analysis to $SNIC_TMP at the start of the job, and store all output there as well. The last thing you do in the job is to copy the files you want to keep back to your project directory.</p>","tags":["node","disk","disc","SNIC_TMP","scratch"]},{"location":"cluster_guides/nodes_own_disk/#long-version","title":"Long version","text":"<p>Parallel network file systems are very fast when accessed from many nodes, but can nevertheless become a bottleneck. For instance, if many jobs on a single are doing many file operations, all those jobs may be fighting each other and degrading performance. Additionally, the metadata server on these kinds of file systems can be overburdened if very large numbers of files are created and/or removed.</p> <p>For this reason, jobs that perform a lot of file accesses, especially on temporary files, should use the compute node's local hard drive. If you do, then any slow-down due to file I/O is limited to the node(s) on which these jobs are running.</p> <p>The hard drive of the node is located at <code>/scratch</code>, and each job that runs on a node gets a folder created automatically with the same name as the jobid, <code>/scratch/&lt;jobid&gt;</code>. This folder name is also stored in the environment variable <code>$SNIC_TMP</code> for ease of use. The idea is that you copy files that you will be reading randomly, such as indices and databases but not files of reads, to <code>$SNIC_TMP</code> first thing in the job. Files that you read as a stream from beginning to end, like files of reads, should remain in project storage and read from there. You then run your analysis and have all the output files written to <code>$SNIC_TMP</code> as well. After the analysis is done, you copy back all the output files you want to keep to your project storage folder. Everything in <code>/scratch/&lt;jobid&gt;</code> will be deleted as soon as the job is finished, and you have no hope of recovering it after the job is completed.</p> <p>An example would be a script that runs bwa to align read. Usually they look something like this:</p> <pre><code>#!/bin/bash -l\n#SBATCH -A snic2022-X-YYY\n#SBATCH -t 12:00:00\n#SBATCH -p core\n#SBATCH -n 20\n\n# load modules\nmodule load bioinfo-tools bwa/0.7.17 samtools/1.14\n\n# run the alignment and convert its output directly to\n# a sorted bam format\nbwa mem -t 16 /proj/snic2022-X-YYY/nobackup/ref/hg19.fa /proj/snic2022-X-YYY/rawdata/sample.fq.gz | samtools sort -@ 4 -M 10G -O bam - &gt; /proj/snic2022-X-YYY/nobackup/results/sample.bam\n</code></pre> <p>The steps to be added are (1) copy the index to $SNIC_TMP but not the reads; (2) adjust your script to read read the index from $SNIC_TMP; and (3) copy the results back to project storage once the alignment is done.</p> <pre><code>#!/bin/bash\n#SBATCH -A snic2022-X-YYY\n#SBATCH -t 12:00:00\n#SBATCH -p core\n#SBATCH -n 20\n\n# load modules\nmodule load bioinfo-tools bwa/0.7.17 samtools/1.14\n\n# copy the index files used in the analysis to $SNIC_TMP\ncp /proj/snic2022-X-YYY/nobackup/ref/hg19.fa* $SNIC_TMP/\n\n# go to the $SNIC_TMP folder to make sure any temporary files\n# are created there as well\ncd $SNIC_TMP\n\n# run the alignment using the index in $SNIC_TMP and the reads\n# from project storage. write the sorted BAM containing\n# alignments directly to $SNIC_TMP. Use 16 threads for\n# alignment and 4 threads for sorting and compression, and\n# 20GB RAM for sorting. These values are appropriate for a\n# full standard rackham node.\nbwa mem -t 16 $SNIC_TMP/hg19.fa /proj/snic2022-X-YYY/rawdata/sample.fq.gz | samtools sort -@ 4 -m 20G -O bam - &gt; $SNIC_TMP/sample.bam\n\n# copy the results back to the network file system\ncp $SNIC_TMP/sample.bam /proj/snic2022-X-YYY/nobackup/results/\n</code></pre> <p>It's not harder than that. This way, the index files are copied to $SNIC_TMP in a single operation, which is much less straining for the file system than small random read/writes. The network filesystem is used when gathering reads for alignment, and streaming reads are easy for that filesystem. When the alignment is finished the results is copied back to project directory so that it can be used in other analysis.</p> <p>One problem that can happen is if your files and the results are too large for the node's hard drive. The drive is 2TiB on Rackham and 4TiB on Bianca, so it is unusual for the hard drive to be too small for the results of such analyses. If you run into this problem, please email UPPMAX at <code>support@uppmax.uu.se</code> and we can look into the problem.</p>","tags":["node","disk","disc","SNIC_TMP","scratch"]},{"location":"cluster_guides/optimizing_jobs/","title":"Checking and optimizing jobs","text":""},{"location":"cluster_guides/optimizing_jobs/#optimizing-jobs","title":"Optimizing jobs","text":"<p>The UPPMAX clusters use the Slurm job scheduler. However, a job may not have run optimally, i.e. reserving CPU power and/or memory that is not used.</p> <p>This page describes how to optimize your Slurm jobs.</p>"},{"location":"cluster_guides/optimizing_jobs/#commands","title":"Commands","text":"<p>You will probably have good use of the following commands:</p> Command Description finishedjobinfo information about finished jobs jobinfo telling you about running and waiting jobs jobstats see CPU and memory use of finished job in a plot projinfo telling you about the CPU hour usage of your projects projmembers telling you about project memberships projsummary [project id] summarizes some useful information about projects uquota telling you about your file system usage Working on Snowy? Use -M snowy <p>For Slurm commands and for commands like projinfo, jobinfo and finishedjobinfo, you may use the <code>-M</code> flag to ask for the answer to be given for a system that you are not logged in to. For example, when logged into Rackham, you may ask about information about current core hour usage on Snowy, with the command <code>projinfo -M Snowy</code></p>"},{"location":"cluster_guides/optimizing_jobs/#check-you-storage-with-uquota","title":"Check you storage with <code>uquota</code>","text":""},{"location":"cluster_guides/optimizing_jobs/#check-your-cpu-hour-usage-with-projinfo","title":"Check your CPU hour usage with <code>projinfo</code>","text":""},{"location":"cluster_guides/pelle/","title":"Pelle","text":"","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle/#pelle","title":"Pelle","text":"Why such a bad image? <p>Copyright. This is one of the few images that shows Pelle Svansl\u00f6s with a Creative Commons license.</p> <p>Pelle is an upcoming general-purpose UPPMAX cluster, paid by Uppsala University.</p> <p>Uppsala users of Rackham will be moved to Pelle by UPPMAX after applying to a Pelle project.</p> <p>Non-Uppsala users of Rackham can move their data to Dardel, see the Rackham to Dardel migration guide.</p> <p>What is the status of Pelle?</p> <p>See the Pelle status page</p>","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle/#how-to-apply-to-a-pelle-project","title":"How to apply to a Pelle project","text":"<p>See how to apply to a Pelle project.</p>","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle/#how-to-log-in-to-a-pelle","title":"How to log in to a Pelle","text":"<p>See how to log in to a Pelle.</p>","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle/#features-of-pelle-compared-to-rackham","title":"Features of Pelle, compared to Rackham","text":"<p>Although we from UPPMAX cannot say exactly how Pelle will look, we try to have Pelle be as similar to Rackham as possible.</p>","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle/#migration-from-rackham-to-pelle","title":"Migration from Rackham to Pelle","text":"<p>As both clusters are UPPMAX clusters, we will transfer your data from Rackham to Pelle. Users will have to apply to a Pelle project.</p>","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle/#pelle-hardware","title":"Pelle hardware","text":"<p>Pelle/Maja hardware</p>","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle/#information-for-test-pilots","title":"Information for test pilots","text":"<p>We have a separate page with information for Pelle test pilot users</p>","tags":["Pelle","cluster","general-purpose"]},{"location":"cluster_guides/pelle_file_transfer_using_gui/","title":"File transfer to/from Pelle using a graphical tool","text":"","tags":["Pelle","File transfer","Graphical tool","Visual tool"]},{"location":"cluster_guides/pelle_file_transfer_using_gui/#file-transfer-tofrom-pelle-using-a-graphical-tool","title":"File transfer to/from Pelle using a graphical tool","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>Data transfer to/from Pelle using a graphical tool is one of the ways to transfer files to/from Pelle</p> What are the other ways? <p>See other ways to transfer data to/from Pelle</p> <p>There are many graphical tools that can do this:</p> <ul> <li>File transfer to/from Pelle using FileZilla</li> </ul> <p><code>TODO ![FileZilla connected to Pelle](../software/img/filezilla_login_to_pelle_480_x_270.png)</code></p>","tags":["Pelle","File transfer","Graphical tool","Visual tool"]},{"location":"cluster_guides/pelle_file_transfer_using_transit/","title":"Data transfer to/from Pelle using Transit","text":""},{"location":"cluster_guides/pelle_file_transfer_using_transit/#data-transfer-tofrom-pelle-using-transit","title":"Data transfer to/from Pelle using Transit","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>Data transfer to/from Pelle using Transit is one of the ways ways to transfer files to/from Pelle</p> What is Transit? <p>See the page about the UPPMAX Transit server.</p> What are the other ways? <p>See the other ways to transfer data to/from Pelle</p> <p>This page assumes your files are 'posted' to Transit. Transit is a service, not a file server.</p> How to transfer files to/from Transit? <p>See how to transfer files to/from Transit</p> <p>To transfer files between Pelle and Transit can be done in multiple ways too:</p> <ul> <li>Using SCP</li> <li>Using SFTP</li> </ul>"},{"location":"cluster_guides/pelle_file_transfer_using_transit/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[SUNET]\n      direction LR\n      user(User)\n      subgraph sub_transit_env[Transit]\n        transit_login(Transit login):::calculation_node\n        files_on_transit(Files posted to Transit):::transit_node\n      end\n      subgraph sub_pelle_shared_env[Pelle]\n          files_in_pelle_home(Files in Pelle home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#ccc,color:#000,stroke:#000\n    style sub_transit_env fill:#cfc,color:#000,stroke:#000\n    style sub_pelle_shared_env fill:#fcc,color:#000,stroke:#000\n\n    user --&gt; |logs in |transit_login\n\n    transit_login --&gt; |can use|files_on_transit\n    %% user_local_files &lt;--&gt; |graphical tool|files_in_pelle_home\n    %% user_local_files &lt;--&gt; |SCP|files_in_pelle_home\n    files_on_transit &lt;==&gt; |transfer|files_in_pelle_home</code></pre> <p>Overview of file transfer on Pelle The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>"},{"location":"cluster_guides/pelle_fingerprints/","title":"Pelle fingerprints","text":"","tags":["Pelle","fingerprint","SHA256"]},{"location":"cluster_guides/pelle_fingerprints/#pelle-fingerprints","title":"Pelle fingerprints","text":"<p>Below are the valid fingerprints on Pelle. If you see these, you can assume you are actually connecting to Pelle.</p> <pre><code>3072 SHA256:Q4yuOETwO9AFwwS5bT8w28nQLtRxEV0KRtM513Uy418 pelle.uppmax.uu.se (RSA)\n256 SHA256:1ooqfYQ4sFTxFlk4WxYWb4pUb178JOTKsaX9dpWruHU pelle.uppmax.uu.se (ECDSA)\n256 SHA256:/mb9IaG8GQnBKyA1VffHed7t6ijNA6xUTjcachn95CY pelle.uppmax.uu.se (ED25519)\n</code></pre>","tags":["Pelle","fingerprint","SHA256"]},{"location":"cluster_guides/pelle_modules/","title":"Working with environment modules on Pelle","text":"","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_modules/#working-with-environment-modules-on-pelle","title":"Working with environment modules on Pelle","text":"<p>Here we show how to use the environment module system on Pelle</p> <p>After describing the background/reasoning why such a system is needed, we show how to work with the module system.</p> <p>There is a table of commonly used shorthand syntaxes, as well as links to almost all installed software and databases on UPPMAX.</p>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_modules/#background","title":"Background","text":"<p>Pelle is, like the other clusters Rackham and Bianca, a shared Linux computer with all the standard Linux tools installed, on which all users should be able to do their work independently and undisturbed.</p> <p>To ensure this, users cannot modify, upgrade or uninstall research software themselves and instead an environment module system (from now on: 'module system') is used. This allow users to independently use their favorite versions of their favorite software.</p> <p>Using a module system keeps installed software hidden by default, and users have to explicitly tell their terminal which version of which software they need.</p> <p>To have new software installed on an UPPMAX cluster, users must explicitly request a version of a piece of software. As of today, there are nearly 800+ programs and packages, with multiple versions available on all UPPMAX clusters. Using explicit versions of software is easy to do and improves the reproducibility of the scripts written.</p> <p>Info</p> <ul> <li>Pelle is set up with completely new research software installations compared to Rackham (and Bianca).</li> <li>The Module tree therefore looks differently.</li> <li>The installation directories also have another structure compared to Rackham</li> <li>There are fewer software and versions of them comapred to Rackham/Bianca</li> <li>Ask support and we installe specific software and version on demand</li> </ul> <p>Warning</p> <p>There is no bioinfo-tools module to load. Instead all software is findable directly.</p>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_modules/#working-with-the-module-system","title":"Working with the module system","text":"<p>Info</p> Command Description <code>module avail</code> Search for a module that is available <code>module spider</code> Search for a module <code>module spider [module]</code> Get info about a module, e.g. <code>module spider cowsay</code> <code>module load [module]</code> Load a module, e.g. <code>module load cowsay</code> <code>module load [module]/[version]</code> Load a module of a specific versions, e.g. <code>module load cowsay/3.03</code> <code>module list</code> List all activated modules <code>module help</code> Show the help for a module <code>module unload [module]</code> Unload the module <code>[module]</code>, e.g. <code>module unload cowsay</code> What is the difference between <code>module spider</code> and <code>module avail</code>? <ul> <li><code>module spider</code>: search for an installed module,   also those that are hidden</li> <li><code>module avail</code>: search for a module that is available to load</li> </ul> <p>Working with the module system means:</p> <ul> <li>searching for a module</li> <li>activating ('loading') a module</li> <li>deactivate ('unloading') a module</li> </ul> <p>This section describes these steps in more details.</p> <p>The <code>module</code> command is the basic interface to the module system.</p> <p>To search for a module, use <code>module spider [module]</code>, for example <code>module spider cowsay</code>.</p> Would you like to see a video instead? <p>Will come</p> What is <code>cowsay</code>? <p>See the UPPMAX page on <code>cowsay</code></p> What is <code>R</code>? <p><code>R</code> is the module for the R programming language. R is a free and open-source programming language, commonly used in data analysis and visualization.</p> How does the output of <code>module spider R</code> look like? <pre><code>ml spider R\n\n-------------------------------------------------------------------------------------------------------------------------\n  R: R/4.4.2-gfbf-2024a\n-------------------------------------------------------------------------------------------------------------------------\n    Description:\n      R is a free software environment for statistical computing and graphics.\n\n\n     Other possible modules matches:\n        ADMIXTURE, AdapterRemoval, Armadillo, Arrow, BioPerl, Brotli, Brunsli, CellRanger, Cereal, Exonerate, ...\n\n    This module can be loaded directly: module load R/4.4.2-gfbf-2024a\n\n    This module provides the following extensions:\n\n       askpass/1.2.1 (E), base (E), base64enc/0.1-3 (E), brew/1.0-10 (E), brio/1.1.\n5 (E), bslib/0.8.0 (E), cachem/1.1.0 (E), callr/3.7.6 (E), cli/3.6.3 (E), clipr/\n0.8.0 (E), commonmark/1.9.2 (E), compiler (E), cpp11/0.5.0 (E), crayon/1.5.3 (E),\ncredentials/2.0.2 (E), curl/6.0.1 (E), datasets (E), desc/1.4.3 (E), devtools/2.4.5\n(E), diffobj/0.3.5 (E), digest/0.6.37 (E), downlit/0.4.4 (E), ellipsis/0.3.2 (E), e\n\n...\n\nHelp:\n\n  Description\n  ===========\n  R is a free software environment for statistical computing\n   and graphics.\n\n\n  More information\n  ================\n   - Homepage: https://www.r-project.org/\n\n  ...\n\n-------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*R.*'\n</code></pre> What is <code>samtools</code>? <p><code>samtools</code> is the module for SAMtools. From wikipedia:</p> <p>SAMtools is a set of utilities for interacting with and post-processing short DNA sequence read alignments in the SAM (Sequence Alignment/Map), BAM (Binary Alignment/Map) and CRAM formats</p> How does the output of <code>module spider samtools</code> look like? <pre><code>$ module spider samtools\n\n-------------------------------------------------------------------------------------------------------------------------\n Rsamtools: Rsamtools/2.22.0 (E)\n-------------------------------------------------------------------------------------------------------------------------\n   This extension is provided by the following modules. To access the extension you must load one of the following modules.\nNote that any module names in parentheses show the module location in the software hierarchy.\n\n\n      R-bundle-Bioconductor/3.20-foss-2024a-R-4.4.2\n\n\nNames marked by a trailing (E) are extensions provided by another module.\n\n-------------------------------------------------------------------------------------------------------------------------\n SAMtools:\n-------------------------------------------------------------------------------------------------------------------------\n   Description:\n     SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging,\n     indexing and generating alignments in a per-position format.\n\n    Versions:\n       SAMtools/0.1.20-GCC-13.3.0\n       SAMtools/1.3.1-GCC-13.3.0\n       SAMtools/1.13-GCC-13.3.0\n       SAMtools/1.19.2-GCC-13.3.0\n       SAMtools/1.21-GCC-13.3.0\n       SAMtools/1.22-GCC-13.3.0\n\n-------------------------------------------------------------------------------------------------------------------------\n For detailed information about a specific \"SAMtools\" package (including how to load the modules) use the module's full name.\n Note that names that have a trailing (E) are extensions provided by other modules.\n For example:\n\n    $ module spider SAMtools/1.22-GCC-13.3.0\n------------------------------------------------------------------------------------------------------------------------- \n</code></pre> How does the output of <code>module spider samtools/1.22</code> look like? <pre><code>$ module spider samtools/1.22\n\n-------------------------------------------------------------------------------------------------------------------------\n  SAMtools: SAMtools/1.22-GCC-13.3.0\n-------------------------------------------------------------------------------------------------------------------------\n    Description:\n      SAM Tools provide various utilities for manipulating alignments in the SAM format, including sorting, merging,\n      indexing and generating alignments in a per-position format.\n\n\n    This module can be loaded directly: module load SAMtools/1.22-GCC-13.3.0\n\n...\n</code></pre> <p>If there is an exact match, that module is reported first. Of the module shown, also the different versions are reported.</p> <p>Note</p> <ul> <li>Many module names are slightly different on Pelle compared to the other clusters.</li> <li>Many tools are in CAPITAL letters or at least start with a Capital.</li> </ul> <p>You are not supposed to load a <code>module load bioinfo-tools</code> first</p> <p><code>bioinfo-tools</code> module is not required on Pelle. It is not even there so remove that line in you old scripts</p> What to do when <code>module load</code> gives an 'Lmod has detected the following error:  These module(s) or extension(s) exist but cannot be loaded as requested' error? <p>Ouch, now it is time to try out many things.</p> <p>Do not hesitate to contact support so that you can spend time on your research and we figure this out :-)</p> <p>To load a module, use <code>module load [module]</code>, for example <code>module load cowsay</code>. This will load the default version of that module, which is almost always the latest version. Loading a module always results in a helpful message (such as that it worked fine), however, it is not general help for using the tool itself.</p> How can I see which modules I've loaded? <p>Use the command <code>module list</code>.</p> How does the output of <code>module list</code> look like? <p>This is an example of how is looks lik when SAMtols is loaded</p> <pre><code>$ module list\n\nCurrently Loaded Modules:\n  1) GCCcore/13.3.0                 5) bzip2/1.0.8-GCCcore-13.3.0       9) Java/17 -&gt; Java/17.0.15  \n  2) zlib/1.3.1-GCCcore-13.3.0      6) ncurses/6.5-GCCcore-13.3.0      10) OpenSSL/3\n  3) binutils/2.42-GCCcore-13.3.0   7) XZ/5.4.5-GCCcore-13.3.0         11) cURL/8.7.1-GCCcore-13.3.0\n  4) GCC/13.3.0                     8) libdeflate/1.20-GCCcore-13.3.0  12) SAMtools/1.22-GCC-13.3.0\n</code></pre> <p>In this example case, we can see that the many more modules than SAMtools are loaded. This is the effect of using the installation method that is prevailent in most NAISS clusters.</p> <p>Getting help on a module</p> <p>Run <code>module help [module]</code>, e.g. <code>module help cowsay</code> to get the general help on a module</p> <p>For reproducible research, however, it is good practice to load a specific version. The information given by <code>module spider</code> contains the versions of the module. For example, to load the <code>SAMtools/1.22</code> module, do <code>module load samtools/1.22&lt;TAB&gt;</code>.</p> How does the output of <code>module load GATK/4.3.0.0</code> look like? <pre><code>$ module load GATK/4.3.0.0\nNote that all versions of GATK starting with 4.0.8.0 use a different wrapper\nscript (gatk) than previous versions of GATK.  You might need to update your\njobs accordingly.\n\nThe complete GATK resource bundle is in /sw/data/GATK\n\nSee 'module help GATK/4.3.0.0' for information on activating the GATK Conda\nenvironment for using DetermineGermlineContigPloidy and similar other tools.\n</code></pre> <p>This message references the command <code>module help GATK/4.3.0.0</code> for additional help with this module.</p> <p>To see a module-specific help, use <code>module help [module]</code> (e.g. <code>module help cowsay</code>).</p> How does the output of <code>module help GATK/4.3.0.0</code> look like? <pre><code>$ module help GATK/4.6.1.0-GCCcore-13.3.0-Java-17\n\n------------------------------ Module Specific Help for \"GATK/4.6.1.0-GCCcore-13.3.0-Java-17\" ------------------------------\n\nDescription\n===========\nThe Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute\n to analyse next-generation resequencing data. The toolkit offers a wide variety of tools,\n with a primary focus on variant discovery and genotyping as well as strong emphasis on\n data quality assurance. Its robust architecture, powerful processing engine and\n high-performance computing features make it capable of taking on projects of any size.\n\n\nMore information\n================\n - Homepage: https://www.broadinstitute.org/gatk/\n</code></pre> <p>To unload a module, do <code>module unload [module]</code> (e.g. <code>module unload cowsay</code>). This will also unload module that depend on the unloaded one. For example, <code>module unload GATK</code> will unload GCCcore, Java, Python etcetera.</p>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_modules/#using-modules-in-an-executable-script","title":"Using modules in an executable script","text":"<p>Using modules in an executable script is straightforward: just load the module needed before using the software in that module.</p> <p>For example, this is a valid bash script:</p> <pre><code>#!/bin/bash\nmodule load cowsay/3.04\ncowsay hello\n</code></pre>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_modules/#common-shorthand-syntaxes","title":"Common shorthand syntaxes","text":"Full command Shorthand syntax <code>module</code> - <code>module avail</code> <code>ml av</code> <code>module spider</code> <code>ml spider</code> <code>module load [module]</code> <code>ml [module]</code> <code>module list</code> <code>ml</code> <code>module unload [module]</code> <code>ml -[module]</code>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_modules/#links","title":"Links","text":"<ul> <li>Almost all installed software on UPPMAX</li> <li>Almost all installed databases on UPPMAX</li> <li>Wikipedia page on environment modules</li> <li>lmod homepage</li> </ul>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_modules/#conflicting-modules","title":"Conflicting modules","text":"<p>Sometimes some tools cannot be run together, that is working when another module is loaded. Read about this in the page:</p> <ul> <li>Conflicting modules</li> </ul>","tags":["module","modules","software module","software modules","lmod"]},{"location":"cluster_guides/pelle_test/","title":"Pelle test pilot guide","text":"","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#pelle-test-pilot-guide","title":"Pelle test pilot guide","text":"<p>This page contains information and instructions for test pilot users of Pelle.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#state-of-the-system","title":"State of the system","text":"<ul> <li>Not everything is finished</li> <li>Some things are broken and/or does not work as expected</li> <li>Not all configuration changes are final</li> </ul> <p>We reserve the right to restart login nodes, compute nodes, kill running jobs and similar with little to no warning. We will avoid this when possible, but if necessary our continuing work on getting the cluster ready have higher priority than the test usage.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#provide-feedback","title":"Provide feedback","text":"<p>Getting information from you about what works and what does not work is an important part of the test pilot usage. This will help us find issues and to prioritise our work. Please send feedback to support@uppmax.uu.se.</p> <p>Some ideas of things we would like to know about:</p> <ul> <li> <p>You find something that is not working. But please check if it   already mentioned as a known issue on this page first.</p> </li> <li> <p>You find something confusing or weird</p> </li> <li> <p>Performance issues</p> </li> <li> <p>Software not working</p> </li> <li> <p>Missing software</p> </li> <li> <p>Something that can be improved</p> </li> <li> <p>Things that are working great</p> </li> </ul>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#log-in","title":"Log in","text":"<p>To log in to Pelle two-factor authentication using TOTP is required.</p> <p>Follow our guide to setup UPPMAX two factor authentication if you don't already have this.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#new-operating-system","title":"New operating system","text":"<p>Pelle is running Rocky Linux 9. Most system software, including the Linux kernel, have newer versions compared to Rackham and other UPPMAX clusters.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#hardware","title":"Hardware","text":"<p>Pelle's compute nodes have much more CPU cores and memory compared to previous UPPMAX cluster. All compute nodes have AMD Zen4 processors.</p> <p>Details about the hardware is on the Pelle/Maja hardware page.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#software-installations","title":"Software installations","text":"<p>Software available on Pelle is not included in the Software list on this web page yet. Please use the module system to list available software, for example by running <code>module avail</code> or <code>module spider</code>.</p> <p>We are working on installing more software packages.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#slurm","title":"Slurm","text":"<p>Slurm on Pelle have been upgraded to version 25.05.</p> <p>Several UPPMAX-specific Slurm changes from previous clusters have been removed, to make the config use more Slurm defaults. This makes the system easier to maintain and will behave more similar to clusters at other sites. Unfortunately this means that some extra changes to job scripts can be needed when moving from Rackham/Snowy.</p> <p>Our Slurm documentation have not yet been updated with the changes on Pelle.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#job-priority","title":"Job priority","text":"<p>Slurm's default fair-share priority are used for the job queue.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#48-hour-max-job-time-limit","title":"48 hour max job time limit","text":"<p>To prevent jobs occupying nodes for too long during the test period we have reduced the max timelimit for jobs to 48 hours.</p> <p>This is only during the pilot test period, after this the allowed time limit will be increased.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#no-nodecore-partitions","title":"No node/core partitions","text":"<p>On Rackham you have used <code>-p node</code> or <code>-p core</code> to specify node/core jobs. This is not used on Pelle. Instead Slurm's standard options is used to specify the job requirements.</p> <p>Example to request 2 full nodes: <code>--nodes=2 --exclusive</code></p> <p>One task, using two threads: <code>--ntasks=1 --cpus-per-task=2</code></p> <p>Two tasks, using one thread each: <code>--ntasks=2 --cpus-per-task=2</code></p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#no-devel-partition","title":"No devel partition","text":"<p>A way to get high priority for short devel jobs will be added later.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#job-memory-specification","title":"Job memory specification","text":"<p>Currently you do not have to request additional CPUs to get additional memory. You can use all Slurm options <code>--mem</code>, <code>--mem-per-cpu</code>, <code>--mem-per-gpu</code> to specify memory requirements.</p> <p>This might be changed later, the configuration is not final.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#23-tib-memory","title":"2/3 TiB memory","text":"<p>Pelle have two fat nodes. One with 2 TiB of memory and one with 3 TiB.</p> <p>Jobs on these nodes always allocate the entire node with all cores.</p> <p>To use the 2 TiB node: <code>sbatch --partition=fat --constraint=2TB</code></p> <p>To use the 3 TiB node: <code>sbatch --partition=fat --constraint=3TB</code></p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#interactive","title":"interactive","text":"<p>The script <code>interactive</code> used to submit interactive jobs still works, but does not provide as much information as on Rackham. It is now a very thin wrapper around the <code>salloc</code> command.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#smt","title":"SMT","text":"<p>The compute node CPUs have Simultaneous multithreading (SMT) enabled. Each CPU core runs two Threads. In Slurm the Threads are referred to as CPUs.</p> <p>Different jobs are never allocated to the same CPU core. The smallest possible job always gets one Core with two Threads (CPUs).</p> <p>Jobs requesting multiple tasks or cpus gets threads by default.</p> <p>Some examples:</p> <ul> <li><code>--ntasks=2</code> - one core, two threads</li> <li><code>--ntasks=1 --cpus-per-task=4</code> - two cores, four threads</li> <li><code>--ntasks=2 --cpus-per-task=3</code> - three cores, six threads.</li> </ul>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#one-thread-per-core-to-avoid-smt","title":"One thread per core to avoid SMT","text":"<p>If you suspect SMT degrades the performance of your jobs, you can you can specify <code>--threads-per-core=1</code> in your job.</p> <p>Same examples as before but with <code>--threads-per-core=1</code>:</p> <ul> <li><code>--ntasks=2 --threads-per-core=1</code> - two cores, (4 threads, 2 used)</li> <li><code>--ntasks=1 --cpus-per-task=4 --threads-per-core=1</code> - 4 cores (8 threads, 4 unused)</li> <li><code>--ntasks=2 --cpus-per-task=3 --threads-per-core=1</code> - 6 cores (12 threads, 6 unused)</li> </ul> <p>When doing this you should launch your tasks using <code>srun</code> to ensure your processes gets pinned to the correct CPUs (threads), one per core.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#other-changes","title":"Other changes","text":"","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#per-user-temporary-directories-on-login-nodes","title":"per-user temporary directories on login nodes","text":"<p>Each user have their own temporary directories (<code>/tmp</code>, <code>/var/tmp</code>, <code>/scratch</code>, <code>/dev/shm</code>) on the login nodes. These directories are only intended for short term storage of files during interactive work and will be purged regularly. You cannot use these directories to share files with other users, please use the project directories instead.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#memory-limits-on-login-nodes","title":"Memory-limits on login nodes","text":"<p>Each user can use no more than 128 GiB memory on the login nodes. This is 25% of the available 512 GiB and we typically have more than four logged in users so the memory can still run out.</p> <p>If possible please run applications that use large amounts of memory in an interactive session on compute nodes.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#known-issues","title":"Known issues","text":"","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#missing-tools","title":"Missing tools","text":"<p>Some UPPMAX specific tools are not yet working on Pelle.</p> <ul> <li><code>uquota</code> - works.</li> <li><code>projinfo</code> - not implemented yet.</li> <li><code>finishedjobinfo</code> - use <code>sacct</code> instead</li> <li><code>jobinfo</code> - use tools like <code>squeue</code> and <code>sprio</code> instead</li> <li><code>jobstats</code> - not working on Pelle. Might be replaced by an alternative solution.</li> </ul>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#core-jobs-can-get-allocated-resources-across-several-compute-nodes","title":"Core jobs can get allocated resources across several compute nodes","text":"<p>A job requesting multiple tasks can get resources allocated on multiple nodes. For example 3 tasks <code>sbatch --ntasks=3</code> might get allocated three CPUs on three different compute nodes.</p> <p>We will update the configuration to avoid this. For not the problem can be avoided by specifying <code>--nodes=1</code> when submitting jobs.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#devshm-size","title":"/dev/shm size","text":"<p>The size of /dev/shm on compute nodes is 50% of the nodes memory. This is the Linux default. We plan to increase the size to closer to 100% as some applications needs to store larger files there.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#apptainer-missing-bind-mounts","title":"Apptainer missing bind mounts","text":"<p>Apptainer on Pelle is missing configuration to bind file systems like project directories, and node local scratch into the running container. We will fix the configuration, but until then you can use the <code>--bind</code> option manually.</p>","tags":["Pelle"]},{"location":"cluster_guides/pelle_test/#missing-projects-fixed","title":"Missing projects [FIXED]","text":"<p>FIXED - All uppmax2025 projects with a Pelle allocation is now available for use.</p> <p>Only the test pilot project (uppmax2025-2-325) have been enabled on the cluster. Other uppmax2025-projects cannot be used to run jobs yet.</p> <p>Use the test pilot project for your test pilot jobs.</p>","tags":["Pelle"]},{"location":"cluster_guides/project_management/","title":"Managing your project","text":""},{"location":"cluster_guides/project_management/#manage-you-projects","title":"Manage you projects","text":""},{"location":"cluster_guides/project_management/#check-the-cpu-hours-of-your-projects","title":"Check the CPU hours of your project(s)","text":""},{"location":"cluster_guides/project_management/#on-an-uppmax-cluster","title":"On an UPPMAX cluster","text":"<p>To get an overview of how much of your project allocation that has been used, please use the <code>projinfo</code> command. Please use the command <code>projinfo -h</code> to get details on usage.</p> <pre><code>Usage: projinfo [-OPTIONS [-MORE_OPTIONS]] [--] [PROGRAM_ARG1 ...]\n\nThe following single-character options are accepted:\n        With arguments: -s -e -M\n        Boolean (without arguments): -h -q -v -m -y\n</code></pre> <ul> <li> <p>With no flags given, projinfo will tell you your usage during the current month.</p> </li> <li> <p>Usage in project \"testproj\" during the current year: <code>projinfo -y testproj</code></p> </li> <li> <p>Usage in project testproj during the specified two months: <code>projinfo -s 2010-02 -e 2010-03 testproj</code></p> </li> </ul> <p>Usage in your projects today until the moment you run the command: <code>projinfo -s today</code></p>"},{"location":"cluster_guides/project_management/#supr","title":"SUPR","text":"<p>Log in to SUPR and view your projects there. You can also get information about usage levels, storage levels, membership, etc.</p>"},{"location":"cluster_guides/project_management/#storage","title":"Storage","text":"<ul> <li>Disk storage guide</li> <li>Backup</li> </ul>"},{"location":"cluster_guides/project_management/#display-storage-quota","title":"Display storage quota","text":"<ul> <li>Display your project quota with the command uquota:</li> </ul> <pre><code>uquota\n</code></pre> <ul> <li>Display the disk quota on the more detailed level</li> </ul>"},{"location":"cluster_guides/project_management/#other","title":"Other","text":"<p>How do I specify that I do not need my large datasets to be backed up?</p> How do I specify that I do not need my large datasets to be backed up? <p>If you create a folder named <code>nobackup</code>, inside any folder, then all data stored inside this folder will not be backed-up.</p> <p>Simply move (<code>mv</code> or <code>rsync</code>) your data into a folder with the proper name.</p> <p>Also note that all projects  have a separate nobackup folder under the <code>/proj/xyz/</code> hierarchy (and also under the <code>/proj/xyz/private/</code> hierarchy) with a separate quota limit than the ordinary backed up project folder. You can read more about this in our disk storage guide.</p> What is this 'glob' folder in my home folder? <ul> <li>The glob directory found in your home has been deprecated since early 2017.</li> <li>It is now a normal directory and shared your default 32GByte sized home.</li> <li> <p>The glob directory remains to not interfere with scripts who might reference ~/glob in the source code.</p> </li> <li> <p>Historically, the glob directory was the main storage area for storage of user data.</p> <ul> <li>It was shared by all nodes.</li> <li>The directory was used for files needed by all job instances and could house files exceeding the quota of the home directory.</li> <li>Job input and output files was (and can still be) stored here.</li> </ul> </li> </ul>"},{"location":"cluster_guides/project_management/#members","title":"Members","text":"<p>Check the current project members with:</p> <pre><code>projmembers &lt;project-name&gt;\n</code></pre> <p>If you want to check which members that presently belong to a certain (Linux) group you do:</p> <pre><code>getent groups &lt;project name&gt;\n</code></pre> <ul> <li>You can also check in SUPR - Swedish User and Project Repository</li> </ul>"},{"location":"cluster_guides/rackham/","title":"Rackham","text":"","tags":["Rackham","cluster","general-purpose"]},{"location":"cluster_guides/rackham/#rackham","title":"Rackham","text":"<p>Rackham is one of the UPPMAX clusters that is a general-purpose cluster.</p> <p>Consider migrating to Dardel already</p> <p>The Rackham cluster will be decommissioned at the end of 2024 so all projects have to migrate their data and calculations to other resources. The plan from NAISS is that all Rackham users will move to the Dardel cluster at PDC.</p> <p>See the page on file transfer to Dardel.</p> <p>In the near future, Rackham will be replaced by Pelle and will be only accessible to Uppsala researchers.</p> <ul> <li>Rackham's name</li> <li>Rackham's design</li> <li>Rackham's hardware</li> <li>Log in</li> <li>Starting an interactive session</li> <li>File transfer<ul> <li>using a graphical program</li> <li>using SCP</li> <li>using SFTP</li> </ul> </li> <li>The module system</li> <li>IDEs<ul> <li>Jupyter</li> <li>RStudio</li> <li>VSCode</li> <li> VSCodium</li> </ul> </li> <li>Isolated environments<ul> <li>venv</li> </ul> </li> <li>Run webexport</li> <li>Best practices</li> <li>Rackham installation guides</li> <li>Rackham workshops</li> </ul>","tags":["Rackham","cluster","general-purpose"]},{"location":"cluster_guides/rackham4/","title":"Rackham4 and Rocky9","text":""},{"location":"cluster_guides/rackham4/#rackham4-and-rocky9","title":"Rackham4 and Rocky9","text":"<p>Info</p> <ul> <li>UPPMAX are phasing out the old operating system CentOS, that has been installed on all UPPMAX clusters.</li> <li>In the future systems (Pelle and Maja) the Operating System Rocky9 will be used instead.</li> <li>The transition has started with the Login node <code>Rackham4</code></li> <li>We are also preparing for building software for several computer architectures; procedures to be used on the future systems.</li> <li>We invite \"power users\" to test their work here.</li> </ul> <p>Warning</p> <p>Please inform us of every bug you find, or about missing software.</p>"},{"location":"cluster_guides/rackham4/#about","title":"About","text":"<ul> <li>The Rocky9 OS works somewhat differently than CentOS and the same \"system files\" that we had before will not automatically be available directly under Rocky9.</li> <li>This will cause some of our earlier installed software to break.</li> <li>We hope that we have identified most of them and that you will not notice any changes.</li> </ul>"},{"location":"cluster_guides/rackham4/#modules","title":"Modules","text":"<ul> <li>We have added many modules built by the tool EasyBuild.</li> <li>Presently they are found in a separate tree.</li> <li>[FIX]</li> </ul>"},{"location":"cluster_guides/rackham4/#slurm","title":"Slurm","text":"<ul> <li>As of today you need to allocate compute nodes with Rocky9 installed with the flag <code>--res=rocky9</code>.<ul> <li>Not necessary any longer</li> </ul> </li> <li>Running interactive sessions only work with the <code>salloc</code> command.</li> <li>[FIX]</li> </ul>"},{"location":"cluster_guides/rackham_file_transfer_using_gui/","title":"File transfer to/from Rackham using a graphical tool","text":"","tags":["Rackham","File transfer","Graphical tool","Visual tool"]},{"location":"cluster_guides/rackham_file_transfer_using_gui/#file-transfer-tofrom-rackham-using-a-graphical-tool","title":"File transfer to/from Rackham using a graphical tool","text":"<p>Data transfer to/from Rackham using a graphical tool is one of the ways to transfer files to/from Rackham</p> What are the other ways? <p>See the other ways to transfer data to/from Rackham</p> <p>There are many graphical tools that can do this:</p> <ul> <li>File transfer to/from Rackham using FileZilla</li> </ul> <p></p>","tags":["Rackham","File transfer","Graphical tool","Visual tool"]},{"location":"cluster_guides/rackham_file_transfer_using_transit/","title":"Data transfer to/from Rackham using Transit","text":""},{"location":"cluster_guides/rackham_file_transfer_using_transit/#data-transfer-tofrom-rackham-using-transit","title":"Data transfer to/from Rackham using Transit","text":"<p>Data transfer to/from Rackham using Transit is one of the ways ways to transfer files to/from Rackham</p> What is Transit? <p>See the page about the UPPMAX Transit server.</p> What are the other ways? <p>See the other ways to transfer data to/from Rackham</p> <p>This page assumes your files are 'posted' to Transit. Transit is a service, not a file server.</p> How to transfer files to/from Transit? <p>See how to transfer files to/from Transit</p> <p>To transfer files between Rackham and Transit can be done in multiple ways too:</p> <ul> <li>Using SCP</li> <li>Using SFTP</li> </ul>"},{"location":"cluster_guides/rackham_file_transfer_using_transit/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[SUNET]\n      direction LR\n      user(User)\n      subgraph sub_transit_env[Transit]\n        transit_login(Transit login):::calculation_node\n        files_on_transit(Files posted to Transit):::transit_node\n      end\n      subgraph sub_rackham_shared_env[Rackham]\n          files_in_rackham_home(Files in Rackham home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#ccc,color:#000,stroke:#000\n    style sub_transit_env fill:#cfc,color:#000,stroke:#000\n    style sub_rackham_shared_env fill:#fcc,color:#000,stroke:#000\n\n    user --&gt; |logs in |transit_login\n\n    transit_login --&gt; |can use|files_on_transit\n    %% user_local_files &lt;--&gt; |graphical tool|files_in_rackham_home\n    %% user_local_files &lt;--&gt; |SCP|files_in_rackham_home\n    files_on_transit &lt;==&gt; |transfer|files_in_rackham_home</code></pre> <p>Overview of file transfer on Rackham The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>"},{"location":"cluster_guides/rackham_fingerprints/","title":"Rackham fingerprints","text":"","tags":["Rackham","fingerprint","SHA256"]},{"location":"cluster_guides/rackham_fingerprints/#rackham-fingerprints","title":"Rackham fingerprints","text":"<p>Below are the valid fingerprints on Rackham. If you see these, you can assume you are actually connecting to Rackham.</p> <pre><code>2048 SHA256:hkhuV+0mUDL7N4Jpr8/OWInrORSAL5ZRpvAqfjyg7Jg rackham (RSA)\n256 SHA256:W/MazH3WrH0wKrHBOJpPbDaU4qeYGqiv3FRPsdXIsb4 rackham (ECDSA)\n256 SHA256:y241gg8SExSktACnpD+OvROrMPTJcXYYdT/zYReef+k rackham (ED25519)\n</code></pre>","tags":["Rackham","fingerprint","SHA256"]},{"location":"cluster_guides/rackham_modules/","title":"Working with environment modules on Rackham","text":""},{"location":"cluster_guides/rackham_modules/#working-with-environment-modules-on-rackham","title":"Working with environment modules on Rackham","text":"<p>Rackham is shared Linux computer with all the standard Linux tools installed, on which all users should be able to do their work independently and undisturbed.</p> <p>Because this is the same for nearly all UPPMAX clusters, see the UPPMAX general page on modules.</p>"},{"location":"cluster_guides/rackhams_design/","title":"Rackham's design","text":""},{"location":"cluster_guides/rackhams_design/#rackhams-design","title":"Rackham's design","text":"<p>Rackham is an (general-purpose) high-performance computing (HPC) cluster.</p> What is an HPC cluster? <p>See the UPPMAX page about its HPC clusters.</p> <p>Or: Rackham is a group of computers that can effectively run many calculations, as requested by multiple people, at the same time. Rackham runs the Linux operating system and all users need some basic Linux knowledge to use Rackham.</p> Using Linux <p>Using Linux (and especially the so-called command-line/terminal) is essential to use Rackham. See the UPPMAX page about Linux.</p>"},{"location":"cluster_guides/rackhams_design/#folder-structure","title":"Folder structure","text":"<p>These are the most important folders on Rackham:</p> General name Example name Description Purpose <code>/home/[username]</code> <code>/home/sven</code> Your home folder Small/general/personal things <code>/proj/[project_name]</code> <code>/proj/uppmax2025-2-262</code> Your project folder Work on your project here <code>/proj/[project_name]/nobackup</code> <code>/proj/uppmax2025-2-262/nobackup</code> The folder of your project that has no backup Folder for intermediate/temporary files <p>To be able to retrieve (accidentally) lost files, Rackham has backups and snapshots.</p>"},{"location":"cluster_guides/rackhams_name/","title":"Rackham's name","text":""},{"location":"cluster_guides/rackhams_name/#rackhams-name","title":"Rackham's name","text":"<p>Rackham, like most UPPMAX clusters between 2013 and 2025, is named after a Tintin character, in this case after Red Rackham.</p> <p></p> What are the UPPMAX clusters? <p>See the UPPMAX page listing all its HPC clusters.</p>"},{"location":"cluster_guides/runtime_tips/","title":"Runtime tips","text":""},{"location":"cluster_guides/runtime_tips/#runtime-tips","title":"Runtime tips","text":""},{"location":"cluster_guides/runtime_tips/#general","title":"General","text":"How can I run X11 applications inside GNU screen? <p>If I log in to the login node with ssh -XA user@hostname as supposed when wanting to run X applications, and then try to start an X application inside a screen session, why does this not work?</p> <p>(This applies also for trying to do PNG output in R, since it depends on X11)</p> <p>When starting a screen session, your DISPLAY environment can sometimes change from the one that you had when you logged in.</p> <p>To solve this problem, you simply have to set the DISPLAY variable inside the screen session, to the same value that you have outside it.</p> <p>So, outside the screen session, do:</p> <pre><code>echo $DISPLAY\n</code></pre> <p>You might see something like:</p> <pre><code>localhost:45.0\n</code></pre> <p>Then, inside your screen session, set your DISPLAY env variable to that same value using the export command, like so:</p> <pre><code>export DISPLAY=localhost:45.0\n</code></pre> <p>(NOTE: The actual number above might be different for you, and should be changed accordingly!)</p> I want my program to send data to both stdout and to a file but nothing comes until the program ends <p>There is a program called unbuffer. You could try using it like (tee takes care of sending both to stdout and to a file):</p> <pre><code>unbuffer your_program |tee some_output_file\n</code></pre> My program suddenly seems to stop executing but it does not crash, the process is still alive. What is wrong? <ul> <li>This may happen if your executable binary file is deleted while the program is running.</li> <li>For example, if you recompile your program the previous executable file is deleted, which can cause running instances of the program to crash with \"Bus error\".</li> <li>The recommended solution is that if you need to recompile or reinstall while the program is running, create a copy of the executable file and execute the copy.</li> <li>Then, the original executable file can be safely deleted. -</li> <li>Alternatively, rename the currently executing file to something new and unique (using the mv command) before recompiling/reinstalling your program.</li> </ul> My program crashes with the error message 'Bus error'. Why? <ul> <li>This may happen if your executable binary file is deleted while the program is running.</li> <li>For example, if you recompile your program the previous executable file is deleted, which can cause running instances of the program to crash with \"Bus error\".</li> <li>The recommended solution is that if you need to recompile or reinstall while the program is running, create a copy of the executable file and execute the copy.</li> <li>Then, the original executable file can be safely deleted. -</li> <li>Alternatively, rename the currently executing file to something new and unique (using the mv command) before recompiling/reinstalling your program.</li> </ul> I have strange problems with my text-files / scripts when they have been copied from other computers For UPPMAX staff <p>TODO: InfoGlue link: <code>https://www.uppmax.uu.se/support/faq/running-jobs-faq/strange-problems-with-text-files---scripts-copied-from-other-computers/</code></p> <p>One reason is that copy-and-paste sometimes doesn't work. Rich text files and PDF's often replace symbols like quotes and white space with different symbols to improve readability, and copying from sources like these is generally not a good idea.</p> <p>Another possible reason is that lines of text files are terminated differently on UNIX/Windows/MAC. Read on for information on how to solve this:</p> <p>This might happen because your file was created, for instance, on a Windows computer and later copied to UPPMAX Linux machines. Text files have different line terminations on for instance Windows and Linux/Unix. If this is an ordinary textfile you can test this by using the \"file\" command, like this:</p> <pre><code>$ file myfile\nmyfile: ASCII text, with CRLF line terminators\n</code></pre> <p><code>CRLF terminators</code> tells you that each line of the file is ended by both a carriage-return and a line-feed, as on Windows. On all UPPMAX systems, the file can simply be converted to UNIX style text files using the \"dos2unix\" command:</p> <pre><code>$ dos2unix myfile\ndos2unix: converting file myfile to UNIX format ...\n</code></pre> <p>Checking the file again with the \"file\" command reveals that it now has ordinary UNIX line terminators (only LF):</p> <pre><code>$ file myfile\nmyfile: ASCII text\n</code></pre> <p>Similarly, a file from a Mac can be converted using the \"mac2unix\" command.</p> <p>If a shell script is behaving strangely, it can be due to the same problem. Trying to execute a program where the end of line marker is wrong might result in an error message such as the one below:</p> <pre><code>$ cat myscript.sh\n#!/bin/sh\n./program\n$ ./myscript.sh\n: No such file or directory\n</code></pre> <p>The \"file\" command does not work in this case as it simply tells us that the script is a \"Bourne shell script text executable\". Opening the script using \"vi\" shows at the bottom of the screen \"myscript.sh\" [dos] 2L, 22C. The \"[dos]\" is a sure marker of the same problem. Opening the same file in emacs reveals the same thing (-uu-(DOS)---F1 myscript.sh). Convert the script to unix-format using the \"dos2unix\" command as described above. An alternative is to copy the file and use the \"dos2unix\" command on the copy and compare the file sizes using \"ls -l\":</p> <pre><code>$ ls -l testme.sh\nrwxr-xr-x  1 daniels uppmax_staff 22 Dec 15 10:53 testme.sh\n$ dos2unix testme.sh\ndos2unix: converting file testme.sh to UNIX format ...\n$ ls -l testme.sh\n-rwxr-xr-x  1 daniels uppmax_staff 20 Dec 15 10:54 testme.sh\n</code></pre> <p>Note that the file size went from 22 bytes to 20, reflecting that the two CR bytes at the (almost) end of the line were removed.</p> How to run interactively on a compute node? <ul> <li>Start an interactive session</li> <li>More about interactive</li> </ul> I got problems running Perl on UPPMAX with messages about 'locale' <ul> <li>Edit your <code>.bashrc</code> file (located in your home folderon a UPPMAX cluster, like Rackham) and add the following lines:</li> </ul> <pre><code>export LC_CTYPE=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8\n</code></pre> <ul> <li>... then restart your terminal, or run, when located in your home folder:</li> </ul> <pre><code>source .bashrc\n</code></pre>"},{"location":"cluster_guides/runtime_tips/#related-to-batch-jobs","title":"Related to Batch jobs","text":"Looking at \"jobinfo\" output, PRIORITY and REASON for my waiting jobs change over time. Please explain what is going on! <p>What do the fields PRIORITY and REASON mean in \"jobinfo\" output?</p> How do I use the modules in batch jobs? <ul> <li>In order to make running installed programs easier you should use the module command.</li> <li> <p>The different modules that are installed sets the correct environments that are needed for the programs to run, like <code>PATH</code>, <code>LD_LIBRARY_PATH</code> and <code>MANPATH</code>. -To see what what modules that are available, type <code>module avail</code>. To see what modules you have loaded, type <code>module list</code>.</p> </li> <li> <p>Note. For the batch system slurm to work with modules you must have</p> </li> </ul> <pre><code>#!/bin/bash -l\n</code></pre> <p>in your submit script.</p> <ul> <li>For more information, read the module system guide</li> </ul> What is causing the sbatch script error 'Unknown shell type <code>load</code>'? <ul> <li>If you're getting the error message</li> </ul> <pre><code>init.c(379):ERROR:109: Unknown shell type load\n</code></pre> <p>when running your sbatch script, then your script is probably starting with the line</p> <pre><code>#!/bin/bash\n</code></pre> <p>To remedy this you need to make sure that your script starts with</p> <pre><code>#!/bin/bash -l\n</code></pre> <p>i.e. notice the trailing \"-l\". This tells bash to load the correct environment settings, which makes the module system usable.</p> I get <code>slurmstepd: error: _get_pss:ferror() /proc/$$/smaps</code> <p>Sometimes, this error message occurs in the Slurm output file: <code>slurmstepd: error: _get_pss: ferror() indicates error on file /proc/$$/smaps</code></p> <p>This error does not affect the results and can be ignored.</p> <p>Statistics are collected when a job has finished, including PSS, which is a measure of memory usage. The error message means that when Slurm tries to collect all info to calculate PSS, the file exposing kernel statistics for the process is already gone. This is probably due to the cleaning process being slightly out of sync.</p> <p>Job statistics based on the PSS value, like how much memory a job has used, might not be reliable. But since this is something that happens after the job has finished, results should not be affected.</p> How can I see my job's memory usage? <ul> <li> <p>Historical information can first of all be found by issuing the command <code>finishedjobinfo -j</code>. That will print out the maximum memory used by your job.</p> </li> <li> <p>If you want more details then we also save some memory information each 5 minute interval for the job in a file under <code>/sw/share/slurm/[cluster-name]/uppmax_jobstats/</code>. Notice that this is only stored for 30 days.</p> </li> <li> <p>You can also ask for an e-mail containing the log, when you submit your job with sbatch or start an \"interactive\" session, by adding a \"-C usage_mail\" flag to your command. Two examples:</p> </li> </ul> <pre><code>sbatch -A testproj -p core -n 5 -C usage_mail batchscript1\n</code></pre> <p>or, if interactive</p> <pre><code>interactive -A testproj -p node -n 1 -C \"fat&amp;usage_mail\"\n</code></pre> <ul> <li> <p>As you see, you have to be careful with the syntax when asking for two features, like \"fat\" and \"usage_mail\", at the same time. The logical AND operator \"&amp;\" combines the flags.</p> </li> <li> <p>If you overdraft the RAM that you asked for, you will probably get an automatic e-mail anyway.</p> </li> <li> <p>If, on the other hand, you want to view your memory consumption in real time then you will have to login to the node in question in another SSH session. (You will probably find a more recently updated memory information file there, named /var/spool/uppmax_jobstats/.)</p> </li> <li> <p>By naively looking at the memory consumption with tools like <code>ps</code> and <code>top</code> you as a user can easily get the wrong impression of the system, as the Linux kernel uses free memory for lots of buffers and caches to speed up other processes (but releases this as soon as applications requests it).</p> </li> <li> <p>If you know that you are the only user running on the node (from requesting a node job for example), then you could issue the command \"free -g\" instead. That will show you how much memory is used/free by the whole system, exclusive to these caches. Look for the row called \"-/+ buffers/cache\".</p> </li> <li> <p>If you require more detailed live information,   then it would probably be best if the tool called <code>smem</code> is used.   Download the latest version from http://www.selenic.com/smem/download/ and unpack it in your home directory.   Inside you will find an executable Python script, and by executing the command <code>smem -utk</code> you will see your user's memory usage reported in three different ways.</p> <ul> <li>USS is the total memory used by the user without shared buffers or caches.</li> <li>RSS is the number reported in \"top\" and \"ps\"; i.e. including ALL shared buffered/cached memory.</li> <li>And then there's also the PSS figure which tries to calculate a proportional memory usage per user for all shared memory buffers and caches (i.e. the figure will fall between USS and RSS).</li> </ul> </li> </ul> My job has very low priority! What can be wrong? <ul> <li> <p>One reason could be that your project has consumed its allocated hours.</p> </li> <li> <p>Background: Every job is associated with a project.</p> <ul> <li>Suppose that that you are working for a SNIC project s00101-01 that's been granted 10000 core hours per 30-days running.</li> <li>At the start of the project, s00101-01 is credited with 10000 hours and jobs that runs in that project are given a high priority.</li> <li>All the jobs that are finished or are running during the last 30 days is compared with this granted time.</li> <li>If enough jobs have run to consume this amount of hours the priority is lowered.</li> <li>The more you have overdrafted your granted time, the lower the priority.</li> </ul> </li> <li> <p>If you have overdrafted your granted time it's still possible to run jobs. You will probably wait for a longer time in the queue.</p> </li> <li> <p>To check status for your projects, run</p> </li> </ul> <pre><code>$ projinfo\n(Counting the number of core hours used since 2010-05-12/00:00:00 until now.)\n\nProject             Used[h]   Current allocation [h/month]\nUser\n-----------------------------------------------------\ns00101-01          72779.48               50000\nsome-user       72779.48\n</code></pre> <ul> <li> <p>If there are enough jobs left in projects that have not gone over their allocation, jobs associated with this project are therefore stuck wating at the bottom of the jobinfo list until the usage for the last 30 days drops down under its allocated budget again.</p> </li> <li> <p>On the other side they may be lucky to get some free nodes, so it could happen that they run as a bonus job before this happens.</p> </li> <li> <p>The job queue, that you can see with the jobinfo command, is ordered on job priority. Jobs with a high priority will run first, if they can (depending on number of free nodes and any special demands on e.g. memory).</p> </li> <li> <p>Job priority is the sum of the following numbers (you may use the sprio command to get exact numbers for individual jobs):</p> <ul> <li>A high number (100000 or 130000) if your project is within its allocation and a lower number otherwise. There are different grades of lower numbers, depending on how many times your project is overdrafted. As an example, a 2000 core hour project gets priority 70000 when it has used more than 2000 core hours, gets priority 60000 when it has used more than 4000 core hours, gets priority 50000 when it has used more than 6000 core hours, and so on. The lowest grade gives priority 10000 and does not go down from there.</li> <li>The number of minutes the job has been waiting in queue (for a maximum of 20160 after fourteen days).</li> <li>A job size number, higher for more nodes allocated to your job, for a maximum of 104.</li> <li>A very, very high number for \"short\" jobs, i.e. very short jobs that is not wider than four nodes.</li> <li>If your job priority is zero or one, there are more serious problems, for example that you asked for more resources than the batch system finds on the system.</li> </ul> </li> <li> <p>If you ask for a longer run time (TimeLimit) than the maximum on the system, your job will not run. The maximum is currently ten days. If you must run a longer job, submit it with a ten-day runtime and contact UPPMAX support.</p> </li> </ul>"},{"location":"cluster_guides/slurm/","title":"Slurm","text":"","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#slurm","title":"Slurm","text":"<p>The UPPMAX clusters are a shared resource. To ensure fair use, UPPMAX uses a scheduling system. A scheduling system decides at what time which calculation is done. The software used is called Slurm.</p> Why not write SLURM? <p>Indeed, Slurm started as an abbreviation of 'Simple Linux Utility for Resource Management'. However, the Slurm homepage uses 'Slurm' to describe the tool, hence we use Slurm too.</p> I thought Slurm was a drink? <p>Ah, yes, it is a drink in the tv-show Futurama too.</p> <p>Here we discuss the software called Slurm.</p> <p>This page describes how to use Slurm in general. See optimizing jobs how to optimize Slurm jobs. See Slurm troubleshooting how to fix Slurm errors.</p> <p>For information specific to clusters, see:</p> <ul> <li>Slurm on Bianca</li> <li>Slurm on Pelle</li> <li>Slurm on Rackham</li> <li>Slurm on Snowy</li> </ul>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#slurm-commands","title":"Slurm Commands","text":"<p>The Slurm system is accessed using the following commands:</p> <ul> <li><code>interactive</code> - Start an interactive session. This is described   in-depth for Bianca   and Rackham</li> <li><code>sbatch</code> - Submit and run a batch job script</li> <li><code>srun</code> - Typically used inside batch job scripts for running parallel jobs   (See examples further down)</li> <li><code>scancel</code> - Cancel one or more of your jobs.</li> <li><code>sinfo</code>: view information   about Slurm nodes and partitions</li> </ul> <pre><code>flowchart TD\n  login_node(User on login node)\n  interactive_session(User in interactive session)\n  computation_node(Computation node):::calculation_node\n\n  login_node --&gt; |move user, interactive|interactive_session\n  login_node ==&gt; |submit jobs, sbatch|computation_node\n  computation_node -.-&gt; |can become| interactive_session</code></pre> <p>The different types of nodes an UPPMAX cluster has. The thick edge shows the topic of this page: how to submit jobs to a computation node.</p>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#job-parameters","title":"Job parameters","text":"<p>This session describes how to specify a Slurm job:</p> <ul> <li>Getting started redirects to the cluster-specific pages</li> <li>Partitions specify the type of job</li> </ul>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#getting-started","title":"Getting started","text":"<p>To let Slurm schedule a job, one uses <code>sbatch</code>, like:</p> <pre><code>sbatch -A [project_code] [script_filename]\n</code></pre> <p>for example:</p> <pre><code>sbatch -A sens2017625 my_script.sh\n</code></pre> <p>Minimal and complete examples of using <code>sbatch</code> is described at the respective cluster guides:</p> <ul> <li>Bianca</li> <li>Rackham</li> <li>Snowy</li> </ul>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#specify-duration-of-the-run","title":"Specify duration of the run","text":"<p>To let Slurm schedule a job with a certain, one uses <code>sbatch</code>, like:</p> <pre><code>sbatch -A [project_code] --time [duration] [script_filename]\n</code></pre> <p>for example, for a job of 1 day, 23 hours, 59 minutes and 0 seconds:</p> <pre><code>sbatch -A sens2017625 --time 1-23:59:00 my_script.sh\n</code></pre> <p>If the job takes too long, this will result in a timeout error and the job will be aborted.</p> <p>The maximum duration of the run depends on the cluster you use.</p>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#partitions","title":"Partitions","text":"<p>Partitions are a way to tell what type of job you are submitting, e.g. if it needs to reserve a whole node, or part of a node.</p> <p>To let Slurm schedule a job using a partition, use the <code>--partition</code> (or <code>-p</code>) flag like this:</p> <pre><code>sbatch -A [project_code] --partition [partition_name] [script_filename]\n</code></pre> <p>for example:</p> <pre><code>sbatch -A sens2017625 --partition core my_script.sh\n</code></pre> <p>These are the partition names and their descriptions:</p> Partition name Description <code>core</code> Use one or more cores <code>node</code> Use a full node's set of cores <code>devel</code> Development job <code>devcore</code> Development job","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#the-core-partition","title":"The <code>core</code> partition","text":"<p>The <code>core</code> partition allows one to use one or more cores.</p> <p>Here is the minimal use for one core:</p> <pre><code>sbatch -A [project_code] --partition core [script_filename]\n</code></pre> <p>For example:</p> <pre><code>sbatch -A sens2017625 --partition core my_script.sh\n</code></pre> <p>To specify multiple cores, use <code>--ntasks</code> (or <code>-n</code>) like this:</p> <pre><code>sbatch -A [project_code] --partition core --ntasks [number_of_cores] [script_filename]\n</code></pre> <p>For example:</p> <pre><code>sbatch -A sens2017625 --partition core --ntasks 2 my_script.sh\n</code></pre> <p>Here, two cores are used.</p> What is the relation between <code>ntasks</code> and number of cores? <p>Agreed, the flag <code>ntasks</code> only indicates the number of threads. However, by default, the number of tasks per core is set to one. One can make this link explicit by using:</p> <pre><code>sbatch -A [project_code] --partition core --ntasks [number_of_cores] --ntasks-per-core 1 [script_filename]\n</code></pre> <p>This is especially important if you might adjust core usage of the job to be something less than a full node.</p>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#the-node-partition","title":"The <code>node</code> partition","text":"<p>Whenever -p node is specified, an entire node is used, no matter how many cores are specifically requested with -n [no_of_cores].</p> <p>For example, some bioinformatics tools show minimal increase in performance when more than 8-10 cores/job; in this case, specify \"-p core -n 8\" to ensure that only 8 cores (less than a single node) are allocated for such a job.</p>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#the-devel-partition","title":"The <code>devel</code> partition","text":"","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#the-devcore-partition","title":"The <code>devcore</code> partition","text":"","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#specifying-job-parameters","title":"Specifying job parameters","text":"<p>Whether you use the UPPMAX clusters interactively or in batch mode, you always have to specify a few things, like number of cores needed, running time etc. These things can be specified in two ways:</p> <p>Either as flags sent to the different Slurm commands (<code>sbatch</code>, <code>srun</code>, the <code>interactive</code> command, etc.), like so:</p> <pre><code>sbatch -A p2012999 -p core -n 1 -t 12:00:00 -J some_job_name my_job_script_file.sh\n</code></pre> <p>or, when using the <code>sbatch</code> command, it can be specified inside the job script file itself, by using special <code>SBATCH</code> comments, for example:</p> job_script.sh<pre><code>#!/bin/bash -l\n\n#SBATCH -A p2012999\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 12:00:00\n#SBATCH -J some_job_name\n</code></pre> <p>If doing this, then one will only need to start the script like so, without any flags:</p> <pre><code>sbatch job_script.sh\n</code></pre> How to see how many resources my project has used? <p>Use projplot.</p>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#need-more-resources-or-gpu","title":"Need more resources or GPU?","text":"","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#more-memory","title":"More memory","text":"<p>If you need extra memory (128 GB is available in common nodes) you can allocate larger nodes. The number and sizes differ among the clusters.</p> <p>Table below shows the configurations and flags to use.</p> RAM Rackham Snowy Bianca 256 GB <code>-C mem256GB</code> <code>-C mem256GB</code> <code>-C mem256GB</code> 512 GB N/A <code>-C mem512GB</code> <code>-C mem512GB</code> 1 TB <code>-C mem1TB</code> N/A N/A 2 TB N/A <code>-p veryfat -C mem2TB</code> N/A 4 TB N/A <code>-p veryfat -C mem4TB</code> N/A","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#gpus","title":"GPUs","text":"<ul> <li>Bianca: Nodes with Nvidia A100 40 GB<ul> <li>All GPU nodes have at least 256 GB RAM (fat nodes) with 16 CPU cores and 2 GPUs per node</li> </ul> </li> <li>Snowy: Nodes with Tesla T4 16 GB<ul> <li>The GPU nodes have either 128 or 256 GB memory and one GPU per node</li> </ul> </li> </ul> <p>Slurm options:</p> <ul> <li>Snowy 128 GB: <code>-M snowy -p node --gres=gpu:1 -t 1:0:1</code>   (Please note that -t has to be more than 1 hr)</li> <li>Snowy 256 GB: <code>-M snowy -p node -C mem256GB --gres=gpu:1  -t 1:0:1</code></li> <li> <p>Bianca: <code>-C gpu --gres=gpu:1 -t 01:10:00</code></p> </li> <li> <p>https://slurm.schedmd.com/gres.html#Running_Jobs</p> </li> </ul>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#the-queue","title":"The queue","text":"Do you want to see a graphical representation of the scheduler? <p>Slurm scheduler</p>","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm/#more-about-slurm-on-uppmax","title":"More about Slurm on UPPMAX","text":"","tags":["slurm","Simple Linux Utility for Resource Management"]},{"location":"cluster_guides/slurm_details/","title":"Compute nodes, Slurm and debugging jobs","text":""},{"location":"cluster_guides/slurm_details/#compute-nodes-slurm-and-debugging-jobs","title":"Compute nodes, Slurm and debugging jobs","text":""},{"location":"cluster_guides/slurm_details/#more-slurm-and-other-advanced-uppmax-techniques","title":"More Slurm and other advanced UPPMAX techniques","text":"<ul> <li>A closer look at Slurm</li> <li>Using the GPUs</li> <li>Debugging</li> <li>Job efficiency with the <code>jobstats</code> tool</li> <li>Advanced job submission</li> </ul>"},{"location":"cluster_guides/slurm_details/#the-slurm-workload-manager","title":"The Slurm Workload Manager","text":"<ul> <li>Free, popular, lightweight</li> <li>Open source: https://slurm.schedmd.com</li> <li>Available at all SNIC centres</li> <li>UPPMAX Slurm user guide</li> </ul>"},{"location":"cluster_guides/slurm_details/#the-queue","title":"The queue","text":"Do you want to see a graphical representation of the scheduler? <p>Slurm scheduler</p>"},{"location":"cluster_guides/slurm_details/#more-on-sbatch","title":"More on sbatch","text":"<p>Recap:</p> sbatch -A naiss20YY-XX-ZZ -t 10:00 -p core -n 10 my_job.sh slurm batch project name max runtime partition (\"job type\") #cores job script"},{"location":"cluster_guides/slurm_details/#more-on-time-limits","title":"More on time limits","text":"<ul> <li>Format <code>-t dd-hh:mm:ss</code></li> <li> <p>Examples and variants on syntax</p> <ul> <li><code>0-00:10:00 = 00:10:00 = 10:00 = 10</code></li> <li><code>0-12:00:00 = 12:00:00</code></li> <li><code>3-00:00:00 =                    3-0</code></li> <li><code>3-12:10:15</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#job-walltime","title":"Job walltime","text":"When you have no idea how long a program will take to run, what should you book? <p>A: very long time, e.g. 10-00:00:00</p> When you have an idea of how long a program would take to run, what should you book? <p>A: overbook by 50%</p>"},{"location":"cluster_guides/slurm_details/#more-on-partitions","title":"More on partitions","text":"<ul> <li> <p><code>-p core</code></p> <ul> <li>\u201ccore\u201d is the default partition</li> <li>\u2264 16 cores on Bianca and Snowy</li> <li>\u2264 20 cores in Rackham</li> <li>a script or program written without any thought on parallelism will use 1 core</li> </ul> </li> <li> <p><code>-p node</code></p> <ul> <li>if you wish to book full node(s)</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#quick-testing","title":"Quick testing","text":"<ul> <li>The \u201cdevel\u201d partition<ul> <li>max 2 nodes per job</li> <li>up to 1 hour in length</li> <li>only 1 at a time</li> <li><code>-p devcore</code>, <code>-p devel</code></li> </ul> </li> </ul> Any free nodes in the devel partition? Check status with <ul> <li><code>sinfo -p devel</code></li> <li><code>jobinfo -p devel</code></li> <li>more on these tools later</li> </ul> <ul> <li> <p>High priority queue for short jobs</p> <ul> <li>4 nodes</li> <li>up to 15 minutes</li> <li><code>--qos=short</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#debugging-or-complicated-workflows","title":"Debugging or complicated workflows","text":"<ul> <li> <p>Interactive jobs</p> <ul> <li>handy for debugging a code or a script by executing it line by line or for using programs with a graphical user interface</li> <li><code>salloc -n 80 -t 03:00:00 -A sens2023598</code></li> <li> <p><code>interactive -n 80 -t 03:00:00 -A sens2023598</code></p> </li> <li> <p>up to 12 hours</p> </li> <li>useful together with the <code>--begin=&lt;time&gt; flag</code></li> <li> <p><code>salloc -A naiss20YY-XX-ZZ --begin=2022-02-17T08:00:00</code></p> </li> <li> <p>asks for an interactive session that will start earliest tomorrow at 08:00</p> </li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#parameters-in-the-job-script-or-the-command-line","title":"Parameters in the job script or the command line?","text":"<ul> <li>Command line parameters override script parameters</li> <li>A typical script may be:</li> </ul> <pre><code>#!/bin/bash\n#SBATCH -A naiss20YY-XX-ZZ\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 24:00:00\n</code></pre> <p>Just a quick test:</p> <pre><code>sbatch -A naiss20YY-XX-ZZ -p devcore -t 00:15:00 jobscript.sh\n</code></pre> Hands-on #1: sbatch/jobinfo <ul> <li>login to Bianca</li> <li>find out which projects you\u2019re a member of using projinfo</li> <li>submit a short (10 min) test job; note the job ID</li> <li>find out if there are any free nodes in the devel partition</li> <li>submit a new job to use the devel partition</li> <li>write in the HackMD when you\u2019re done</li> </ul>"},{"location":"cluster_guides/slurm_details/#memory-in-core-or-devcore-jobs","title":"Memory in core or devcore jobs","text":"<ul> <li><code>-n X</code></li> <li>Bianca: 8GB per core</li> <li>Slurm reports the available memory in the prompt   at the start of an interactive session</li> </ul>"},{"location":"cluster_guides/slurm_details/#more-flags","title":"More flags","text":"<ul> <li><code>-J &lt;jobname&gt;</code></li> <li> <p>email:</p> <ul> <li><code>--mail-type=BEGIN,END,FAIL,TIME_LIMIT_80</code></li> <li> <p><code>--mail-user</code></p> <ul> <li>Don\u2019t use. Set your email correctly in SUPR instead.</li> </ul> </li> </ul> </li> <li> <p>out/err redirection:</p> <ul> <li> <p><code>--output=slurm-%j.out</code> and <code>--error=slurm-%j.err</code></p> <ul> <li>by default, where <code>%j</code> will be replaced by the job ID<ul> <li><code>--output=my.output.file</code></li> <li><code>--error=my.error.file</code></li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#monitoring-jobs","title":"Monitoring jobs","text":"<ul> <li> <p><code>jobinfo</code> - a wrapper around <code>squeue</code></p> <ul> <li>lists running and pending jobs</li> <li><code>jobinfo -u username</code></li> <li><code>jobinfo -A naiss20YY-XX-ZZ</code></li> <li><code>jobinfo -u username --state=running</code></li> <li><code>jobinfo -u username --state=pending</code></li> </ul> </li> <li> <p>You may also use the <code>squeue</code> command.</p> </li> <li> <p><code>bianca_combined_jobinfo</code> (queued jobs of all projects)</p> </li> </ul>"},{"location":"cluster_guides/slurm_details/#monitoring-and-modifying-jobs","title":"Monitoring and modifying jobs","text":"<ul> <li> <p><code>scontrol</code></p> <ul> <li><code>scontrol show job [jobid]</code></li> </ul> </li> <li> <p>possible to modify the job details after the job has been submitted; some options, like maximum runtime, may be modified (=shortened) even after the job started</p> <ul> <li><code>scontrol update JobID=jobid QOS=short</code></li> <li><code>scontrol update JobID=jobid TimeLimit=1-00:00:00</code></li> <li><code>scontrol update JobID=jobid NumNodes=10</code></li> <li><code>scontrol update JobID=jobid Features=mem1TB</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#when-a-job-goes-wrong","title":"When a job goes wrong","text":"<ul> <li> <p><code>scancel [jobid]</code></p> <ul> <li><code>-u username</code> - to cancel all your jobs</li> <li><code>-t [state]</code> - cancel pending or running jobs</li> <li><code>-n name</code> - cancel jobs with a given name</li> <li><code>-i</code> - ask for confirmation</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#priority","title":"Priority","text":"<ul> <li> <p>Roughly:</p> <ul> <li>The first job of the day has elevated priority</li> <li>Other normal jobs run in the order of submission (subject to scheduling)</li> <li>Projects exceeding their allocation get successively into the lower priority category</li> <li>Bonus jobs run after the jobs in the higher priority categories</li> </ul> </li> <li> <p>In practice:</p> <ul> <li>submit early = run early</li> <li>bonus jobs always run eventually, but may need to wait until the night or weekend</li> <li>In detail: jobinfo</li> </ul> </li> </ul> Hands-on #2: sbatch/squeue/scancel/scontrol/jobinfo <ul> <li>submit a new job; note the job ID</li> <li>check all your running jobs</li> <li>what is the priority or your recently-submitted job?</li> <li>submit a new job to run for 24h; note the job ID</li> <li>modify the name of the job to \u201cwrongjob\u201d</li> <li>cancel your job with name \u201cwrongjob\u201d</li> </ul>"},{"location":"cluster_guides/slurm_details/#determining-job-efficiency","title":"Determining job efficiency","text":"<ul> <li><code>jobstats</code> - custom-made UPPMAX tool</li> </ul>"},{"location":"cluster_guides/slurm_details/#job-efficiency","title":"Job efficiency","text":"<ul> <li> <p><code>jobstats</code> - a tool in the fight for productivity</p> <ul> <li>it works only for jobs longer than 5-15 minutes</li> <li><code>-r jobid</code> - check running jobs</li> <li><code>A project</code> - check all recent jobs of a given project</li> <li><code>p jobid</code> - produce a CPU and memory usage plot</li> </ul> </li> <li> <p>Jobstats user guide</p> </li> </ul> Hands-on #3: jobstats <ul> <li> <ul> <li>Firstly, find some job IDs from this month, using   <code>finishedjobinfo -m username</code></li> <li>Write down the IDs from some interesting jobs.</li> <li>Generate the images:</li> </ul> <p>Generate jobstats plots for your jobs</p> <pre><code>$ jobstats -p ID1 ID2 ID3\n</code></pre> </li> <li> <p>Look at the images</p> </li> </ul> <pre><code>$ eog *png &amp;\n</code></pre> <ul> <li>Which of the plots<ul> <li>Show good CPU or memory usage?</li> <li>Indicate that the job requires a fat node?</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_details/#different-flavours-of-slurm-job-script-examples-and-workflows","title":"Different flavours of Slurm: Job script examples and workflows","text":""},{"location":"cluster_guides/slurm_details/#simple-workflow","title":"Simple workflow","text":"<pre><code>#!/bin/bash\n#SBATCH -J jobname\n#SBATCH -A naiss20YY-XX-ZZ\n#SBATCH -p core\n#SBATCH -n 10\n#SBATCH -t 10:00:00\n\nmodule load software/version\nmodule load python/3.9.5\n\n./my-script.sh\n./another-script.sh\n./myprogram.exe\n</code></pre>"},{"location":"cluster_guides/slurm_details/#job-dependencies","title":"Job dependencies","text":"<ul> <li><code>sbatch jobscript.sh</code>   submitted job with jobid1</li> <li><code>sbatch anotherjobscript.sh</code>   submitted job with jobid2</li> <li><code>--dependency=afterok:jobid1:jobid2 job</code> will only start running after the successful end of jobs jobid1:jobid2</li> <li>very handy for clearly defined workflows</li> <li>You may also use -<code>-dependency=afternotok:jobid</code> in case you\u2019d like to resubmit a failed job, OOM (out of memory) for example, to a node with a higher memory: <code>-C mem256GB</code> or <code>-C mem512GB</code></li> </ul>"},{"location":"cluster_guides/slurm_details/#io-intensive-jobs-snic_tmp","title":"I/O intensive jobs: $SNIC_TMP","text":"<pre><code>#!/bin/bash\n#SBATCH -J jobname\n#SBATCH -A naiss20YY-XX-ZZ\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 10:00:00\n\nmodule load bioinfotools\nmodule load bwa/0.7.17 samtools/1.14\n\nexport SRCDIR=$HOME/path-to-input\n\ncp $SRCDIR/foo.pl $SRCDIR/bar.txt $SNIC_TMP/.\ncd $SNIC_TMP\n\n./foo.pl bar.txt\n\ncp *.out $SRCDIR/path-to-output/.\n</code></pre>"},{"location":"cluster_guides/slurm_details/#openmp-or-multi-threaded-job","title":"OpenMP or multi-threaded job","text":"<pre><code>#!/bin/bash\n#SBATCH -A naiss20YY-XX-ZZ\n#SBATCH --exclusive\n#SBATCH -p node\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=20\n#SBATCH -t 01:00:00\n\nmodule load uppasd\nexport OMP_NUM_THREADS=20\n\nsd &gt; out.log\n</code></pre>"},{"location":"cluster_guides/slurm_details/#gpu-nodes","title":"GPU nodes","text":"<ul> <li>Bianca: Nodes with Nvidia A100 40 GB</li> <li>Snowy: Nodes with Tesla T4 16 GB</li> <li> <p>All GPU nodes have at least 256 GB RAM (fat nodes) with 16 CPU cores and 2 GPUs per node</p> </li> <li> <p>slurm options:</p> <ul> <li>Snowy: <code>-M snowy --gres=gpu:1</code></li> <li>Bianca: ``-C gpu --gres=gpu:1 -t 01:10:00</li> </ul> </li> <li> <p>https://slurm.schedmd.com/gres.html#Running_Jobs</p> </li> </ul>"},{"location":"cluster_guides/slurm_details/#running-on-several-nodes-mpi-jobs","title":"Running on several nodes: MPI jobs","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J rsptjob\n#SBATCH \u2014mail-type=FAIL\n#SBATCH -A naiss20YY-XX-ZZ\n#SBATCH -t 00-07:00:00\n#SBATCH -p node\n#SBATCH -N 4\n### for jobs shorter than 15 min (max 4 nodes):\n###SBATCH --qos=short\n\n\nmodule load RSPt/2021-10-04\nexport RSPT_SCRATCH=$SNIC_TMP\n\nsrun -n 80 rspt\n\nrm -f apts dmft_lock_file e_entropy efgArray.dat.0 efgData.out.0 energy_matrices eparm_last interstitialenergy jacob1 jacob2 locust.* out_last pot_last rspt_fft_wisdom.* runs.a symcof_new\n</code></pre>"},{"location":"cluster_guides/slurm_details/#job-arrays","title":"Job arrays","text":"<ul> <li>Submit many jobs at once with the same or similar parameters</li> <li>Use <code>$SLURM_ARRAY_TASK_ID</code> in the script in order to find the correct path</li> </ul> <pre><code>#!/bin/bash\n#SBATCH -A naiss20YY-XX-ZZ\n#SBATCH -p node\n#SBATCH -N 2\n#SBATCH -t 01:00:00\n#SBATCH -J jobarray\n#SBATCH --array=0-19\n#SBATCH --mail-type=ALL,ARRAY_TASKS\n\n# SLURM_ARRAY_TASK_ID tells the script which iteration to run\necho $SLURM_ARRAY_TASK_ID\n\ncd /pathtomydirectory/dir_$SLURM_ARRAY_TASK_ID/\n\nsrun -n 40 my-program\nenv\n</code></pre> <ul> <li>You may use <code>scontrol</code> to modify some of the job arrays.</li> </ul>"},{"location":"cluster_guides/slurm_details/#snakemake-and-nextflow","title":"Snakemake and Nextflow","text":"<ul> <li>Conceptually similar, but with different flavours</li> <li>First define steps, each with an input, an output, and a command that transforms the input into output</li> <li>Then just ask for the desired output and the system will handle the rest</li> <li>Snakemake hackathon (re-occurring event)</li> <li>Nextflow training</li> </ul> Hands-on #4: make it your own <ul> <li>use 2 or 3 of the sample job scripts as a starting point for your own job script</li> <li>tweak them so that you run something closer to your research; or just feel free to experiment</li> <li>paste at least one of the examples in the HackMD</li> <li>great if you could add a comment what the job script is about</li> </ul>"},{"location":"cluster_guides/slurm_details/#where-to-go-from-here","title":"Where to go from here?","text":"<ul> <li>Code documentation</li> <li>NAISS training newsletter - software-specific training events included</li> <li>https://coderefinery.org/workshops/upcoming/</li> <li>https://nbis.se/training/events.html (bio)</li> <li>Contact support</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca/","title":"Using Slurm on Bianca","text":"","tags":["Slurm","Bianca"]},{"location":"cluster_guides/slurm_on_bianca/#using-slurm-on-bianca","title":"Using Slurm on Bianca","text":"<p>This page describes how to use Slurm on Bianca.</p> What is Slurm? <p>See the general page on Slurm</p> What is Bianca? <p>See the general page on Bianca</p> <p>See Slurm troubleshooting how to fix Slurm errors.</p>","tags":["Slurm","Bianca"]},{"location":"cluster_guides/slurm_on_bianca/#sbatch-and-interactive-on-bianca","title":"<code>sbatch</code> (and <code>interactive</code>) on Bianca","text":"<p><code>sbatch</code> (and <code>interactive</code>) work the same as on Rackham.</p> Want to start an interactive session? <p>See how to start an interactive session on Bianca</p> <p>Here it is shown how to submit a job with:</p> <ul> <li>command-line Slurm parameters</li> <li>Slurm parameters in the script</li> </ul>","tags":["Slurm","Bianca"]},{"location":"cluster_guides/slurm_on_bianca/#sbatch-a-script-with-command-line-slurm-parameters","title":"<code>sbatch</code> a script with command-line Slurm parameters","text":"<p>The minimal command to use <code>sbatch</code> with command-line Slurm parameters is:</p> <pre><code>sbatch -A [project_code] [script_filename]\n</code></pre> <p>where <code>[project_code]</code> is the project code, and <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch -A sens2017625 my_script.sh\n</code></pre> Forgot your Bianca project? <p>When login to Bianca's remote desktop environment webpage at https://bianca.uppmax.uu.se is helpful in showing you your Bianca projects:</p> <p></p> <p>An example of the Bianca projects for this user</p> What is in the script file? <p>The script file <code>my_script.sh</code> is a minimal example script. Such a minimal example script could be:</p> <pre><code>#!/bin/bash\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>.</p>","tags":["Slurm","Bianca"]},{"location":"cluster_guides/slurm_on_bianca/#sbatch-a-script-with-slurm-parameters-in-script","title":"<code>sbatch</code> a script with Slurm parameters in script","text":"<p>The minimal command to use <code>sbatch</code> with Slurm parameters in the script:</p> <pre><code>sbatch [script_filename]\n</code></pre> <p>where <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch my_script.sh\n</code></pre> <p>The script must contain at least the following lines:</p> <pre><code>#SBATCH -A [project_code]\n</code></pre> <p>where <code>[project_code]</code> is the project code, for example:</p> <pre><code>#SBATCH -A sens2017625\n</code></pre> Forgot your Bianca project? <p>When login to Bianca's remote desktop environment webpage at https://bianca.uppmax.uu.se is helpful in showing you your Bianca projects:</p> <p></p> <p>An example of the Bianca projects for this user</p> <p>A full example script would be:</p> <pre><code>#!/bin/bash\n#SBATCH -A sens2017625\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>. See the general page on Slurm.</p>","tags":["Slurm","Bianca"]},{"location":"cluster_guides/slurm_on_bianca/#more-about-slurm-on-bianca","title":"More about slurm on Bianca","text":"","tags":["Slurm","Bianca"]},{"location":"cluster_guides/slurm_on_bianca_details/","title":"Compute nodes, Slurm and debugging jobs on Bianca","text":""},{"location":"cluster_guides/slurm_on_bianca_details/#compute-nodes-slurm-and-debugging-jobs-on-bianca","title":"Compute nodes, Slurm and debugging jobs on Bianca","text":""},{"location":"cluster_guides/slurm_on_bianca_details/#more-slurm-and-other-advanced-uppmax-techniques","title":"More Slurm and other advanced UPPMAX techniques","text":"<ul> <li>A closer look at Slurm</li> <li>Using the GPUs</li> <li>Debugging</li> <li>Job efficiency with the <code>jobstats</code> tool</li> <li>Advanced job submission</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#the-slurm-workload-manager","title":"The Slurm Workload Manager","text":"<ul> <li>Free, popular, lightweight</li> <li>Open source: https://slurm.schedmd.com</li> <li>Available at all SNIC centres</li> <li>UPPMAX Slurm user guide</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#more-on-sbatch","title":"More on sbatch","text":"<p>Recap:</p> sbatch -A sens2023598 -t 10:00 -p core -n 10 my_job.sh slurm batch project name max runtime partition (\"job type\") #cores job script"},{"location":"cluster_guides/slurm_on_bianca_details/#more-on-time-limits","title":"More on time limits","text":"<ul> <li>Format <code>-t dd-hh:mm:ss</code></li> <li> <p>Examples and variants on syntax</p> <ul> <li><code>0-00:10:00 = 00:10:00 = 10:00 = 10</code></li> <li><code>0-12:00:00 = 12:00:00</code></li> <li><code>3-00:00:00 =                    3-0</code></li> <li><code>3-12:10:15</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#job-walltime","title":"Job walltime","text":"When you have no idea how long a program will take to run, what should you book? <p>A: very long time, e.g. 10-00:00:00</p> When you have an idea of how long a program would take to run, what should you book? <p>A: overbook by 50%</p>"},{"location":"cluster_guides/slurm_on_bianca_details/#more-on-partitions","title":"More on partitions","text":"<ul> <li> <p><code>-p core</code></p> <ul> <li>\u201ccore\u201d is the default partition</li> <li>\u2264 16 cores on Bianca</li> <li>a script or program written without any thought on parallelism will use 1 core</li> </ul> </li> <li> <p><code>-p node</code></p> <ul> <li>if you wish to book full node(s)</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#quick-testing","title":"Quick testing","text":"<ul> <li>The \u201cdevel\u201d partition<ul> <li>max 2 nodes per job</li> <li>up to 1 hour in length</li> <li>only 1 at a time</li> <li><code>-p devcore</code>, <code>-p devel</code></li> </ul> </li> </ul> Any free nodes in the devel partition? Check status with <ul> <li><code>sinfo -p devel</code></li> <li><code>jobinfo -p devel</code></li> <li>more on these tools later</li> </ul> <ul> <li> <p>High priority queue for short jobs</p> <ul> <li>4 nodes</li> <li>up to 15 minutes</li> <li><code>--qos=short</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#debugging-or-complicated-workflows","title":"Debugging or complicated workflows","text":"<ul> <li> <p>Interactive jobs</p> <ul> <li>handy for debugging a code or a script by executing it line by line or for using programs with a graphical user interface</li> <li><code>salloc -n 80 -t 03:00:00 -A sens2023598</code></li> <li> <p><code>interactive -n 80 -t 03:00:00 -A sens2023598</code></p> </li> <li> <p>up to 12 hours</p> </li> <li>useful together with the <code>--begin=&lt;time&gt; flag</code></li> <li> <p><code>salloc -A snic2022-22-50 --begin=2022-02-17T08:00:00</code></p> </li> <li> <p>asks for an interactive job that will start earliest tomorrow at 08:00</p> </li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#parameters-in-the-job-script-or-the-command-line","title":"Parameters in the job script or the command line?","text":"<ul> <li>Command line parameters override script parameters</li> <li>A typical script may be:</li> </ul> <pre><code>#!/bin/bash\n#SBATCH -A sens2023598\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 24:00:00\n</code></pre> <p>Just a quick test:</p> <pre><code>sbatch -p devcore -t 00:15:00 jobscript.sh\n</code></pre> Hands-on #1: sbatch/jobinfo <ul> <li>login to Bianca</li> <li>find out which projects you\u2019re a member of using projinfo</li> <li>submit a short (10 min) test job; note the job ID</li> <li>find out if there are any free nodes in the devel partition</li> <li>submit a new job to use the devel partition</li> <li>write in the HackMD when you\u2019re done</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#memory-in-core-or-devcore-jobs","title":"Memory in core or devcore jobs","text":"<ul> <li><code>-n X</code></li> <li>Bianca: 8GB per core</li> <li>Slurm reports the available memory   in the prompt at the start of an interactive job</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#more-flags","title":"More flags","text":"<ul> <li><code>-J &lt;jobname&gt;</code></li> <li> <p>email:</p> <ul> <li><code>--mail-type=BEGIN,END,FAIL,TIME_LIMIT_80</code></li> <li> <p><code>--mail-user</code></p> <ul> <li>Don\u2019t use. Set your email correctly in SUPR instead.</li> </ul> </li> </ul> </li> <li> <p>out/err redirection:</p> <ul> <li> <p><code>--output=slurm-%j.out</code> and <code>\u2014-error=slurm-%j.err</code></p> <ul> <li>by default, where %j will be replaced by the job ID</li> </ul> </li> <li> <p><code>--output=my.output.file</code></p> </li> <li><code>--error=my.error.file</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#monitoring-jobs","title":"Monitoring jobs","text":"<ul> <li> <p><code>jobinfo</code> - a wrapper around <code>squeue</code></p> <ul> <li>lists running and pending jobs</li> <li><code>jobinfo -u username</code></li> <li><code>jobinfo -A sens2023598</code></li> <li><code>jobinfo -u username --state=running</code></li> <li><code>jobinfo -u username --state=pending</code></li> </ul> </li> <li> <p>You may also use the <code>squeue</code> command.</p> <ul> <li>This will give you a list of jobs in the present project (possibly other users within the project)</li> </ul> </li> </ul> <p>Get a view of the whole queue, including all projects</p> <ul> <li>Use the command <code>bianca_combined_jobinfo</code> (queued jobs of all projects)</li> <li>That makes it easier to see how the resources are used and what the odds are that you can start your job soon!</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#monitoring-and-modifying-jobs","title":"Monitoring and modifying jobs","text":"<ul> <li> <p><code>scontrol</code></p> <ul> <li><code>scontrol show job [jobid]</code></li> </ul> </li> <li> <p>possible to modify the job details after the job has been submitted; some options, like maximum runtime, may be modified (=shortened) even after the job started</p> <ul> <li><code>scontrol update JobID=jobid QOS=short</code></li> <li><code>scontrol update JobID=jobid TimeLimit=1-00:00:00</code></li> <li><code>scontrol update JobID=jobid NumNodes=10</code></li> <li><code>scontrol update JobID=jobid Features=mem1TB</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#when-a-job-goes-wrong","title":"When a job goes wrong","text":"<ul> <li> <p><code>scancel [jobid]</code></p> <ul> <li><code>-u username</code> - to cancel all your jobs</li> <li><code>-t [state]</code> - cancel pending or running jobs</li> <li><code>-n name</code> - cancel jobs with a given name</li> <li><code>-i</code> - ask for confirmation</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#priority","title":"Priority","text":"<ul> <li> <p>Roughly:</p> <ul> <li>The first job of the day has elevated priority</li> <li>Other normal jobs run in the order of submission (subject to scheduling)</li> <li>Projects exceeding their allocation get successively into the lower priority category</li> <li>Bonus jobs run after the jobs in the higher priority categories</li> </ul> </li> <li> <p>In practice:</p> <ul> <li>submit early = run early</li> <li>bonus jobs always run eventually, but may need to wait until the night or weekend</li> <li>In detail: jobinfo</li> </ul> </li> </ul> Hands-on #2: sbatch/squeue/scancel/scontrol/jobinfo <ul> <li>submit a new job; note the job ID</li> <li>check all your running jobs</li> <li>what is the priority or your recently-submitted job?</li> <li>submit a new job to run for 24h; note the job ID</li> <li>modify the name of the job to \u201cwrongjob\u201d</li> <li>cancel your job with name \u201cwrongjob\u201d</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#determining-job-efficiency","title":"Determining job efficiency","text":"<ul> <li><code>jobstats</code> - custom-made UPPMAX tool</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#job-efficiency","title":"Job efficiency","text":"<ul> <li> <p><code>jobstats</code> - a tool in the fight for productivity</p> <ul> <li>it works only for jobs longer than 5-15 minutes</li> <li><code>-r jobid</code> - check running jobs</li> <li><code>A project</code> - check all recent jobs of a given project</li> <li><code>p jobid</code> - produce a CPU and memory usage plot</li> </ul> </li> <li> <p>Jobstats user guide</p> </li> </ul> Hands-on #3: jobstats <ul> <li> <ul> <li>Firstly, find some job IDs from this month</li> <li>Run <code>finishedjobinfo -m username</code></li> <li>Write down the IDs from some interesting jobs</li> <li>Generate the images:</li> </ul> <p>Generate jobstats plots for your jobs</p> <pre><code>$ jobstats -p ID1 ID2 ID3\n</code></pre> </li> <li> <p>Look at the images</p> </li> </ul> <pre><code>$ eog *png &amp;\n</code></pre> <ul> <li>Which of the plots<ul> <li>Show good CPU or memory usage?</li> <li>Indicate that the job requires a fat node?</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#different-flavours-of-slurm-job-script-examples-and-workflows","title":"Different flavours of Slurm: Job script examples and workflows","text":""},{"location":"cluster_guides/slurm_on_bianca_details/#simple-workflow","title":"Simple workflow","text":"<pre><code>#!/bin/bash\n#SBATCH -J jobname\n#SBATCH -A sens2023598\n#SBATCH -p core\n#SBATCH -n 10\n#SBATCH -t 10:00:00\n\nmodule load software/version\nmodule load python/3.9.5\n\n./my-script.sh\n./another-script.sh\n./myprogram.exe\n</code></pre>"},{"location":"cluster_guides/slurm_on_bianca_details/#job-dependencies","title":"Job dependencies","text":"<ul> <li><code>sbatch jobscript.sh</code>   submitted job with jobid1</li> <li><code>sbatch anotherjobscript.sh</code>   submitted job with jobid2</li> <li><code>--dependency=afterok:jobid1:jobid2 job</code> will only start running after the successful end of jobs jobid1:jobid2</li> <li>very handy for clearly defined workflows</li> <li>You may also use -<code>-dependency=afternotok:jobid</code> in case you\u2019d like to resubmit a failed job, OOM (out of memory) for example, to a node with a higher memory: <code>-C mem215GB</code> or <code>-C mem512GB</code></li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#io-intensive-jobs-snic_tmp","title":"I/O intensive jobs: $SNIC_TMP","text":"<pre><code>#!/bin/bash\n#SBATCH -J jobname\n#SBATCH -A sens2023598\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 10:00:00\n\nmodule load bioinfotools\nmodule load bwa/0.7.17 samtools/1.14\n\nexport SRCDIR=$HOME/path-to-input\n\ncp $SRCDIR/foo.pl $SRCDIR/bar.txt $SNIC_TMP/.\ncd $SNIC_TMP\n\n./foo.pl bar.txt\n\ncp *.out $SRCDIR/path-to-output/.\n</code></pre>"},{"location":"cluster_guides/slurm_on_bianca_details/#openmp-or-multi-threaded-job","title":"OpenMP or multi-threaded job","text":"<pre><code>#!/bin/bash\n#SBATCH -A sens2023598\n#SBATCH --exclusive\n#SBATCH -p node\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=20\n#SBATCH -t 01:00:00\n\nmodule load uppasd\nexport OMP_NUM_THREADS=20\n\nsd &gt; out.log\n</code></pre>"},{"location":"cluster_guides/slurm_on_bianca_details/#gpu-nodes-on-bianca","title":"GPU nodes on Bianca","text":"<ul> <li>Nodes with Nvidia A100 40 GB.</li> <li>All GPU nodes have at least 256 GB RAM (fat nodes) with 16 CPU cores and 2 GPUs per node.</li> <li>In order to avoid GPU misuse, a project cannot request more than 7 GPU nodes, in total.</li> <li>SBATCH options:</li> </ul> <pre><code>#SBATCH -C gpu\n#SBATCH --gpus=2            #number of GPUs requested\n#SBATCH --gpus-per-node=2   #number of GPUs per node\n\nnvidia-smi\n</code></pre> <ul> <li>https://slurm.schedmd.com/gres.html#Running_Jobs</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#running-on-several-nodes-mpi-jobs","title":"Running on several nodes: MPI jobs","text":"<pre><code>#!/bin/bash -l\n#SBATCH -J rsptjob\n#SBATCH \u2014mail-type=FAIL\n#SBATCH -A sens2023598\n#SBATCH -t 00-07:00:00\n#SBATCH -p node\n#SBATCH -N 4\n### for jobs shorter than 15 min (max 4 nodes):\n###SBATCH --qos=short\n\nmodule load RSPt/2021-10-04\nexport RSPT_SCRATCH=$SNIC_TMP\n\nsrun -n 80 rspt\n\nrm -f apts dmft_lock_file e_entropy efgArray.dat.0 efgData.out.0 energy_matrices eparm_last interstitialenergy jacob1 jacob2 locust.* out_last pot_last rspt_fft_wisdom.* runs.a symcof_new\n</code></pre>"},{"location":"cluster_guides/slurm_on_bianca_details/#job-arrays","title":"Job arrays","text":"<ul> <li>Submit many jobs at once with the same or similar parameters</li> <li>Use <code>$SLURM_ARRAY_TASK_ID</code> in the script in order to find the correct path</li> </ul> <pre><code>#!/bin/bash\n#SBATCH -A sens2023598\n#SBATCH -p node\n#SBATCH -N 2\n#SBATCH -t 01:00:00\n#SBATCH -J jobarray\n#SBATCH --array=0-19\n#SBATCH --mail-type=ALL,ARRAY_TASKS\n\n# SLURM_ARRAY_TASK_ID tells the script which iteration to run\necho $SLURM_ARRAY_TASK_ID\n\ncd /pathtomydirectory/dir_$SLURM_ARRAY_TASK_ID/\n\nsrun -n 40 my-program\nenv\n</code></pre> <ul> <li>You may use <code>scontrol</code> to modify some of the job arrays.</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#snakemake-and-nextflow","title":"Snakemake and Nextflow","text":"<ul> <li>Conceptually similar, but with different flavours</li> <li>First define steps, each with an input, an output, and a command that transforms the input into output</li> <li>Then just ask for the desired output and the system will handle the rest</li> <li>Snakemake hackathon (re-occurring event)</li> <li>Nextflow training</li> </ul> Hands-on #4: make it your own <ul> <li>use 2 or 3 of the sample job scripts as a starting point for your own job script</li> <li>tweak them so that you run something closer to your research; or just feel free to experiment</li> <li>paste at least one of the examples in the HackMD</li> <li>great if you could add a comment what the job script is about</li> </ul>"},{"location":"cluster_guides/slurm_on_bianca_details/#where-to-go-from-here","title":"Where to go from here?","text":"<ul> <li>Code documentation</li> <li>NAISS training newsletter - software-specific training events included</li> <li>https://coderefinery.org/workshops/upcoming/</li> <li>https://nbis.se/training/events.html (bio)</li> <li>email support@uppmax.uu.se or https://supr.naiss.se/support/</li> </ul>"},{"location":"cluster_guides/slurm_on_pelle/","title":"Using Slurm on Pelle","text":""},{"location":"cluster_guides/slurm_on_pelle/#using-slurm-on-pelle","title":"Using Slurm on Pelle","text":"<p>This page describes how to use Slurm on Pelle.</p> What is Slurm? <p>See the general page about Slurm</p> What is Pelle? <p>See the general page about Pelle</p> <p>See Slurm troubleshooting how to fix Slurm errors.</p>"},{"location":"cluster_guides/slurm_on_pelle/#sbatch-and-interactive-on-pelle","title":"<code>sbatch</code> (and <code>interactive</code>) on Pelle","text":"<p><code>sbatch</code> (and <code>interactive</code>) work the same as on the other clusters, the only difference is that some flags/options may be different, like partition name, see below.</p> Want to start an interactive session? <p>See how to start an interactive session on Pelle</p> <p>Here it is shown how to submit a job with:</p> <ul> <li>Command-line Slurm parameters</li> <li>Slurm parameters in the script</li> </ul>"},{"location":"cluster_guides/slurm_on_pelle/#partitions-on-pelle","title":"Partitions on Pelle","text":"<p>Partition flag is either <code>--partition</code> or <code>-p</code></p> Partition name Description <code>pelle</code> (Default) Use one or more CPU cores <code>fat</code> Use a fat node with 2 or 3 TB memory, see below <code>gpu</code> GPU node, 2 types see below"},{"location":"cluster_guides/slurm_on_pelle/#the-pelle-partition","title":"The <code>pelle</code> partition","text":"<p>The <code>pelle</code> partition is default so you can omit specifying <code>-p</code> or <code>--partition</code></p> <p>Its allocates an ordinary CPU node (allows one to use one or more cores, up to 96 cores).</p> <p>Here is the minimal use for one core:</p> <pre><code>sbatch -A [project_code] [script_filename]\n</code></pre> <p>For example:</p> <pre><code>sbatch -A staff my_script.sh\n</code></pre> <p>To specify multiple cores, use <code>--ntasks</code> (or <code>-n</code>) like this:</p> <pre><code>sbatch -A [project_code] --ntasks [number_of_cores] [script_filename]\n</code></pre> <p>For example:</p> <pre><code>sbatch -A staff --ntasks 2 my_script.sh\n</code></pre> <p>Here, two cores are used.</p> What is the relation between <code>ntasks</code> and number of cores? <p>Agreed, the flag <code>ntasks</code> only indicates the number of tasks. However, by default, the number of tasks per core is set to one. One can make this link explicit by using:</p> <pre><code>sbatch -A [project_code] --partition core --ntasks [number_of_cores] --ntasks-per-core 1 [script_filename]\n</code></pre> <p>This is especially important if you might adjust core usage of the job to be something less than a full node.</p>"},{"location":"cluster_guides/slurm_on_pelle/#the-fat-partition","title":"The <code>fat</code> partition","text":"<p>With the <code>fat</code> partition you reach compute nodes with more memory. There are at the moment just one 2 TB node and one 3 TB node.</p> <ul> <li> <p>To allocate 2 TB: <code>-p fat -C 2TB</code></p> <ul> <li>Example: <code>interactive -A staff -t 1:0:0 -p fat -C 2TB</code></li> </ul> </li> <li> <p>To allocate 3 TB: <code>-p fat -C 3TB</code></p> <ul> <li>Example: <code>interactive -A staff -t 1:0:0 -p fat -C 3TB</code></li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_pelle/#the-gpu-partition","title":"The <code>gpu</code> partition","text":"<p>With the <code>gpu</code> partition you reach the nodes with GPUs.</p> <p>There are two kinds of GPUs at the moment.</p> <ul> <li>4 of the lighter type <code>L40s</code>, enough for most problems. Each node has 10 (!) GPUs. Most often just one GPU is needed, so remember to state that you need just 1, see below.</li> <li>2 of the large type <code>H100</code>, which can be suitable for large training runs. Each node has 2 GPUs. Most often just one GPU is needed, so remember to state that you need just 1, see below.</li> </ul> <p>Therefore, at first hand, allocate the default <code>L40s</code> and one of them</p> <ul> <li> <p>To allocate L40s: <code>-p gpu --gres=gpu:&lt;number of GPUs&gt;</code> or <code>-p gpu --gpus:l40s:&lt;number of GPUs&gt;</code></p> <ul> <li>Example with 1 GPU: <code>interactive -A staff -t 1:0:0 -p gpu --gres=gpu:1</code></li> <li>Example with 11 GPUs: <code>interactive -A staff -t 1:0:0 -p gpu --gres=gpu:11</code> will fail because there are just 10 GPUs on one node!</li> </ul> </li> <li> <p>To allocate H100: <code>-p gpu --gpus=h100:&lt;number of GPUs&gt;</code></p> <ul> <li>Example with 1 GPU: <code>`interactive -A staff -t 1:0:0 -p gpu --gpus=h100:1</code></li> <li>Example with 3 GPU: <code>`interactive -A staff -t 1:0:0 -p gpu --gpus=h100:3</code> will fail because there are just 2 GPUs on one node!</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_on_pelle/#sbatch-a-script-with-command-line-slurm-parameters","title":"<code>sbatch</code> a script with command-line Slurm parameters","text":"<p>The minimal command to use <code>sbatch</code> with command-line Slurm parameters is:</p> <pre><code>sbatch -A [project_code] [script_filename]\n</code></pre> <p>where <code>[project_code]</code> is the project code, and <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch -A uppmax2023-2-25 my_script.sh\n</code></pre> Forgot your Rackham project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>Example of the Rackham project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Rackham, has a slightly different name: the account name to use on Rackham is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> What is in the script file? <p>The script file <code>my_script.sh</code> is a minimal example script. Such a minimal example script could be:</p> <pre><code>#!/bin/bash\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>.</p>"},{"location":"cluster_guides/slurm_on_pelle/#sbatch-a-script-with-slurm-parameters-in-script","title":"<code>sbatch</code> a script with Slurm parameters in script","text":"<p>The minimal command to use <code>sbatch</code> with Slurm parameters in the script:</p> <pre><code>sbatch [script_filename]\n</code></pre> <p>where <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch my_script.sh\n</code></pre> <p>The script must contain at least the following lines:</p> <pre><code>#SBATCH -A [project_code]\n</code></pre> <p>where <code>[project_code]</code> is the project code, for example:</p> <pre><code>#SBATCH -A uppmax2023-2-25\n</code></pre> Forgot your Rackham project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>Example of the Rackham project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Rackham, has a slightly different name: the account name to use on Rackham is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> <p>A full example script would be:</p> <pre><code>#!/bin/bash\n#SBATCH -A uppmax2023-2-25\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>.</p>"},{"location":"cluster_guides/slurm_on_rackham/","title":"Using Slurm on Rackham","text":""},{"location":"cluster_guides/slurm_on_rackham/#using-slurm-on-rackham","title":"Using Slurm on Rackham","text":"<p>This page describes how to use Slurm on Rackham.</p> What is Slurm? <p>See the general page about Slurm</p> What is Rackham? <p>See the general page about Rackham</p> <p>See Slurm troubleshooting how to fix Slurm errors.</p>"},{"location":"cluster_guides/slurm_on_rackham/#sbatch-and-interactive-on-rackham","title":"<code>sbatch</code> (and <code>interactive</code>) on Rackham","text":"<p><code>sbatch</code> (and <code>interactive</code>) work the same as on other clusters, the only difference is that one need specify one want to use the Rackham computer nodes.</p> Want to start an interactive session? <p>See how to start an interactive session on Rackham</p> <p>Here it is shown how to submit a job with:</p> <ul> <li>command-line Slurm parameters</li> <li>Slurm parameters in the script</li> </ul>"},{"location":"cluster_guides/slurm_on_rackham/#sbatch-a-script-with-command-line-slurm-parameters","title":"<code>sbatch</code> a script with command-line Slurm parameters","text":"<p>The minimal command to use <code>sbatch</code> with command-line Slurm parameters is:</p> <pre><code>sbatch -A [project_code] [script_filename]\n</code></pre> <p>where <code>[project_code]</code> is the project code, and <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch -A uppmax2023-2-25 my_script.sh\n</code></pre> Forgot your Rackham project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>Example of the Rackham project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Rackham, has a slightly different name: the account name to use on Rackham is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> What is in the script file? <p>The script file <code>my_script.sh</code> is a minimal example script. Such a minimal example script could be:</p> <pre><code>#!/bin/bash\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>.</p>"},{"location":"cluster_guides/slurm_on_rackham/#sbatch-a-script-with-slurm-parameters-in-script","title":"<code>sbatch</code> a script with Slurm parameters in script","text":"<p>The minimal command to use <code>sbatch</code> with Slurm parameters in the script:</p> <pre><code>sbatch [script_filename]\n</code></pre> <p>where <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch my_script.sh\n</code></pre> <p>The script must contain at least the following lines:</p> <pre><code>#SBATCH -A [project_code]\n</code></pre> <p>where <code>[project_code]</code> is the project code, for example:</p> <pre><code>#SBATCH -A uppmax2023-2-25\n</code></pre> Forgot your Rackham project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>Example of the Rackham project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Rackham, has a slightly different name: the account name to use on Rackham is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> <p>A full example script would be:</p> <pre><code>#!/bin/bash\n#SBATCH -A uppmax2023-2-25\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>.</p>"},{"location":"cluster_guides/slurm_on_snowy/","title":"Using Slurm on Snowy","text":""},{"location":"cluster_guides/slurm_on_snowy/#using-slurm-on-snowy","title":"Using Slurm on Snowy","text":"<p>This page describes how to use Slurm on Snowy.</p> What is Slurm? <p>See the UPPMAX general page on Slurm</p> What is Snowy? <p>See the UPPMAX general page on Snowy</p> <p>See Slurm troubleshooting how to fix Slurm errors.</p>"},{"location":"cluster_guides/slurm_on_snowy/#sbatch-and-interactive-on-snowy","title":"<code>sbatch</code> (and <code>interactive</code>) on Snowy","text":"<p><code>sbatch</code> (and <code>interactive</code>) work the same as on other clusters, the only difference is that one need specify one want to use the Snowy computer nodes.</p> Want to start an interactive session? <p>See how to start an interactive session on Snowy</p> <p>Here it is shown how to submit a job with:</p> <ul> <li>command-line Slurm parameters</li> <li>Slurm parameters in the script</li> </ul>"},{"location":"cluster_guides/slurm_on_snowy/#sbatch-a-script-with-command-line-slurm-parameters","title":"<code>sbatch</code> a script with command-line Slurm parameters","text":"<p>The minimal command to use <code>sbatch</code> with command-line Slurm parameters is:</p> <pre><code>sbatch -A [project_code] -M snowy [script_filename]\n</code></pre> <p>where <code>[project_code]</code> is the project code, and <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch -A uppmax2023-2-25 -M snowy my_script.sh\n</code></pre> Forgot your Snowy project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>An example of the Snowy project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Snowy, has a slightly different name: the account name to use on Snowy is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> What is in the script file? <p>The script file <code>my_script.sh</code> is a minimal example script. Such a minimal example script could be:</p> <pre><code>#!/bin/bash\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>. See the general page on Slurm.</p>"},{"location":"cluster_guides/slurm_on_snowy/#sbatch-a-script-with-slurm-parameters-in-script","title":"<code>sbatch</code> a script with Slurm parameters in script","text":"<p>The minimal command to use <code>sbatch</code> with Slurm parameters in the script:</p> <pre><code>sbatch [script_filename]\n</code></pre> <p>where <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch my_script.sh\n</code></pre> <p>The script must contain at least the following lines:</p> <pre><code>#SBATCH -A [project_code]\n#SBATCH -M snowy\n</code></pre> <p>where <code>[project_code]</code> is the project code, for example:</p> <pre><code>#SBATCH -A uppmax2023-2-25\n#SBATCH -M snowy\n</code></pre> Forgot your Snowy project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>An example of the Snowy project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Snowy, has a slightly different name: the account name to use on Snowy is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> <p>A full example script would be:</p> <pre><code>#!/bin/bash\n#SBATCH -A uppmax2023-2-25\n#SBATCH -M snowy\necho \"Hello\"\n</code></pre> <p>Again, what is shown here is a minimal use of <code>sbatch</code>.</p>"},{"location":"cluster_guides/slurm_scheduler/","title":"The job scheduler and the queue","text":""},{"location":"cluster_guides/slurm_scheduler/#the-job-scheduler-and-the-queue","title":"The job scheduler and the queue","text":"<ul> <li> <p>How does the queue work?</p> </li> <li> <p>Let's look graphically at jobs presently running.</p> </li> </ul> <p></p> <ul> <li>x-axis: cores, one thread per core</li> <li> <p>y-axis: time</p> </li> <li> <p>We see some holes where we may fit jobs already!</p> </li> <li>Let's see which type of jobs that can fit!</li> </ul> <p></p> <ul> <li> <p>4 one-core jobs can run immediately (or a 4-core wide job).*</p> <ul> <li>The jobs are too long to fit at core number 9-13.</li> </ul> </li> </ul> <p></p> <ul> <li> <p>A 5-core job has to wait.*</p> <ul> <li>Too long to fit in cores 9-13 and too wide to fit in the last cores.</li> </ul> </li> <li> <p>Easiest to schedule single-threaded, short jobs</p> </li> </ul> <p>Tip</p> <ul> <li>You don't see the queue graphically, however.</li> <li>But, overall:<ul> <li>short and narrow jobs will start fast</li> <li>test and development jobs can get use of specific development nodes if they are shorter than 1 hour and uses up to two nodes.</li> <li>waste of resources unless you have a parallel program or need all the memory, e.g. 128 GB per node</li> </ul> </li> </ul>"},{"location":"cluster_guides/slurm_troubleshooting/","title":"Slurm troubleshooting","text":""},{"location":"cluster_guides/slurm_troubleshooting/#slurm-troubleshooting","title":"Slurm troubleshooting","text":"<p>When using Slurm, unexpected things may happen. This page describes Slurm errors.</p>"},{"location":"cluster_guides/slurm_troubleshooting/#1-invalid-account-or-accountpartition-combination-specified","title":"1. Invalid account or account/partition combination specified","text":""},{"location":"cluster_guides/slurm_troubleshooting/#11-full-error-message","title":"1.1. Full error message","text":"<pre><code>sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified\n</code></pre>"},{"location":"cluster_guides/slurm_troubleshooting/#12-to-reproduce","title":"1.2. To reproduce","text":"<pre><code>touch do_something.sh\necho '#!/bin/bash' &gt;&gt; do_something.sh\nsbatch -A some_invalid_account do_something.sh\n</code></pre>"},{"location":"cluster_guides/slurm_troubleshooting/#13-problem","title":"1.3. Problem","text":"<p>As stated by the error message: you've used either:</p> <ul> <li>an invalid account (for example, <code>some_invalid_account</code> in the example above)</li> <li>an invalid combination of account and partition,   for example using a Rackham account for a Snowy partition</li> </ul> <p>Or, in less formal terms, you are using a NAISS project that is not an active UPPMAX project for that UPPMAX cluster.</p>"},{"location":"cluster_guides/slurm_troubleshooting/#14-solution","title":"1.4. Solution","text":"<ul> <li>View your NAISS projects   and see if the project you used is indeed an active UPPMAX project that can   be used on the cluster you expect</li> <li>Use these in your scripts</li> </ul>"},{"location":"cluster_guides/slurm_troubleshooting/#2-invalid-project","title":"2. Invalid project","text":""},{"location":"cluster_guides/slurm_troubleshooting/#21-full-error-message","title":"2.1. Full error message","text":"<pre><code>````\n\nsbatch: error: Errors in job submission:\nsbatch: error: ERROR 1: Invalid project.\nsbatch: error: Use the flag -A to specify an active project with allocation on this cluster.\nsbatch: error: Batch job submission failed: Unspecified error```\n\n### 2.2. To reproduce\n\n```bash\nsbatch my_script.sh -A my_project\n</code></pre>"},{"location":"cluster_guides/slurm_troubleshooting/#23-problem","title":"2.3. Problem","text":"<p>The order of the arguments is incorrect. The script to be submitted must be the last argument.</p>"},{"location":"cluster_guides/slurm_troubleshooting/#24-solution","title":"2.4. Solution","text":"<pre><code>sbatch -A my_project my_script.sh\n</code></pre>"},{"location":"cluster_guides/snapshot/","title":"Snapshot","text":"","tags":["snapshot","snapshots"]},{"location":"cluster_guides/snapshot/#snapshot","title":"Snapshot","text":"<p>Besides the UPPMAX backup service, UPPMAX provides snapshot on your home folder.</p> <p>Snapshot makes a frozen \"picture\" of some file structure as it looks at the time the snapshot was taken. This allows you to restore a particular file as it was at some time point.</p> <p>Snapshots reside on the same storage system as the original data. This means that when the storage system fails catastrophically, then the snapshots are gone as well.</p> <p>Snapshots are taken on regular basis and only available for home directories.</p> <p>You can easily access snapshots in every directory by   <code>ls .snapshot</code> or <code>cd .snapshot</code> in a terminal.   The <code>.snapshot</code> is a hidden directory that does not show up in <code>ls --all</code>.</p>","tags":["snapshot","snapshots"]},{"location":"cluster_guides/snowy/","title":"Snowy","text":""},{"location":"cluster_guides/snowy/#snowy","title":"Snowy","text":"<p>Snowy is one of the UPPMAX clusters.</p> <ul> <li>Snowy's name</li> <li>Snowy's design</li> <li>Snowy's hardware</li> <li>Log in</li> <li>Submitting jobs, using Slurm</li> <li>Courses and workshops</li> </ul>"},{"location":"cluster_guides/snowy/#accounts-and-log-in","title":"Accounts and log in","text":"<p>Snowy is different from other clusters at UPPMAX in that there are no login nodes for Snowy. All access to this system is done via secure shell (a.k.a SSH) interactive login to the login node, using the domain name rackham.uppmax.uu.se</p> <pre><code>ssh -X user@rackham.uppmax.uu.se\n</code></pre> <p>See the UPPMAX page on how to get a user account page. on how to get a user account.</p> <p>For questions concerning accounts and access to Rackham and Snowy, please contact UPPMAX support.</p> <p>Note that the machine you arrive at when logged in, is only a so called login node, where you can do various smaller tasks. We have some limits in place that restricts your usage. For larger tasks you should use our batch system that pushes your jobs onto other machines within the cluster.</p> <p>All access to Snowy is done using the batch system Slurm, either as an interactive session or non-interactive batch jobs.</p>"},{"location":"cluster_guides/snowy/#using-the-batch-system","title":"Using the batch system","text":"<p>To allow a fair and efficient usage of the system we use a resource manager to coordinate user demands. On Snowy we use the Slurm resource manager, as is discussed in more detail there.</p> <p>Note: When accessing snowy from Rackhams login nodes you must\u00a0always use the flag -M for all Slurm commands. Examples:</p> <ul> <li><code>squeue</code> <code>-M snowy</code></li> <li><code>jobinfo</code> <code>-M snowy</code></li> <li><code>sbatch</code> <code>-M snowy slurm_script_file</code></li> <li><code>scancel -u username -M snowy</code></li> <li><code>interactive</code> <code>-A projectname -M snowy -p node -n 32 -t 01:00:00</code></li> </ul> <p>Note: We always recommend loading all your modules in your job script file. This is even more important when running on Snowy since the module environment is not the same on the Rackham login nodes as on Snowy compute nodes.</p>"},{"location":"cluster_guides/snowy/#some-limits","title":"Some Limits","text":"<ul> <li>There is a job walltime limit of 30\u00a0days (720 hours).</li> <li>We restrict each user to at most 5000 running and waiting jobs in total.</li> <li>Each project has a 30 days running allocation of CPU hours. We do not forbid running jobs after the allocation is overdrafted, but instead allow to submit jobs with a very low queue priority, so that you may be able to run your jobs anyway, if a sufficient number of nodes happens to be free on the system.</li> <li>Very wide jobs will only be started within a maintenance window (just before the maintenance window or at the end of the maintenance window). These are planned for the first Wednesday of each month. On Snowy a \"very wide\" job asks for 100 nodes or more.</li> </ul>"},{"location":"cluster_guides/snowy/#convenience-variables","title":"Convenience Variables","text":"<ul> <li>$SNIC_TMP - Path to node-local temporary disk space</li> </ul> <p>The $SNIC_TMP variable contains the path to a node-local temporary file directory that you can use when running your jobs, in order to get maximum disk performance (since the disks are local to the current compute node). This directory will be automatically created on your (first) compute node before the job starts and automatically deleted when the job has finished.</p> <p>The path specified in $SNIC_TMP is equal to the path: /scratch/$SLURM_JOB_ID, where the job variable $SLURM_JOB_ID contains the unique job identifier of your job.</p> <p>WARNING: Please note, that in your \"core\" (see below) jobs, if you write data in the /scratch directory but outside of the /scratch/$SLURM_JOB_ID directory, your data may be automatically deleted during your job run.</p>"},{"location":"cluster_guides/snowy/#details-about-the-core-and-node-partitions","title":"Details about the \"core\" and \"node\" partitions","text":"<p>A normal Snowy node contains 128 GB of RAM and 16\u00a0compute cores. An equal share of RAM for each core would mean that each core gets at most 8 GB of RAM. This simple calculation gives one of the limits mentioned below for a \"core\" job.</p> <p>You need to choose between running a \"core\" job or a \"node\" job. A \"core\" job must keep within certain limits, to be able to run together with up to 15 other \"core\" jobs on a shared node. A job that cannot keep within those limits must run as a \"node\" job.</p> <p>Some serial jobs must run as \"node\" jobs. You tell Slurm that you need a \"node\" job with the flag \"-p node\". (If you forget to tell Slurm, you are by default choosing to run a \"core\" job.)</p> <p>A \"core\" job:</p> <ul> <li>Will use a part of the resources on a node, from a 1/16\u00a0share to a 15/16\u00a0share of a node.</li> <li>Must specify less cores than 16, i.e.between \"-n 1\" to \"-n 15\".</li> <li>Must not demand \"-N\", \"--nodes\", or \"--exclusive\".</li> <li>Is recommended not to demand \"--mem\"</li> <li>Must not demand to run on a fat node (see below, for an explanation of \"fat\"), a devel node.</li> <li>Must not use more than 8\u00a0GB of RAM for each core it demands. If a job needs half of the RAM, i.e. 64 GB, you need to reserve also at least half of the cores on the node, i.e. 8\u00a0cores, with the \"-n 8\" flag.</li> </ul> <p>A \"core\" job is accounted on your project as one \"core hour\" (sometimes also named as a \"CPU hour\") per core you have been allocated, for each wallclock hour that it runs. On the other hand, a \"node\" job is accounted on your project as sixteen core hours for each wallclock hour that it runs, multiplied with the number of nodes that you have asked for.</p>"},{"location":"cluster_guides/snowy/#node-types","title":"Node types","text":"<p>Rackham has two node types, thin being the typical cluster node and fat nodes having double the amount of memory available normally (256 GB). You may specify a node with more RAM, by adding the words \"-C mem256GB\" or \"-C fat\" to your job submission line and thus making sure that you will get 256 GB of RAM on each node in your job. If you need even more memory you can request a 512 GB\u00a0memory node by adding \"-C mem512GB\". Please note that there are only 13\u00a0nodes with 256GB and 17\u00a0with 512GB of RAM.</p> <p>To request a fat node, use -C mem256GB or -C fat in your Slurm command.</p> <p>To request the fattest nodes, use -C mem512GB in your Slurm command.</p>"},{"location":"cluster_guides/snowy/#file-storage-and-disk-space","title":"File storage and disk space","text":"<p>At UPPMAX we have a few different kinds of storage areas for files, see Disk Storage User Guide for more information and recommended use.</p>"},{"location":"cluster_guides/snowy/#very-long-jobs","title":"Very Long jobs","text":"<p>If you have very long jobs that require more than 10 days of CPU-time. We recommend using Snowy. But in order for our job to successfully run for several weeks you should implement the following;</p> <ul> <li>Use only local disk for your job. Copy all input and data files needed to $SNIC_TMP at the start of your job and at the end, copy all output back to your project directory.</li> <li>Book a full node with the Slurm flags -p node\u00a0(you won't be able to submit these jobs in the core partition).</li> <li>If\u00a0possible make sure you do not rely on files stored outside the node. One way of achieving this may be to copy program files to\u00a0$SNIC_TMP.</li> </ul> <p>Even if you do this we can't promise that a 20 or 30 day long job will finish without being interrupted by the global file systems, the network or problems on the node,</p>"},{"location":"cluster_guides/snowy/#snowys-design","title":"Snowy's design","text":"<p>Snowy is an (general-purpose) high-performance computing (HPC) cluster, with GPUs and suitable for longer jobs.</p> What is an HPC cluster? <p>See the UPPMAX page about HPC clusters.</p> <p>Or: Snowy is a group of computers that can effectively run many calculations, as requested by multiple people, at the same time. Snowy runs the Linux operating system and all users need some basic Linux knowledge to use Snowy.</p> <p>Additionally, Snowy has GPUs and allows for jobs running for maximally 30 days.</p> <p>Snowy does not have a login node. Instead, it uses a login node on Rackham.</p> Using Linux <p>Using Linux (and especially the so-called command-line/terminal) is essential to use Snowy. See the UPPMAX page about Linux.</p>"},{"location":"cluster_guides/snowy/#snowys-system-configuration","title":"Snowy's system configuration","text":"<p>Snowy consists of 228 compute servers (nodes) where each compute server consists of two 8-core Xeon E5-2660 processors running at 2.2 GHz. We provide 198\u00a0nodes with 128 GB memory\u00a0(<code>s1-s120</code>, <code>s151-s228</code>), 13 nodes with 256 GB (<code>s138-s150</code>) and\u00a017\u00a0nodes with 512\u00a0GB (<code>s121-s137</code>). All nodes are interconnected with a 2:1 oversubscribed FDR (40\u00a0GB/s) InfiniBand fabric. In total Snowy provides 3548\u00a0CPU cores in compute nodes.</p>"},{"location":"cluster_guides/snowy/#compiling-on-snowy","title":"Compiling on Snowy","text":"<p>There are several compilers available through the module system on Snowy. This gives you flexibility to obtain programs that run optimally on Snowy.</p> <ul> <li>gcc - the newest version usually generates the best code, if you tell it to use the new instructions. Check which version is the newest by doing module avail.   The compiler executable is named gcc for C, g++ for C++, and gfortran for Fortran.   To use the new instructions available on Snowy (AVX2 and FMA3), give the additional options \"-mavx2 -mfma3\" to gcc. For good performance with this compiler you should also specify optimization at least at level -O2 or -O3. Also try using -march=broadwell for GCC &gt;= 4.9.0 or -march=core-avx2 for GCC 4.8.x, which will enable all the instructions on the CPU.</li> <li>Intel+MKL - usually generates the fastest code. As with gcc, it is good to use the latest version. The compiler executable is named icc for C, icpc for C++, and ifort for Fortran. You should give optimization options at least -O2, preferably -O3 or -fast. You can also try to use the -xCORE-AVX2 option to the compiler to output AVX2 instructions.</li> <li>pgi - often generates somewhat slower code, but it is stable so often it is easier to obtain working code, even with quite advanced optimizations. The compiler executable is named pgcc for C, pgCC for C++, and pgfortran, pgf77, pgf90, or pgf95 for Fortran. For this compiler, you can generate code for Snowy using the following options \"UPDATES NEEDED\". Also give optimization options at least -O2, preferably -Ofast, even though the compile times are much longer, the result is often worth the wait.</li> </ul>"},{"location":"cluster_guides/snowys_design/","title":"Snowy's design","text":""},{"location":"cluster_guides/snowys_design/#snowys-design","title":"Snowy's design","text":"<p>Snowy is an (general-purpose) high-performance computing (HPC) cluster, with GPUs and suitable for longer jobs.</p> What is an HPC cluster? <p>See the UPPMAX page about our HPC clusters.</p> <p>Or: Snowy is a group of computers that can effectively run many calculations, as requested by multiple people, at the same time. Snowy runs the Linux operating system and all users need some basic Linux knowledge to use Snowy.</p> <p>Additionally, Snowy has GPUs and allows for jobs running for maximally 30 days.</p> <p>Snowy does not have a login node. Instead, it uses a login node on Rackham.</p> Using Linux <p>Using Linux (and especially the so-called command-line/terminal) is essential to use Snowy. See the UPPMAX page with the most important Linux commands.</p>"},{"location":"cluster_guides/snowys_design/#snowys-system-configuration","title":"Snowy's system configuration","text":"<p>Snowy consists of 228 compute servers (nodes) where each compute server consists of two 8-core Xeon E5-2660 processors running at 2.2 GHz. We provide 198\u00a0nodes with 128 GB memory\u00a0(<code>s1-s120</code>, <code>s151-s228</code>), 13 nodes with 256 GB (<code>s138-s150</code>) and\u00a017\u00a0nodes with 512\u00a0GB (<code>s121-s137</code>). All nodes are interconnected with a 2:1 oversubscribed FDR (40\u00a0GB/s) InfiniBand fabric. In total Snowy provides 3548\u00a0CPU cores in compute nodes.</p>"},{"location":"cluster_guides/snowys_name/","title":"Snowy's name","text":""},{"location":"cluster_guides/snowys_name/#snowys-name","title":"Snowy's name","text":"<p>Snowy, like most UPPMAX clusters between 2013 and 2025, is named after a Tintin character, in this case after Snowy, Tintin's dog:</p> <p></p> What are the UPPMAX clusters? <p>See our UPPMAX clusters.</p>"},{"location":"cluster_guides/software_on_transit/","title":"Software on Transit","text":""},{"location":"cluster_guides/software_on_transit/#software-on-transit","title":"Software on Transit","text":"<p>Transit is an UPPMAX service that can be used to securely transfer files.</p> <p>This page describes the software on Transit.</p> <p>After logging in to Transit, you cannot make lasting changes to anything, except for mounted wharf directories. However, anything you have added to your Rackham home directory is available on Transit.</p> <p>In addition, some modules are available.</p> <ul> <li>SciLifeLab Data Delivery System - https://delivery.scilifelab.se/</li> </ul> <pre><code># Load the tool from the software module tree\nmodule load bioinfo-tools dds-cli\n\n# Run the tool\ndds\n</code></pre> <p></p> <p>To download data from TCGA, log in to Rackham and install the GDC client to your home directory. Then log in to Transit, mount the wharf, and run <code>./gdc-client</code>.</p> <p>2FA on transit</p> <p>If you connect from abroad and you are asked for the 2FA (two factor authentication), there is a grace period (about 5 minutes) in which you can <code>ssh</code>/<code>scp</code>/<code>rsync</code>/<code>sftp</code> to transit without the need for 2FA. This allows you to use these and other tools that might experience problems with the 2FA.</p>"},{"location":"cluster_guides/spirula/","title":"Object-storage Spirula","text":""},{"location":"cluster_guides/spirula/#spirula","title":"Spirula","text":"<p>Spirula is a DDLS-funded SciLifeLab FAIR Data Storage system which exposes 10 PB of raw S3 compatible object storage (or blob storage) through Ceph Object Gateway, also known as RADOS Gateway (RGW).</p> <ul> <li>Spirula is not in production.</li> <li>There is no backup of any data stored on Spirula.</li> <li>You should not store any primary data on Spirula.</li> <li>Spirula does not expose any public endpoints.</li> </ul> <p>See storage systems for other storage systems at UPPMAX.</p>"},{"location":"cluster_guides/spirula/#support","title":"Support","text":"<p>The responsibility for Spirula is shared between UPPMAX and SciLifeLab.</p> <p>Spirula is accessible from within UPPMAX and from certain whitelisted SciLifeLab addresses. Access to Rackham/Pelle is not granted for projects with Spirula allocations, in cases where you don't have access to Rackham/Pelle through some other means please refer to FAIRstorage@scilifelab.se for more information.</p> <p>In general follow these guidelines when requesting support.</p> <p>Contact SciLifeLab DC at FAIRstorage@scilifelab.se for issues regarding:</p> <ul> <li>Project grants, allocations and quota in SUPR.</li> <li>Access to Spirula through SciLifeLab addresses.</li> <li>FAIR Storage and the intended use-cases for Spirula.</li> </ul> <p>Contact UPPMAX Support at support@uppmax.uu.se for issues regarding:</p> <ul> <li>Retrieving access tokens and/or quota from the SSH service.</li> <li>Technical implementation and/or limitations of the system.</li> </ul>"},{"location":"cluster_guides/spirula/#usage","title":"Usage","text":""},{"location":"cluster_guides/spirula/#access-tokens-and-displaying-usage-quota","title":"Access tokens and displaying usage quota","text":"<p>If you are able to <code>ping</code> and <code>ssh</code> to <code>s3.spirula.uppmax.uu.se</code> you can interact with UPPMAX's SSH to S3 credential service. You will login the service using your UPPMAX user account and second factor authentication.</p> <pre><code>ssh s3.spirula.uppmax.uu.se\nPassword: ********\nTwo factor (UPPMAX): 123456\n _   ________________  ___  ___  __   __      _____       _            _\n| | | | ___ \\ ___ \\  \\/  | / _ \\ \\ \\ / /     /  ___|     (_)          | |\n| | | | |_/ / |_/ / .  . |/ /_\\ \\ \\ V /______\\ `--. _ __  _ _ __ _   _| | __ _\n| | | |  __/|  __/| |\\/| ||  _  | /   \\______|`--. \\ '_ \\| | '__| | | | |/ _` |\n| |_| | |   | |   | |  | || | | |/ /^\\ \\     /\\__/ / |_) | | |  | |_| | | (_| |\n \\___/\\_|   \\_|   \\_|  |_/\\_| |_/\\/   \\/     \\____/| .__/|_|_|   \\__,_|_|\\__,_|\n                                                   | |\n                                                   |_|\nYou have 2 available credentials. Please select which credential to use:\n0: User tintin in sll1234001\n1: Project user for sll1234001\n</code></pre> <p>After selecting which credentials you want to use you can either retrieve your access credentials or view your current usage and quota.</p> <pre><code>SSH to S3 credentials service - v0.2\n-------------------------------------------\nYou have several options. Please select what you want to do.\n1: Retrieve S3 Access Credentials\n2: View Current Quota\n</code></pre>"},{"location":"cluster_guides/spirula/#configuring-a-s3-client","title":"Configuring a S3 client","text":"<p>Spirula through the Ceph Object Gateway supports multiple S3 compliant clients such as <code>aws cli</code> or <code>s3cmd</code>. Setup differs between tools, adopt the configuration below to your particular needs.</p>"},{"location":"cluster_guides/spirula/#aws-cli","title":"AWS CLI","text":"<p>For setting up AWS command line interface <code>aws cli</code> to use Spirula you can create or update the <code>~/.aws/config</code> configuration file with the following options:</p> <pre><code># ~/.aws/config\n[default]\nregion = None\nendpoint_url = https://s3.spirula.uppmax.uu.se:8443\n</code></pre> <p>Full usage instructions and configuration options for <code>aws cli</code>  are available through AWS documentation.</p>"},{"location":"cluster_guides/spirula/#example-of-using-aws-cli","title":"Example of using AWS CLI","text":"<p>After retrieving your credentials using the SSH to S3 credential service, you can store them e.g. in a credentials file in your AWS CLI configuration directory, as per the following instructions in the AWS documentation.</p> <p>With that done, you can access your storage area on Spirula using the AWS CLI. To see which areas you have access to, run <code>aws s3 ls</code> and you should see something like:</p> <pre><code>aws s3 ls\n2023-11-29 13:07:57 testuser-nobackup-insecure\n2025-03-06 10:44:15 sll2021001-testuser-nobackup\n</code></pre> <p>To view the contents of an area, run e.g. <code>aws s3 ls sll2021001-testuser-nobackup</code>.</p> <p>Suppose you have a file called <code>testfile</code> in your current working directory. To upload it, run:</p> <pre><code>aws s3 cp ./testfile s3://sll2021001-testuser-nobackup/\nupload: ./testfile to s3://sll2021001-testuser-nobackup/testfile \n</code></pre> <p>For more information about basic <code>aws s3</code> commands, see the AWS documentation.</p>"},{"location":"cluster_guides/start_interactive_session/","title":"Start an interactive session","text":"","tags":["start","interactive","session"]},{"location":"cluster_guides/start_interactive_session/#starting-an-interactive-session","title":"Starting an interactive session","text":"<p>Below we describe the general ideas of using an interactive session:</p> <ul> <li>the types of nodes</li> <li>When to use an interactive session</li> </ul> <p>To start an interactive session on specific cluster:</p> <ul> <li>Start an interactive session on Bianca</li> <li>Start an interactive session on Rackham</li> <li>Start an interactive session on Snowy</li> </ul>","tags":["start","interactive","session"]},{"location":"cluster_guides/start_interactive_session/#types-of-nodes","title":"Types of nodes","text":"<p>The UPPMAX HPC clusters have three types of nodes:</p> What are nodes? <p>See the UPPMAX page about nodes.</p> <ul> <li>login nodes:   nodes where a user enters and interacts with the system</li> <li>calculation nodes: nodes that do the calculations</li> </ul> Requesting a calculation to run <p>Requesting a calculation is described at the UPPMAX page about our job scheduler.</p> How can I find out on which node I am? <p>In a terminal, type <code>hostname</code>:</p> <ul> <li>the login node   has name <code>rackham[number]</code>,   where <code>[number]</code> is the number of the login node</li> <li>an interactive session has name <code>r[number]</code>,   where <code>[number]</code> is the compute node number</li> </ul> <p>As a login is shared with all users, there is a simple rule to use it fairly:</p> <p>Only do short and light things on the login node</p> <p>Examples of short and light things are:</p> <ul> <li>Editing files</li> <li>Copying, deleting, moving files</li> <li>Scheduling jobs</li> <li>Starting an interactive session</li> </ul> <p>Examples of heavy things are:</p> <ul> <li>Running code with big calculations</li> <li>Develop code with big calculations line-by-line</li> </ul> Develop code with big calculations line-by-line  <p>This usage is typically done an interactive session</p>","tags":["start","interactive","session"]},{"location":"cluster_guides/start_interactive_session/#when-to-use-an-interactive-session","title":"When to use an interactive session","text":"<p>Some users develop computer code on an HPC cluster in a line-by-line fashion. These users typically want to run a (calculation-heavy) script frequently, to test if the code works.</p> <p>However, scheduling each new line is too slow, as it can take minutes before the new code is run. Instead, there is a way to directly work with such code: use an interactive session.</p> <p>An interactive session is a type of calculation node, where one can run heavy calculations directly.</p> <pre><code>flowchart TD\n    UPPMAX(What to run on which node?)\n    operation_type[What type of operation/calculation?]\n    interaction_type[What type of interaction?]\n    login_node(Work on login node)\n    interactive_session(Work on interactive session)\n    calculation_node(Schedule for calculation node)\n\n    UPPMAX--&gt;operation_type\n    operation_type--&gt;|light,short|login_node\n    operation_type--&gt;|heavy,long|interaction_type\n    interaction_type--&gt;|Direct|interactive_session\n    interaction_type--&gt;|Indirect|calculation_node</code></pre>","tags":["start","interactive","session"]},{"location":"cluster_guides/start_interactive_session_on_bianca/","title":"Starting an interactive session on Bianca","text":"","tags":["start","interactive","session","Bianca"]},{"location":"cluster_guides/start_interactive_session_on_bianca/#starting-an-interactive-session-on-bianca","title":"Starting an interactive session on Bianca","text":"<p>This page describes how to start an interactive session on Bianca. See the general information on starting an interactive session on how to do so in general.</p> Prefer a video? <p>See the video Starting an interactive session on Bianca</p> <p>To use an interactive session, in a terminal, type:</p> <pre><code>interactive -A [project name] -n [number_of_cores] -t [session_duration]\n</code></pre> <p>Where</p> <ul> <li><code>[project name]</code> is your project name, for example <code>sens2023598</code></li> <li><code>[number_of_cores]</code> is the number of core, for example <code>2</code></li> <li><code>[session_duration]</code> is the duration of the session, for example <code>8:00:00</code>   for eight hours</li> </ul> <p>For example:</p> <pre><code>interactive -A sens2023598 -n 2 -t 8:00:00\n</code></pre> <p>This starts an interactive session using project <code>sens2023598</code> that uses 2 cores and has a maximum duration of 8 hours.</p> <p>The script interactive has many same arguments as sbatch.</p> Forgot your Bianca projects? <p>One easy way to see your Bianca projects is to use the Bianca remote desktop login screen at https://bianca.uppmax.uu.se/.</p> <p></p> <p>Has Bianca frozen?</p> <p>It can take tens of minutes before an interactive session is allocated.</p> <p>Bianca has not frozen, go ahead and have a coffee break :-)</p> <p>You can see the regular timings at cluster speeds.</p>","tags":["start","interactive","session","Bianca"]},{"location":"cluster_guides/start_interactive_session_on_pelle/","title":"Starting an interactive session on Pelle","text":"","tags":["start","interactive","session","Pelle"]},{"location":"cluster_guides/start_interactive_session_on_pelle/#starting-an-interactive-session-on-pelle","title":"Starting an interactive session on Pelle","text":"<p>This page describes how to start an interactive session on Pelle, unlike the general information on starting an interactive session.</p> <p>To use an interactive session, in a terminal, type:</p> <pre><code>interactive -A [project name]\n</code></pre> <p>For example:</p> <pre><code>interactive -A uppmax2023-2-25\n</code></pre> <p>This starts an interactive session using project <code>uppmax2023-2-25</code> that has a default duration of 1 hour.</p> Forgot your Pelle project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>An example of the Pelle project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Pelle, has a slightly different name: the account name to use on Pelle is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> <p>To increase the duration of the interactive session, add the use of <code>-t</code>:</p> <pre><code>interactive -A [project name] -t [session_duration]\n</code></pre> <p>For example:</p> <pre><code>interactive -A uppmax2023-2-25 -t 8:00:00\n</code></pre> <p>This starts an interactive session using project <code>uppmax2023-2-25</code> that has a maximum duration of 8 hours.</p> <p>To reach high-memory or GPU nodes</p> <p>See Partitions on Pelle</p> Has Pelle frozen? <p>It can take tens of seconds before an interactive session is allocated.</p> <p>Pelle has not frozen, just be a bit more patient.</p> <p>To stop the session, do:</p> <pre><code>exit\n</code></pre> <p>This will take you back to the login node.</p>","tags":["start","interactive","session","Pelle"]},{"location":"cluster_guides/start_interactive_session_on_rackham/","title":"Starting an interactive session on Rackham","text":"","tags":["start","interactive","session","Rackham"]},{"location":"cluster_guides/start_interactive_session_on_rackham/#starting-an-interactive-session-on-rackham","title":"Starting an interactive session on Rackham","text":"<p>This page describes how to start an interactive session on Rackham, unlike the general information on starting an interactive session.</p> <p>To use an interactive session, in a terminal, type:</p> <pre><code>interactive -A [project name]\n</code></pre> <p>For example:</p> <pre><code>interactive -A uppmax2023-2-25\n</code></pre> <p>This starts an interactive session using project <code>uppmax2023-2-25</code> that has a default duration of 1 hour.</p> Forgot your Rackham project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>An example of the Rackham project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Rackham, has a slightly different name: the account name to use on Rackham is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> <p>To increase the duration of the interactive session, add the use of <code>-t</code>:</p> <pre><code>interactive -A [project name] -t [session_duration]\n</code></pre> <p>For example:</p> <pre><code>interactive -A uppmax2023-2-25 -t 8:00:00\n</code></pre> <p>This starts an interactive session using project <code>uppmax2023-2-25</code> that has a maximum duration of 8 hours.</p> <p>Has Rackham frozen?</p> <p>It can take tens of seconds before an interactive session is allocated.</p> <p>Rackham has not frozen, just be a bit more patient.</p> <p>To stop the session, do:</p> <pre><code>exit\n</code></pre> <p>This will take you back to the login node.</p>","tags":["start","interactive","session","Rackham"]},{"location":"cluster_guides/start_interactive_session_on_snowy/","title":"Starting an interactive session on Snowy","text":"","tags":["start","interactive","session","Snowy"]},{"location":"cluster_guides/start_interactive_session_on_snowy/#starting-an-interactive-session-on-snowy","title":"Starting an interactive session on Snowy","text":"<p>This page describes how to start an interactive session on Snowy, unlike the general information on starting an interactive session.</p> <p>To start an interactive session, in a terminal, type:</p> <pre><code>interactive -A [project name] -M snowy\n</code></pre> <p>For example:</p> <pre><code>interactive -A uppmax2023-2-25 -M snowy\n</code></pre> <p>This starts an interactive session using project <code>uppmax2023-2-25</code> that has a default duration of 1 hours.</p> Forgot your Snowy project? <p>One can go to the SUPR NAISS pages to see one's projects,</p> <p></p> <p>Example of the Snowy project called 'UPPMAX 2023/2-25'</p> <p>On the SUPR NAISS pages, projects are called 'UPPMAX [year]/[month]-[day]', for example, 'UPPMAX 2023/2-25'. The UPPMAX project name, as to be used on Snowy, has a slightly different name: the account name to use on Snowy is <code>uppmax[year]-[month]-[day]</code>, for example, <code>uppmax2023-2-25</code></p> <p>To increase the duration of the interactive session, add the use of <code>-t</code>:</p> <pre><code>interactive -A [project name] -M snowy -t [session_duration]\n</code></pre> <p>For example:</p> <pre><code>interactive -A uppmax2023-2-25 -M snowy -t 8:00:00\n</code></pre> <p>This starts an interactive session using project <code>uppmax2023-2-25</code> that has a maximum duration of 8 hours.</p> <p>Has Snowy frozen?</p> <p>It can take tens of seconds before the computer core(s)/node(s) are allocated.</p> <p>Snowy has not frozen, just be a bit more patient.</p>","tags":["start","interactive","session","Snowy"]},{"location":"cluster_guides/storage_compression/","title":"Storage and compression","text":"","tags":["storage","compression"]},{"location":"cluster_guides/storage_compression/#storage-and-compression","title":"Storage and compression","text":"","tags":["storage","compression"]},{"location":"cluster_guides/storage_compression/#storage","title":"Storage","text":"<p>Disk storage guide</p> How does automatic backup of project areas work at UPPMAX? <p>Backup</p> What is this 'glob' folder in my home folder? <ul> <li>The glob directory found in your home has been deprecated since early 2017.</li> <li>It is now a normal directory and shared your default 32GByte sized home.</li> <li> <p>The glob directory remains to not interfere with scripts who might reference ~/glob in the source code.</p> </li> <li> <p>Historically, the glob directory was the main storage area for storage of user data.</p> <ul> <li>It was shared by all nodes.</li> <li>The directory was used for files needed by all job instances and could house files exceeding the quota of the home directory.</li> <li>Job input and output files was (and can still be) stored here.</li> </ul> </li> <li> <p>You might also be interested in our disk storage guide.</p> </li> </ul> Disk quota exceeded when copying data <p>The problem is that if you have data in a project directory, e.g. <code>/proj/snic2017-1-999</code>, and are copying the data to another project directory, e.g. <code>/proj/uppstore2017-999</code>, then you may get a \"disk quota exceeded\" error.</p> <p>This happens when your (snic2017-1-999) project quota is almost full and you're copying the data without changing the group ownership of the files. Even though the destination folder is owned by a project with sufficient quota, the files will for a short time be owned by the original project. By copying the files, the earlier project's disk usage is increased and the quota is exceeded.</p> <p>The solution is one of these options:</p> <ol> <li>Use <code>mv</code> instead of <code>cp</code></li> <li>Give the flag <code>--no-g</code> to <code>rsync</code> to set the group ownership of the destination files to that of the destination directory</li> <li>Use <code>newgroup [the-group-i-want]</code> to change the group ownership of the files first, then <code>rsync -rlpt /old-location /new-location</code></li> </ol> <p>Explanation:</p> <pre><code>`-r` is for recursive\n`-l` is to preserve links\n`-p` is to preserve permissions\n`-t` is to preserve times\n</code></pre>","tags":["storage","compression"]},{"location":"cluster_guides/storage_compression/#compression","title":"Compression","text":"File compression guide <p>Compression guide</p> How can I compress my files as quickly and efficiently as possible? <ul> <li> <p>You can use this SBATCH script [1] to run the compression in parallel as a node job, with a parallel version of the highly efficient bzip2 compression software.</p> </li> <li> <p>Remember to modify the appropriate #SBATCH parameters at the top of the file, according to your project, and the estimated time to compress your files.</p> </li> <li> <p>[1] Thanks Roman Valls Guimera, for contributing this script.</p> </li> </ul> How should I compress FastQ-format files? <p>Compress FastQ</p> Which compression format should I use for NGS-related files (FastQ, Fasta, VCF, GFF, etc.)? <p>Compression format</p>","tags":["storage","compression"]},{"location":"cluster_guides/system_status/","title":"System status","text":"","tags":["system","status","down","up"]},{"location":"cluster_guides/system_status/#system-status","title":"System status","text":"<p>The UPPMAX system status shows the current status of our HPC clusters.</p> <p>View the UPPMAX system status page</p>","tags":["system","status","down","up"]},{"location":"cluster_guides/transfer_bianca/","title":"Transfer to/from Bianca","text":""},{"location":"cluster_guides/transfer_bianca/#file-transfer-tofrom-bianca","title":"File transfer to/from Bianca","text":"<pre><code>flowchart LR\n  subgraph sunet[SUNET]\n    subgraph bianca[Bianca]\n      wharf\n    end\n    transit[transit server]\n    sftp_server[SFTP server]\n    user[User in SUNET or user on Rackham or user on other NAISSS clusters]\n    wharf &lt;--&gt; transit\n    wharf &lt;--&gt; sftp_server\n    transit &lt;--&gt; user\n    sftp_server &lt;--&gt; user\n  end</code></pre> <p>File transfer is the process of getting files from one place to the other. This page shows how to do file transfer to/from the Bianca UPPMAX cluster.</p> <p>For all file transfer on Bianca:</p> <ul> <li>The user needs to be inside of SUNET</li> <li>The files are moved from/to the <code>wharf</code> folder</li> </ul>"},{"location":"cluster_guides/transfer_bianca/#file-transfer-methods","title":"File transfer methods","text":"<p>There are multiple ways to transfer files to/from Bianca:</p> Method Features Using a graphical program Graphical interface, intuitive, for small amounts of data only Using <code>rsync</code> Terminal, recommended Using <code>sftp</code> Terminal, easy to learn, can use terminal commands to select files Using <code>lftp</code> Terminal Transit server from/to Rackham, see below Terminal, can be used to transfer data between clusters in general Mounting <code>wharf</code> on your local computer Both graphical and terminal, need a computer with <code>sshfs</code> installed"},{"location":"cluster_guides/transfer_bianca/#using-a-graphical-program","title":"Using a graphical program","text":"<p>FileZilla connected to Bianca</p> <p>To transfer files to/from Bianca one can use a graphical tool, such as FileZilla and WinSCP. See Bianca file transfer using a graphical program for details.</p>"},{"location":"cluster_guides/transfer_bianca/#using-sftp","title":"Using <code>sftp</code>","text":"<p><code>sftp</code> is a terminal SFTP client to transfer files to/from Bianca. See Bianca file transfer using sftp.</p>"},{"location":"cluster_guides/transfer_bianca/#using-lftp","title":"Using <code>lftp</code>","text":"<p><code>sftp</code> is a terminal SFTP client to transfer files to/from Bianca. See Bianca file transfer using lftp.</p>"},{"location":"cluster_guides/transfer_bianca/#using-rsync","title":"Using <code>rsync</code>","text":"<p>rsync is a terminal program to transfer files to/from Bianca. See Bianca file transfer using rsync.</p>"},{"location":"cluster_guides/transfer_bianca/#transit-server","title":"Transit server","text":"<p>To facilitate secure data transfers to, from, and within the system for computing on sensitive data a special service is available via SSH at <code>transit.uppmax.uu.se</code>.</p> <p></p> <p>See the UPPMAX documentation on the Transit server.</p> <ul> <li> <p>Note that your home directory is mounted read-only, any changes you do to your \"local\" home directory (on transit) will be lost upon logging out.</p> </li> <li> <p>You can use commands like <code>rsync</code>, <code>scp</code> to fetch data and transfer it to your bianca wharf.</p> <ul> <li>You can use cp to copy from Rackham to the wharf</li> </ul> </li> <li>Remember that you cannot make lasting changes to anything except for mounted wharf directories. Therefore you have to use rsync and scp to transfer from the <code>wharf</code> to Rackham.</li> <li>The mounted directory will be kept for later sessions.</li> </ul>"},{"location":"cluster_guides/transfer_bianca/#moving-data-from-transit-to-rackham","title":"Moving data from transit to Rackham","text":"<ul> <li>On Rackham: (or other computer) copy files to Bianca via transit:</li> </ul> <pre><code># scp\nscp path/my_files my_user@transit.uppmax.uu.se:sens2023531/\n\n# rsync\nrsync -avh path/my_files my_user@transit.uppmax.uu.se:sens2023531/\n</code></pre> <ul> <li>On transit: copy files to Bianca from Rackham (or other computer)</li> </ul> <pre><code># scp\nscp my_user@rackham.uppmax.uu.se:path/my_files ~/sens2023531/\n\n# rsync\nrsync -avh my_user@rackham.uppmax.uu.se:path/my_files ~/sens2023531/\n</code></pre> <pre><code>:book:  `rsync` [tutorial](https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories) for beginners.\n</code></pre> <p> Keep in mind that project folders on Rackham are not available on transit.</p>"},{"location":"cluster_guides/transfer_bianca/#moving-data-between-projects","title":"Moving data between projects","text":"<ul> <li>You can use transit to transfer data between projects   by mounting the wharfs for the different projects   and transferring data with <code>rsync</code>.</li> <li>Note that you may only do this if this is allowed   (agreements, permissions, etc.)</li> </ul>"},{"location":"cluster_guides/transfer_bianca/#mounting-wharf-on-your-local-computer","title":"Mounting <code>wharf</code> on your local computer","text":"<p>Mounting <code>wharf</code> means that a <code>wharf</code> folder is added to the filesystem of your local computer, after which you can use it like any other folder.</p> <p>See the UPPMAX documentation of <code>wharf</code> on how to do so.</p> <p>Summary</p> <ul> <li>For simple transfers use SFTP to connect to <code>bianca-sftp.uppmax.uu.se</code> - use command line <code>sftp</code> or tools that support SFTP protocol.</li> <li>For <code>rsync</code> - sync files to pre-mounted wharf folder from Rackham or secure local computer.</li> <li>Keep in mind that project folders on Rackham are not available on transit.</li> </ul>"},{"location":"cluster_guides/transfer_bianca/#bianca-file-transfer-as-image","title":"Bianca file transfer as image","text":""},{"location":"cluster_guides/transfer_dardel/","title":"File transfer to/from Dardel","text":""},{"location":"cluster_guides/transfer_dardel/#file-transfer-tofrom-dardel","title":"File transfer to/from Dardel","text":"<p>This page describes how to transfer files to Dardel, the HPC cluster at PDC in Stockholm.</p>"},{"location":"cluster_guides/transfer_dardel/#why-do-i-need-this","title":"Why do I need this?","text":"<p>The Rackham cluster will be decommissioned at the end of 2024 so all projects have to migrate their data and calculations to other resources. The plan from NAISS is that all Rackham users will move to the Dardel cluster at PDC.</p>"},{"location":"cluster_guides/transfer_dardel/#how-do-i-do-this","title":"How do I do this?","text":"<p>First, we are here to help. Please contact support if you run into problems when trying the guide below.</p> <p>To transfer your files to Dardel, follow the steps below.</p> <pre><code>flowchart TD\n  get_supr_project[1 Access to a SUPR project with Dardel]\n  get_pdc_account[2 Access to a PDC account]\n  create_ssh_key[3 Create SSH key pair on Rackham]\n  add_ssh_key[4 Add public SSH key to PDC Login Portal]\n  transfer_files[5 Transfer files to Dardel]\n\n  get_supr_project --&gt; |requires| get_pdc_account\n  create_ssh_key --&gt; |requires| add_ssh_key\n  get_pdc_account --&gt; |requires| add_ssh_key\n  add_ssh_key --&gt; |requires| transfer_files</code></pre>"},{"location":"cluster_guides/transfer_dardel/#3-create-an-ssh-key-pair","title":"3. Create an SSH key pair","text":"<p>How to create an SSH key pair is described in detail at the PDC page on how to create an SSH key pair.</p> <p>On Rackham, do:</p> <pre><code># generate the key\nssh-keygen -t ed25519 -N \"\" -f ~/.ssh/id_ed25519_pdc\n</code></pre> <p>and you have created a SSH key pair.</p> How do I know this worked? <p>On Rackham, in a terminal, type:</p> <pre><code>$ cat ~/.ssh/id_ed25519_pdc.pub\n</code></pre> <p>This will show a text similar to:</p> <pre><code>ssh-ed25519 AAAA63Nz1C1lZkI1NdE5ABAAIA7RHe4jVBRTEvHVbEYxV8lnOQl22N+4QcUK+rDv1gPS user@rackham2.uppmax.uu.se\n</code></pre>"},{"location":"cluster_guides/transfer_dardel/#5-add-the-public-ssh-key-to-pdcs-login-portal","title":"5. Add the public SSH key to PDC:s Login Portal","text":"<p>How to add the SSH public key is described in detail in the PDC documentation on how to add SSH keys.</p> <p>You will need to get the public part of the key in order to complete this step.i On Rackham, in a terminal, type:</p> <pre><code>cat ~/.ssh/id_ed25519_pdc.pub\n</code></pre> <p>This will show a text similar to:</p> <pre><code>ssh-ed25519 AAAA63Nz1C1lZkI1NdE5ABAAIA7RHe4jVBRTEvHVbEYxV8lnOQl22N+4QcUK+rDv1gPS user@rackham2.uppmax.uu.se\n</code></pre> <p>Select and copy that text, it is the public key you will add.</p> <p>In short,</p> <ol> <li>Open the PDC Login Portal</li> <li>Follow the instructions there to login.</li> <li>Click on the <code>Add new key</code> link.</li> <li>Paste the public key you copied after running the <code>cat</code> command above.</li> <li>Make up a name for the key so you know which computer it resides on. E.g. <code>rackham-darsync</code></li> <li>Press the <code>Save</code> button.</li> </ol> How does the adding the key look like? <p></p> <p>Click on 'Prove Indentity'</p> <p></p> <p>PDC key managements before any keys are added.</p> <p></p> <p>How it looks when adding a new key.</p> <p>After having added your public SSH key, you will be able to see your registered keys.</p> How does that look like? <p></p> <p>Here we see that there is an SSH key added.</p> <p>The next thing you have to do is to add UPPMAX as a placer permitted to use your newly added key. Do that by pressing the <code>Add address</code> link for the key you just added. At the bottom of the form you have a section called <code>Custom domain</code>. Add <code>*.uppmax.uu.se</code> in that field and press <code>Save</code>.</p> How does that look like? <p></p> <p>This is where you enter that UPPMAX is allowed to use this key.</p> For staff only <p>@Richel, need a screenshot of adding custom domain.</p> <p>To validate that it work you can connect to Dardel via SSH:</p> <pre><code># replace your_dardel_username with your actual Dardel username\nssh -i ~/.ssh/id_ed25519_pdc your_dardel_username@dardel.pdc.kth.se\n</code></pre> For staff only <p>@Richel, need a screenshot of ssh working</p>"},{"location":"cluster_guides/transfer_dardel/#6-transfer-files","title":"6. Transfer files","text":"<p>To facilitate this move we have created Darsync, a tool that can inspect your files and make suggestions to make the transfer easier, as well as generating a script file you can submit to Slurm to perform the actual file transfer. Read more about how to use Darsync here.</p> <p>Here is a summary of how to run it, using <code>/path/to/dir</code> as a placeholder for the actual path to the directory you want to copy to Dardel:</p> <pre><code>module load darsync\n\ndarsync check --local-dir /path/to/dir\n# fix any errors the check step found\ndarsync gen --local-dir /path/to/dir --outfile ~/dardel_transfer_script.sh\n</code></pre>"},{"location":"cluster_guides/transfer_dardel/#6-submit-the-script-created-by-darsync","title":"6. Submit the script created by Darsync","text":"<p>Submit the transfer script created by Darsync to Slurm:</p> <pre><code>sbatch --output=~/dardel_transfer.out --error=~/dardel_transfer.err ~/dardel_transfer_script.sh\n</code></pre>"},{"location":"cluster_guides/transfer_dardel/#7-check-logs","title":"7. Check logs","text":"<p>Once the submitted job has finished, have a look at the log file produced by the job and make sure it did not end in a error message.</p> <pre><code>tail ~/dardel_transfer.out\ntail ~/dardel_transfer.err\n</code></pre> For staff only <p>@Richel, need a screenshot of successful rsync command, as well as a failed one?</p> <p>If there are any errors you can either run <code>darsync gen</code> again and correct any mistakes you made and submit the new script file.</p> <p>If you have updated your files at UPPMAX and want to sync over the changes, just submit the same script file again and it will only transfer over the modified files.</p> <p>If your data transfer took too long and got killed by Slurm, or if it crashed for some other reason, just submit the same script again and it till pick up from where it left off.</p>"},{"location":"cluster_guides/transfer_dardel/#8-delete-the-ssh-key-pair","title":"8. Delete the SSH key pair","text":"<p>When you are done with transferring files you should delete your SSH keys you created in the previous steps in this guide. The SSH keys created where created without a password to protect them (required to run darsync as a unattended job), and it's best to delete them.</p> <pre><code>rm ~/.ssh/id_ed25519_pdc*\n</code></pre> <p>Create new ones if you still need to connect to Dardel from UPPMAX. To create new keys with password on them, simply run:</p> <pre><code>ssh-keygen -t ed25519\n</code></pre> <p>and add the new public key (<code>~/.ssh/id_ed25519.pub</code>) to the PDC Login Portal following the same steps as above.</p> <p>Once you are sure your data has been transferred, we recommend that you switch over to only work on Dardel. If you keep working on both clusters you will easily forget which cluster has the most up-to-date version of the files.</p>"},{"location":"cluster_guides/transfer_dardel/#link","title":"Link","text":"<ul> <li>PDC's page on getting access to Dardel</li> </ul>"},{"location":"cluster_guides/transfer_pelle/","title":"File transfer to/from Pelle","text":"","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_pelle/#file-transfer-tofrom-pelle","title":"File transfer to/from Pelle","text":"<p>There are multiple ways to transfer files to/from Pelle:</p> Method Features Using a graphical program Graphical interface, intuitive, for small amounts of data only Using rsync Terminal, easy to learn, can be used in scripts, efficient for backups Using SCP Terminal, easy to learn, can be used in scripts Using SFTP Terminal, easy to learn, secure Using transit Terminal, easy to learn, secure, can transfer between HPC clusters <p>Each of these methods is discussed below.</p>","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_pelle/#using-a-graphical-program","title":"Using a graphical program","text":"<p>One can transfer files to/from Pelle using a graphical program. A graphical interface is intuitive to most users. However, it can be used for small amounts of data only and whatever you do cannot be automated.</p> <p>See Pelle file transfer using a graphical program for a step-by-step guide how to transfer files using a graphical tool.</p>","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_pelle/#using-rsync","title":"Using <code>rsync</code>","text":"<p>One can transfer files to/from Pelle using rsync in a terminal. This works similar to a regular copy of files, except that a remote (instead of a local) address needs to be specified. <code>rsync</code> can be used in scripts for regular file transfer. However, <code>rsync</code> shines by providing a so-called 'delta' file transfer: when you transfer files twice, <code>rsync</code> will only transfer the files that have changed. This is ideal for backups.</p> <p>See Pelle file transfer using rsync for a step-by-step guide how to transfer files using <code>rsync</code>.</p>","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_pelle/#using-scp","title":"Using SCP","text":"<p>One can transfer files to/from Pelle using SCP in a terminal. This works similar to a regular copy of files, except that a remote address needs to be specified. The advantage of SCP is that is can be used in scripts.</p> <p>See Pelle file transfer using SCP for a step-by-step guide how to transfer files using SCP.</p>","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_pelle/#using-sftp","title":"Using SFTP","text":"<p>One can transfer files to/from Pelle using SFTP in a terminal. One connects a local and a remote folder, after which one can upload and download files. SFTP is considered a secure file transfer protocol.</p> <p>See Pelle file transfer using SFTP for a step-by-step guide how to transfer files using SFTP.</p>","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_pelle/#using-transit","title":"Using <code>transit</code>","text":"<p>One can transfer files to/from Pelle using the UPPMAX <code>transit</code> server. One connects a local folder and the <code>transit</code> server, after which one can upload and download files.</p> <p>See Pelle file transfer using <code>transit</code> for a step-by-step guide how to transfer files using the <code>transit</code> UPPMAX server.</p>","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_pelle/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[SUNET]\n      direction LR\n      user(User)\n      user_local_files(Local user files):::file_node\n\n      subgraph sub_transit_env[Transit]\n        transit_login(Transit login):::calculation_node\n        files_on_transit(Files posted to Transit):::transit_node\n      end\n      subgraph sub_pelle_shared_env[Pelle]\n          pelle_login(Pelle login node):::calculation_node\n          files_in_pelle_home(Files in Pelle home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#ccc,color:#000,stroke:#000\n    style sub_transit_env fill:#cfc,color:#000,stroke:#000\n    style sub_pelle_shared_env fill:#fcc,color:#000,stroke:#000\n\n    user --&gt; |has|user_local_files\n    user --&gt; |logs in |transit_login\n    user --&gt; |logs in |pelle_login\n\n    user_local_files &lt;--&gt; |graphical tool|files_in_pelle_home\n    user_local_files &lt;--&gt; |SCP|files_in_pelle_home\n    user_local_files &lt;--&gt; |SFTP|files_in_pelle_home\n    user_local_files &lt;--&gt; |graphical tool|files_on_transit\n    user_local_files &lt;--&gt; |SFTP|files_on_transit\n\n    pelle_login --&gt; |can use|files_in_pelle_home\n\n    transit_login --&gt; |can use|files_on_transit\n    files_on_transit &lt;--&gt; |transfer|files_in_pelle_home\n\n    files_in_pelle_home ~~~ transit_login</code></pre> <p>Overview of file transfer on Pelle The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>","tags":["Pelle","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/","title":"Transfer to/from Rackham","text":"","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/#file-transfer-tofrom-rackham","title":"File transfer to/from Rackham","text":"<p>There are multiple ways to transfer files to/from Rackham:</p> Method Features Using a graphical program Graphical interface, intuitive, for small amounts of data only Using rsync Terminal, easy to learn, can be used in scripts, efficient for backups Using SCP Terminal, easy to learn, can be used in scripts Using SFTP Terminal, easy to learn, secure Using transit Terminal, easy to learn, secure, can transfer between HPC clusters <p>Each of these methods is discussed below.</p>","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/#using-a-graphical-program","title":"Using a graphical program","text":"<p>One can transfer files to/from Rackham using a graphical program. A graphical interface is intuitive to most users. However, it can be used for small amounts of data only and whatever you do cannot be automated.</p> <p>See Rackham file transfer using a graphical program for a step-by-step guide how to transfer files using a graphical tool.</p>","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/#using-rsync","title":"Using <code>rsync</code>","text":"<p>One can transfer files to/from Rackham using rsync in a terminal. This works similar to a regular copy of files, except that a remote (instead of a local) address needs to be specified. <code>rsync</code> can be used in scripts for regular file transfer. However, <code>rsync</code> shines by providing a so-called 'delta' file transfer: when you transfer files twice, <code>rsync</code> will only transfer the files that have changed. This is ideal for backups.</p> <p>See Rackham file transfer using rsync for a step-by-step guide how to transfer files using <code>rsync</code>.</p>","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/#using-scp","title":"Using SCP","text":"<p>One can transfer files to/from Rackham using SCP in a terminal. This works similar to a regular copy of files, except that a remote address needs to be specified. The advantage of SCP is that is can be used in scripts.</p> <p>See Rackham file transfer using SCP for a step-by-step guide how to transfer files using SCP.</p>","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/#using-sftp","title":"Using SFTP","text":"<p>One can transfer files to/from Rackham using SFTP in a terminal. One connects a local and a remote folder, after which one can upload and download files. SFTP is considered a secure file transfer protocol.</p> <p>See Rackham file transfer using SFTP for a step-by-step guide how to transfer files using SFTP.</p>","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/#using-transit","title":"Using <code>transit</code>","text":"<p>One can transfer files to/from Rackham using the UPPMAX <code>transit</code> server. One connects a local folder and the <code>transit</code> server, after which one can upload and download files.</p> <p>See Rackham file transfer using <code>transit</code> for a step-by-step guide how to transfer files using the <code>transit</code> UPPMAX server.</p>","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_rackham/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[SUNET]\n      direction LR\n      user(User)\n      user_local_files(Local user files):::file_node\n\n      subgraph sub_transit_env[Transit]\n        transit_login(Transit login):::calculation_node\n        files_on_transit(Files posted to Transit):::transit_node\n      end\n      subgraph sub_rackham_shared_env[Rackham]\n          rackham_login(Rackham login node):::calculation_node\n          files_in_rackham_home(Files in Rackham home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#ccc,color:#000,stroke:#000\n    style sub_transit_env fill:#cfc,color:#000,stroke:#000\n    style sub_rackham_shared_env fill:#fcc,color:#000,stroke:#000\n\n    user --&gt; |has|user_local_files\n    user --&gt; |logs in |transit_login\n    user --&gt; |logs in |rackham_login\n\n    user_local_files &lt;--&gt; |graphical tool|files_in_rackham_home\n    user_local_files &lt;--&gt; |SCP|files_in_rackham_home\n    user_local_files &lt;--&gt; |SFTP|files_in_rackham_home\n    user_local_files &lt;--&gt; |graphical tool|files_on_transit\n    user_local_files &lt;--&gt; |SFTP|files_on_transit\n\n    rackham_login --&gt; |can use|files_in_rackham_home\n\n    transit_login --&gt; |can use|files_on_transit\n    files_on_transit &lt;--&gt; |transfer|files_in_rackham_home\n\n    files_in_rackham_home ~~~ transit_login</code></pre> <p>Overview of file transfer on Rackham The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>","tags":["Rackham","file","transfer","data"]},{"location":"cluster_guides/transfer_transit/","title":"File transfer to/from Transit","text":""},{"location":"cluster_guides/transfer_transit/#file-transfer-tofrom-transit","title":"File transfer to/from Transit","text":"<p>There are multiple ways to transfer files to/from Transit:</p> What is Transit? <p>Transit is an UPPMAX service to send files around. It is not a file server.</p> <p>See the page about Transit for more detailed information.</p> Method Features Using a graphical program Graphical interface, intuitive, for small amounts of data only Using rsync Terminal, easy to learn, secure Using SFTP Terminal, easy to learn, secure Using SCP  only download, terminal, easy to learn, can be used in scripts <p>Each of these methods is discussed below.</p>"},{"location":"cluster_guides/transfer_transit/#using-a-graphical-program","title":"Using a graphical program","text":"<p>One can transfer files to/from Transit using a graphical program. A graphical interface is intuitive to most users. However, it can be used for small amounts of data only and whatever you do cannot be automated.</p> <p>See Transit file transfer using a graphical program for a step-by-step guide how to transfer files using a graphical tool.</p>"},{"location":"cluster_guides/transfer_transit/#using-rsync","title":"Using <code>rsync</code>","text":"<p>Transit is used as a stepping-stone to transfer files to Bianca using rsync.</p>"},{"location":"cluster_guides/transfer_transit/#using-scp","title":"Using SCP","text":"<p>One cannot upload files to Transit using SCP in a terminal: Transit only allows for sending files from A to B, not for storing them.</p> <p>One can download the files on Transit. However, Transit is not a file server. Instead, the files that appear to be on Transit are the files in your Rackham home folder. Due to this, it makes more sense to use SCP to transfer files to/from Rackham.</p> <p>For completeness sake, see Transit file transfer using SCP for a step-by-step guide how to transfer files using SCP. It show one cannot upload files to Transit.</p>"},{"location":"cluster_guides/transfer_transit/#using-sftp","title":"Using SFTP","text":"<p>One can transfer files to/from Transit using SFTP in a terminal. One connects a local and a remote folder, after which one can upload and download files. SFTP is considered a secure file transfer protocol.</p> <p>See Transit file transfer using SFTP for a step-by-step guide how to transfer files using SFTP.</p>"},{"location":"cluster_guides/transfer_transit/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fff,color:#000,stroke:#000\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[SUNET]\n      user_local_files(Local user files):::file_node\n\n      subgraph sub_transit_env[Transit]\n        files_on_transit(Files posted to Transit):::transit_node\n      end\n      subgraph sub_rackham_shared_env[Rackham]\n        files_in_rackham_home(Files in Rackham home folder):::file_node\n      end\n      subgraph sub_bianca_private_env[Bianca]\n        files_in_bianca_project(Files in Bianca project folder):::file_node\n      end\n      subgraph sub_other_clusters[Other clusters]\n        files_on_other_clusters(Files on other clusters):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#ccc,color:#000,stroke:#000\n    style sub_transit_env fill:#cfc,color:#000,stroke:#000\n    style sub_rackham_shared_env fill:#fcc,color:#000,stroke:#000\n    style sub_bianca_private_env fill:#ccf,color:#000,stroke:#000\n    style sub_other_clusters fill:#ffc,color:#000,stroke:#000\n\n    user_local_files &lt;--&gt; |graphical tool|files_on_transit\n    user_local_files &lt;--&gt; |SFTP|files_on_transit\n\n    files_on_transit &lt;--&gt; |SCP|files_in_rackham_home\n    files_on_transit &lt;--&gt; |SFTP|files_in_rackham_home\n\n    files_on_transit &lt;--&gt; |SCP|files_in_bianca_project\n    files_on_transit &lt;--&gt; |SFTP|files_in_bianca_project\n\n    files_on_transit &lt;--&gt; |transfer|files_on_other_clusters\n</code></pre> <p>Overview of file transfer on Transit</p>"},{"location":"cluster_guides/transit/","title":"Transit","text":""},{"location":"cluster_guides/transit/#transit","title":"Transit","text":"<p>Transit is an UPPMAX service that can be used to securely transfer files between online locations, such as your local computer, Bianca, Rackham and other sensitive data clusters.</p> Is Transit a file server? <p>Transit is a service, not a file server. Transit is not a file server, as it does not store files.</p> <p>This can be observed by uploading files to Transit and then closing this connection before sending the files to a permanent location: the Transit-only files will disappear.</p> What is Transit? <p></p> <p>A Swedish post box. The yellow post box is for non-regional mail, the blue for regional mail.</p> <p>Transit can be viewed as a post box, where the file you upload is a letter.</p> <p>If you put a letter without an address in a post box, it will be thrown away.</p> <p>If you put an address on the letter, the letter will be delivered. Here, 'putting an address on the letter' is to copy the file to the desired location.</p> <ul> <li>how to log in to Transit</li> <li>file transfer using Transit.<ul> <li>Bianca file transfer using rsync</li> </ul> </li> <li>software on Transit</li> </ul>"},{"location":"cluster_guides/transit_file_transfer_using_gui/","title":"File transfer to/from Transit using a graphical tool","text":""},{"location":"cluster_guides/transit_file_transfer_using_gui/#file-transfer-tofrom-transit-using-a-graphical-tool","title":"File transfer to/from Transit using a graphical tool","text":"<p>There are multiple ways to transfer files to/from Transit. Here we describe how to do so using a graphical tool.</p> <p>There are multiple graphical tools to do so:</p> Documentation Tool Description Documentation FileZilla Free, open source, works on all platforms (recommended) Documentation WinSCP Only works under Windows"},{"location":"cluster_guides/uppmax/","title":"UPPMAX","text":""},{"location":"cluster_guides/uppmax/#uppmax","title":"UPPMAX","text":"<p>UPPMAX in an organization that provides HPC infrastructure that is physically located in Uppsala. To do so, it provides the UPPMAX systems below.</p>"},{"location":"cluster_guides/uppmax/#uppmax-systems","title":"UPPMAX systems","text":"<p>Here we place Bianca between the other UPPMAX systems.</p> <p>There are three types of UPPMAX systems:</p> <ul> <li>Computing systems</li> <li>Storage systems</li> <li>Cloud services</li> </ul> <p>Apply for these resources.</p>"},{"location":"cluster_guides/uppmax/#uppmax-computing-systems","title":"UPPMAX computing systems","text":"<p>Computing systems allow a user to do heavier computational calculations. At UPPMAX, we use multiple HPC clusters</p>"},{"location":"cluster_guides/uppmax/#uppmax-storage-systems","title":"UPPMAX storage systems","text":"<p>See UPPMAX systems.</p>"},{"location":"cluster_guides/uppmax/#uppmax-cloud-services","title":"UPPMAX Cloud services","text":"<p>See UPPMAX systems.</p> <p>Cloud services allow a user to have something active (typically a website) that can be accessed by the internet.</p>"},{"location":"cluster_guides/uppmax_as_an_organization/","title":"UPPMAX as an organization","text":""},{"location":"cluster_guides/uppmax_as_an_organization/#uppmax-as-an-organization","title":"UPPMAX as an organization","text":"<p>UPPMAX is a provider of HPC infrastructure that is physically located in Uppsala.</p> Where can I find an overview of UPPMAX? <p>See our overview of UPPMAX.</p> <p>Here we place UPPMAX within the bigger, national, picture, starting from the biggest source of money for research in Sweden.</p> <p></p> <p>Vetenskapsr\u00e5det ('Science counsel', VR) is biggest funder of research in Sweden and funds the national HPC infrastructure.</p> <p></p> <p>The National Academic Infrastructure for Supercomputing in Sweden (NAISS) provides such HPC infrastructure: computing power, storage and data services. Applications for these resources starts at this NAISS page. These resources are physically located in multiple places in Sweden, among other Uppsala.</p> <p></p> <p>Uppsala Multidisciplinary Center for Advanced Computational Science (UPPMAX = UppMACS) provides the HPC infrastructure that is physically located in Uppsala. Part of this is to provide training and support.</p> <pre><code>flowchart TD\n    HPC_Sweden(HPC in Sweden)\n    HPC_others(HPC in other cities)\n    HPC_Uppsala(HPC in Uppsala)\n    NAISS(NAISS)\n    UPPMAX(UPPMAX)\n    UU(Uppsala University)\n    Users(Users)\n    VR(Vetenskapsr\u00e5det)\n\n    VR --&gt; |money| HPC_Sweden\n    HPC_Sweden --&gt;|done by| NAISS\n    NAISS --&gt; |money| HPC_others\n    NAISS --&gt; |money| HPC_Uppsala\n    HPC_Uppsala --&gt;|done by| UPPMAX\n    UU --&gt;|money| HPC_Uppsala\n    Users --&gt;|apply for HPC|NAISS</code></pre>"},{"location":"cluster_guides/uppmax_cloud/","title":"UPPMAX cloud","text":""},{"location":"cluster_guides/uppmax_cloud/#uppmax-cloud","title":"UPPMAX cloud","text":"<p>Cloud services allow a user to have something active (typically a website) that can be accessed by the internet.</p> <p>The NAISS 'Swedish Science Cloud (SSC)', consists out of multiple regions. The eastern region (called <code>EAST-1</code>) of SCC is named 'Dis' (the Swedish word for 'haze') and is hosted by Uppsala university (the service is called 'UPPMAX cloud') and Ume\u00e5 University (north, HPC2N).</p>"},{"location":"cluster_guides/uppmax_cloud/#history-of-dis","title":"History of Dis","text":"<p>The UPPMAX cloud 'Dis' (Swedish word for 'haze') and successor of 'Smog' (Swedish for 'smog') was introduced in October 2017 and upgraded during 2020.</p>"},{"location":"cluster_guides/uppmax_cloud/#apply-for-an-scc-project","title":"Apply for an SCC project","text":"<p>See the UPPMAX pages on 'Apply for an SCC project'</p>"},{"location":"cluster_guides/uppmax_cloud/#technical-specifications","title":"Technical specifications","text":"<ul> <li>40 compute nodes, 24 dedicated for NAISS and 16 for local projects.   Each compute node is equipped with 128-256GB memory   and dual CPU E5-2660 at 2.2GHz for a total of 16 cores per compute node</li> <li>VM flavors for small (2 vCPUs) up to large (16 vCPUs) compute allocations</li> <li>250 TB of total volume storage.</li> <li>Interconnect is 10GbE.</li> </ul> <p>Object storage is planned for 2021 but currently unavailable.</p>"},{"location":"cluster_guides/uppmax_cluster/","title":"The UPPMAX clusters","text":""},{"location":"cluster_guides/uppmax_cluster/#the-uppmax-clusters","title":"The UPPMAX clusters","text":"<p>UPPMAX is an organization that provides HPC clusters.</p> Where can I find an overview of UPPMAX? <p>See the UPPMAX page about itself</p> Where can I find an overview of UPPMAX's systems? <p>See the UPPMAX page about its systems</p> <p>After giving an overview of the different UPPMAX clusters, it is discussed what a computer cluster is, how it differs from a supercomputer, what the restrictions of a computer cluster are, as well as some added restrictions on a sensitive data computer cluster.</p> <p>This is followed by a detailed technical summary of the clusters and a detailed overview of the clusters.</p>"},{"location":"cluster_guides/uppmax_cluster/#overview-of-uppmax-clusters","title":"Overview of UPPMAX clusters","text":"<p>UPPMAX clusters are computing systems, i.e. they allow a user to do heavy computational calculations.</p> Name Purpose Description Bianca Sensitive data, general use Will be replaced by Maja Pelle Regular data, general purpose Is replacing Rackham Rackham Regular data, general purpose Is being replaced by Pelle Snowy Regular data, long runs, GPUs .  [System usage](https://docs.uppmax.uu.se/cluster_guides/system_usage/system_usage.html) { .md-button .md-button--primary }  <p>Another cluster UPPMAX is involved in:</p> <ul> <li>Dardel: a general purpose HPC cluster in Stockholm.   Consider moving your files to it already</li> </ul> <pre><code>flowchart TD\n    UPPMAX(Which UPPMAX cluster?)\n    Bianca\n    Dardel\n    Maja\n    Pelle\n    Rackham\n    Snowy\n    is_sensitive[Do you use sensitive data?]\n    is_long[Do you use long runs and/or GPUs?]\n\n    UPPMAX --&gt; is_sensitive\n    is_sensitive --&gt; |yes|Bianca\n    is_sensitive --&gt; |no|is_long\n    is_long --&gt; |no|Rackham\n    is_long --&gt; |yes|Snowy\n    Bianca --&gt; |near future| Maja\n\n    Rackham --&gt; |not UU, before 2025-01-01| Dardel\n    Rackham --&gt; |UU, near future| Pelle</code></pre> <p>All UPPMAX clusters follow the same file system, with special folders.</p>"},{"location":"cluster_guides/uppmax_cluster/#what-is-a-computer-cluster-technically","title":"What is a computer cluster technically?","text":"<p>A computer cluster is a machine that consists out of many computers. These computers work together.</p> <p>Each computer of a cluster is called a node.</p> <p>There are three types of nodes:</p> <ul> <li>login nodes: nodes where a user enters and interacts with the system</li> </ul> Logging in <p>Logging in is described separately per cluster:</p> <ul> <li>Bianca.</li> <li>Rackham.</li> <li>Snowy.</li> </ul> <ul> <li>calculation nodes: nodes that do the calculations</li> </ul> Requesting a calculation to run <p>Requesting a calculation to run is described on the UPPMAX page about the job scheduler.</p> <ul> <li>interactive sessions: a user on a calculation node</li> </ul> Requesting an interactive session <p>Requesting an interactive session is described per cluster:</p> <ul> <li>Bianca</li> <li>Rackham</li> </ul> <p>This is done by requesting an interactive session from the Slurm scheduler.</p> <p>Each node contains several CPU/GPU cores, RAM and local storage space.</p> <p>A user logs in to a login node via the Internet.</p> <pre><code>flowchart TD\n\n  login_node(User on login node)\n  interactive_session(User on interactive session)\n  computation_node(Computation node)\n\n  login_node --&gt; |move user, interative|interactive_session\n  login_node --&gt; |submit jobs, sbatch|computation_node\n  computation_node -.-&gt; |can become| interactive_session</code></pre> <p>The different types of nodes an UPPMAX cluster has.</p>"},{"location":"cluster_guides/uppmax_cluster/#difference-between-a-supercomputer-and-a-high-performing-computer-cluster","title":"Difference between a supercomputer and a (high-performing) computer cluster","text":"<p>A supercomputer is a machine that is optimized for doing calculations quickly. For example, to predict the weather for tomorrow, the calculation may not take a week. The image above is a supercomputer.</p> <p></p> <p>A computer cluster is a set of computers that work together so that they can be viewed as a single system. The image above shows a home-made computer cluster. This home-made computer cluster may not be suitable for high-performance computing.</p> <p></p> <p>The image above shows Rackham, another UPPMAX computer cluster, suitable for high-performance computing. This makes Rackham an high-performance computing (HPC) cluster. Bianca and Rackham are HPC clusters.</p> <p>When using this definition:</p> <p>a supercomputer is one big computer, while high-performance computing is many computers working toward the same goal</p> <p>Frank Downs</p> <p>one could conclude that the UPPMAX HPC cluster can be used as a supercomputer when a user runs a calculation on all nodes.</p>"},{"location":"cluster_guides/uppmax_cluster/#restrictions-on-a-computer-cluster","title":"Restrictions on a computer cluster","text":"<p>A computer cluster is a group of computers that can run many calculations, as requested by multiple people, at the same time.</p> <p>To ensure fair use of this shared resource, regular users are restricted in some ways:</p> <ul> <li>Users cannot run calculations directly.   Instead, users need to request either (1) a calculation to be run,   or (2) an interactive session</li> </ul> Requesting a calculation to run <p>Requesting a calculation to run is described at the UPPMAX page about the job scheduler.</p> Requesting an interactive session <p>Requesting an interactive session is described per cluster:</p> <ul> <li>Bianca</li> <li>Rackham</li> </ul> <p>This is done by requesting an interactive session from the Slurm scheduler.</p> <ul> <li>Users cannot install software directly.   Instead, users need to use pre-installed software or learn   techniques how to run custom software anyway</li> </ul> Using pre-installed software <p>Using pre-installed software is described at the UPPMAX page about the software module system.</p> How to run custom software <p>Using a Singularity container allows you to run most custom software on any HPC cluster</p> <p>These restrictions apply to most general-purpose clusters and all UPPMAX clusters.</p>"},{"location":"cluster_guides/uppmax_cluster/#restrictions-on-a-sensitive-data-computer-cluster","title":"Restrictions on a sensitive data computer cluster","text":"<p>Next to the general restrictions above, a sensitive data cluster has additional restrictions.</p> <p>Here is an overview which clusters are designed for sensitive data:</p> Cluster name Sensitive data yes/no? Bianca Yes Rackham No Snowy No <p>On a sensitive data cluster, (sensitive) data must be protected to remain there, due to which there are these additional restrictions to users:</p> <ul> <li>Users have no direct access to internet.   Instead, users can up/download files from/to a special folder.</li> </ul> File transfer <p>Transferring files is described per sensitive data cluster:</p> <ul> <li>Bianca.</li> </ul> <p>The goal is to prevent the accidental up/download of sensitive data. As these up/downloads are monitored, in case of an accident, the extent of the leak and the person (accidentally) causing it is known. Identifying a responsible person in case of such an accident is required by law.</p>"},{"location":"cluster_guides/uppmax_cluster/#uppmax-clusters-technical-summary","title":"UPPMAX clusters technical summary","text":"<p>This is a technical summary of the UPPMAX clusters:</p> . Rackham Snowy Bianca Purpose General-purpose General-purpose Sensitive # Intel CPU Nodes 486+144 228 288 # GPU Nodes - 50, Nvidia T4 10, 2x Nvidia A100 each Cores per node 20/16 16 16/64 Memory per node 128 GB 128 GB 128 GB Fat nodes 256 GB &amp; 1 TB 256, 512 GB &amp; 4 TB 256 &amp; 512 GB Local disk (scratch) 2/3 TB 4 TB 4 TB Login nodes Yes No (reached from Rackham) Yes (2 cores and 15 GB) \"Home\" storage Domus Domus Castor/Cygnus \"Project\" Storage Crex, Lutra Crex, Lutra Castor/Cygnus"},{"location":"cluster_guides/uppmax_cluster/#detailed-overview-of-the-uppmax-systems","title":"Detailed overview of the UPPMAX systems","text":"<pre><code>\n  graph TB\n\n  Node1 -- interactive --&gt; SubGraph2Flow\n  Node1 -- sbatch --&gt; SubGraph2Flow\n  subgraph \"Snowy\"\n  SubGraph2Flow(calculation nodes)\n        end\n\n        thinlinc -- usr-sensXXX + 2FA + VPN ----&gt; SubGraph1Flow\n        terminal -- usr --&gt; Node1\n        terminal -- usr-sensXXX + 2FA + VPN ----&gt; SubGraph1Flow\n        Node1 -- usr-sensXXX + 2FA + no VPN ----&gt; SubGraph1Flow\n\n        subgraph \"Bianca\"\n        SubGraph1Flow(Bianca login) -- usr+passwd --&gt; private(private cluster)\n        private -- interactive --&gt; calcB(calculation nodes)\n        private -- sbatch --&gt; calcB\n        end\n\n        subgraph \"Rackham\"\n        Node1[Login] -- interactive --&gt; Node2[calculation nodes]\n        Node1 -- sbatch --&gt; Node2\n        end</code></pre>"},{"location":"cluster_guides/uppmax_filesystem/","title":"UPPMAX filesystem","text":"","tags":["filesystem","file system","UPPMAX"]},{"location":"cluster_guides/uppmax_filesystem/#uppmax-filesystem","title":"UPPMAX filesystem","text":"<p>One can store files on the UPPMAX clusters.</p> <p>Here we show some common directories and best practices.</p> Directory name Description <code>backup</code> A folder that is guaranteed to have a backup for 30 days Home folder Your home folder, <code>/home/[username]</code>, e.g. <code>/home/sven</code> <code>nobackup</code> A folder without a backup Project folder Your project folder, <code>/proj/[project_name]</code>, e.g. <code>/proj/snic2021-22-780</code> Wharf A Bianca-only folder for file transfer <p>Some folders do and some do not have a backup. See the UPPMAX documentation on 'backup' for details.</p>","tags":["filesystem","file system","UPPMAX"]},{"location":"cluster_guides/uppmax_filesystem/#best-practices","title":"Best practices","text":"Are there any horror story on this? <p>Yes, ask the UPPMAX staff :-)</p> <ol> <li>Keep an inventory of important data and make a plan    for how it should be treated. Inform collaborators of this plan</li> <li>Make sure you keep a separate copy of the most important data</li> <li>Put important data in folders that have a backup</li> <li>Put intermediate/temporary data in a <code>nobackup</code> directory</li> <li>Run <code>chmod -R -w .</code> on directories containing critical    data that should be preserved.</li> </ol>","tags":["filesystem","file system","UPPMAX"]},{"location":"cluster_guides/uppmax_history/","title":"UPPMAX history","text":""},{"location":"cluster_guides/uppmax_history/#uppmax-history","title":"UPPMAX history","text":"Resource Out of commission Size Price Start End Features Grendel 2004-06-30 ?16 nodes - ? ? Together with NSC Ngorongoro 2008-12-31 48 CPUs - ? ? SunFire 15k Hagrid 2008-01-31 100 nodes - 2003-12-01 2007-12-31 SNIC/SweGrid Ra 2009-08-03 100 nodes - 2005-02-01 2009-01-31 SNIC/matvet Set 2010-12-31 10 nodes 0.9 2006-07-01 2010-06-30 SNIC, power5+IB Isis 2010-12-31 200 nodes 4.49 2007-02-01 2010-01-31 SNIC/matvet Os 2010-12-31 10 nodes 0 2007-02-01 2011-12-31 SNIC, IB Grad 2013-01-31 64 nodes - 2008-04-01 2012-03-31 SNIC/SweGrid Cell 2012-01-31 2 nodes - 2008-09-01 2009-08-31 2 nodes with cell-processors Kalkyl 2013-12-31 348 nodes 8.6 2009-12-12 2013-12-31 KAW/SNIC Bubo 2013-12-09 500TB 5.3 2009-09-12 2013-12-01 KAW/SNIC H\u00f6keborg . ca 230 m2, 7 cooling aggregates, 90 kVA UPS, racks A-D - 2011-09-01 - Computer hall, faculty means H\u00f6keborg . +3 cooling aggregates, racks E-F - 2013-06-01 - Computer hall, faculty means H\u00f6keborg . +3 cooling aggregates, +30kVA UPS, moved batteries, racks G-H - 2015-04-15 - Computer hall, faculty means Lynx 2015-11-09 500 TB 4.7 2011-12-01 2015-12-01 KAW/SNIC Halvan 2016-04-06 64 core, 2TB 1.2 2011-02-11 2016-02-29 Misc, extended support 1 year Tintin . 164 nodes 5.3 2012-02-01 2016-02-01 SNIC Kali . 1 nod, 30TB disk 0.1 X X+1 year iRODS, KAW? dCache . 600 TB 0.6 2012-11-19 2016-11-18 SNIC Gulo . 1.2 PB 1.9 2012-11-19 2016-11-18 KAW/BILS Pica . 5.5 PB 10 2013-10-01 2017-10-01 KAW Host . 8 nodes 0.45 2013-11-01 2017-11-01 Used Ganeti, UPPMAX Milou . 248 nodes 9.9 2013-11-01 2017-11-01 KAW/BILS Milou-f2 . 1 nod, 4 TB 1 2014-02-01 2018-02-01 Login node Nestor 2016-05-31 48 nodes - 2014-04-08 2018-04-08 . Apus 2016-05-31 500 TB - 2014-01-13 2018-01-13 . Topolino . 24 nodes - 2014-04-08 2018-04-08 BILS Meles . 279 TB - 2014-01-13 2018-01-13 . Das . 48 TB 0.07 2015-07-01 2020-06-30 New back mount, HP, data network redesign IT 2015/25 Core network . 2 switches 0.22 2015-07-01 2020-06-30 Dell, core network, data network redesign IT 2015/50 Irma . 250 nodes 15.8 2015-10-01 2019-09-30 Supermicro, data network redesign IT2014/93 Lupus . 1 PB Lustre 2.1 2016-03-03 2021-03-02 Dell, data network redesign IT214/92 CEPH . 252 TB 0.35 2015-12-14 2019-12-13 Dell, 7 servers, data network redesign IT 2015/84 Bianca . 100 nodes 3.1 2016-04-01 2020-03-31 SouthPole, Huawei data network redesign IT 2015/65 Castor . 1 PB, 18 servers 1 2016-04-01 2020-03-31 SouthPole, Huawei data network redesign IT 2015/65 Castor, +1 PB . 1 PB, 18 servers 2.3 2016-07-01 2020-07-31 SouthPole, Huawei data network redesign IT 2015/65 Grus . 1.5 PB, 14 servers 1.8? 2016-07-01 2020-07-31 SouthPole, Huawei data network redesign IT 2015/65 Irham . . . 2016-07-01 2024-01-12 Decommissioned Irma nodes added to Rackham, became <code>r[1001-1072,1179-1250]</code> Miarka . . . 2021 . . Rackham -2025 . . . . . Snowy . . . . . . Pelle 2025- . . . . . Maja . . . . . . Gorilla 2025- . . . . . <ul> <li>Price is in millions of Swedish kroner</li> <li>'Start': start of the guarantee</li> <li>'End': end of the guarantee</li> <li>'data network redesign' is assumed to be the unabbreviated form of <code>dnr</code></li> <li>'processors' is assumed to be the unabbreviated form of <code>procs</code></li> </ul>"},{"location":"cluster_guides/uppmax_storage_system/","title":"UPPMAX storage system","text":""},{"location":"cluster_guides/uppmax_storage_system/#uppmax-storage-system","title":"UPPMAX storage system","text":"<p>A system to store data on.</p> <ul> <li>Castor</li> <li>Crex</li> <li>Cygnus</li> <li>Domus</li> <li>Lutra</li> <li>Spirula<ul> <li>User guide </li> </ul> </li> <li>Vulpes</li> </ul>"},{"location":"cluster_guides/uppmax_systems/","title":"UPPMAX systems","text":""},{"location":"cluster_guides/uppmax_systems/#uppmax-systems","title":"UPPMAX systems","text":"<p>UPPMAX is an organization that provides HPC infrastructure that is physically located in Uppsala.</p> Where can I find an overview of UPPMAX? <p>See our overview of UPPMAX</p> <p>This HPC infrastructure consists out of:</p> <ul> <li>Computing systems, to do calculations</li> <li>Storage systems, to store data</li> <li>Cloud services, to provide webservices</li> </ul> <p>Below these systems are discussed.</p>"},{"location":"cluster_guides/uppmax_systems/#uppmax-computing-systems","title":"UPPMAX computing systems","text":"<p>Computing systems allow a user to do heavier computational calculations.</p> <p>UPPMAX has, among others, the following clusters:</p> <ul> <li>Rackham: regular data, general purpose</li> <li>Snowy: regular data, long runs and GPU:s</li> <li>Bianca: for sensitive data, general use</li> </ul> <p>A technical summary can be found below.</p> <pre><code>flowchart TD\n    UPPMAX(Which UPPMAX cluster?)\n    Bianca\n    Rackham\n    Snowy\n    is_sensitive[Do you use sensitive data?]\n    is_long[Do you use long runs and/or GPUs?]\n\n    UPPMAX --&gt; is_sensitive\n    is_sensitive --&gt; |yes|Bianca\n    is_sensitive --&gt; |no|is_long\n    is_long --&gt; |no|Rackham\n    is_long --&gt; |yes|Snowy</code></pre>"},{"location":"cluster_guides/uppmax_systems/#uppmax-storage-systems","title":"UPPMAX storage systems","text":"<p>Storage systems allow a user to storage (big amounts of) data, for either active use (i.e. in calculations) or to archive it (cold data).</p> <p>You are not supposed to do calculations on the cold data. This is stored on off-load storage where the file system is much slower. You need to transfer the data to an active storage first.</p> <p>The UPPMAX storage systems are:</p> <ul> <li>Active: Cygnus for Bianca, Crex for Rackham</li> <li>Off-load: Lutra for Rackham</li> </ul> <pre><code>flowchart TD\n    UPPMAX[Which UPPMAX storage system?]\n    which_cluster[Which UPPMAX cluster?]\n    Cygnus\n    Lutra\n    usage_type{Type of use?}\n\n    UPPMAX--&gt;which_cluster\n    which_cluster--&gt;|Rackham|usage_type\n    which_cluster--&gt;|Bianca|Cygnus\n    usage_type--&gt;|active|Crex\n    usage_type--&gt;|off-load|Lutra</code></pre> <p>See the UU page on UPPMAX storage for more information.</p>"},{"location":"cluster_guides/uppmax_systems/#uppmax-cloud-services","title":"UPPMAX Cloud services","text":"<p>See the UPPMAX cloud.</p>"},{"location":"cluster_guides/uppmax_systems/#difference-between-supercomputer-and-high-performing-computer-cluster","title":"Difference between supercomputer and (high-performing) computer cluster","text":"<p>A supercomputer is a machine that is optimized for doing calculations quickly. For example, to predict the weather for tomorrow, the calculation may not take a week. The image above is a supercomputer.</p> <p></p> <p>A computer cluster is a machine that is optimized for doing a lot of calculations. The image above shows a home-made computer cluster. This home-made computer cluster may not be suitable for high-performance.</p> <p></p> <p>The image above shows Rackham, another UPPMAX computer cluster, suitable for high-performance computing. This makes Rackham an high-performance computing (HPC) cluster. Bianca and Rackham are HPC clusters.</p>"},{"location":"cluster_guides/uppmax_systems/#restrictions-on-a-computer-cluster","title":"Restrictions on a computer cluster","text":"<p>A computer cluster is a group of computers that can run many calculations, as requested by multiple people, at the same time.</p> <p>To ensure fair use of this shared resource, regular users are restricted in some ways:</p> <ul> <li>Users cannot run calculations directly.   Instead, users need to request either (1) a calculation to be run,   or (2) an interactive session</li> </ul> Requesting a calculation to run <p>Requesting a calculation to run is described on the UPPMAX page about the job scheduler.</p> Requesting an interactive session <p>See the UPPMAX page about requesting an interactive session.</p> <ul> <li>Users cannot install software directly.   Instead, users need to use pre-installed software or learn   techniques how to run custom software anyway</li> </ul> Using pre-installed software <p>Using pre-installed software is described on the UPPMAX page about the software module system.</p> How to run custom software <p>One can use Singularity containers to run software on an HPC cluster.</p> <p>These restrictions apply to most general-purpose clusters. However, Bianca is a sensitive data cluster, to which more restrictions apply.</p>"},{"location":"cluster_guides/uppmax_systems/#restrictions-on-a-sensitive-data-computer-cluster","title":"Restrictions on a sensitive data computer cluster","text":"<p>Next to the general restrictions above, Bianca also is a sensitive data cluster. This sensitive data must be protected to remain only on Bianca, due to which there are these additional restrictions to users:</p> <ul> <li>Users have no direct access to internet.   Instead, users can up/download files from/to a special folder.</li> </ul> File transfer <p>See the UPPMAX pages about file transfer.</p> <p>The goal is to prevent the accidental up/download of sensitive data. As these up/downloads are monitored, in case of an accident, the extent of the leak and the person (accidentally) causing it is known. Identifying a responsible person in case of such an accident is required by law.</p>"},{"location":"cluster_guides/uppmax_systems/#what-is-a-computer-cluster-technically","title":"What is a computer cluster technically?","text":"<p>A computer cluster is a machine that consists out of many computers. These computers work together.</p> <p>Each computer of a cluster is called a node.</p> <p>There are three types of nodes:</p> <ul> <li>login nodes: nodes where a user enters and interacts with the system</li> </ul> Logging in <p>See the UPPMAX page about logging in to Bianca.</p> <ul> <li>calculation nodes: nodes that do the calculations</li> </ul> Requesting a calculation to run <p>Requesting a calculation to run is part of this course and is described at the UPPMAX page about the job scheduler.</p> <ul> <li>interactive sessions: a user on a calculation node,   where he/she can do calculations directly</li> </ul> Requesting an interactive session <p>See the UPPMAX page on how to start an interactive session on Bianca.</p> <p>Each node contains several CPU/GPU cores, RAM and local storage space.</p> <p>A user logs in to a login node via the Internet.</p>"},{"location":"cluster_guides/uppmax_systems/#summary","title":"Summary","text":"<p>keypoints</p> <ul> <li>NAISS provides HPC resources for Swedish research.</li> <li>UPPMAX takes care of the Uppsala HPC facilities</li> <li>Bianca is an HPC cluster for sensitive data</li> <li>The restrictions on Bianca follow from Bianca being a shared resource   that uses sensitive data</li> </ul>"},{"location":"cluster_guides/uppmax_systems/#extra-material","title":"Extra material","text":""},{"location":"cluster_guides/uppmax_systems/#uppmax-clusters-technical-summary","title":"UPPMAX clusters technical summary","text":"Rackham Snowy Bianca Purpose General-purpose General-purpose Sensitive # Intel CPU Nodes 486+144 228 288 # GPU Nodes - 50, Nvidia T4 10, 2x Nvidia A100 each Cores per node 20/16 16 16/64 Memory per node 128 GB 128 GB 128 GB Fat nodes 256 GB &amp; 1 TB 256, 512 GB &amp; 4 TB 256 &amp; 512 GB Local disk (scratch) 2/3 TB 4 TB 4 TB Login nodes Yes No (reached from Rackham) Yes (2 cores and 15 GB) \"Home\" storage Domus Domus Castor \"Project\" Storage Crex, Lutra Crex, Lutra Castor"},{"location":"cluster_guides/uppmax_systems/#detailed-overview-of-the-uppmax-systems","title":"Detailed overview of the UPPMAX systems","text":"<pre><code>\n  graph TB\n\n  Node1 -- interactive --&gt; SubGraph2Flow\n  Node1 -- sbatch --&gt; SubGraph2Flow\n  subgraph \"Snowy\"\n  SubGraph2Flow(calculation nodes)\n        end\n\n        thinlinc -- usr-sensXXX + 2FA + VPN ----&gt; SubGraph1Flow\n        terminal -- usr --&gt; Node1\n        terminal -- usr-sensXXX + 2FA + VPN ----&gt; SubGraph1Flow\n        Node1 -- usr-sensXXX + 2FA + no VPN ----&gt; SubGraph1Flow\n\n        subgraph \"Bianca\"\n        SubGraph1Flow(Bianca login) -- usr+passwd --&gt; private(private cluster)\n        private -- interactive --&gt; calcB(calculation nodes)\n        private -- sbatch --&gt; calcB\n        end\n\n        subgraph \"Rackham\"\n        Node1[Login] -- interactive --&gt; Node2[calculation nodes]\n        Node1 -- sbatch --&gt; Node2\n        end</code></pre>"},{"location":"cluster_guides/webexport/","title":"Webexport","text":""},{"location":"cluster_guides/webexport/#webexport-guide","title":"Webexport guide","text":"<p>You can enable webexport by creating a publicly readable folder called webexport in your project directory (<code>/proj/[project id]</code>). The contents of that folder will be accessible through <code>https://export.uppmax.uu.se/[project id]/</code>.</p> <p>This will not work on Bianca for security reasons.</p>"},{"location":"cluster_guides/webexport/#publicly-readable-folder","title":"Publicly readable folder","text":"<ul> <li>A publicly readable folder has the execute permission set for \"other\" users.</li> <li>Run the command <code>chmod o+x webexport</code> to ensure that the webexport directory has the correct permissions.</li> </ul>"},{"location":"cluster_guides/webexport/#control-access","title":"Control access","text":"<ul> <li> <p>A subset of .htaccess/.htpasswd functionality is available to control access.</p> </li> <li> <p>Example:</p> <ul> <li><code>/crex/proj/naiss2024-1-123/webexport/Project_portal/.htaccess</code></li> <li> <p><code>/crex/proj/naiss2024-1-123/Nisse/.htpasswd</code></p> </li> <li> <p>Note that you need the full physical <code>/crex/proj...</code> path. This full path is given from the command <code>pwd -P</code>.</p> </li> </ul> </li> </ul> <p>see also</p> <p>You may want to check the external Easy_webshare_on_UPPMAX while we update this page</p>"},{"location":"cluster_guides/wharf/","title":"wharf","text":""},{"location":"cluster_guides/wharf/#wharf","title":"<code>wharf</code>","text":"<p><code>wharf</code> is a folder on Bianca used for file transfer on Bianca.</p> <p>He it is described:</p> <ul> <li>What is <code>wharf</code>?</li> <li>The <code>wharf</code> location</li> <li><code>wharf</code> use</li> <li>mounting <code>wharf</code></li> </ul>"},{"location":"cluster_guides/wharf/#what-is-wharf","title":"What is <code>wharf</code>?","text":"<p>The <code>wharf</code> is like a \"postbox\"  for data/file exchange between the Internet restricted Bianca cluster and the remaining of the World Wide Internet. This \"postbox\" is reachable to transfer data from two internal servers - <code>bianca-sftp.uppmax.uu.se</code> and <code>transit.uppmax.uu.se</code>.</p>"},{"location":"cluster_guides/wharf/#the-wharf-location","title":"The <code>wharf</code> location","text":"<p>The path to this special folder is:</p> <pre><code>/proj/nobackup/[project_id]/wharf/[user_name]/[user_name]-[project_id]\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[user_name]</code> is the name of your UPPMAX user account</li> </ul> <p>For example:</p> <pre><code>/proj/nobackup/sens2023598/wharf/sven/sven-sens2023598\n</code></pre>"},{"location":"cluster_guides/wharf/#wharf-use","title":"<code>wharf</code> use","text":"<p>To transfer data from/to Bianca, <code>wharf</code> is to folder where files are sent to/from.</p> <p>Do not keep files in <code>wharf</code>, as this folder is connected to the outside world and hence is a security risk. Instead, move your data to your project folder.</p> <p>You have full access to your <code>wharf</code> and read-only access to other users' <code>wharf</code> folders in that same project.</p> <p><code>wharf</code> is only accessible when inside the university networks.</p>"},{"location":"cluster_guides/wharf/#mounting-wharf","title":"Mounting <code>wharf</code>","text":"<p>Mounting <code>wharf</code> means that a <code>wharf</code> folder is added to the filesystem of your local computer or other place outside Bianca, after which you can use it like any other folder. The data shown in the folder is on Bianca, not on your local storage.</p>"},{"location":"cluster_guides/wharf/#on-transit","title":"On Transit","text":"<p>One can mount <code>wharf</code> on on Transit using the command <code>mount_wharf [project_id]</code></p> <p>More details at Mount a Bianca project on Transit</p>"},{"location":"cluster_guides/wharf/#local-computer","title":"Local computer","text":"<p>One can mount <code>wharf</code> on your local computer using <code>sshfs</code> when inside the university networks. <code>sshfs</code> is available on most Linux distributions:</p> Distribution Package name Ubuntu <code>sshfs</code> Fedora <code>fuse-sshfs</code> RHEL7/CentOS7 [1] <code>fuse-sshfs</code> RHEL8 [2] <code>fuse-sshfs</code> CentOS8 [3] <code>fuse-sshfs</code> <ul> <li>[1] Enable EPEL repository</li> <li>[2] Enable <code>codeready-builder</code> repository</li> <li>[3] Enable <code>powertools</code> repository</li> </ul> <p>UPPMAX does not have <code>sshfs</code> installed for security reasons.</p>"},{"location":"cluster_guides/wharf2/","title":"wharf v 2.0","text":""},{"location":"cluster_guides/wharf2/#wharf-v-20","title":"<code>wharf</code> v 2.0","text":"<p><code>wharf</code> is a folder on Bianca used for file transfer on Bianca.</p> <p>He it is described:</p> <ul> <li>What is <code>wharf</code>?</li> <li>The <code>wharf</code> location</li> <li><code>wharf</code> use</li> <li>mounting <code>wharf</code></li> </ul>"},{"location":"cluster_guides/wharf2/#what-is-wharf","title":"What is <code>wharf</code>?","text":"<p>The <code>wharf</code> is like a \"postbox\"  for data/file exchange between the Internet restricted Bianca cluster and the remaining of the World Wide Internet. This \"postbox\" is reachable to transfer data from two internal servers - <code>bianca-sftp.uppmax.uu.se</code> and <code>transit.uppmax.uu.se</code>.</p>"},{"location":"cluster_guides/wharf2/#the-wharf-location","title":"The <code>wharf</code> location","text":"<p>The path to this special folder is:</p> <pre><code>/proj/nobackup/[project_id]/wharf/[user_name]/[user_name]-[project_id]\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[user_name]</code> is the name of your UPPMAX user account</li> </ul> <p>For example:</p> <pre><code>/proj/nobackup/sens2023598/wharf/sven/sven-sens2023598\n</code></pre>"},{"location":"cluster_guides/wharf2/#wharf-use","title":"<code>wharf</code> use","text":"<p>To transfer data from/to Bianca, <code>wharf</code> is to folder where files are sent to/from.</p> <p>Do not keep files in <code>wharf</code>, as this folder is connected to the outside world and hence is a security risk. Instead, move your data to your project folder.</p> <p>You have full access to your <code>wharf</code> and read-only access to other users' <code>wharf</code> folders in that same project.</p> <p><code>wharf</code> is only accessible when inside the university networks.</p>"},{"location":"cluster_guides/wharf2/#mounting-wharf","title":"Mounting <code>wharf</code>","text":"<p>Mounting <code>wharf</code> means that a <code>wharf</code> folder is added to the filesystem of your local computer or other place outside Bianca, after which you can use it like any other folder. The data shown in the folder is on Bianca, not on your local storage.</p>"},{"location":"cluster_guides/wharf2/#on-transit","title":"On Transit","text":"<p>One can mount <code>wharf</code> on on Transit using the command <code>mount_wharf [project_id]</code></p> <p>More details at Mount a Bianca project on Transit</p>"},{"location":"cluster_guides/wharf2/#local-computer","title":"Local computer","text":"<p>One can mount <code>wharf</code> on your local computer using <code>sshfs</code> when inside the university networks. <code>sshfs</code> is available on most Linux distributions:</p> Distribution Package name Ubuntu <code>sshfs</code> Fedora <code>fuse-sshfs</code> RHEL7/CentOS7 [1] <code>fuse-sshfs</code> RHEL8 [2] <code>fuse-sshfs</code> CentOS8 [3] <code>fuse-sshfs</code> <ul> <li>[1] Enable EPEL repository</li> <li>[2] Enable <code>codeready-builder</code> repository</li> <li>[3] Enable <code>powertools</code> repository</li> </ul> <p>UPPMAX does not have <code>sshfs</code> installed for security reasons.</p>"},{"location":"cluster_guides/to_add_to_doc/","title":"To add","text":""},{"location":"cluster_guides/to_add_to_doc/#to-add","title":"To add","text":""},{"location":"cluster_guides/to_add_to_doc/#2fa-key-removes-need-for-2fa","title":"2FA key removes need for 2FA","text":"<p>Application expert:</p> <p>Lovely - it seems that both ssh and thinlic will not ask me for 2FA if I use my key \ud83d\udc4d\ufe0f</p> <p>Sysadmin:</p> <p>check we discussed that so intentional</p>"},{"location":"cluster_guides/to_add_to_doc/#names-of-resources-on-pelle","title":"Names of resources on Pelle","text":"<p>Application expert:</p> <pre><code>--gres=gpu:h100:1\n</code></pre> <p>for the H100 nodes</p>"},{"location":"courses_workshops/R_matlab_julia/","title":"R-MATLAB-Julia","text":"","tags":["course","workshop","R","Julia","MATLAB","intro","introduction"]},{"location":"courses_workshops/R_matlab_julia/#introduction-to-running-julia-r-and-matlab-in-hpc","title":"Introduction to running Julia, R, and MATLAB in HPC","text":"<p>Learn how to run R, MATLAB, and Julia at Swedish HPC centres. We will show you how to find and load the needed modules, how to write a batch script, as well as how to install and use your own packages, and more. The course will consist of lectures interspersed with hands-on sessions where you get to try out what you have just learned.</p> <p>We will mainly use Tetralith at NSC for the examples for the course, but there is little difference in how you use the various HPC centres in Sweden and you should have no problems applying the knowledge to the other systems.</p> <p>NOTE: the course will NOT cover the topic of improving your programming skills in R, MATLAB, and Julia. Likewise, we will not cover advanced techniques for code optimization.</p> <p>NOTE if you are interested in running Python at Swedish HPC centres, then we recommend the course \"Introduction to Python and Using Python in an HPC environment\" which will run 24-25 April + 28-29 April. The first day is the introduction to Python and it is possible to just participate that day.</p> <p>Remote/online participation: The course will be completely online and we will use Zoom. More information about connecting and such will be sent to the participants close to the course.</p> <p>Prerequisites: some familiarity with the LINUX command line (recordings from HPC2N's Linux intro here and UPPMAX Linux Intro here and also here), basic R, MATLAB, or Julia, depending on which language(s) you are interested in. See below for links to useful material if you need a refresher before the course.</p>","tags":["course","workshop","R","Julia","MATLAB","intro","introduction"]},{"location":"courses_workshops/R_matlab_julia/#schedule","title":"Schedule","text":"<p>This course will consist of three days (9:00-16:00), one for each language. It is a cooperation between HPC2N, LUNARC, and UPPMAX.</p> <p>Full schedule can be found on the rendered presentations for each course day: https://uppmax.github.io/R-matlab-julia-HPC/</p> <ul> <li> <p>Day 1, Mon. 24. March</p> <ul> <li>9:00 - 16:00 R</li> </ul> </li> <li> <p>Day 2, Tue. 25. March</p> <ul> <li>9:00 - 16:00 MATLAB</li> </ul> </li> <li> <p>Day 3, Wed. 26. March</p> <ul> <li>9:00 - 16:00 Julia</li> </ul> </li> </ul>","tags":["course","workshop","R","Julia","MATLAB","intro","introduction"]},{"location":"courses_workshops/R_matlab_julia/#materials","title":"Materials","text":"<pre><code>Exercises and .rst files can be downloaded from the course's GitHub page: &lt;https://github.com/UPPMAX/R-matlab-julia-HPC&gt;\nRendered presentations can be found here: &lt;https://uppmax.github.io/R-matlab-julia-HPC/&gt;\nRecordings are here: TBA\nQ/A document for each day, as PDF: TBA\n</code></pre>","tags":["course","workshop","R","Julia","MATLAB","intro","introduction"]},{"location":"courses_workshops/R_matlab_julia/#links-to-refresher-material","title":"Links to refresher material","text":"<p>This is NOT in any way mandatory for participation or part of the course. It is a list of links to useful refresher material for those who would like to read up on Julia/R/MATLAB/Linux/etc. before the course.</p> <ul> <li>Julia<ul> <li>Aalto Univ.: https://github.com/AaltoRSE/julia-introduction</li> <li>Software Carpentry: https://carpentries-incubator.github.io/julia-novice/</li> </ul> </li> <li>R<ul> <li>Software Carpentry: https://swcarpentry.github.io/r-novice-gapminder/index.html</li> <li>Parallel R: https://github.com/menzzana/parallel_R_course</li> </ul> </li> <li>MATLAB<ul> <li>Software Carpentry: https://swcarpentry.github.io/matlab-novice-inflammation/</li> <li>MATLAB documentation at MathWorks: https://se.mathworks.com/help/matlab/index.html</li> </ul> </li> <li>Linux intro<ul> <li>Linux intro from \"Introduction to Kebnekaise\": https://hpc2n.github.io/intro-linux/  (Recordings)</li> <li>Material in the UPPMAX introduction course</li> </ul> </li> <li>Slurm<ul> <li>Contained in the \"Introduction to Kebnekaise\" course: https://hpc2n.github.io/intro-course/batch/ (Recordings)</li> <li>UPPMAX Slurm guide</li> <li>Material in the UPPMAX introduction course</li> </ul> </li> </ul> <p>Time and Dates: 24-26 March 2025, three days, one for each language. 9:00 - 16:00 each day. The last hour each day will be used for extra time for exercises.</p> <p>Onboarding: Friday, 21. March (1 hour - time to be decided)</p> <p>Location: ONLINE. Zoom link will be sent to participants a few days before the course.</p> <p>Deadline for registration: 17. March 2025</p> <p>Registration from HPC2N page</p> <p>Participation in the course is free.</p> <p>Please make sure have an account at SUPR as well as at NSC if you want to participate in the hands-on part of the training. There will be a course project on NSC that can be used to run the examples in during the hands-on. If you are affiliated with IRF, LTU, UMU, MIUN, or SLU and have account/project at HPC2N you can use HPC2N's local cluster if you prefer. Also, if you have an account/project at LUNARC or one at UPPMAX, you may use that instead if you want. If you do not have an account at SUPR and/or UPPMAX/HPC2N/LUNARC/NSC, you will be contacted with further instructions for how to create those. You are STRONGLY encouraged to sign up to SUPR as soon as possible after registering for the course.</p> <p>NOTE:</p> <pre><code>Kebnekaise has become a local resource. Please also read the page about \"Kebnekaise will be retired as a national resource\". HPC2N accounts are ONLY meant for people who are at Ume\u00e5 university, one of HPC2N's partnersites (IRF, LTU, MIUN, SLU), or are in a research group with a PI at one of those.\nCosmos (LUNARC) is also a local resource, for those at Lund University.\nUPPMAX accounts are only for local Uppsala people.\nEveryone else must use NSC for the course.\n</code></pre> <p>Course project: As part of the hands-on, you may be given temporary access to a course project, which will be used for running the hands-on examples. There are some policies regarding this, that we ask that you follow:</p> <pre><code>You may be given access to the project before the course; please do not use the allocation for running your own codes in. Usage of the project before the course means the priority of jobs submitted to it goes down, diminishing the opportunity for you and your fellow participants to run the examples during the course. You can read more detailed information about the job policies of NSC here and NSC usage rules here.\nThe course project will be open 1-2 weeks after the course, giving the participants the opportunity to test run examples and shorter codes related to the course. During this time, we ask that you only use it for running course related jobs. Use your own discretion, but it could be: (modified) examples from the hands-on, short personal codes that have been modified to test things learned at the course, etc.\nAnyone found to be misusing the course project, using up large amounts of the allocation for their own production runs, will be removed from the course project.\nYou will likely also be given access to a storage area connected to the compute project. Any data you store there should be course-related and if you wish to save it you should copy it to somewhere else soon after the course as it will be deleted about a month later.\n</code></pre> <p>The course uses compute resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) at NSC partially funded by the Swedish Research Council through grant agreement no. 2022-06725.</p>","tags":["course","workshop","R","Julia","MATLAB","intro","introduction"]},{"location":"courses_workshops/awk/","title":"AWK","text":"","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#awk-workshop","title":"Awk workshop","text":"<p>AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool.</p> <p>This two-days workshop aims to promote and demonstrate the flexibility of the tool, where the overhead of more sophisticated approaches and programming languages is not worth the bother.</p>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#learn-how-to","title":"Learn how to","text":"<ul> <li>use Awk as an advanced grep command, capable of arithmetic selection rules with control over the content of the matched lines</li> <li>perform simple conversions, analysis or filter you data on the fly making it easy to plot or read in your favorite research tool</li> <li>handle and take advantage on data split over multiple file data sets.</li> <li>use Awk as simple function or data generator</li> <li>perform simple sanity checks on your results</li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#awk-for-bioinformaticians","title":"Awk for bioinformaticians","text":"<p>Use what you learn and dive into the basic concepts of bioinformatics with simple exercises on typical scientific problems and tasks.</p> <p>Venue and registration:</p> <ul> <li>Date: 28 and 29 August, 2025</li> <li>Time: 9:15 - 12:00 and 13:15 -16:00</li> <li> <p>Location: Uppsala, EBC, House 7, room 1003, MazeMap.</p> </li> <li> <p>Application: form.</p> </li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#schedule","title":"Schedule","text":"","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#1-st-day-915-1200","title":"1-st day 9:15 - 12:00","text":"<p>Seminar session</p> <ul> <li>Examples of typical problems suitable for Awk \u201ctreatment\u201d</li> <li>Introduction to the basics of Awk scripting language</li> <li>Solving interactively simple problems</li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#1-st-day-lunch-break","title":"1-st day lunch break","text":"<p>Exercises 13:15 -16:00</p> <ul> <li>Solving interactively the exercise problems</li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#2-nd-day-915-1200","title":"2-nd day 9:15 - 12:00","text":"<ul> <li>Awk for bioinformaticians - seminar</li> <li>Case Study: Manipulating the output from a genome analysis - vcf and gff</li> <li>Filtering and formatting raw data</li> <li>Counting and piling features</li> <li>Indexing and hashing to compare variants and annotations</li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#2-nd-day-lunch-break","title":"2-nd day lunch break","text":"<p>Walk-through session on various topics:</p> <ul> <li>Awk parsing \u201csimultaneously\u201d multiple input files</li> <li>Multiple input files - second approach scenario will be discussed.</li> <li>How to trick <code>awk</code> to accept options on the command line   like regular program,   i.e. <code>$ script.awk filename parameter1 parameter2</code> from Pavlin's notes</li> <li>Declaring and calling functions in awk</li> <li>Input/output to/from an external programs</li> <li>Learn how to send input to an external program (might be based on your data)   and read the result back,   from Pavlin's notes</li> <li>Handy tips: awk oneliners use with Vim, gnuplot\u2026</li> </ul> <p>Also: Suggest topic for discussion or see recently suggested topics.</p>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#prerequisites","title":"Prerequisites","text":"","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#macos","title":"MacOS","text":"<p>The system provided awk version will work for most of the examples during the workshop with few exceptions, which are noted in the online material.</p> <p>Tilda <code>~</code> sign on Mac with Swedish keyboard layout - Alt + ^</p>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#linux","title":"Linux","text":"<p>Several distributions have other awk flavors installed by default. The easiest fix is to install the gnu version gawk i.e. for Ubuntu: <code>sudo apt install gawk</code></p>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#windows-1011","title":"Windows 10/11","text":"<ul> <li>Ubuntu for Windows 10 - it is better to read from the source, despite it might not be the easiest tutorial. To my experience, this is the best Linux environment without virtualization.</li> <li>MobaXterm use the internal package manager to install gawk. The default is provided by Busybox and is not enough for the purpose of the workshop.</li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#linux-computer-center","title":"Linux computer center","text":"<ul> <li>Just login to your account and use the provided awk - any version newer than 4 will work.</li> </ul> <pre><code>rackham3:[~] awk -V GNU Awk 4.0.2 Copyright (C) 1989, 1991-2012 Free Software Foundation.\n</code></pre>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#virtual-linux-machine","title":"Virtual Linux Machine","text":"<p>Just follow some tutorial on how to setup and use the virtual Linux environment.</p> <ul> <li>VirtualBox</li> <li>Ubuntu on Public Clouds</li> <li>GitHub &amp; Binder (you need only a browser)</li> <li>Singularity <code>singularity run shub://pmitev/Teoroo-singularity:gawk 'BEGIN{ for(i=1;i&lt;=4;i++) print i}'</code></li> </ul> Feedback from previous workshops <ul> <li>2024.08 | 2024.01</li> <li>2023.09 | 2023.01 (not enough data to be anonymous)</li> <li>2022.09 | 2022.01 \u200b- 2021.09 | 2021.01</li> <li>2020.08 | 2020.01</li> <li>2019.08 | 2019.01</li> <li>2018.08 | 2018.01</li> <li>2017.01 | 2017.08</li> <li>2016.08 | 2016.01</li> <li>2015.10</li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/awk/#contacts-for-the-course","title":"Contacts for the course","text":"<ul> <li>Pavlin Mitev</li> <li>Jonas S\u00f6derberg</li> <li>Lars Eklund</li> <li>Richel Bilderbeek</li> <li>UPPMAX</li> </ul>","tags":["course","workshop","AWK"]},{"location":"courses_workshops/bianca_intermediate/","title":"Bianca intermediate","text":""},{"location":"courses_workshops/bianca_intermediate/#bianca-in-depth-workshophackathon-improve-your-handling-of-sensitive-research-data","title":"Bianca In-Depth Workshop/Hackathon: Improve Your Handling of Sensitive Research Data","text":""},{"location":"courses_workshops/bianca_intermediate/#overview-and-schedule","title":"Overview and schedule","text":"<p>Are you already working with sensitive data in your research and feel your workflow can be improved? If yes, welcome to a full day of learning of smarter ways to work on the Bianca UPPMAX cluster. We will tell you how to login from a terminal (bypassing ThinLinc), do file transfer from a terminal (allowing scripts), advanced Slurm, using IDEs (i.e. RStudio and/or VSCode), and installing custom software and packages.</p> <p>This is a Q&amp;A-based hackathon for intermediate users of Bianca.</p> <p>Prerequisites: being able to login to Bianca, submit a Slurm bash script, and know how to transfer files.</p> <p>You do not need to be a member of a NAISS-SENS project in order to join the workshop. A SUPR course project will be available to all participants. The workshop will consist of both lectures and exercise sessions.</p> <p>When: Friday, May 23, 2025.</p> <p>Time: 09:00 - 12:00, and 13:00 - 16:00.</p> <p>Where: online via Zoom. Connection details will be sent to registered participants.</p> <p>Registration form: Closing May 19</p> <p>The workshop will consist of short lectures and longer Q&amp;A sessions.</p>"},{"location":"courses_workshops/bianca_intermediate/#tentative-schedule","title":"Tentative schedule","text":"<ul> <li>Introduction</li> <li>NAISS-SENS summary</li> <li>Transferring files to and from Bianca<ul> <li>several methods</li> </ul> </li> <li>Slurm jobs<ul> <li>jobstats</li> <li>GPU:s</li> </ul> </li> <li>Software and packages installation on Bianca<ul> <li>packages/libraries</li> <li>tools</li> <li>containers</li> </ul> </li> <li>Summary</li> <li>Q/A and extra material</li> </ul> <p>Course material</p>"},{"location":"courses_workshops/bianca_intro/","title":"Bianca intro","text":"","tags":["course","workshop","Bianca","intro","introduction"]},{"location":"courses_workshops/bianca_intro/#introduction-to-bianca-handling-sensitive-research-data","title":"Introduction to Bianca: Handling Sensitive Research Data","text":"<p>Are you just beginning to work with sensitive data in your research? If yes, welcome to a 1-day introduction to handling sensitive data on the UPPMAX cluster, Bianca. We will tell you about NAISS-SENS, how to login to Bianca, transfer files via wharf, basics of the Slurm workload manager and the module system.</p> <p>This workshop is intended for beginner users of Bianca.</p> <p>You do not need to be a member of a NAISS-SENS project in order to join the workshop. A SUPR course project will be available to all participants. The workshop will consist of both lectures and exercise sessions.</p> <p>Prerequisites: none.</p> <p>When: Monday, September 15, 2025.</p> <p>Time: 09:00 - 12:00, and 13:00 - 16:00.</p> <p>Where: online via Zoom. Connection details will be sent to registered participants.</p> <p>Login help session: No</p> <p>Registration form closing Wednesday 10 Sep at 16.00</p> <ul> <li>Coordinator: Bj\u00f6rn Claremar</li> </ul>","tags":["course","workshop","Bianca","intro","introduction"]},{"location":"courses_workshops/bianca_intro/#content","title":"Content","text":"<ul> <li>Introduction</li> <li>Intro to NAISS-Sens</li> <li>Login: ThinLinc</li> <li>Command line intro specific to Bianca</li> <li>Module system</li> <li>Intro to transferring files to and from Bianca</li> <li>Compute nodes and Slurm</li> <li>Using RStudio on Bianca</li> <li>Summary</li> <li>Q/A</li> </ul> <p>Workshop material</p>","tags":["course","workshop","Bianca","intro","introduction"]},{"location":"courses_workshops/courses_workshops/","title":"Overview","text":"","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#courses-and-workshops","title":"Courses and workshops","text":"<p>At UPPMAX, we teach, by providing workshops and courses. This page gives an overview of these.</p> <p>Course dates are (or should be) provided at each course's website.</p> <ul> <li>Intro UPPMAX</li> </ul>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#upcoming-courses-administered-by-uppmax","title":"Upcoming courses administered by UPPMAX","text":"","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#august","title":"August","text":"<p>Awk workshop  Aug 28-29</p>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#september","title":"September","text":"<p>Log in and transfer files to/from HPC Clusters  Sep 5</p> <p>Introduction to Bianca: Handling Sensitive Research Data  Sep 15</p>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#october","title":"October","text":"<p>Introduction to running R, MATLAB, and Julia in HPC  Oct 6-8 + 10</p> <p>Basic Singularity: Running and building Singularity containers  Oct 13</p> <p>Introduction to Python  Oct 14</p> <p>Introduction to Linux and UPPMAX  Oct 15-17</p> <p>Programming formalisms  Oct 23-24 + 27-29</p>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#november","title":"November","text":"<p>Log in and transfer files to/from HPC Clusters  Nov 14</p> <p>Bianca In-Depth Workshop/Hackathon:  Improve Your Handling of Sensitive Research Data  Nov 18</p> <p>Using Python in an HPC environment part 2  Nov 27-28</p>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#december","title":"December","text":"<p>Using Python in an HPC environment part 1  Dec 1-2</p>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#about-the-courses","title":"About the courses","text":"","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#learn-uppmax-systems","title":"Learn UPPMAX systems","text":"<p>The courses on how to use our local clusters, such as Rackham and Snowy.</p> <ul> <li>Introduction to Linux and UPPMAX</li> <li>Whisper for transcriptions</li> </ul>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#naiss-sens","title":"NAISS-Sens","text":"<p>The courses on how to use Bianca, a NAISS HPC cluster for sensitive data.</p> <ul> <li>Introduction to Bianca: Handling Sensitive Research Data</li> <li>Bianca In-Depth Workshop/Hackathon: Improve Your Handling of Sensitive Research Data</li> </ul>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#learn-linux-and-tools","title":"Learn Linux and tools","text":"<ul> <li>NAISS Introduction to LINUX</li> <li>NAISS Intermediate Bash and Linux</li> <li>NAISS File transfer</li> <li>Awk workshop</li> <li>Basic Singularity</li> </ul>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#general-programming","title":"General Programming","text":"<ul> <li>Intro to Python</li> <li>Programming formalisms</li> </ul>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#programming-in-hpc-environments","title":"Programming in HPC environments","text":"<p>UPPMAX is part of NAISS and we do teach things that apply to all NAISS HPC clusters.</p> <ul> <li>HPC-Python</li> <li>Introduction to running R, MATLAB, and Julia in HPC</li> </ul>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/courses_workshops/#other-centers-and-organizations","title":"Other centers and organizations","text":"<ul> <li>The SCoRe user documentation's overview of courses   shows all NAISS, SciLifeLab, ENCCS and more courses</li> <li>The NAISS calendar   shows all NAISS courses and also Zoom-ins (virtual open house)</li> </ul>","tags":["courses","workshops","teaching","classes","sessions"]},{"location":"courses_workshops/hpc_python/","title":"HPC Python","text":"","tags":["course","workshop","Python"]},{"location":"courses_workshops/hpc_python/#using-python-in-an-hpc-environment","title":"Using Python in an HPC environment","text":"<p>The course is given under NAISS and is a collaboration between UPPMAX, HPC2N, LUNARC, and InfraVis</p> <p>This online 4-day workshop aims to give a brief, but comprehensive introduction to using Python in a Swedish academic High-Performance Computing (HPC) environment. It covers a wide range of levels in using Python and you can decide which days to follow.</p> <p>If you are a complete beginner, perhaps the first day(s) is sufficient for you, but you may follow the other days as well for inspiration. If you already are a Python programmer you could still benefit from attending the HPC parts of the first day.</p> <p>The goals of the different days are</p> <p>Day 1 (summarized from all learning outcomes):</p> <ul> <li>Run Python on an HPC cluster</li> <li>Use variables, expressions and statements in Python</li> </ul> <p>Day 2</p> <ul> <li>Load modules for site-installed Python packages</li> <li>Use and install python packages (pip and/or conda)</li> <li>Use virtual environments</li> <li>Ask for compute resources, write batch scripts for running Python</li> <li>Do basic analysis with Matplotlib in Jupyter</li> </ul> <p>Day 3-4</p> <ul> <li>More about compute resources, including parallelism and GPUs</li> <li>Use Pandas and Seaborn,</li> <li>Big data and Python</li> <li>Use Python for ML/DL</li> <li>Guest lecture by Anders Hast and InfraVis introduction</li> </ul> <p>This course will consist of lectures and type-alongs interspersed with hands-on sessions where you get to try out what you have just learned.</p> <p>Remote/online participation: We will use Zoom. More information about connecting and about accounts and course projects and such will be sent to the participants close to the course. Tetralith at NSC will mainly be used for demos, but written material should guide users within academic Sweden on the resources they already are using.</p> <ul> <li> <p>Prerequisites:</p> <ul> <li>Day 1: none</li> <li>Days 2-4: familiarity with the LINUX command line. Basic Python knowledge (as taught on day 1) is recommended</li> </ul> </li> <li> <p>Pre-requirements</p> </li> <li>Prepare the environment</li> </ul> I'm used to Python but haven't used an HPC cluster. What to do? <ul> <li>You need to:<ul> <li>be able to login to your HPC cluster</li> <li>be able to start Python interpreter or run scripts.</li> </ul> </li> <li>How to get these skills?<ul> <li>Please attend the Login session 9-10 Day 1 (Thursday 24 April)</li> <li>Or be sure you can do the steps yourself</li> </ul> </li> </ul> <p>Preliminary Schedule:</p> <p>Materials:</p> Day Course materials More information 1 Course materials More information 2-4 Course materials More information <p>Time and date: Thursday 27-Friday 28 November, Monday 1-Tuesday 2 December 2025, 09:00-16:00 each day</p> <p>Location: ONLINE. Zoom link will be sent to participants a few days before the course.</p> <p>Instructors: Birgitte Bryds\u00f6, Bj\u00f6rn Claremar, Jayant Yadav, Pedro Ojeda-May, Rebeccas Pitts, Rich\u00e8l Bilderbeek, Anders Hast(InfraVis)</p> <p>Deadline for registration: TBA</p> <p>Please register by filling in the form at TBA</p>","tags":["course","workshop","Python"]},{"location":"courses_workshops/intro_to_python/","title":"Intro to Python","text":"","tags":["course","workshop","Python","HPC","intro","introduction"]},{"location":"courses_workshops/intro_to_python/#introduction-to-python","title":"Introduction to Python","text":"<p>Tip</p> <p>Great course. Now, Python is not scary anymore.</p> <p>A learner from this course (source)</p> <p>This 1-day course helps you get started with Python, by working ourselves through an online and free book. Although the focus is on learning Python, we do work in an HPC environment and you will be taught the bare minimum to run Python on an UPPMAX HPC cluster. The pace of this highly interactive course is set by the majority of learners, ensuring that any complete beginner has enough time for exercises. At the end of the day, you should feel comfortable with the basics of Python and feel familiar to a book on Python to help you on your next steps.</p> <p>You will:</p> <ul> <li>Feel comfortable with learning Python</li> <li>Feel comfortable using an online and free book on Python</li> <li>Write Python code on an HPC cluster</li> <li>Run Python scripts on an HPC cluster</li> </ul>","tags":["course","workshop","Python","HPC","intro","introduction"]},{"location":"courses_workshops/intro_to_python/#practical-matters","title":"Practical matters","text":"<ul> <li>Registration form:<ul> <li>Registration form</li> <li>Registration closes at 2025-10-09</li> </ul> </li> <li>Course prerequisites</li> <li>Date: Tuesday October 14th 2025 (course dates)</li> <li>When: 9:00-16:00 (course schedule)</li> <li>Where: Online via Zoom. Zoom room and password will be mailed </li> <li>Course material</li> <li>Earlier evaluations of the course</li> </ul>","tags":["course","workshop","Python","HPC","intro","introduction"]},{"location":"courses_workshops/intro_to_python/#questions","title":"Questions","text":"<ul> <li>The FAQ page of the course</li> <li>About the course content: contact the main teacher, Rich\u00e8l Bilderbeek,   at <code>richel.bilderbeek@uppmax.uu.se</code></li> <li>About higher-level things: contact the coordinator, Diana Iusan</li> </ul>","tags":["course","workshop","Python","HPC","intro","introduction"]},{"location":"courses_workshops/intro_to_python/#coordinators","title":"Coordinators","text":"<ul> <li>Diana Iusan</li> </ul>","tags":["course","workshop","Python","HPC","intro","introduction"]},{"location":"courses_workshops/naiss_transfer/","title":"File transfer","text":"","tags":["course","workshop","NAISS","file","data","transfer"]},{"location":"courses_workshops/naiss_transfer/#transferring-files-tofrom-hpc-clusters","title":"Transferring files to/from HPC Clusters","text":"<p>In this 3-hour workshop you learn to transfer files to or from Swedish academic HPC clusters. We will cover graphical as well as terminal tools and you will work highly interactively. A log in session will be included as well. At the end of the day, you should be comfortable in transferring files between local computer and a cluster and cross-clusters, and choosing the right tool for your use cases.</p> <p>The workshop is intended for beginner users but with some Linux experience, see the prerequisites link below. You do not need to be a member of a NAISS project in order to join the workshop. A course project on one of the NAISS clusters will be available to those.</p>","tags":["course","workshop","NAISS","file","data","transfer"]},{"location":"courses_workshops/naiss_transfer/#coming-course-instance","title":"Coming course instance","text":"<ul> <li>When: Fri 5 Sep, 2025, 9.00-12.00</li> <li>Where: Online via Zoom</li> <li>Registration</li> <li>Prerequisites</li> </ul>","tags":["course","workshop","NAISS","file","data","transfer"]},{"location":"courses_workshops/naiss_transfer/#more-information","title":"More information","text":"<ul> <li>Course website</li> <li>Future  course dates</li> <li>Course schedule</li> </ul>","tags":["course","workshop","NAISS","file","data","transfer"]},{"location":"courses_workshops/programming_formalisms/","title":"Programming formalisms","text":""},{"location":"courses_workshops/programming_formalisms/#programming-formalisms","title":"Programming formalisms","text":"<p>The goal of this highly-interactive 5-day course is to be able to develop academic software that you can trust to be 'good enough'. We assume you have written code 'that (sometimes) just works'.</p> <p>The course follows a formal development process from start to finish, with a selection of topics and best practices we think are most important, with the goal of developing academic software that is actually good enough. The course aims to strengthen the understanding of more advanced programming concepts, ability to produce more reusable scripts through modular programming and to enable a better understanding of how to evaluate a script or programs performance.</p> <p>We will cover:</p> <ul> <li>an introduction to Algorithms and Data structures</li> <li>programming Paradigms especially structured and object oriented programming</li> <li>an overview of other paradigms like functional programming.</li> <li>modular development and (code) reusability, testing and optimisation.</li> </ul> <p>We will cover theory with bridging practical examples and applications to enhance the theoretical understanding of the principles.</p> <p>Prerequisites: We recommend basic knowledge in the following topics.</p> <ul> <li>Git, of either<ul> <li>NBIS/ELIXIR-SE Tools for Reproducible research</li> <li>or the CodeRefinery courses Introduction to version control with Git and Collaborative distributed version control.</li> </ul> </li> <li> <p>We assume familiarity with python at least equivalent of the Intro to Python course or Python programming with applications to bioinformatics.</p> </li> <li> <p>Setups</p> </li> </ul> <p>Course material.</p>"},{"location":"courses_workshops/programming_formalisms/#coming-course-instance","title":"Coming course instance","text":"<ul> <li>When: May 5-9, 9:00-16:00</li> <li>Where: Online (Zoom). The link will be sent to registered participants prior to the course.</li> <li> <p>Online on-boarding: TBD. Get help with setups.</p> </li> <li> <p>Registration form</p> </li> </ul>"},{"location":"courses_workshops/singularity/","title":"Singularity","text":""},{"location":"courses_workshops/singularity/#basic-singularity","title":"Basic Singularity","text":""},{"location":"courses_workshops/singularity/#running-and-building-singularity-containers","title":"Running and building Singularity containers","text":"<p>The online workshop is an introduction to the basic concepts of containerized software environment solution within the Singularity framework https://sylabs.io/singularity/.</p> <p>During the workshop you will have the opportunity to follow the interactive guide on how to</p> <ul> <li>run Singularity containers</li> <li>how to build your own</li> <li>good (and bad) practices on designing and building Singularity recipes</li> <li>build and/or host container remotely</li> </ul> <p>To fully benefit from the workshop, basic Linux system administration experience is highly desirable i.e. knowledge of package management and common tools for building and managing of software: git, pip, conda, wget, curl \u2026</p> <p>\ud83d\udcda Workshop tutorial material</p> <p>Please follow the instructions to install Singularity on your computer or, alternatively, use apptainer provided by your computer center. Please, follow the center provided instructions.</p>"},{"location":"courses_workshops/singularity/#workshop-information","title":"WORKSHOP INFORMATION","text":"<p>\ud83c\udf0d Place: Zoom: link will be sent to applicants</p> <p>\ud83d\udcc5 Date and time: 13 October, 2025: 9:15-12:00; 13:15-16:00</p> <p>\ud83d\udcdd Registration form</p>"},{"location":"courses_workshops/singularity/#contacts-for-the-workshop","title":"Contacts for the workshop","text":"<p>Please direct any questions or comments to</p> <ul> <li>Pavlin Mitev - UPPMAX</li> <li>Pedro Ojeda May - HPC2N</li> </ul>"},{"location":"courses_workshops/uppmax_intro_course/","title":"UPPMAX intro","text":"","tags":["course","workshop","UPPMAX","Linux","intro","introduction"]},{"location":"courses_workshops/uppmax_intro_course/#introduction-to-linux-and-uppmax","title":"Introduction to Linux and UPPMAX","text":"","tags":["course","workshop","UPPMAX","Linux","intro","introduction"]},{"location":"courses_workshops/uppmax_intro_course/#overview-and-schedule","title":"Overview and Schedule","text":"<p>UPPMAX application experts want to share their skills in a 3-day series of lectures. We will help you move from being a Linux novice to an UPPMAX expert. If you already have the fundamentals down, you are still sure to enjoy the tips and tricks in the later parts of the course. The lectures covering Linux and bash scripting are cluster-agnostic and may be attended by non-UPPMAX users as well. It is possible to sign up only for the lectures that are interesting to you.</p> <p>August 2025 is cancelled</p> <p>The August 2025 course has been cancelled.</p> <p>When: October 15-17, 2025.</p> <p>Where: online via Zoom.</p> <p>Registration form</p>","tags":["course","workshop","UPPMAX","Linux","intro","introduction"]},{"location":"courses_workshops/uppmax_intro_course/#schedule","title":"Schedule","text":"Wednesday, October 15 Thursday, October 16 Friday, August 17 Morning Intro to UPPMAX and Linux 1/2Rich\u00e8l Bilderbeek Linux IIDouglas Scofield Bash ScriptingDouglas Scofield Afternoon Intro to UPPMAX and Linux 2/2Rich\u00e8l Bilderbeek Linux IIIDouglas Scofield Slurm at UPPMAXDiana Iusan <p>The lectures are scheduled 09:00 to 12:00 and 13:00 to 16:00 daily.</p>","tags":["course","workshop","UPPMAX","Linux","intro","introduction"]},{"location":"courses_workshops/uppmax_intro_course/#startup-instructions-to-course-participants","title":"Startup instructions to course participants","text":"<p>Approximately two weeks before the course starts, you will receive a set of instructions for creating an account and joining the course project. It is important that you complete these steps well in advance of the course.</p>","tags":["course","workshop","UPPMAX","Linux","intro","introduction"]},{"location":"databases/1000-genome_project/","title":"1000-genome project","text":""},{"location":"databases/1000-genome_project/#1000-genomes-project","title":"1000 genomes project","text":"<p>The 1000-genome project is an international collaboration to sequence the genomes of a large number of people. The complete archive is available from NCBI and EBI but downloading this massive quantity of next-gen data is time- and resource-consuming. UPPMAX now has a local copy of the sequencing and index files (BAM, BAI and BAS) as a shared resource.</p> <p>The main archive is stored at /sw/data/KGP/central. Within this folder, \"low\" holds the primary dataset with one individual per folder (eg, \"HG00096\", \"NA11831\") holding data files for each sequencing technology applied. In the main folder, \"high\" holds the high-coverage data for a subset of the individuals.</p> <p>One level up in the file system, /sw/data/KGP/regional holds sequence data for some individual countries outside the 1000-genome project. So far, very little data has been stored but this may be expanded.</p> <p>Users interesting in any of this data should request membership in the \"KGP\" group (via <code>support@uppmax.uu.se</code>). This requirement is not intended to restrict the resource in any way, but makes it easier to inform interested users of possible changes. Considering the large storage space used, it is possible that the data would need to be reorganized or possibly even reduced in the future, depending of course on the perceived need for the resource by the members of the KGP group.</p>"},{"location":"databases/blast/","title":"BLAST databases available locally","text":""},{"location":"databases/blast/#blast-databases-available-locally","title":"Blast databases available locally","text":"<p>Many pipelines involving annotation/assembly comparison involve Blast. Several Blast versions are available as modules, for example:</p> <ul> <li><code>blast/2.12.0+</code>, etc. : the Blast+ suites (blastp, tblastn, etc.), recommended</li> <li><code>diamond/2.0.14</code> : the DIAMOND protein aligner, recommended for protein databases. See UPPMAX's DIAMOND database webpage for more information.</li> <li><code>blast/2.2.26</code>, etc. : 'legacy' Blast (blastall, megablast, etc)</li> </ul> <p>Use module spider blast to see available versions. As for all bioinformatics tools at Uppmax, module load bioinfo-tools is required before the blast modules are available.</p> <p>Uppmax maintains local copies of many Blast databases, including many available at NCBI:</p> <ul> <li><code>ftp://ftp.ncbi.nih.gov/blast/db/README</code></li> <li><code>ftp://ftp.ncbi.nlm.nih.gov/blast/documents/blastdb.html</code></li> <li><code>https://www.ncbi.nlm.nih.gov/books/NBK62345/</code></li> <li><code>https://ncbiinsights.ncbi.nlm.nih.gov/2020/02/21/rrna-databases/</code></li> <li><code>https://www.ncbi.nlm.nih.gov/sars-cov-2/</code></li> <li><code>https://www.ncbi.nlm.nih.gov/refseq/refseq_select/</code></li> <li><code>https://blast.ncbi.nlm.nih.gov/smartblast/smartBlast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs#searchSets</code></li> </ul> <p>as well as several UniProt databases.</p> <p>Note that:</p> <pre><code>The local UPPMAX copies are found at /sw/data/blast_databases\nDoing module load blast_databases sets the environment variable BLASTDB to this directory; this is loaded as a prerequisite when loading any blast modules\nNew versions are installed the first day of each month at 00.01 from local copies updated the 28th of the previous month beginning at 00.01\nWhen new versions are installed, the directory containing the previous versions is renamed to blast_databases_old\nblast_databases_old is deleted the second data of each month at 00.01\n</code></pre> <p>These databases use the \"v5\" format, which includes rich taxonomic information with sequences, and will only work with the Blast tools from the module blast/2.8.0+ and later. Earlier module versions can still be used, but you will need to provide/build your own databases. NCBI no longer updates databases with the older \"v4\" databases as of February 2020, and they have been deleted from UPPMAX. The final updates of these databases (again, as of this writing nearly two years old) are available from NCBI over FTP at ftp://ftp.ncbi.nlm.nih.gov/blast/db/v4.</p> <p>Each NCBI-hosted database also includes a JSON file containing additional metadata for that particular database. These are found in /sw/data/blast_databases/ and are named databasename*.json. The exact name varies based on the format of the database. For example, the contents of the JSON file for the nr database can be see by running</p> <pre><code>cat /sw/data/blast_databases/nr*.json\n</code></pre> <p>The Blast databases available at UPPMAX are:</p> Name Type Source Notes 16S_ribosomal_RNA nucleotide NCBI 16S ribosomal RNA (Bacteria and Archaea type strains) 18S_fungal_sequences nucleotide NCBI 18S ribosomal RNA sequences (SSU) from Fungi type and reference material (BioProject PRJNA39195) 28S_fungal_sequences nucleotide NCBI 28S ribosomal RNA sequences (LSU) from Fungi type and reference material (BioProject PRJNA51803) Betacoronavirus nucleotide NCBI Betacoronavirus nucleotide sequences cdd_delta protein NCBI Conserved domain database for use with delta-blast env_nr protein NCBI Protein sequences for metagenomes (EXCLUDED from nr) env_nt nucleotide NCBI Nucleotide sequences for metagenomes human_genome nucleotide NCBI Current RefSeq human genome assembly with various database masking ITS_eukaryote_sequences nucleotide NCBI Internal transcribed spacer region (ITS) for eukaryotic sequences ITS_RefSeq_Fungi nucleotide NCBI Internal transcribed spacer region (ITS) from Fungi type and reference material (BioProject PRJNA177353) landmark protein NCBI Proteomes of 27 model organisms. The landmark database includes complete proteomes from a few selected representative genomes spanning a wide taxonomic range, the main database used by the SmartBLAST services. LSU_eukaryote_rRNA nucleotide NCBI Large subunit ribosomal RNA sequences for eukaryotic sequences LSU_prokaryote_rRNA nucleotide NCBI Large subunit ribosomal RNA sequences for prokaryotic sequences mito nucleotide NCBI NCBI Genomic Mitochondrial Reference Sequences mouse_genome nucleotide NCBI Current RefSeq mouse genome assembly with various database masking nr protein NCBI Non-redundant protein sequences from GenPept, Swissprot, PIR, PDF, PDB, and NCBI RefSeq nt nucleotide NCBI Partially non-redundant nucleotide sequences from all traditional divisions of GenBank, EMBL, and DDBJ pataa protein NCBI Patent protein sequences patnt nucleotide NCBI Patent nucleotide sequences. Both patent databases are directly from the USPTO, or from the EPO/JPO via EMBL/DDBJ pdbaa protein NCBI Sequences for the protein structure from the Protein Data Bank pdbnt nucleitide NCBI Sequences for the nucleotide structure from the Protein Data Bank. They are NOT the protein coding sequences for the corresponding pdbaa entries. ref_euk_rep_genomes nucleotide NCBI Refseq Representative Eukaryotic genomes (1000+ organisms) ref_prok_rep_genomes nucleotide NCBI Refseq Representative Prokaryotic genomes (5700+ organisms) ref_viroid_rep_genomes nucleotide NCBI Refseq Representative Viroid genomes (46 organisms) ref_viruses_rep_genomes nucleotide NCBI Refseq Representative Virus genomes (9000+ organisms) refseq_protein protein NCBI NCBI protein reference sequences refseq_rna nucleotide NCBI NCBI Transcript reference sequences refseq_select_prot protein NCBI NCBI RefSeq protein sequences from human, mouse, and prokaryotes, restricted to the RefSeq Select set of proteins. RefSeq Select includes one representative protein per protein-coding gene for human and mouse, and RefSeq proteins annotated on reference and representative genomes for prokaryotes refseq_select_rna nucleotide NCBI NCBI RefSeq transcript sequences from human and mouse, restricted to the RefSeq Select set with one representative transcript per protein-coding gene SSU_eukaryote_rRNA nucleotide NCBI Small subunit ribosomal RNA sequences for eukaryotic sequences swissprot protein NCBI Swiss-Prot sequence database (last major update) tsa_nr protein NCBI Protein sequences from the Transcriptome Shotgun Assembly. Its entries are EXCLUDED from the nr database. tsa_nt nucleotide NCBI A database with earlier non-project based Transcriptome Shotgun Assembly (TSA) entries. Project-based TSA entries are NOT included. Entries are EXCLUDED from the nt database. uniprot_sprot protein UniProt Swiss-Prot high quality manually annotated and non-redundant protein sequence database uniprot_trembl protein UniProt TrEMBL high quality but unreviewed protein sequence database uniprot_sptrembl protein uniprot_sprot and uniprot_trembl combined uniprot_all protein alias for uniprot_sptrembl uniprot_all.fasta protein alias for uniprot_sptrembl uniprot_sprot_varsplic protein UniProt UniProt canonical and isoform sequences (see link) uniprot_uniref50 protein UniProt Clustered sets of 50%-similar protein sequences (see link) uniprot_uniref90 protein UniProt Clustered sets of 90%-similar protein sequences (see link) uniprot_uniref100 protein UniProt Clustered sets of identical protein sequences (see link) UniVec nucleotide UniVec Sequences commonly attached to cDNA/genomic DNA during the cloning process UniVec_Core nucleotide UniVec A subset of UniVec chosen to minimise false positives <p>Additionally, taxdb.btd and taxdb.bti are downloaded, which provide additional taxonomy information for these databases. Local copies of the NCBI Taxonomy databases are also available; further details are available on a separate page.</p> <p>For UniVec and UniVec_Core, Fasta-format files containing the vector sequences are also available with the given names (e.g., /sw/data/uppnex/blast_databases/UniVec), alongside the Blast-format databases built from the same Fasta files.</p> <p>The exact times all databases were updated are provided by database.timestamp files located in the directory Databases are available automatically after loading any blast module</p> <p>When any of the blast modules is loaded, the BLASTDB environment variable is set to the location of the local database copies (/sw/data/uppnex/blast_databases). The various Blast tools can use this variable to find the locations of databases, so that only the name needs to be specified.</p> <pre><code>module load bioinfo-tools blast/2.7.1+\nblastp -db nr -query input.fasta\n</code></pre> <p>After loading the blast/2.7.1+ module, specifying blastp -db nr results in blastp searching the local copy of nr, because the BLASTDB environment variable is set when the module is loaded. Similarly, each of these would result in searching the local copy of the given database:</p> <pre><code>blastp -db pdbaa ...\nblastp -db uniprot_sprot ...\nblastp -db uniprot_uniref90 ...\nblastn -db nt ...\nblastn -db refseq_genomic ...\n</code></pre> <p>WGS and SRA sequence databases are not included</p> <p>The NCBI Whole-Genome Shotgun is not available locally. NCBI provides special versions of Blast and other tools that can be used to search the remote versions of WGS and the Sequence Read Archive.</p> <p>These special blast versions and other tools are part of NCBI's SRA Tools, which is available at Uppmax as the sratools module. We have also include auxiliary NCBI scripts in the sratools module to convert taxonomic IDs to WGS and SRA identifiers.</p> <p>Note that NCBI's TSA database is available at UPPMAX, just use the database name tsa_nr or tsa_nt.</p>"},{"location":"databases/diamond/","title":"DIAMOND protein alignment databases","text":""},{"location":"databases/diamond/#diamond-protein-alignment-databases","title":"DIAMOND protein alignment databases","text":"<p>The DIAMOND protein aligner is a recent tool offering much faster (100\u00d7 to 1000\u00d7 faster than Blast) alignment of protein sequences against reference databases. On UPPMAX, DIAMOND is available by loading the <code>diamond</code> module, the most recent installed version of which which as of this writing is diamond/2.0.14.</p> <p>As for BLAST databases, UPPMAX provides several pre-built databases suitable for direct usage with the --db flag to diamond, as well as runs diamond prepdb on each of its downloaded BLAST protein databases whenever they are installed. The BLAST databases are updated according to the schedule given on their webpage. The diamond-format NCBI protein databases are updated once a month.</p> <p>For each of the databases listed below, the method of versioning is indicated. To determine the version at UPPMAX, check the path given below after removing the database name from the last position; latest is a symbolic link that points to a directory with a name equivalent to the version of the most recent update. Old database versions will be removed after updates, so please use latest rather than directly addressing a database version.</p> <p>Each of the database locations below is also available in the indicated environment variable set when any version of the diamond module is loaded. These are simple to use, for example to search nr:</p> <pre><code>diamond --db $DIAMOND_NR ...\n</code></pre> <p>NCBI BLAST Protein Databases</p> <p>Whenever the BLAST databases are updated and installed, diamond prepdb is run on each of the protein-format databases so that they can be searched directly by diamond. See the BLAST databases webpage for a description of these.</p> <p>To search any of them using diamond, load the blast_databases/latest module. This defines the environment variable BLASTDB, which contains the directory holding these databases. Once this module is loaded, you can run diamond on any of the protein databases. For example:</p> <pre><code>diamond --db $BLASTDB/nr ...\ndiamond --db $BLASTDB/cdd_delta ...\ndiamond --db $BLASTDB/swissprot ...\ndiamond --db $BLASTDB/pdbaa ...\n</code></pre> <p>According to DIAMOND's developer, these are faster to load than DIAMOND's own <code>.dmnd</code>-format databases. So, you may want to load the <code>blast_databases/latest data</code> module and use <code>--db $BLASTDB/nr</code> for your NCBI <code>nr</code> searches, for example, instead of <code>--db $DIAMOND_NR</code>.</p> <p>Diamond-format NCBI Protein Databases</p> <p>Downloaded from <code>ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA</code>. These are updated frequently at NCBI, so they are versioned here by the monthly download date. There is no longer a separate FASTA version of <code>env_nr</code>, so its Blast database is downloaded from <code>ftp://ftp.ncbi.nlm.nih.gov/blast/db</code> and FASTA sequences are extracted using <code>blastdbcmd -entry all from module blast/2.12.0+</code>.</p> Database Environment variable for diamond --db UPPMAX path nr DIAMOND_NR /sw/data/diamond_databases/Blast/latest/nr env_nr DIAMOND_ENV_NR /sw/data/diamond_databases/Blast/latest/env_nr swissprot DIAMOND_SWISSPROT /sw/data/diamond_databases/Blast/latest/swissprot pdbaa DIAMOND_PDBAA /sw/data/diamond_databases/Blast/latest/pdbaa <p>NCBI RefSeq Proteins</p> <p>RefSeq protein databases are downloaded from <code>ftp://ftp.ncbi.nlm.nih.gov/refseq/release/complete/</code>, with an update occurring if there is a new release as indicated by the contents of <code>ftp://ftp.ncbi.nlm.nih.gov/refseq/release/RELEASE_NUMBER</code>.</p> Database Environment variable for diamond --db UPPMAX path complete.nonredundant_protein.protein DIAMOND_REFSEQ_NONREDUNDANT /sw/data/diamond_databases/RefSeq/latest/complete.nonredundant_protein.protein complete.protein DIAMOND_REFSEQ /sw/data/diamond_databases/RefSeq/latest/complete.protein <p>UniRef90</p> <p>The UniRef90 protein database is downloaded as Fasta from its UK mirror at <code>ftp://ftp.expasy.org/databases/uniprot/current_release/uniref/uniref90/</code>, with an update occurring if there is a new version as indicated by the <code>&lt;version&gt;</code> tag in the XML description available at <code>ftp://ftp.expasy.org/databases/uniprot/current_release/uniref/uniref90/RELEASE.metalink</code>.</p> Database Environment variable for diamond --db UPPMAX path uniref90 DIAMOND_UNIREF90 /sw/data/diamond_databases/UniRef90/latest/uniref90 <p>UniProt Reference Proteomes</p> <p>The UniProt Reference Proteomes protein database is downloaded as Fasta from its UK mirror at <code>ftp://ftp.expasy.org/databases/uniprot/current_release/knowledgebase/reference_proteomes</code>, with an update occurring if there is a new version as indicated by the <code>&lt;version&gt;</code> tag in the XML description available at <code>ftp://ftp.expasy.org/databases/uniprot/current_release/knowledgebase/reference_proteomes/RELEASE.metalink</code>. If there is a new release, then the file <code>Reference_Proteomes_RELEASE.tar.gz</code> is downloaded, with <code>RELEASE</code> replaced by the release number. The <code>reference_proteomes.dmnd</code> database is created from this file using the protocol described after the table.</p> Database Environment variable for diamond --db UPPMAX path UniProt Reference Proteomes DIAMOND_REFERENCE_PROTEOMES /sw/data/diamond_databases/reference_proteomes/latest/reference_proteomes <p>The reference_proteomes.dmnd database is created using the following protocol for the BlobToolKit. This uses UPPMAX's most recently downloaded NCBI taxonomy database for its taxonomic metadata.</p> <pre><code>module load bioinfo-tools\nmodule load diamond/2.0.14\nmodule load ncbi_taxonomy/latest\n</code></pre>"},{"location":"databases/diamond/#after-downloading","title":"after downloading","text":"<pre><code>tar xf Reference_Proteomes_RELEASE.tar.gz\ntouch reference_proteomes.fasta.gz\nfind . -mindepth 2 | grep \"fasta.gz\" | grep -v 'DNA' | grep -v 'additional' | xargs cat &gt;&gt; reference_proteomes.fasta.gz\nprintf \"accession\\taccession.version\\ttaxid\\tgi\\n\" &gt; reference_proteomes.taxid_map\nzcat */*/*.idmapping.gz | grep \"NCBI_TaxID\" | awk '{print $1 \"\\t\" $1 \"\\t\" $3 \"\\t\" 0}' &gt;&gt; reference_proteomes.taxid_map\ndiamond makedb --db reference_proteomes.dmnd --in reference_proteomes.fasta.gz --threads 10 --taxonmap reference_proteomes.taxid_map --taxonnames $NCBI_TAXONOMY_ROOT/names.dmp --taxonnodes $NCBI_TAXONOMY_ROOT/nodes.dmp\n</code></pre>"},{"location":"databases/ncbi/","title":"NCBI taxonomy databases","text":""},{"location":"databases/ncbi/#ncbi-taxonomy-databases","title":"NCBI taxonomy databases","text":"<p>Uppmax maintains local copies of the full set of NCBI Taxonomy databases. Note that:</p> <ul> <li>The local copies are found at /sw/data/ncbi_taxonomy/latest</li> <li>The data module ncbi_taxonomy/latest defines the environment variable NCBI_TAXONOMY_ROOT to this location. We recommend loading this module and using this environment variable to access these data.</li> <li>This also contains the subdirectories new_taxdump, accession2taxid and biocollections containing those databases, see the tables below for their contents</li> <li>latest is a symbolic link to a directory named from the date of the most recent update</li> <li>There is also a subdirectory download containing the files as downloaded from NCBI</li> <li>The installation of new versions begins Sunday of each week at 00.10. The update may take several minutes up to an hour, depending on network speeds.</li> <li>When new versions are successfully installed, the latest/ symbolic link is updated to point to the new dated directory</li> <li>The previous version of the taxonomy databases are removed when the new versions have completed installation</li> </ul> <p>See the links for each database for specifics on file format and contents. Many tools know how to make use of these databases; follow each tool's specific instructions. The files can be found in the indicated directories.</p> <p>The databases available within /sw/data/ncbi_taxonomy/latest are below. For more on each, see the links.</p> Name Source Notes taxdump NCBI NCBI taxonomic database, in multiple .dmp files (see taxdump_readme.txt or link) taxcat NCBI NCBI taxonomic categories, in categories.dmp (see taxcat_readme.txt or link) taxdump_readme.txt NCBI NCBI taxdump file description taxcat_readme.txt NCBI NCBI taxcat file description gi_taxid_nucl.dmp NCBI Mappings of nucleotide GI to taxid (DEPRECATED) gi_taxid_prot.dmp NCBI Mappings of protein GI to taxid (DEPRECATED) <p>The databases available within /sw/data/ncbi_taxonomy/latest/new_taxdump are below. For more on each, see the links.</p> Name Source Notes new_taxdump NCBI NCBI new-format taxonomic database, in multiple .dmp files (see this taxdump_readme.txt or link) taxdump_readme.txt NCBI NCBI new-format taxonomic database file description <p>The databases available within /sw/data/ncbi_taxonomy/latest/accession2taxid are below. The dead_ files contain accession-to-TaxID mappings for dead (suppressed or withdrawn) sequence records. For more on each, see the links.</p> Name Source Notes nucl_wgs.accession2taxid NCBI TaxID mapping for nucleotide records of type WGS or TSA nucl_gb.accession2taxid NCBI TaxID mapping for nucleotide records not of the above types prot.accession2taxid NCBI TaxID mapping for protein records pdb.accession2taxid NCBI TaxID mapping for PDB protein records dead_nucl.accession2taxid NCBI TaxID mapping for dead nucleotide records dead_prot.accession2taxid NCBI TaxID mapping for dead protein records dead_wgs.accession2taxid NCBI TaxID mapping for dead WGS or TSA records <p>The biocollections databases contain collections location information. coll_dump.txt is located within the /sw/data/ncbi_taxonomy/latest directory. Those marked biocollections are located within the /sw/data/ncbi_taxonomy/latest/biocollections directory.</p> Name Source Notes coll_dump.txt NCBI . Collection_codes.txt NCBI biocollections Institution_codes.txt NCBI biocollections Unique_institution_codes.txt NCBI biocollections"},{"location":"databases/other_local/","title":"Other bioinformatics-oriented local data resources","text":""},{"location":"databases/other_local/#other-bioinformatics-oriented-local-data-resources","title":"Other bioinformatics-oriented local data resources","text":"<p>Haplotype Reference Consortium</p> <p>The Haplotype Reference Consortium VCF database is a large reference panel of human haplotypes produced by combining together sequencing data from multiple cohorts.  Version r1.1 is installed on all systems as data module HaplotypeReferenceConsortium/r1.1. GnomAD: Genome Aggregation Database</p> <p>The Genome Aggregation Database (gnomAD) VCF database is downloaded and located in /sw/data/gnomad_data/vcf/{exomes, genomes}. ExAC: Exome Aggregation Consortium</p> <p>The ExAC Exome Aggregation Consortium database releases 0.1, 0.2, 0.3 and 0.3.1 are downloaded in their entirety and are available at /sw/data/ExAC/release{0.1,0.2,0.3,0.3.1}. Pfam</p> <p>The Pfam database versions 2011, 28.0, 31.0 and 35.0 are downloaded in their entirety and available via the data modules Pfam/{2011,28.0,31.0,35.0} which each define the environment variable PFAM_ROOT to the location of the Pfam downloads. See the appropriate module help for further information. In particular, the family-specific trees are available in $PFAM_ROOT/trees. The given directory can be used for the -dir argument to the pfam_scan.pl script provided by the pfam_scan modules, which each load the appropriate Pfam data module.  Module version pfam_scan/1.5 is for Pfam/28.0, and module version pfam_scan/1.6 is for Pfam/31.0. This latter module might also work with Pfam/35.0</p> <pre><code>pfam_scan.pl -dir $PFAM_ROOT ...\n</code></pre> <p>The pfam_scan.pl script is designed to work with the Pfam database. dbCAN</p> <p>The dbCAN 4.0 database for automated carbohydrate-active enzyme annotation is now available in directory /sw/data/dbCAN/4.0 on Uppmax servers. The database is formatted for use with the hmmer/3.1b1-{gcc,intel} modules. For more information see /sw/data/dbCAN/4.0/readme.txt or the remote version.</p> <p>The local path to the script for post-processing hmmscan --domtblout output is /sw/data/dbCAN/4.0/hmmscan-parser.sh. The CAZyDB trees have also been unpacked and are available in /sw/data/dbCAN/4.0/CAZyDB-phylogeny. Variant Effect Predictor cache files</p> <p>A local cache for all database files available for Ensembl Variant Effect Predictor 87, 89 and 91 are available in directories /sw/data/vep/{87,89,91}. When module version vep/89 or vep/91 is loaded, the environment variable VEP_CACHE is set to the directory for the appropriate version.  Local caches for versions 82, 84 and 86 exist only for homo_sapiens.  To use the cached databases, run the script using the --cache option to indicate the use of a locally-cached database, and the --dir option to specify where this is:</p> <pre><code>vep --cache --dir $VEP_CACHE  ...\n</code></pre> <p>If you are using vep/89, use:</p> <pre><code>variant_effect_predictor.pl --cache --dir $VEP_CACHE  ...\n</code></pre> <p>All plugins are also available.  For more script options, see its online help page. CDD - Position-Specific Scoring Matrices for CD-Search</p> <p>The CDD database versions 3.14 and 3.16 are downloaded in their entirety and are available at /sw/data/cdd/{3.14,3.16}. These directories contains collections of position-specific scoring matrices (PSSMs) that have been created for the CD-Search service.</p> <p>The PSSMs are meant to be used for compiling RPS-BLAST search databases, which can be used with the standalone RPS-BLAST programs (rpsblast and rpsblastn). These programs, as well as the makeprofiledb application needed to convert files in this directory, are part of the BLAST+ executables (available on Uppmax as part of <code>bioinfo-tools</code>, e.g., module blast/2.2.31+). The makeprofiledb application is described at http://www.ncbi.nlm.nih.gov/books/NBK1763.</p> <p>More information is available in the CDD README either via FTP or its local copy /sw/data/cdd/README. iGenomes - Collection of reference sequences and annotation files</p> <p>A local copy of illumina's iGenomes collection of commonly analyzed organisms is available at /sw/data/igenomes. In addition to the annotations provided by the collection, Bismark and STAR indexes have been added. UK Biobank institutional data set (GENETICS)</p> <p>The UKBB data set is available for eligible projects in the system for sensitive research SNIC-SENS Bianca. If you believe you are eligible, contact Professor Tove Fall to gain access.</p>"},{"location":"databases/overview/","title":"Overview","text":"","tags":["database","databases","overview","list"]},{"location":"databases/overview/#overview-of-databases","title":"Overview of databases","text":"<p>Many commonly used data sources are stored locally at UPPMAX. This page provides an index to pages where they are described in more detail. Available databases:</p> <ul> <li>BLAST databases available locally</li> <li>NCBI taxonomy databases</li> <li>DIAMOND protein alignment databases</li> <li>Reference genomes</li> <li>1000-genome project</li> <li>Simons Genome Diversity Project datasets</li> <li>Other bioinformatics-oriented local data resources</li> </ul> <p>In order for you to access Swegen or 1000-genomes you must first send an email to <code>datacentre@scilifelab.se</code> and ask for access. When they approve you, they will contact UPPMAX and we will grant access to Swegen.</p>","tags":["database","databases","overview","list"]},{"location":"databases/reference_genomes/","title":"Reference genomes","text":""},{"location":"databases/reference_genomes/#reference-genomes","title":"Reference genomes","text":"<p>NOTE: The Illumina igenomes are also available at UPPMAX, with additional indices built for Bismarck and STAR.  The scripts used to build the additional indices are available at the UPPMAX/bio-data github repository.</p> <p>Many next-generation sequencing applications involves alignment of the sequence reads to a reference genome. We store reference sequences in a directory that is accessible for all users in the system. The table below shows all currently available genomes.</p> Reference genome Assembly version Homo sapiens Feb. 2009 (GRCh37/hg19) Pan troglodytes Mar. 2006 (CGSC2.1/PanTro2) Macaca mulatta Jan. 2006 (RheMac2) Sus scrofa Apr. 2009 (Sscrofa9) Canis familiaris Sep. 2011 (CanFam3) Mus musculus July 2007 (NCBIM37/mm9), Jan. 2012 (GRCm38) Gallus gallus May 2006 (WASHUC2/galGal3) Taeniopygia guttata Mar. 2010 (TaeGut3.2.4) Saccharomyces cerevisiae Mar 2010 (ScereEF2) Equus caballus Sep. 2007 (EquCab2) Pichia stipitis Picst3 Rattus norvegicus Nov. 2004 (RGSC3.4.61) Schizosaccharomyces pombe 20090701 <p>Directory structure</p> <p>The data files are located at /sw/data/reference and the directory structure is e.g.: Homo_sapiens/GRCh37.</p> <p>Each directory contains several subdirectories, explained below:</p> <p>dna_ftp.ensembl.org_ contains the original data files from the ENSEMBL ftp server, and should not be modified.</p> <p>chromosomes contains fasta files for individual chromosomes.</p> <p>chromosomes_rm contains the same files, masked with RepeatMasker.</p> <p>concat contains most of the fasta files in \"chromosome\" concatenated into a single fasta file. The exceptions are alternate contig files and DNA not mapped to any chromosome.</p> <p>concat_rm contains most of the fasta files in \"chromosome_rm\" concatenated into a single fasta file. The exceptions are alternate contig files and DNA not mapped to any chromosome.</p> <p>program_files contains index files and metadata for software packages used to work with reference genomes, e.g. SAMtools and aligners such as Bowtie, BWA.</p> <p>Requests for additional reference genomes or software data/index files should be directed to UPPMAX support.</p>"},{"location":"databases/simons_genome/","title":"Simons Genome Diversity Project datasets","text":""},{"location":"databases/simons_genome/#simons-genome-diversity-project-datasets","title":"Simons Genome Diversity Project datasets","text":"<p>The Simons Foundation's Genome Diversity Project datasets are now available on UPPMAX. These represent deep human genome sequence data sampled to represent as much diversity as possible:</p> <p>sgdp geographical distribution</p> <p>There are currently approximately 14 TB of data, in the form of CRAM files with associated indices and summaries of the BAM files from which the CRAM files werre derived.</p> <p>Our current SGDP data are those aligned to human reference genome GRCh38DH found at <code>ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/simons_diversity_data/</code>. The local UPPMAX directory for these data is <code>/sw/data/SGDP/</code>. The command used to collect the data was</p> <pre><code>echo \"mirror data\" | lftp ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/simons_diversity_data\n</code></pre> <p>As a result, the local UPPMAX archive is found at <code>/sw/data/SGDP/data/</code>. Within this directory are subdirectories for each of the populations included in the full dataset, with individual samples found within each population directory. For example,</p> <pre><code>rackham1: /sw/data/SGDP $ ls -l data/Greek\ntotal 8\ndrwxr-s--- 3 douglas kgp 4096 Apr 29 14:03 SAMEA3302732\ndrwxr-s--- 3 douglas kgp 4096 Apr 29 14:03 SAMEA3302763\n</code></pre> <p>and one of these sample directories contains</p> <pre><code>rackham1: /sw/data/SGDP $ ls -l data/Greek/SAMEA3302732/alignment/\ntotal 34529204\n-rw-r----- 1 douglas kgp         635 Nov 30  2020 SAMEA3302732.alt_bwamem_GRCh38DH.20200922.Greek.simons.bam.bas\n-rw-r----- 1 douglas kgp 35355769475 Nov 30  2020 SAMEA3302732.alt_bwamem_GRCh38DH.20200922.Greek.simons.cram\n-rw-r----- 1 douglas kgp     2079029 Dec  1  2020 SAMEA3302732.alt_bwamem_GRCh38DH.20200922.Greek.simons.cram.crai\n</code></pre> <p>To access this data, please request membership in the kgp group by emailing <code>support@uppmax.uu.se</code>. As for the 1000 Genomes Project, this is not to restrict access in any way, but rather to make it easier to inform UPPMAX users using the datasets of any relevant changes. Because the local copies of these datasets are hosted on UPPMAX systems, access is restricted to UPPMAX users; non-UPPMAX users will need to follow the procedures described on the SGDP website to download their own copies of the datasets.</p>"},{"location":"databases/swegen/","title":"Access to Swegen","text":""},{"location":"databases/swegen/#access-to-swegen","title":"Access to Swegen","text":"<p>In order for you to access Swegen (or 1000 genomes) you must first send an email to <code>datacentre@scilifelab.se</code> and ask for access.</p> <p>When they approve you, they will contact UPPMAX and we will grant access to Swegen.</p>"},{"location":"getting_started/bianca_usage_prerequisites/","title":"Prerequisites for using Bianca","text":""},{"location":"getting_started/bianca_usage_prerequisites/#prerequisites-for-using-bianca","title":"Prerequisites for using Bianca","text":"<p>To be allowed to log in to Bianca, one needs all of these:</p> <ul> <li>An active research project</li> <li>An UPPMAX account</li> <li>An UPPMAX password</li> </ul> <p>These prerequisites are discussed in detail below.</p>"},{"location":"getting_started/bianca_usage_prerequisites/#an-active-research-project","title":"An active research project","text":"<p>One prerequisite for using Bianca is that you need to be a member of an active SNIC SENS or SIMPLER research project (these are called <code>sens[number]</code> or <code>simp[number]</code>, where <code>[number]</code> represent a number, for example <code>sens123456</code> or <code>simp123456</code>).</p> Forgot your Bianca projects? <p>One easy way to see your Bianca projects is to use the Bianca remote desktop login screen at https://bianca.uppmax.uu.se/.</p> <p></p> <p>SUPR (the 'Swedish User and Project Repository') is the website that allows one to request access to Bianca and to get an overview of the requested resources.</p> How does the SUPR website look like? <p></p> <p>First SUPR page</p> <p></p> <p>SUPR 2FA login. Use the SUPR 2FA (i.e. not UPPMAX)</p> <p>After logging in, the SUPR website will show all projects you are a member of, under the 'Projects' tab.</p> How does the 'Projects' tab of the SUPR website look like? <p></p> <p>Example overview of SUPR projects</p> <p>To see if a project has access to Bianca, click on the project and scroll to the 'Resources' section. In the 'Compute' subsection, there is a table. Under 'Resource' it should state 'Bianca @ UPPMAX'.</p> How does the 'Resources' page of an example project look like? <p></p> <p>The 'Resources' page of an example project.</p> <p>Note that the 'Accounts' tab can be useful to verify your username.</p> How does the 'Accounts' tab help me find my username? <p></p> <p>An example of a SUPR 'Accounts' tab. The example user has username <code>sven-sens2023598</code>, which means his/her UPPMAX username is <code>sven</code></p> <p>You can become a member of an active SNIC SENS by:</p> <ul> <li>request membership to an existing project in SUPR</li> <li>Apply for an UPPMAX project</li> </ul>"},{"location":"getting_started/bianca_usage_prerequisites/#an-uppmax-user-account","title":"An UPPMAX user account","text":"<p>Another prerequisite for using Bianca is that you must have a personal UPPMAX user account.</p>"},{"location":"getting_started/bianca_usage_prerequisites/#an-uppmax-password","title":"An UPPMAX password","text":"<p>Another prerequisite for using Bianca is that you need to know your UPPMAX password. If you change it, it may take up to an hour before changes are reflected in Bianca.</p> <p>For advice on handling sensitive personal data correctly on Bianca, see our FAQ page.</p>"},{"location":"getting_started/change_uppmax_password/","title":"Change your UPPMAX password","text":"","tags":["UPPMAX","password","change","edit","modify"]},{"location":"getting_started/change_uppmax_password/#change-your-uppmax-password","title":"Change your UPPMAX password","text":"Prefer a video? <p>See the YouTube video 'How to reset your UPPMAX password' at 1:53</p> <p>If you know your UPPMAX password, here is how to change it.</p> Forgot your UPPMAX password? <p>Go to How to reset your UPPMAX password.</p>","tags":["UPPMAX","password","change","edit","modify"]},{"location":"getting_started/change_uppmax_password/#procedure","title":"Procedure","text":"","tags":["UPPMAX","password","change","edit","modify"]},{"location":"getting_started/change_uppmax_password/#1-log-in-to-rackham","title":"1. Log in to Rackham","text":"<p>See How to login to Rackham.</p> I have have a SENS project only and just have Bianca <ul> <li>Log in to Transit instead</li> </ul>","tags":["UPPMAX","password","change","edit","modify"]},{"location":"getting_started/change_uppmax_password/#2-open-a-terminal","title":"2. Open a terminal","text":"<p>When logged in to an UPPMAX cluster, open a terminal. If you've logged in via SSH, you are already in a terminal :-)</p>","tags":["UPPMAX","password","change","edit","modify"]},{"location":"getting_started/change_uppmax_password/#3-set-your-own-password","title":"3. Set your own password","text":"<p>In that terminal, type:</p> <pre><code>passwd\n</code></pre> <p>Now you will be asked to repeat the old password and set a new one!</p> <ul> <li>On Rackham your new password will work immediately!</li> <li>On Bianca the new password needs some time to sync.</li> </ul>","tags":["UPPMAX","password","change","edit","modify"]},{"location":"getting_started/first_job/","title":"Run your first job","text":""},{"location":"getting_started/first_job/#run-your-first-job","title":"Run your first job","text":"<p>This page guides you through a possible workflow</p> <ul> <li>This is an example and gives you a quick start of the steps that may be required for you to do your work.</li> <li>There are links to topics on the way, but you should be able to follow the steps anyway.</li> </ul> <ul> <li>managing directory</li> <li>transferring</li> <li>loading modules</li> <li>writing batch script</li> <li>view your CPU hours and disk usage</li> </ul>"},{"location":"getting_started/first_job/#transferring-some-files","title":"Transferring some files","text":""},{"location":"getting_started/first_job/#graphical-file-manager","title":"Graphical file manager","text":"Want more detailed information of file transfer to/from Rackham using a graphical tool? <p>See file transfer to/from Rackham using a graphical tool.</p> <ul> <li>This is good if you want to move many files between host and local and cannot use wildcards.</li> </ul> LinuxMacWindows <p>See file transfer to/from Rackham using a graphical tool</p> <p>One such graphical tool is FileZilla:</p> <p></p> <ul> <li>For copying of files with sftp (secure file transfer protocol) between your client computer (where you are) and the cluster Filezilla can be the choice.</li> <li> <p>https://filezilla-project.org/download.php?type=client</p> </li> <li> <p></p> </li> <li> <p>Cyberduck</p> </li> </ul> <ul> <li> <p>For copying of files between your client computer (where you are) and the cluster WinSCP can also be the choice.</p> <ul> <li>https://winscp.net/eng/download.php</li> </ul> </li> <li> <p></p> </li> <li> <p>Cyberduck</p> </li> <li> <p>Filezilla</p> </li> </ul> <p>Type-along</p> <p>TODO</p>"},{"location":"getting_started/first_job/#using-the-compute-nodes","title":"Using the compute nodes","text":""},{"location":"getting_started/get_inside_sunet/","title":"Get inside the university networks","text":"","tags":["SUNET","University network","University networks","Get inside","edoroam"]},{"location":"getting_started/get_inside_sunet/#get-inside-the-university-networks","title":"Get inside the university networks","text":"<p>One cannot connect to all UPPMAX clusters everywhere around the world. Instead, one needs to get inside the university networks first. This page described how to get inside the university networks, or, to use more precise language, to obtain a SUNET Internet Protocol ('IP') address.</p> How do I know if I am inside the university networks? <p>Go to https://bianca.uppmax.uu.se/.</p> <ul> <li>If nothing happens, you are outside of the university networks</li> </ul> <p></p> <p>A user that is outside of the university network sees nothing.</p> <ul> <li>If you so a login screen, you are inside of the university networks</li> </ul> <p></p> <p>A user that is outside of the university network sees a login screen.</p> <p>There are these ways to do this:</p> <ul> <li>Physically move inside SUNET</li> <li>Use a VPN (a 'virtual private network')</li> <li>Use an HPC cluster within SUNET</li> </ul> <p>Each of these three ways are described below.</p> <pre><code>flowchart TD\n\n    subgraph sub_outside[IP outside SUNET]\n      outside(Physically outside SUNET)\n    end\n\n    subgraph sub_inside[IP inside SUNET]\n      physically_inside(Physically inside SUNET)\n      inside_using_vpn(Inside SUNET using VPN)\n      inside_using_rackham(Inside SUNET using Rackham)\n    end\n\n    %% Outside SUNET\n    outside--&gt;|Move physically|physically_inside\n    outside--&gt;|Use a VPN|inside_using_vpn\n    outside--&gt;|Login to Rackham|inside_using_rackham\n\n    %% Inside SUNET\n    physically_inside-.-&gt;inside_using_rackham\n    physically_inside-.-&gt;inside_using_vpn</code></pre>","tags":["SUNET","University network","University networks","Get inside","edoroam"]},{"location":"getting_started/get_inside_sunet/#physically-move-inside-sunet","title":"Physically move inside SUNET","text":"<p>To connect to all UPPMAX clusters, one must be inside SUNET.</p> <p>All Swedish university buildings are within SUNET. Hence, working from a University building is a non-technical solution to get direct access to Bianca.</p>","tags":["SUNET","University network","University networks","Get inside","edoroam"]},{"location":"getting_started/get_inside_sunet/#use-a-virtual-private-network","title":"Use a virtual private network","text":"Want a video to see how to install the UU VPN? <ul> <li>Install VPN client for Ubuntu and Uppsala university</li> </ul> <p>To connect to all UPPMAX clusters, one must be inside SUNET.</p> <p>A virtual private network (VPN) allows one to access all UPPMAX clusters indirectly: your computer connects to the VPN within SUNET, where that VPN accesses your favorite UPPMAX cluster.</p> <p>To setup a VPN, see the UPPMAX documentation on how to setup a VPN.</p> Want a video to see how the UU VPN is used? <ul> <li>Use the UU VPN with 2FA</li> <li>Use the UU VPN (yet without 2FA) to access the Bianca remote desktop website</li> </ul>","tags":["SUNET","University network","University networks","Get inside","edoroam"]},{"location":"getting_started/get_inside_sunet/#use-an-hpc-cluster-within-sunet","title":"Use an HPC cluster within SUNET","text":"<p>To connect to all UPPMAX clusters, one must be inside SUNET.</p> <p>An HPC cluster within SUNET (for example, Rackham) allows one to access all other clusters: your computer connects to the HPC cluster within SUNET, after which one accesses all other clusters.</p> <p>However, when using this method, one can only use the console environments (i.e. no remote desktop environment).</p>","tags":["SUNET","University network","University networks","Get inside","edoroam"]},{"location":"getting_started/get_started/","title":"Get started","text":""},{"location":"getting_started/get_started/#get-started-here","title":"Get started here","text":"<ul> <li>In order to use UPPMAX resources, you need to be a member of a project and a user account.</li> <li>Your user account is a personal log-in to our systems.</li> <li>Computer resources like CPU-hours and disk storage are allocated to projects.</li> </ul>"},{"location":"getting_started/get_started/#pis","title":"PIs","text":"<p>Do you or members in your research group need compute and storage resources on a HPC cluster or Infrastructure-as-a-Service cloud? Learn how to apply for a project by following the link below:</p> <ul> <li>Project apply</li> </ul> <p>Are you interested in other services, e.g. large volume data storage? Let us know by contacting UPPMAX Support!</p> <p>You may want to see what other NAISS centers can offer, as well.</p> <ul> <li>NAISS resources</li> </ul>"},{"location":"getting_started/get_started/#users","title":"Users","text":"<p>The workflow is like this:</p> <ol> <li>Register in SUPR</li> <li>Accept a user agreement</li> <li>Become a member of a project</li> <li>Apply for an account at UPPMAX (or other resources)</li> </ol> <p>Once you or someone in your group or collaboration has a project, you must apply for a user account by following the link below.</p> <ul> <li>User account</li> </ul> <p>Have an account already? Then scroll down just a little to to \"Getting started: First login to UPPMAX\".</p>"},{"location":"getting_started/get_started/#students","title":"Students","text":"<p>Are you taking a university course that uses UPPMAX and need help? Ask your instructor! If they can't help, contact us through IT Support.</p>"},{"location":"getting_started/get_started/#getting-started-first-login-to-uppmax","title":"Getting started: First login to UPPMAX","text":"<p>See Log in to an UPPMAX cluster.</p>"},{"location":"getting_started/get_started/#changing-your-password","title":"Changing your password","text":"<p>See How to change your UPPMAX password</p>"},{"location":"getting_started/get_started/#copying-files-fromto-your-uppmax-account","title":"Copying files from/to your UPPMAX account","text":"<p>See How to transfer files from/to your UPPMAX account</p>"},{"location":"getting_started/get_started/#where-are-my-files-or-what-are-the-different-file-systems","title":"Where are my files? Or, what are the different file systems?","text":"<p>See files on UPPMAX</p>"},{"location":"getting_started/get_started/#modules","title":"Modules","text":"<p>In order to run installed programs, one uses the module system.</p>"},{"location":"getting_started/get_started/#how-to-run-jobs","title":"How to run jobs","text":"<p>All jobs should be run using the job scheduler.</p>"},{"location":"getting_started/get_started/#uppmax-homepage","title":"UPPMAX homepage","text":"<p>Please check our homepage regularly for information, news and announcements. We will announce maintenance stops and down time there.</p> <ul> <li>https://www.uu.se/en/centre/uppmax</li> <li>https://www.uppmax.uu.se</li> </ul>"},{"location":"getting_started/get_uppmax_2fa/","title":"Setting up two factor authentication for UPPMAX","text":"","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#setting-up-two-factor-authentication-for-uppmax","title":"Setting up two factor authentication for UPPMAX","text":"<p>Two factor authentication (abbreviated to '2FA') increases the security of your UPPMAX account and is mandatory is multiple contexts.</p> Why is this important? <p>See Why is 2FA important?</p> <p>This page describes how to set this up.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#procedure","title":"Procedure","text":"Prefer a video? <p>Watch the YouTube video 'Get your UPPMAX 2FA'</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#1-install-a-2fa-app","title":"1. Install a 2FA app","text":"<p>Install an app to use for 2FA.</p> Which app do you recommend? <p>Any app that works for you.</p> <p>Search for '2FA' or 'OTP' (short for 'one time password') or see the Wikipedia list of 2FA apps.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#2-go-to-httpssuprintegrationuppmaxuusebootstrapotp","title":"2. Go to https://suprintegration.uppmax.uu.se/bootstrapotp/","text":"<p>In your web browser, go to https://suprintegration.uppmax.uu.se/bootstrapotp/.</p> How does this look like? <p>Here is how https://suprintegration.uppmax.uu.se/bootstrapotp/ looks like:</p> <p></p> <p>This will take you to the UU page to request a second factor for your UPPMAX account.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#3-click-on-continue","title":"3. Click on 'Continue'","text":"<p>At this page, click on 'Continue' to be sent to a 'Login to SUPR' page.</p> <p>Click on 'Continue' to be sent to a 'Login to SUPR' page.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#4-log-in-to-supr","title":"4. Log in to SUPR","text":"<p>At the 'Login to SUPR' page, log in, in any way that works for you.</p> How does this look like? <p></p> <p>In case you are not logged in already, log in to SUPR.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#5-press-the-button-prove-my-identity-to-uppmax","title":"5. Press the button 'Prove My Identity to UPPMAX'","text":"<p>Acknowledge to SUPR that they may tell UPPMAX who you are, by pressing the button \"Prove My Identity to UPPMAX\" on the page.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#6-scan-the-qr-code-with-your-2fa-app","title":"6. Scan the QR-code with your 2FA app","text":"<p>Scan the QR-code with your 2FA app.</p> How does that look like? <p></p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#7-enter-the-code-on-the-webpage","title":"7. Enter the code on the webpage","text":"<p>Your application will show you a code, enter this code on the same webpage (as shown in scan the QR-code with your 2FA app).</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#8-see-acknowledgement-that-the-new-two-factor-has-been-registered","title":"8. See acknowledgement that the new two factor has been registered","text":"<p>You should see an acknowledgement that the new two factor has been registered.</p> How does that look like? <p></p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#9-wait-for-a-confirmation-email","title":"9. Wait for a confirmation email","text":"<p>After this procedure, it takes around 15 minutes before you can use the 2FA to log in.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#faq","title":"FAQ","text":"How does the use of a 2FA app looks like? <p>UPPMAX 2FA set up for a fictional UPPMAX user called <code>sven</code></p> How do I know I used my new 2FA too early? <p>Simple answer: when you've used your new 2FA before having received an email.</p> <p>Another way to find out: go to  and try to use your new 2FA. You will get a 'Authentication failed' error when your new 2FA is not active yet.</p> <p></p> How long does it take before my 2FA is active? <p>This is a matter of minutes.</p> <p>It takes a little while before your newly registered factor is usable, but this should be a matter of minutes, not days.</p> Will I get an email when my 2FA is active? <p>No.</p> <p>There is no extra mail sent to let you know that the newly registered factor is usable, just the confirmation mail that mentions that it will be activated soon.</p>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa/#troubleshooting","title":"Troubleshooting","text":"<p>Some of the common problems we've seen include</p> <ul> <li>Not having an account at UPPMAX. This is required to get the second factor for your account.</li> <li>Using a device having it's time set differently from our systems. There are services on the internet (e.g. https://time.is/) you can visit from the device you try to manage the code on that will show you if your device settings are problematic.</li> <li>Noting the code given at first and trying to use it every time when asked for a code. The code to give will change every thirty second and you should give whatever code is shown at the time.</li> <li>Expecting something else to be sent to you. You register the new second factor as part of the process. A confirmation mail is sent as well, but this is mostly to let you know in case your account details in SUPR have gone astray and someone else has registered a second factor for your account.</li> </ul>","tags":["2FA","MFA","UPPMAX"]},{"location":"getting_started/get_uppmax_2fa_qr/","title":"Setting up the QR code for two factor authentication for UPPMAX","text":"","tags":["2FA","MFA","UPPMAX","QR"]},{"location":"getting_started/get_uppmax_2fa_qr/#setting-up-the-qr-code-for-two-factor-authentication-for-uppmax","title":"Setting up the QR code for two factor authentication for UPPMAX","text":"<p>Part of setting up two factor authentication for UPPMAX is to get a QR code.</p> <p></p> <p>You need to scan this QR code to add your account to your software. Most softwares call this \"Add account\" or similar and will offer an option to scan a QR code using the smartphone camera or select an area of the screen where the code is.</p> <p>Note that this must often be done from within the app for two factor authentication.</p> <p>If you see a string similar to</p> <pre><code>otpauth://totp/username@UPPMAX?secret=SOMETEXT&amp;issuer=UPPMAX\n</code></pre> <p>it didn't work and you probably need to do something different (such as starting the app and select scan from within).</p> <p>Once you've scanned the code, you are often allowed to change the name the software will use for the account before you add it. You can change the name if you want - changing the name does not affect the codes generated. Finish adding the account to the software.</p>","tags":["2FA","MFA","UPPMAX","QR"]},{"location":"getting_started/get_uppmax_2fa_qr_code/","title":"Setting up the QR code for two factor authentication for UPPMAX","text":"","tags":["2FA","MFA","UPPMAX","QR","code"]},{"location":"getting_started/get_uppmax_2fa_qr_code/#setting-up-the-qr-code-for-two-factor-authentication-for-uppmax","title":"Setting up the QR code for two factor authentication for UPPMAX","text":"<p>Once you have the new account, you should get one time codes for it when you have it selected. To finish the registration at UPPMAX, you need to enter the code that is displayed in the field where it says \"Code:\" and submit. The codes will change over time, don't worry about this, you just need to use whatever code is current.</p> <p>Once you have entered and submitted the current code, you should see a final page:</p> <p></p> <p>When you see that page, it will take a little while and the token will be activated (you should also receive an e-mail about the new token).</p>","tags":["2FA","MFA","UPPMAX","QR","code"]},{"location":"getting_started/join_existing_project/","title":"Join an existing project","text":""},{"location":"getting_started/join_existing_project/#join-an-existing-project","title":"Join an existing project","text":"<p>To use an UPPMAX cluster, one needs to apply to a project. This page describes how to join an existing project.</p>"},{"location":"getting_started/join_existing_project/#procedure","title":"Procedure","text":""},{"location":"getting_started/join_existing_project/#1-go-to-httpssuprnaissse","title":"1. Go to https://supr.naiss.se/","text":"<p>Example SUPR NAISS main page</p>"},{"location":"getting_started/join_existing_project/#2-click-on-projects","title":"2. Click on 'Projects'","text":"<p>On the main page, click on 'Projects'</p> <p></p> <p>On the main page, click on 'Projects'</p>"},{"location":"getting_started/join_existing_project/#3-scroll-to-request-membership-in-project","title":"3. Scroll to 'Request Membership in Project'","text":"<p>At the 'Projects' page, scroll down to 'Request Membership in Project'.</p> <p></p> <p>At the 'Projects' page, scroll down to 'Request Membership in Project'</p> <p>This is the 'Request Membership in Project' section:</p> <p></p> <p>At the 'Projects' page, here is the 'Request Membership in Project'</p>"},{"location":"getting_started/join_existing_project/#4-search-for-a-project","title":"4. Search for a project","text":"<p>At 'Request Membership in Project' in the 'Projects' page, enter a search term and click 'Search for project'</p> <p></p> <p>At 'Request Membership in Project' in the 'Projects' page, enter a search term and click 'Search for project'. In this example, the search term is 'DNA'</p>"},{"location":"getting_started/join_existing_project/#5-search-for-a-project","title":"5. Search for a project","text":"<p>At the 'Request Membership in Project', click on 'Request' for the project you want to request membership of.</p> <p></p> <p>At the 'Request Membership in Project', click on 'Request' for the project you want to request membership of.</p>"},{"location":"getting_started/join_existing_project/#6-wait-for-email","title":"6. Wait for email","text":"<p>After your request, the PI of the project will receive an email and will accept or reject your proposal.</p>"},{"location":"getting_started/linux/","title":"Working in Linux","text":""},{"location":"getting_started/linux/#linux","title":"Linux","text":"<ul> <li>The \"operating system\" of the UPPMAX and most of the other clusters is Linux.</li> </ul> <p>Questions</p> <ul> <li>What is Linux?</li> <li>How to use the command line?</li> </ul> <p>Objectives</p> <ul> <li>We'll briefly get an overview of Linux</li> <li>How the command line works</li> <li>Some text editors</li> <li>Things to be aware of</li> </ul> Want a video? <p>See this YouTube video for an introduction to Linux</p>"},{"location":"getting_started/linux/#what-is-linux","title":"What is Linux?","text":"<ul> <li>Daily speaking: The Linux Operating system is a UNIX like and UNIX compatible Operating system.</li> <li>Linux is a \"Kernel\" on which many different programs can run.</li> <li>The shell (bash, sh, ksh, csh, tcsh and many more) is one such program.</li> </ul> <ul> <li>Actually, for it to be an OS, it is supplied with GNU software and other additions giving us the name GNU/Linux.<ul> <li>Linux naming controversy</li> </ul> </li> </ul> <ul> <li>Linux has a multiuser platform at its base which means permissions and security comes easy.</li> </ul>"},{"location":"getting_started/linux/#linux-comes-in-different-distributions-dialects-or-say-flavours","title":"Linux comes in different distributions, dialects or, say, flavours","text":"<ul> <li>UPPMAX runs CentOS and RedHat</li> </ul> <p>Local Linux environment</p> <ul> <li>You may sometimes benefit from having a local Linux environment.</li> <li>Examples:<ul> <li>Mimic cluster environment to work with your local files and data as on the Cluster</li> <li>get used to Linux (!)</li> </ul> </li> <li>Mac is UNIX and very Linux-like</li> <li>Windows requires WSL (Windows subsystem for Linux)</li> </ul> For windows users who wants to get started with WSL (not covered here) <ul> <li> <p>Install WSL (Windows Subsystem for Linux)</p> <ul> <li>https://docs.microsoft.com/en-us/windows/wsl/install-win10 (Links to an external site.)</li> <li>Don\u2019t forget to update to WSL2</li> </ul> </li> <li> <p>Install a distribution or a ssh (secure shell) program</p> <ul> <li>Distribution such as ubuntu or</li> <li>(recommended) a ssh program such as MobaXTerm</li> <li>https://mobaxterm.mobatek.net/ (Links to an external site.)<ul> <li><code>sftp</code> frame makes it easy to move, upload and download files.</li> </ul> </li> </ul> </li> <li>You may want to check this webpage as well!<ul> <li>https://hackmd.io/@pmitev/Linux4WinUsers (Links to an external site.)</li> </ul> </li> </ul>"},{"location":"getting_started/linux/#using-the-command-line","title":"Using the command line","text":"Prefer a video? <p>See the YouTube video how to use the command-line on the UPPMAX Bianca cluster.</p>"},{"location":"getting_started/linux/#command-line-with-bash-bourne-again-shell","title":"Command line with bash (Bourne Again Shell)","text":"<ul> <li>A Unix shell and command language.</li> <li>Often default shell</li> </ul> <ul> <li>The command-line interface: the bash prompt $</li> <li>bash can be seen as a program that finds and runs other programs</li> <li>bash is scripting language that is referred to as a shell<ul> <li>(because it sits around the kernel making it easy to interact with)</li> </ul> </li> </ul>"},{"location":"getting_started/linux/#the-prompt","title":"The prompt","text":"<pre><code>[info]$ word1 word2 word3 [...]\n</code></pre> <ul> <li> <p>[info] is configurable, and usually tells you who you are, on what system, and where in the file system.</p> <ul> <li>Example:</li> </ul> <pre><code>[bjornc@rackham3 linux_tutorial]$\n</code></pre> <ul> <li>For changing info (only for advanced users!): How to Change / Set up bash custom prompt</li> <li>The program to run is the first word</li> <li>All words are separated by spaces</li> </ul> </li> </ul> <p></p>"},{"location":"getting_started/linux/#example-bash-command","title":"Example bash command","text":"<ul> <li>Terminal screen shows</li> </ul>"},{"location":"getting_started/linux/#tab-completion","title":"Tab Completion","text":"<ul> <li> <p>Whenever you\u2019re writing a path or filename on the bash prompt, you can strike the \u2018tab\u2019 key to ask Bash to complete what you\u2019re writing.</p> </li> <li> <p>Get in the habit of this \u2014 it will save you many hours!</p> </li> </ul>"},{"location":"getting_started/linux/#editing-files","title":"Editing files","text":"<p>To edit files, you will use a text editor. The UPPMAX HPC clusters have multiple text editors installed, which are described at the UPPMAX 'Text editors' page.</p> <p>Example</p> <p>Start nano and save a file called <code>first.txt</code></p> <pre><code>$ nano first.txt\n</code></pre> <ul> <li>Type <code>test text</code></li> <li>End and save with <code>&lt;ctrl&gt;-X</code> followed by <code>Y</code> and <code>&lt;enter&gt;</code>.</li> </ul>"},{"location":"getting_started/linux/#typical-sources-of-error","title":"Typical sources of error","text":"<p>Warning</p> <ul> <li>Capitalization matters in file names and program names</li> <li>Spaces matter.<ul> <li>Always have a space after the program name.</li> <li>Don\u2019t add spaces within file names.</li> </ul> </li> <li>Check that you are in the right place in the file system.</li> <li>File permissions. Check that the right read, write and execute permission are set. See next session.</li> </ul>"},{"location":"getting_started/linux/#caution","title":"Caution","text":"<p>Warning</p> <ul> <li>There is no undo for:<ul> <li>copy (<code>cp</code>),</li> <li>move (<code>mv</code>), and</li> <li>remove (<code>rm</code>).</li> </ul> </li> <li>Beware of overwriting files and deleting the wrong ones.</li> </ul> <p>Tip</p> <ul> <li> <ul> <li> <p>Within a session: Type in the command prompt</p> <p>alias rm='rm -i'</p> </li> </ul> <p>Tip: make \"<code>rm</code>\" ask if you really want to erase:</p> <ul> <li> <p>Override asking with</p> <p>rm \u2013f &lt;&gt;</p> </li> <li> <p>Edit file <code>.bashrc</code> in <code>home</code> directory by adding the alias line for this to start everytime.</p> </li> </ul> </li> <li> <p>This will also work for <code>mv</code> and <code>cp</code>!</p> </li> </ul> <p>Note</p> <ul> <li>If you do destroy your data, email UPPMAX support, we may be able to help.</li> </ul> <p>Keypoints</p> <ul> <li>Linux Operating system is a UNIX-like and UNIX compatible Operating system.</li> <li>Typical command:     $ program word1 word2 word3 [\u2026]</li> <li>Use text editors to edit files</li> <li>Tips<ul> <li>use Tab completion</li> <li>capitalization and spaces matters</li> <li>no undo:s for copying, moving and removing<ul> <li>Solution: <code>alias rm='rm -i'</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"getting_started/linux/#links","title":"Links","text":"<ul> <li>A free online book about Linux: 'The Linux Command Line'.</li> </ul>"},{"location":"getting_started/linux_basics/","title":"Basic Linux commands","text":""},{"location":"getting_started/linux_basics/#basic-toolkit","title":"Basic toolkit","text":"<p>Objectives</p> <ul> <li>Let's dig into the most important Bash commands</li> <li>We'll do a type-along session</li> </ul> Like videos? <p>See the YouTube video how to use the command-line on the UPPMAX Bianca cluster.</p>"},{"location":"getting_started/linux_basics/#we-will-cover-these-commands","title":"We will cover these commands","text":""},{"location":"getting_started/linux_basics/#navigation-and-file-management","title":"Navigation and file management","text":"<ol> <li><code>pwd</code>  \u2003 present directory</li> <li><code>ls</code>  \u2003list content</li> <li><code>cd</code>  \u2003change directory</li> <li><code>mkdir</code>  \u2003make directory</li> <li><code>cp</code>  \u2003copy</li> <li><code>scp</code>  \u2003securely remotely copy</li> <li><code>mv</code>  \u2003move</li> <li><code>rm</code>  \u2003remove</li> <li><code>rmdir</code>  \u2003remove empty directory</li> </ol>"},{"location":"getting_started/linux_basics/#read-files-and-change-file-properties","title":"Read files and change file properties","text":"<ol> <li><code>cat</code>  \u2003print content on screen</li> <li><code>head</code>  \u2003print first part</li> <li><code>tail</code>  \u2003print last part</li> <li><code>less</code>  \u2003browse content</li> <li><code>tar</code>  \u2003compress or extract file</li> <li><code>chmod</code>  \u2003change file permissions</li> <li><code>man</code>  \u2003info about a command</li> </ol>"},{"location":"getting_started/linux_basics/#file-system-navigation","title":"File system Navigation","text":""},{"location":"getting_started/linux_basics/#pwd-where-are-you-now-print-name-of-currentworking-directory","title":"pwd \u2014 where are you now? \u201cPrint name of current/Working Directory`","text":"<pre><code>pwd\n\npwd -P\n</code></pre> <ul> <li><code>-P</code> gives you the physical path,<ul> <li>ignores how you got there</li> </ul> </li> </ul>"},{"location":"getting_started/linux_basics/#ls-list-directory-contents","title":"ls \u2014 list directory contents","text":"<p>Type <code>ls</code> to display the contents of the current directory.</p> <pre><code>ls -a\n</code></pre> <p><code>-a</code> also shows hidden files and directories.</p> <pre><code>ls -l\n</code></pre> <p><code>-l</code> gives you listed and detailed information.</p> <pre><code>ls -lt\n</code></pre> <p><code>-lt</code> sorts things by time modified.</p> <pre><code>ls -lrt\n</code></pre> <p><code>-r</code> gives reversed order, so in this case newest in last line.</p> <pre><code>man ls\n</code></pre> <ul> <li>for complete information about a command.</li> <li>TIP: <code>-$ man &lt;command&gt;</code> works for almost any command!<ul> <li>scroll with arrows and quit with <code>q</code>.</li> </ul> </li> </ul>"},{"location":"getting_started/linux_basics/#cd-change-the-shell-working-directory","title":"cd \u2014 Change the shell working Directory","text":"<ul> <li>To change directory, use <code>cd &lt;target&gt;</code></li> </ul> <p>Warning</p> <ul> <li>Some of following steps will only be available for the Introduction course members.</li> <li>These involve the `/proj/introtouppmax`` directory</li> </ul> <pre><code>cd /proj/introtouppmax\n\npwd\n\nls\n\ncd labs\n\npwd\n</code></pre> <p>Challenge</p> <ul> <li>Experiment with <code>cd</code></li> <li>Try adding <code>&lt;spaces&gt;</code> or extra <code>/</code> in various places</li> <li> <p>Use tab completion to avoid typos and typing <code>ls</code> a lot</p> </li> <li> <p>Figure out the use of the following:</p> </li> </ul> <pre><code>$ cd -\n\n$ cd ..\n\n$ cd\n\n$ cd ~\n</code></pre> Solution <ul> <li> <p><code>cd -</code> : goes back to your last directory</p> </li> <li> <p><code>cd ..</code> : goes a level up in the hierarchy</p> </li> <li> <p><code>cd</code> : goes to home directory</p> </li> <li> <p><code>cd ~</code> : also goes to home directory</p> </li> </ul>"},{"location":"getting_started/linux_basics/#copy-create-move","title":"Copy, Create, Move","text":""},{"location":"getting_started/linux_basics/#mkdir-make-directories","title":"mkdir \u2014 make directories","text":"<p>Warning</p> <ul> <li>Make sure you\u2019re in your home directory by <code>cd ~</code></li> </ul> <ul> <li>Create a new directory <code>uppmax-intro</code></li> </ul> <pre><code>cd ~\nmkdir uppmax-intro\n</code></pre> <ul> <li>Go in there:</li> </ul> <pre><code>cd uppmax-intro/\n</code></pre>"},{"location":"getting_started/linux_basics/#cp-copy-files-and-directories","title":"cp \u2014 copy files and directories","text":"<ul> <li>Copy files with: <code>cp &lt;source&gt; &lt;target&gt;</code></li> <li>Set target to <code>.</code> to keep name and to point at present directory.</li> </ul> <pre><code>cp /proj/introtouppmax/labs/linux_tutorial/ .\n</code></pre> <ul> <li>Well, that didn\u2019t work. What does the error say?</li> <li>So... try</li> </ul> <pre><code>cp -r /proj/introtouppmax/labs/linux_tutorial/ .\n</code></pre> <p><code>-r</code> is for recursive, meaning including files and subdirectories!</p> <ul> <li>Move to your just created <code>linux_tutorial/</code></li> </ul> <pre><code>cd linux_tutorial\n</code></pre> <ul> <li>Make a copy of the file <code>newfile</code> in the same directory:</li> </ul> <pre><code>cp newfile copyfile\n</code></pre>"},{"location":"getting_started/linux_basics/#scp-secure-copy-remote-file-copy-program","title":"scp \u2014 secure copy (remote file copy program)","text":"<ul> <li>Linux/MacOS: To copy data to/from Rackham, you can use <code>scp</code></li> </ul>"},{"location":"getting_started/linux_basics/#download-from-rackham","title":"Download from Rackham","text":"<ul> <li>Download</li> </ul> <pre><code>[bob@macbook]$ scp bob@rackham.uppmax.uu.se:~/mydata copyofmydata\n\n[bob@macbook]$ scp bob@rackham.uppmax.uu.se:~/mydata .                      # (keeping file name)\n</code></pre> <p>Example</p> <p>Download the file <code>first.txt</code></p> <ul> <li>In your local terminal:</li> </ul> <pre><code>[bob@macbook]$ scp &lt;username&gt;@rackham.uppmax.uu.se:~/first.txt .                      # (keeping file name)\n</code></pre>"},{"location":"getting_started/linux_basics/#upload-to-rackham","title":"Upload to Rackham","text":"<ul> <li>Upload from present directory on local machine to your home directory on cluster.<ul> <li>Example:</li> </ul> </li> </ul> <pre><code>[bob@macbook]$ scp myinput bob@rackham.uppmax.uu.se:~/copyofmyinput\n\n[bob@macbook]$ scp myinput bob@rackham.uppmax.uu.se:~/                      # (keeping filename)\n</code></pre> <p>Example</p> <p>upload the file <code>first.txt</code> after some modification</p> <ol> <li>Open the file you just downloaded in any editor.</li> <li>Add a row, like: <code>A new row</code></li> <li>Save and quit.</li> <li>Upload your file but save it as <code>second.txt</code> on Rackham. In your local terminal:</li> </ol> <pre><code>[bob@macbook]$ scp first.txt &lt;username&gt;@rackham.uppmax.uu.se:~/second.txt                     # (new filename)\n</code></pre> <p>See</p> <ul> <li>Rackham file transfer using scp</li> </ul>"},{"location":"getting_started/linux_basics/#mv-moverename-file","title":"mv \u2014 move/rename file","text":"<ul> <li>Moving files works just like copying files:</li> <li><code>mv &lt;source&gt; &lt;target&gt;</code></li> <li>Move the copy you just made to another place:</li> </ul> <pre><code>mv copyfile ../\n</code></pre> <ul> <li>Rename it.</li> </ul> <pre><code>mv ../copyfile ../renamedfile\n</code></pre>"},{"location":"getting_started/linux_basics/#archiving","title":"Archiving","text":""},{"location":"getting_started/linux_basics/#tar-archiving-and-compression","title":"tar \u2014 archiving and compression","text":"<ul> <li>We\u2019re going to need more files. Let's extract the tar.gz file (tared and gzipped file)</li> </ul> <pre><code>tar -vxzf files.tar.gz\n</code></pre> <ul> <li>The flags mean:         - v*erbosely         - extract         - gzipped         - f**ilename</li> <li>Order of flags may matter!<ul> <li><code>f</code> should be in the start or in the end!</li> </ul> </li> <li>You should see a list of files being extracted</li> </ul> <p>Tip</p> <ul> <li>To compress use the flag <code>-c</code>instead of <code>-x</code></li> </ul> <pre><code>$ tar -czfv &lt;tar file&gt; &lt;path/to/directory/file(s)-or-directory&gt;\n</code></pre>"},{"location":"getting_started/linux_basics/#deleting","title":"Deleting","text":""},{"location":"getting_started/linux_basics/#rm-delete-files-or-directories","title":"rm \u2014 delete files  or directories","text":"<p>Note</p> <ul> <li>Tip: make \"rm\" ask if you really want to erase:</li> <li>Within a session: Type in the command prompt</li> </ul> <pre><code>alias rm='rm -i'\n</code></pre> <ul> <li>Override asking with</li> </ul> <pre><code>rm -f &lt;&gt;\n</code></pre> <ul> <li>Do you want this to be the case everytime you start a new session?<ul> <li>Edit file <code>.bashrc</code> in /home directory by adding the above alias line on any but the first line.</li> </ul> </li> <li>These steps will also work for <code>mv</code> and <code>cp</code>.</li> </ul> <ul> <li> <p>Deleting files works just like copying or moving them: <code>rm &lt;target&gt;</code></p> <ul> <li>Try it out:</li> </ul> </li> </ul> <pre><code>rm ../renamedfile\n\nrm this_is_empty\n</code></pre> <ul> <li>hmmmm...</li> </ul>"},{"location":"getting_started/linux_basics/#rmdir-delete-an-empty-directory","title":"rmdir \u2014 delete an empty directory","text":"<ul> <li>We need another command to delete directories</li> </ul> <pre><code>rmdir this_is_empty\n\nrmdir this_has_a_file\n</code></pre> <ul> <li> <p>Problem again??</p> </li> <li> <p>Is there a way to use rm to delete directories?</p> </li> </ul> <p>Solution</p> <ul> <li>Recursive commands <code>-r</code> are applied to directories and their contents</li> </ul> <pre><code>$ rm -r this_has_a_file\n</code></pre>"},{"location":"getting_started/linux_basics/#help","title":"Help","text":""},{"location":"getting_started/linux_basics/#man-manual-look-up-the-right-flags","title":"man \u2014 manual, look up the right flags","text":"<ul> <li>Nobody can remember whether it\u2019s <code>-R</code> or <code>-r</code> for recursive, or if <code>-f</code> lets you choose a file or forces an action.</li> </ul> <pre><code>man ls\n</code></pre> <ul> <li>shows you how to use <code>ls</code> and all its options</li> <li>Type <code>/&lt;keyword&gt;</code> to search for a keyword, use <code>n</code> (forward) and \u00b4N` (backward) to scan through hits.</li> <li>Scroll with arrows.</li> <li>Type <code>q</code> to quit.</li> </ul> <p>Challenge</p> <ul> <li>Spend some time now to browse the man pages for the commands you\u2019ve just learned!</li> </ul>"},{"location":"getting_started/linux_basics/#lets-get-wild-with-wildcards","title":"Let\u2019s get wild with Wildcards","text":"<pre><code>ls many_files\n\nls many_files/*.txt\n\nls many_files/file_1*1.docx\n</code></pre> <ul> <li>Want to clean out temporary files ending in .tmp in all the subdirectories?</li> </ul> <p>Warning</p> <ul> <li>It could be wise to do <code>ls -a */*.tmp</code> first to see what will be deleted...</li> </ul> <pre><code>$ rm */*.tmp\n</code></pre> <p>Challenge</p> <ul> <li>Exercise:  Create a new directory and move all .txt files in many_files to it.</li> </ul>"},{"location":"getting_started/linux_basics/#reading-files","title":"Reading files","text":"<ul> <li>In Linux, you can (if you wish) also display files without being able to change them</li> </ul> <pre><code>cd old_project\n\nls\n</code></pre> <ul> <li>Hmm, which of these files are useful?</li> </ul>"},{"location":"getting_started/linux_basics/#cat-concatenate-files-and-print-on-the-standard-output","title":"cat - concatenate files and print on the standard output","text":"<ul> <li><code>cat</code> dumps the contents of files to the terminal as text</li> </ul> <pre><code>cat the_best\n</code></pre> <ul> <li>Yummy!</li> </ul> <pre><code>cat a\n</code></pre> <ul> <li> <p>What's this????</p> </li> <li> <p>Concatenate files with this wizardry:</p> </li> </ul> <pre><code>cat a the_best &gt; combinedfiles.txt\n</code></pre> <ul> <li>File <code>a</code> is written first and <code>the_best</code> is appended</li> </ul>"},{"location":"getting_started/linux_basics/#head-display-the-top-heading-of-a-file","title":"head \u2014 display the top (heading) of a file","text":"<pre><code>head a\n</code></pre> <ul> <li>You can choose how many lines to display (default 10)</li> </ul> <pre><code>head -n 4 a\n</code></pre>"},{"location":"getting_started/linux_basics/#tail-display-the-end-of-a-file","title":"tail \u2014 display the end of a file","text":"<ul> <li>Tail is the same as head, but for the other end.</li> </ul> <pre><code>tail -n 5 a\n</code></pre> <ul> <li>Handy to look at log files or to figure out the structure of a text file.</li> </ul>"},{"location":"getting_started/linux_basics/#less-read-a-whole-file","title":"less \u2014 read a whole file","text":"<ul> <li>cat doesn\u2019t really work for long files</li> </ul> <pre><code> less a\n</code></pre> <ul> <li>Search with <code>/&lt;keyword&gt;</code> and <code>n</code>/<code>N</code></li> <li>Hit <code>q</code> to quit.</li> <li>scroll with arrows.</li> <li> <p><code>man</code> uses `less!</p> <p>\u201cless is more`</p> </li> </ul>"},{"location":"getting_started/linux_basics/#history","title":"History","text":"<ul> <li><code>history</code> shows previous commands</li> <li>You can rerun earlier commands by:<ul> <li>copy-pasting and pressing <code>&lt;enter&gt;</code></li> <li><code>!990</code> will run the command of line 990 of last <code>history</code> output.</li> </ul> </li> <li>Search for earlier commands you just remember parts of:<ul> <li>history | grep 'jobstats'</li> </ul> </li> </ul>"},{"location":"getting_started/linux_basics/#file-permissions","title":"File permissions","text":""},{"location":"getting_started/linux_basics/#example","title":"Example","text":"<pre><code>$ ls -l\n\ndrwxrwxr-x 2 marcusl marcusl 4096 Sep 19 2012 external_hdd\n-rwxr-xr-x 1 marcusl marcusl 17198 Jul 16 14:12 files.tar.gz\n</code></pre> <ul> <li>Leading symbol:<ul> <li><code>d</code> directory</li> <li><code>-</code> regular file</li> <li><code>l</code> symbolic link (more on this tomorrow)</li> <li>Others exist, but you can ignore them for now</li> </ul> </li> </ul> <pre><code>$ ls -l\n\n  drwxrwxr-x 2 marcusl marcusl 4096 Sep 19 2012 external_hdd\n\n  -rwxr-xr-x 1 marcusl marcusl 17198 Jul 16 14:12 files.tar.gz\n</code></pre> <ul> <li> <p>Three sets of \u201crwx` permissions</p> <ul> <li>rwx: r ead, w rite, ex ecute</li> <li>User: the user account that owns the file (usually the one that created it)</li> <li>Group: the group that owns the file (usually the project group in /proj/xyz or the user\u2019s group elsewhere)</li> <li>Others: everyone else on the system (literally a thousand strangers)</li> </ul> </li> <li> <p>r - read</p> <ul> <li>Files: Read the contents of the file</li> <li>Directories: List the files in the directory</li> </ul> </li> <li> <p>w - write</p> <ul> <li>Files: Modify the file</li> <li>Directories: Add, rename, or delete files in the directory</li> </ul> </li> <li> <p>x - execute</p> <ul> <li>Files: Run the file as a program</li> <li>Directories: Traverse the directory (e.g. with \u201ccd`)</li> </ul> </li> </ul>"},{"location":"getting_started/linux_basics/#changing-permissions","title":"Changing permissions","text":"<p>chmod \u2014 change file mode bits</p> <p>If you own, i.e. created, the file or directory, you can modify the content.</p> <p>Common issues</p> <ul> <li>Files with <code>w</code> can be modified and destroyed by accident. Protect your data!</li> <li>If you want to share data or scripts with a person not in your project (e.g. support staff like me), you can!</li> <li>If you want to keep non-members from even seeing which files you have, you can!</li> </ul>"},{"location":"getting_started/linux_basics/#syntax","title":"Syntax","text":"<p><code>chmod &lt;mode&gt; &lt;files&gt;</code></p> <ul> <li><code>&lt;mode&gt;</code> is of the form: For whom, Modify, What permission(s)</li> <li>For whom?<ul> <li><code>u</code>: user/owner</li> <li><code>g</code>: group, often the members to a certain project</li> <li><code>o</code>: others</li> <li><code>a</code>: all</li> <li>if not set changes are applied for user AND group</li> </ul> </li> <li>Modify?<ul> <li><code>+</code>: add permissions,</li> <li><code>-</code>: remove</li> <li><code>=</code>: set equal to<ul> <li><code>=</code> usually causes unmentioned bits to be removed except that a directory's unmentioned set user and group ID bits are not affected.</li> </ul> </li> </ul> </li> <li>What permissions?<ul> <li><code>r</code>, <code>w</code>, <code>x</code>, i.e. the actual permission</li> </ul> </li> </ul>"},{"location":"getting_started/linux_basics/#examples","title":"Examples","text":"<ul> <li> <p><code>&lt;mode&gt;</code> can be e.g.:</p> <ul> <li><code>u+x</code> : lets You (owner) run a script you just wrote</li> <li><code>-w</code> : no write permissions for owner+group<ul> <li>warning: if <code>w</code> was already set for others it will be kept!!</li> </ul> </li> <li><code>+rw</code> : let user and group members read and edit this file, not others if not already set</li> <li><code>=xw</code> : let group members go into your directory and put files there, but not see which files are there, others are not affected</li> <li><code>a=xw</code> : set xw for everyone</li> </ul> </li> <li> <p>chmod takes flags as usual, e.g.</p> <ul> <li><code>-R</code> for recursive (i.e. all files and sub-directories therein)</li> </ul> </li> </ul> <p>chmod 755 style \u2014 binary sum \u2014 octal bit mask</p> <ul> <li> <p>Online, you will come across e.g. <code>chmod 755 &lt;file/dir&gt;</code>. What does this mean? It\u2019s an \"octal bit mask`:</p> </li> <li> <p>Each digit corresponds to the binary sum for the owner, group and others, respectively.</p> <ul> <li><code>7 = 4 + 2 + 1 = r + w + x</code>   All permissions</li> <li><code>5 = 4 + 0 + 1 = r +   + x</code>   Read and execute permission</li> </ul> </li> <li> <p>755 then means all permissions for owner, but limiting write permissions for the group and all others</p> </li> <li> <p>What number would <code>rw</code> be?</p> </li> </ul> Solution <p>6</p> chmod \u2014 Hands-on <ul> <li>In your locally created <code>linux_tutorial</code> directory, find important files and old saved data that you wouldn\u2019t want to lose (imagine).</li> <li>Directories: important_results/, old_project/</li> <li>File: last_years_data</li> <li>Use chmod to remove write permission from those files and directories (use the <code>-R</code> flag (not <code>-r</code>) to also do the files in the directories).</li> <li>Take a moment to play around with chmod and explore the effects of permissions on files and directories.</li> </ul> Solution <pre><code>$ chmod -wR &lt;target&gt;\n</code></pre>"},{"location":"getting_started/linux_basics/#links","title":"Links","text":"<ul> <li>A free online book about Linux: 'The Linux Command Line'.</li> </ul>"},{"location":"getting_started/login/","title":"Log in","text":"","tags":["login","log in","general"]},{"location":"getting_started/login/#log-in","title":"Log in","text":"<p>Warning</p> <p>N.B. You are NOT supposed to log in to any webpage with the password and username you get via UPPMAX support, with the exception of the ThinLinc webinterface.</p> <p>One needs to log in into an UPPMAX cluster to use it.</p> <p>There are two environments one can login to:</p> <ul> <li>a remote desktop environment<ul> <li>using a webbrowser</li> <li>using a local ThinLinc client</li> </ul> </li> <li>a console environment, using an SSH client</li> </ul> <p></p> <p>The two environments to work on Bianca. At the left is a remote desktop environment. At the the right is the console environment.</p> <p>Because logging in differs between clusters, each cluster has its own login page:</p> <ul> <li>Login to Bianca</li> <li>Login to Pelle</li> <li>Login to Rackham</li> <li>Login to Snowy</li> </ul> <p>Go to those pages for more details.</p> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> <p>Other things to log in to, shown for completeness:</p> <ul> <li>Login to Dardel (this is not an UPPMAX cluster)</li> <li>Login to Transit (this is an UPPMAX service, not a cluster)</li> </ul>","tags":["login","log in","general"]},{"location":"getting_started/login_bianca/","title":"Log in to Bianca","text":"","tags":["login","log in","Bianca"]},{"location":"getting_started/login_bianca/#log-in-to-bianca","title":"Log in to Bianca","text":"<p>The two Bianca environments to work on Bianca. At the left is a remote desktop environment. At the the right is the console environment.</p> <p>There are multiple UPPMAX clusters one can log in to. Here it is described how to log in to Bianca:</p>","tags":["login","log in","Bianca"]},{"location":"getting_started/login_bianca/#which-way-to-log-in-to-bianca","title":"Which way to log in to Bianca","text":"<p>After you've fulfilled all prerequisites for using Bianca, there are many ways to log in to Bianca.</p> <p>Here is the decision tree, with more detailed explanation below it:</p> <pre><code>flowchart TD\n\n  %% Give a white background to all nodes, instead of a transparent one\n  classDef node fill:#fff,color:#000,stroke:#000\n  %% Graph nodes for files and calculations\n  classDef important_node stroke-width:4px\n\n\n  in_sunet(A.Can you get inside the university networks?)\n  need_remote_desktop(B.Need/prefer a remote desktop?)\n  how_login(C.How to log in?)\n  need_remote_desktop_no_sunet(B.Need/prefer a remote desktop?)\n  how_login_no_sunet(C.How to log in?)\n\n  use_website[1.Use the Bianca remote desktop website]:::important_node\n  use_password[2.Use a terminal and password to access Bianca directly]:::important_node\n  use_ssh_keys[3.Use a terminal and SSH keys to access Bianca directly]\n\n  use_website_no_sunet[4.Use the Rackham remote desktop website]\n  use_password_no_sunet[5.Use a terminal and password via Rackham]\n  use_ssh_keys_no_sunet[Use a terminal and SSH keys via Rackham]\n\n  in_sunet --&gt; |yes| need_remote_desktop\n\n  need_remote_desktop ---&gt; |no| how_login\n  need_remote_desktop --&gt; |yes| use_website\n\n  how_login --&gt; |Using a password| use_password\n  how_login --&gt; |Using SSH keys| use_ssh_keys\n\n  in_sunet ---&gt; |no| need_remote_desktop_no_sunet\n\n  need_remote_desktop_no_sunet ---&gt; |no| how_login_no_sunet\n  need_remote_desktop_no_sunet --&gt; |yes| use_website_no_sunet\n\n  how_login_no_sunet --&gt; |Using a password| use_password_no_sunet\n  how_login_no_sunet ---&gt; |Using SSH keys| use_ssh_keys_no_sunet</code></pre> <p>Decision tree on how to log in to Bianca. The nodes with a thicker outline are the ones used in courses.</p> <p>Question A, 'Can you get inside the university networks?' is commonly answered 'yes' for anyone with an email address at a university in Sweden. The UPPMAX documentation on how to get inside the university networks should allow anyone to do so.</p> <p>Question B, 'Need/prefer a remote desktop?' is about if you prefer a visual/graphical environment to work with Bianca, which will be similar to what most of us are used to. A 'yes' is more suitable for new users, although it is considered a more clunky (it responds slower on user input) and clumsy (copy-pasting to it needs multiple mouse clicks) way to work. A 'no' is more suitable for users comfortable with a terminal and works smoothly.</p> How does the Bianca remote desktop look like? <p>One can pick multiple remote desktop environments, such as GNOME and XFCE (and KDE, don't pick KDE!).</p> <p></p> <p>The Bianca XFCE remote desktop environment</p> <p></p> <p>A more populated Bianca XFCE remote desktop</p> <ul> <li>A remote desktop environment, also called 'graphical environment',   'GUI environment', 'ThinLinc environment'</li> </ul> How does the Bianca console environment look like? <p></p> <p>The Bianca console environment</p> <ul> <li>A console environment, also called 'terminal environment' or 'terminal'</li> </ul> <p>Question C, 'How to log in?' is about how you prefer to login. The option 'Using a password' is more suitable for new users, as it is easy to setup and understand. However, one does need to type his/her password every time one logs in. 'Using SSH keys' is harder to setup, yet more convenient.</p> Will a local ThinLinc client work too? <p>No.</p> <p>One really can only access the Bianca remote desktop environment via the website.</p> <p>Here are the ways to log in to Bianca:</p> <ul> <li>1.Use the Bianca remote desktop website</li> <li>2.Use a terminal and password to access Bianca directly</li> <li>3.Use a terminal and SSH keys to access Bianca directly</li> <li>(Workaround if you are outside of SUNET) 4.Use the Rackham remote desktop website   to log in to Bianca's remote desktop environment</li> <li>(Workaround if you are outside of SUNET) 5.Use a terminal and password via Rackham   to log in to Bianca's console environment</li> <li>(Workaround if you are outside of SUNET) 6.Use a terminal and SSH keys via Rackham   to log in to Bianca's console environment</li> </ul> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p>","tags":["login","log in","Bianca"]},{"location":"getting_started/login_bianca_console_password/","title":"Login to the Bianca console environment with a password","text":"","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#login-to-the-bianca-console-environment-with-a-password","title":"Login to the Bianca console environment with a password","text":"<p>There are multiple ways to log in to Bianca.</p> <p>This page describes how to log in to Bianca using a terminal and a password:</p> <ul> <li>Procedure: describes the procedure</li> <li>Troubleshooting: describes how to fix errors</li> </ul>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#procedure","title":"Procedure","text":"Video: how to use a terminal and SSH to access the Bianca console environment <p>This video shows how to use a terminal and SSH to access the Bianca console environment: YouTube</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#1-get-inside-the-university-networks","title":"1. Get inside the university networks","text":"<p>Get inside the university networks.</p> Forgot how to get within the university networks? <p>See the 'get inside the university networks' page</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#2-use-ssh-to-log-in","title":"2. Use <code>ssh</code> to log in","text":"<p>From a terminal, use <code>ssh</code> to log in:</p> <pre><code>ssh [username]@bianca.uppmax.uu.se\n</code></pre> <p>For example:</p> <pre><code>ssh sven@bianca.uppmax.uu.se\n</code></pre> How does this look like (when inside of SUNET)? <pre><code>sven@svens_computer:~$ ssh sven@bianca.uppmax.uu.se\n\nProvide your normal UPPMAX password. You will supply the TOTP code separately, in the next step.\n\n(sven@bianca.uppmax.uu.se) Password:\n</code></pre> <p>After which a password will be asked. Go to the next step.</p> How does this look like the first time? <pre><code>sven@svens_computer:~$ ssh sven@bianca.uppmax.uu.se\nThe authenticity of host 'bianca.uppmax.uu.se (89.44.250.74)' can't be established.\nECDSA key fingerprint is SHA256:Z9FeKcfgw9PicHfotfkCVZTzWTY0xPjy0qa9Ap/7Aws.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <ul> <li> <p>Type <code>yes</code>!</p> </li> <li> <p>Other valid fingerprints are:</p> <ul> <li><code>SHA256:ZUsJUznqix7DjFbwV90nhfKq5u/x3+GUSX7F6C9s3rA</code></li> <li><code>SHA256:WDimSDqd6+b9bgbcNP+zCMCUp3Cen6Js7lAXPVFscBw</code></li> </ul> </li> </ul> How does it look like when outside of SUNET? <pre><code>sven@svens_computer:~$ ssh sven@bianca.uppmax.uu.se\n</code></pre> <p>After which there is only waiting...</p> Why no <code>-A</code>? <p>On Bianca, one can use <code>-A</code>:</p> <pre><code>ssh -A [username]@bianca.uppmax.uu.se\n</code></pre> <p>this option is only useful when you want to log in to Bianca via the console using an SSH key. As we here use passwords (i.e. no SSH keys) to access Bianca, <code>-A</code> is unused and hence we simplify this documentation by omitting it.</p> Why no <code>-X</code>? <p>On Rackham, one can use <code>-X</code>:</p> <pre><code>ssh -X [username]@rackham.uppmax.uu.se\n</code></pre> <p>However, on Bianca, this so-called X forwarding is disabled. Hence, we do not teach it :-)</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#3-type-your-uppmax-password","title":"3. Type your UPPMAX password","text":"<p>Type your UPPMAX password.</p> How does this look like? <pre><code>Provide your normal UPPMAX password. You will supply the TOTP code separately, in the next step.\n\n(sven@bianca.uppmax.uu.se) Password:\n(sven@bianca.uppmax.uu.se)\n</code></pre> <p>After which you'll asked for your TOTP. Go to the next step.</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#4-type-your-totp","title":"4. Type your TOTP","text":"<p>Type the TOTP from the UPPMAX 2-factor authentication service, for example <code>123456</code>, then press enter.</p> How does this look like? <pre><code>Second factor (TOTP UPPMAX):\n</code></pre> <p>After which you'll asked for your Bianca project. Go to the next step.</p> <p>After authenticated using the UPPMAX password and UPPMAX TOTP, you are now asked to pick a Bianca project.</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#5-type-your-bianca-project","title":"5. Type your Bianca project","text":"<p>You will be asked for your UPPMAX project's name. Type it and press enter.</p> How does this look like? <pre><code>Project name (pick from sens2016001 sens2017625 sens2023598): sens2017625\n</code></pre> <p>The next step is to login to your own private virtual project cluster. As you are already properly authenticated (i.e. using an UPPMAX password and UPPMAX TOTP), you don't need to use your 2FA anymore.</p> What is a virtual project cluster? <p>As Bianca holds sensitive data, by regulations, each Bianca project must be isolated from each other and are not allowed to, for example, share the same memory.</p> <p>One way to achieve this, would be to build one HPC cluster per project. While this would guarantee isolated project environments, this would be quite impractical.</p> <p>Instead, we create isolated project environments by using software, that creates so-called virtual clusters, as if they would be physical clusters. Like physical clusters, a virtual cluster has a guaranteed isolated project environment.</p> <p>When you login to Bianca's shared network, you will get a message of your project's login node status. It can be <code>up and running</code> or <code>down</code>. If it is <code>down</code>, the virtual cluster is started, which may take some minutes.</p> How does this look like? <p>Here is how it looks like when the node is already running:</p> <pre><code>****************************************************************************\n* Login node up and running. Redirecting now!                              *\n****************************************************************************\n</code></pre>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#6-type-your-uppmax-password","title":"6. Type your UPPMAX password","text":"<p>Type your UPPMAX password, for example <code>verysecret</code></p> How does this look like? <pre><code>sven@sens2017625-bianca.uppmax.uu.se's password:\n</code></pre>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#7-you-are-in","title":"7. You are in","text":"<p>Enjoy! You are in! Or, to be precise, you are on the login node of your own virtual project cluster.</p> How does this look like? <p>```bash Last login: Fri Feb  7 12:09:32 2025 from 172.18.144.254  _   _ _    __  __    _    __  __ | | | |  _ |  _ |  \\/  |  /     \\/ /   | System:    sens2017625-bianca | | | | |) | |) | |\\/| | / _      /    | User:      sven | || |  /|  /| |  | |/ ___   /      |  ___/||   ||   ||  |//   _\\//_   |</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> <p>By default, this node has one core, hence if you need more memory or more CPU power, you submit a job (interactive or batch), and an idle node will be moved into your project cluster.</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#_1","title":"Login to the Bianca console environment with a password","text":"<pre><code>    User Guides: http://www.uppmax.uu.se/support/user-guides\n    FAQ: http://www.uppmax.uu.se/support/faq\n\n    Write to support@uppmax.uu.se, if you have questions or comments.\n</code></pre> <p>[sven@sens2017625-bianca ~]$ ````</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#troubleshooting","title":"Troubleshooting","text":"<p>Here are some common errors and their solutions:</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password/#permission-denied-please-try-again","title":"Permission denied, please try again","text":"<pre><code>Permission denied, please try again.\n</code></pre> <p>Here are the questions we will ask to solve your problem:</p> <pre><code>flowchart TD\n    error[Permission denied, please try again.]\n    correct_password[Is your password correct?]\n    correct_totp[Is your TOTP correct?]\n    in_sunet[Are you within the university networks?]\n    active_bianca_project[Is that Bianca project active?]\n    member_of_bianca_project[Are you a member of that Bianca project]\n    contact_support[Contact support]\n\n    error --&gt; correct_password\n    error --&gt; in_sunet\n\n    in_sunet --&gt; |yes| active_bianca_project\n\n    correct_password --&gt; |yes| correct_totp\n    active_bianca_project --&gt;  |yes| member_of_bianca_project\n    member_of_bianca_project --&gt; |yes| contact_support\n    correct_totp --&gt; |yes| contact_support</code></pre> How do I know my password is correct? <p>You don't.</p> <p>It could be a typo: you don't see your password when you type (this is a security measure), so a typo is likely to occur. Also check if 'Caps Lock' is off.</p> <p>It could be that you've forgotten your password. That can happen to all of us. You can then reset your password at https://suprintegration.uppmax.uu.se/getpasswd</p> How do I know my TOTP is correct? <p>You don't.</p> <p>Most likely, you are using the incorrect TOTP generator.</p> What is the correct TOTP generator? <p>The UPPMAX one, titled <code>[username]@UPPMAX</code>, for example <code>sven@UPPMAX</code>.</p> <p>When using UPPMAX, one needs to create other TOTPs too, such as for SUPR or the Uppsala VPN. Don't use those numbers to login to Bianca.</p> How do I know if I am within the university networks? <p>If you login via <code>eduroam</code> you are within the university networks.</p> <p>When unsure, go to the Bianca remote desktop website at https://bianca.uppmax.uu.se: if this page does not load, you are outside of the university networks.</p> <p>See How to get inside of the university networks if you outside of the university networks.</p> How do I know if the Bianca project is active? <p>A quick way to confirm your Bianca project is active: go to https://bianca.uppmax.uu.se and type your username. If the project is displayed, it is active.</p> <p>To confirm your project is active or inactive, use the SUPR NAISS website. See the UPPMAX documentation on projects how to see if your project is active?</p> <p>See the UPPMAX page on contacting support on how to contact us.</p>","tags":["login","log in","Bianca","console","terminal","password"]},{"location":"getting_started/login_bianca_console_password_no_sunet/","title":"Login to the Bianca console environment with a password from outside of the Swedish university networks","text":"","tags":["login","log in","Bianca","console","terminal","password","out","outside","SUNET","university networks"]},{"location":"getting_started/login_bianca_console_password_no_sunet/#login-to-the-bianca-console-environment-with-a-password-from-outside-of-the-swedish-university-networks","title":"Login to the Bianca console environment with a password from outside of the Swedish university networks","text":"<p>There are multiple ways to log in to Bianca.</p> <p>This page describes how to log in to Bianca using a terminal and a password from outside of the Swedish university networks.</p> <p>Danger</p> <ul> <li>Do not log in to Rackham and from there log in to Bianca.</li> <li>This will let all sensitive data land on Rackham uncrypted as a an intermediate step.</li> <li>Rackham is not a secure system and could be spied on.</li> </ul>","tags":["login","log in","Bianca","console","terminal","password","out","outside","SUNET","university networks"]},{"location":"getting_started/login_bianca_console_password_no_sunet/#procedure-with-using-rackham-as-a-jump-host","title":"Procedure with using Rackham as a \"jump host\"","text":"<ul> <li>Log in to Bianca via Rackham in one line</li> </ul> <pre><code>ssh bjornc@bianca.uppmax.uu.se -J bjornc@rackham.uppmax.uu.se\nbjornc@rackham.uppmax.uu.se's password:\n\nProvide your normal UPPMAX password. You will supply the TOTP code separately, in the next step.\n\n(bjornc@bianca.uppmax.uu.se) Password:\n(bjornc@bianca.uppmax.uu.se) Second factor (TOTP UPPMAX):\n</code></pre> <ul> <li>You are now inside the Bianca session and sensitive data is not seen by Rackham at all.</li> </ul>","tags":["login","log in","Bianca","console","terminal","password","out","outside","SUNET","university networks"]},{"location":"getting_started/login_bianca_console_ssh_key/","title":"Login to the Bianca console environment using SSH keys","text":"","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_console_ssh_key/#login-to-the-bianca-console-environment-using-ssh-keys","title":"Login to the Bianca console environment using SSH keys","text":"<p>There are multiple ways to log in to Bianca.</p> <p>This page describes how to log in to Bianca using a terminal and an SSH key pair.</p>","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_console_ssh_key/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>When inside SUNET, one can access a Bianca console environment using SSH and SSH keys.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p> <p>This is considered a bit harder to setup, but one only needs to type one password to login to Bianca. If you don't mind typing your UPPMAX password twice, an easier setup is log in to the Bianca console environment with a password.</p>","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_console_ssh_key/#2-use-ssh-to-log-in","title":"2. Use <code>ssh</code> to log in","text":"<p>From a terminal, use <code>ssh</code> to log in:</p> <pre><code>ssh -A [user]@bianca.uppmax.uu.se\n</code></pre> <p>For example:</p> <pre><code>ssh -A sven@bianca.uppmax.uu.se\n</code></pre> How does it look like when outside of SUNET? <p>See this YouTube video how this looks like when outside of SUNET.</p> <p>Spoiler: quite dull, as nothing happens until these is a timeout.</p> Why no <code>-X</code>? <p>On Rackham, one can use <code>-X</code>:</p> <pre><code>ssh -X username@rackham.uppmax.uu.se\n</code></pre> <p>However, on Bianca, this so-called X forwarding is disabled. Hence, we do not teach it :-)</p>","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_console_ssh_key/#3-type-your-uppmax-password","title":"3. Type your UPPMAX password","text":"<p>Type your UPPMAX password.</p> How does this look like? <pre><code>$ ssh -A sven@bianca.uppmax.uu.se\n\nProvide your normal UPPMAX password. You will supply the TOTP code separately, in the next step.\n\n(sven@bianca.uppmax.uu.se) Password:\n(sven@bianca.uppmax.uu.se)\n</code></pre>","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_console_ssh_key/#4-type-your-totp","title":"4. Type your TOTP","text":"<p>Type your UPPMAX TOTP.</p> How does this look like? <pre><code>Second factor (TOTP UPPMAX):\n</code></pre>","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_console_ssh_key/#5-type-your-bianca-projects-name","title":"5. Type your Bianca project's name","text":"<p>Type your Bianca project's name.</p> How does this look like? <pre><code>Project name (pick from sens2016001 sens2017625 sens2023598): sens2017625\n</code></pre>","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_console_ssh_key/#6-you-are-in","title":"6. You are in","text":"<p>Enjoy! You are in! To be precise, you are on a Bianca login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> <p>In a Bianca console environment:</p> <ul> <li>Text display is limited to 50kBit/s.   This means that if you create a lot of text output,   you will have to wait some time before you get your prompt back.</li> <li>Cut, copy and paste work as usual.   Be careful to not copy-paste sensitive data!</li> </ul> Why does one need two passwords? <p>The first password is needed to get into the shared Bianca environment. This password contains both an UPPMAX password and an UPPMAX 2FA number.</p> <p>The second password is needed to go to the login node of a project's virtual cluster.</p> <pre><code>flowchart TD\n\n    %% Give a white background, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n    classDef focus_node fill:#fff,color:#000,stroke:#000,stroke-width:4px\n\n    subgraph sub_bianca_shared_env[Bianca shared network]\n      bianca_shared_console[Bianca console environment login]\n      bianca_shared_remote_desktop[Bianca remote desktop login]\n      subgraph sub_bianca_private_env[The project's private virtual project cluster]\n        bianca_private_console[Bianca console environment]\n        bianca_private_remote_desktop[Bianca remote desktop]\n        bianca_private_terminal[Terminal]\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    %% style sub_inside fill:#fcc,color:#000,stroke:#fcc\n    style sub_bianca_shared_env fill:#ffc,color:#000,stroke:#ffc\n    style sub_bianca_private_env fill:#cfc,color:#000,stroke:#cfc\n\n    %% Shared Bianca\n    bianca_shared_console --&gt; |UPPMAX password|bianca_private_console\n    bianca_shared_remote_desktop--&gt;|UPPMAX password|bianca_private_remote_desktop\n\n    %% Private Bianca\n    bianca_private_console---|is a|bianca_private_terminal\n    bianca_private_remote_desktop--&gt;|must also use|bianca_private_terminal</code></pre>","tags":["login","log in","Bianca","console","terminal","SSH"]},{"location":"getting_started/login_bianca_remote_desktop_local_thinlinc_client/","title":"Login to the Bianca remote desktop environment via a ThinLinc client","text":""},{"location":"getting_started/login_bianca_remote_desktop_local_thinlinc_client/#login-to-the-bianca-remote-desktop-environment-via-a-thinlinc-client","title":"Login to the Bianca remote desktop environment via a ThinLinc client","text":"<p>There are multiple ways to log in to Bianca. Accessing Bianca's remote desktop environment via a local ThinLinc client, however, is impossible: one can only access Bianca remote desktop environment via a website.</p> <p>This page is here for UPPMAX staff only (no page links to this one), so they remember this is impossible. See this Issue for a dialog.</p>"},{"location":"getting_started/login_bianca_remote_desktop_website/","title":"Log in to the Bianca remote desktop environment website","text":"","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#log-in-to-the-bianca-remote-desktop-environment-website","title":"Log in to the Bianca remote desktop environment website","text":"<p>The Bianca remote desktop environment</p> <p>There are multiple ways to log in to Bianca.</p> <p>This page describes how to log in to Bianca using a remote desktop that is accessible from a webbrowser.</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#procedure","title":"Procedure","text":"Prefer a video? <p>See this page explained in a YouTube video here</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>As Bianca is an HPC cluster for sensitive data, one needs to be within SUNET to be able to access her.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p> How does it look like to try to access a remote desktop from outside of SUNET? <p></p> <p>When accessing the Bianca UPPMAX login website from outside of SUNET, nothing will appear in your browser.</p> <p>You can see it in action in this video you can see how this looks like when outside of SUNET.</p> <p>It looks quite dull, as nothing happens until these is a timeout.</p> Will a local ThinLinc client work too? <p>No.</p> <p>One really can only access Bianca remote desktop environment via a website</p> <p>When inside SUNET, one can access a remote desktop environment using a website:</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#2-go-to-httpsbiancauppmaxuuse","title":"2. Go to https://bianca.uppmax.uu.se","text":"<p>When inside SUNET, in your web browser, go to https://bianca.uppmax.uu.se.</p> How does it look like when outside of SUNET? <p></p> <p>When accessing the Bianca UPPMAX login website from outside of SUNET, nothing will appear in your browser.</p> <p>You can see it in action in this video you can see how this looks like when outside of SUNET.</p> <p>It looks quite dull, as nothing happens until these is a timeout.</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#3-fill-in-the-first-dialog","title":"3. Fill in the first dialog","text":"<p>Fill in the first dialog.</p> <p>Do use the <code>UPPMAX</code> 2-factor authentication (i.e. not SUPR!)</p> How do I setup 2-factor authentication? <p>See the guide at 2-factor authentication to setup an UPPMAX 2-factor authentication method.</p> <p>You really need to use the UPPMAX 2-factor authentication, i.e not the SUPR one, to login to Bianca.</p> <p></p> <p>Screenshot of a two-factor authentication app. Use the 2-factor authentication called 'UPPMAX' to access Bianca</p> How does that web page look like? <p></p> <p>The first page of https://bianca.uppmax.uu.se</p> <p>Sometimes a webpage will be shown that asks you to wait. Simply do that :-)</p> How does that web page look like? <p></p> <p>No c Web Access active The login node for your project cluster is probably asleep. Boot initiated. The startup can take from 2 to 8 minutes.</p> <p>This page will attempt to automatically reload. If nothing happens even after multiple minutes, you can do so manually. It is a bit more controlled in text mode.</p> <p>When this takes long, your original second factor code might expire. In that scenario, you'll be redirected to the first login page again.</p> <p>This is the webpage that is shown when a login node needs to be created.</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#4-fill-in-the-second-dialog-using-your-regular-password","title":"4. Fill in the second dialog, using your regular password","text":"<p>Fill in the second dialog, using your regular password (i.e. no need for two-factor authentication).</p> How does that web page look like? <p></p> <p>The second Bianca remote desktop login dialog. Note that it uses ThinLinc to establish this connection</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#5-picking-a-remote-desktop-flavor-but-not-kde","title":"5. Picking a remote desktop flavor, but not KDE","text":"<p>When picking a remote desktop flavor, pick GNOME or XFCE, avoid picking KDE.</p> How does that look like? <p></p> <p>Here you are told you will need to pick a remote desktop flavor</p> <p></p> <p>Here you are asked to pick a remote desktop flavor, with Xfce as the default. Pick any, except KDE.</p> <p>Avoid choosing KDE</p> <p>Avoid choosing the KDE desktop, as it gives problems when running interactive sessions.</p> <p>Instead, we recommend GNOME or XFCE.</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#6-you-are-in","title":"6. You are in","text":"<p>Enjoy! You are in: you are now on a Bianca login node.</p> How do I copy-paste text? <p>The Bianca remote desktop environment via a website uses ThinLinc.</p> <p>At the ThinLinc page you can find how to work with its interface.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> What is the difference between 'disconnect session' and 'end session'? <p>'disconnect session' will save the current state of your session. When you connect again, you will get the remote desktop back in exactly in the same place you left the system. For example: if you were editing a file before disconnecting, your prompt will be in the same place you left it.</p> <p>'end session' will not save the current state of your session. Instead, you will start with a clean slate at the next login.</p> <p>Bianca has a automatically disconnect after 30 minutes of inactivity. In the future it is possible that we implement some kind of \"automatic log out from active graphical session\".</p> <pre><code>flowchart TD\n\n    subgraph sub_inside[IP inside SUNET]\n\n      user(User)\n\n      subgraph sub_bianca_shared_env[Bianca shared network]\n        bianca_shared_remote_desktop[Bianca remote desktop login]\n        subgraph sub_bianca_private_env[The project's private virtual project cluster]\n          bianca_private_remote_desktop[Bianca remote desktop]\n\n          %% Ensure the innermost square gets big enough\n          END:::hidden\n        end\n      end\n    end\n\n    %% Inside SUNET\n    user--&gt;|Bianca website, UPPMAX password and 2FA|bianca_shared_remote_desktop\n\n    bianca_shared_remote_desktop --&gt; |UPPMAX password| bianca_private_remote_desktop</code></pre>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#troubleshooting","title":"Troubleshooting","text":"","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#access-denied","title":"Access denied","text":"How does that look like? <p>Contact support.</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website/#authentication-failed","title":"Authentication failed","text":"How does that look like? <p>Contact support.</p>","tags":["login","log in","Bianca","remote desktop","website","URL","in","inside","within","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website_no_sunet/","title":"Log in to the Bianca remote desktop environment website from outside of the Swedish university networks","text":"","tags":["login","log in","Bianca","remote desktop","website","URL","out","outside","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website_no_sunet/#log-in-to-the-bianca-remote-desktop-environment-website-from-outside-of-the-swedish-university-networks","title":"Log in to the Bianca remote desktop environment website from outside of the Swedish university networks","text":"<p>If you cannot use VPN this may be the solution for you.</p> <p>Danger</p> <ul> <li>Do not log in \"normally\" with ThinLinc (web or client) to Rackham and from there log in to Bianca.</li> <li>This will let all sensitive data land on Rackham uncrypted as a an intermediate step.</li> <li>Rackham is not a secure system and could be spied on.</li> </ul>","tags":["login","log in","Bianca","remote desktop","website","URL","out","outside","SUNET","university networks"]},{"location":"getting_started/login_bianca_remote_desktop_website_no_sunet/#procedure","title":"Procedure","text":"<p>Info is coming!</p> Info to elaborate <p>Exemplet h\u00e4r med -W i thinlincs ssh-konf uppn\u00e5r samma effekt.</p> <p>https://community.thinlinc.com/t/jumphost-support/275</p>","tags":["login","log in","Bianca","remote desktop","website","URL","out","outside","SUNET","university networks"]},{"location":"getting_started/login_dardel/","title":"Log in to Dardel (at PDC)","text":"","tags":["login","log in","Dardel"]},{"location":"getting_started/login_dardel/#log-in-to-dardel","title":"Log in to Dardel","text":"<p>There are multiple clusters one can log in to. Here it is described how to login to Dardel.</p>","tags":["login","log in","Dardel"]},{"location":"getting_started/login_dardel/#procedure","title":"Procedure","text":"Prefer a video? <p>Go to a YouTube video on how to log in to Dardel to view what to do from step 3 and onwards.</p> <p>First, we are here to help. Please contact support if you run into problems when trying the guide below.</p> <p>Note that step 1 requires some hours of waiting and step 2 requires an overnight wait.</p> <pre><code>flowchart TD\n  get_supr_project[1.Access to a SUPR project with Dardel]\n  get_pdc_account[2.Access to a PDC account]\n  create_ssh_key[3.Create temporary SSH keys]\n  add_ssh_key[4.Add the SSH keys to the PDC Login Portal]\n  login[5.Login]\n\n  get_supr_project --&gt; |needed for| get_pdc_account\n\n  create_ssh_key --&gt; |needed for| add_ssh_key\n  get_pdc_account --&gt; |needed for| add_ssh_key\n  add_ssh_key --&gt; |needed for| login</code></pre> <p>Overview of the migration process. Note that step 1 requires some hours of waiting and step 2 requires an overnight wait.</p>","tags":["login","log in","Dardel"]},{"location":"getting_started/login_dardel/#1-get-access-to-a-supr-project-with-dardel","title":"1. Get access to a SUPR project with Dardel","text":"<p>First step is to get get access to a SUPR project with Dardel. This is described at PDC's page on getting access to Dardel. You will get an email when you are added to a project, this can take some hours.</p> How do I know I have access to a Dardel project? <p>Login to https://supr.naiss.se/. If there is a PDC project, you may have access to a project with Dardel.</p> <p></p> <p>Example user that has access to a PDC project</p> <p>If you may a PDC project that does not use Dardel, click on the project to go the the project overview.</p> <p></p> <p>Example PDC project overview</p> <p>From there, scroll down to 'Resources'. If you see 'Dardel' among the compute resources, you have confirmed you have access to a Dardel project.</p> <p></p> <p>Resources from an example PDC project</p>","tags":["login","log in","Dardel"]},{"location":"getting_started/login_dardel/#2-get-a-pdc-account-via-supr","title":"2. Get a PDC account via SUPR","text":"<p>Get a PDC account via SUPR. This is described at the PDC page on getting an account. You will get a PDC account overnight.</p> How do I know I have a PDC account? <p>Login to https://supr.naiss.se/. and click on 'Accounts' in the main menu bar at the left.</p> <p>If you see 'Dardel' among the resources, and status 'Enabled' in the same row, you have a PDC account!</p> <p></p> <p>Example of a user having an account at PDC's Dardel HPC cluster</p> How do I find out my PDC username? <p>In the PDC login portal, after logging in, you can see your Dardel username in the top-right corner:</p> <p></p> <p>Example screenshot of the PDC login portal. The Dardel username of this user is <code>svenbi</code></p>","tags":["login","log in","Dardel"]},{"location":"getting_started/login_dardel/#3-create-ssh-key-pair","title":"3. Create SSH key pair","text":"<p>Create SSH key and add it to the PDC Login Portal.</p> <ul> <li>Create the password less SSH key in a Linux terminal (e.g. from Rackham):</li> </ul> <pre><code>module load darsync\n</code></pre> <pre><code>darsync sshkey\n</code></pre>","tags":["login","log in","Dardel"]},{"location":"getting_started/login_dardel/#4-add-the-public-key-to-the-pdc-login-portal","title":"4. Add the public key to the PDC Login Portal","text":"<p>When creating the SSH key pair, <code>darsync</code> will already display the public key.</p> <p>If, however, you missed it, you can view the public SSH key again; in a terminal logged into Rackham:</p> <pre><code>cat ~/id_ed25519_pdc.pub\n</code></pre> How does that look like? <p>The text will look similar to this:</p> <pre><code>ssh-ed25519 AAAA69Nz1C1lZkI1NdE5ABAAIA7RHe4jVBRTEvHVbEYxV8lnOQl22N+4QcUK+rDv1gPS user@rackham2.uppmax.uu.se\n</code></pre> <p>Open the PDC Login Portal.</p> <p>Follow our step-by-step instructions on how to add SSH keys.</p>","tags":["login","log in","Dardel"]},{"location":"getting_started/login_dardel/#5-login","title":"5. Login","text":"<ul> <li>the PDC documentation on 'SSH login'</li> </ul> <p>On a terminal, do:</p> <pre><code>ssh [username]@dardel.pdc.kth.se\n</code></pre> <p>where <code>[username]</code> is your PDC username, for example <code>ssh sven@dardel.pdc.kth.se</code>.</p>","tags":["login","log in","Dardel"]},{"location":"getting_started/login_pelle/","title":"Log in to Pelle","text":"","tags":["login","log in","Pelle"]},{"location":"getting_started/login_pelle/#log-in-to-pelle","title":"Log in to Pelle","text":"<p>There are multiple UPPMAX clusters one can log in to. Here we describe how to log in to Pelle.</p> <p>Pelle is not ready yet</p> <ul> <li>Pelle is not completely ready yet, hence the content of this page will change.</li> <li>Also, the content will be shorter and sloppier, until procedures solidify.</li> <li>The instructions below will not work as of today.</li> </ul> <p>Test users are testing the Pelle environment</p> <ul> <li>We have pilot test users testing Pelle right now.</li> <li>Soon all Pelle users will be let in.</li> <li>In addition to SSH also ThinLinc will be available</li> </ul> <ul> <li>Prerequisites describes what is needed before one can access Pelle</li> <li>Which way to login?<ul> <li>Website</li> <li>Terminal</li> <li>Local ThinLinc client</li> </ul> </li> </ul>","tags":["login","log in","Pelle"]},{"location":"getting_started/login_pelle/#which-way-to-login","title":"Which way to login?","text":"<p>There are multiple ways to log in to Pelle:</p> Login Description Screenshot Website  Does not work yet. Remote desktop, no installation needed, slow  Under construction Terminal Console environment, recommended Local ThinLinc client Remote desktop, recommended, need installation <p>Here is a decision tree, to determine which way to log in:</p> <pre><code>flowchart TD\n  need_gui(Need to run a graphical program?)\n  use_terminal[Use a terminal]\n  use_website[1.Use the remote desktop website]\n  need_easy_or_speedy(Need easiest or fastest?)\n  use_local[2.Use a local ThinLinc client]\n\n  need_gui --&gt; |no| use_terminal\n  need_gui --&gt; |yes| need_easy_or_speedy\n  need_easy_or_speedy --&gt; |easiest| use_website\n  need_easy_or_speedy --&gt; |fastest| use_local\n\n  how_login(How to log in?)\n\n  use_password[3.Use password]\n  use_ssh_keys[4.Use SSH keys]\n\n  use_terminal --&gt; how_login\n  how_login --&gt; |easiest| use_password\n  how_login --&gt; |most convenient| use_ssh_keys</code></pre> <p>The procedures can be found at:</p> <ul> <li> Does not work yet. 1.Login to the Pelle remote desktop environment using the website</li> <li>2.Login to the Pelle remote desktop environment using a local ThinLinc client</li> <li>3.Login to the Pelle console environment with a password.</li> <li>4.Login to the Pelle console environment with an SSH key</li> </ul> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p>","tags":["login","log in","Pelle"]},{"location":"getting_started/login_pelle_console_password/","title":"Login to the Pelle console environment with a password","text":"","tags":["login","log in","Pelle","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_pelle_console_password/#login-to-the-pelle-console-environment-with-a-password","title":"Login to the Pelle console environment with a password","text":"<p>There are multiple ways to log in to Pelle. This page describes how to do so using a terminal and a password.</p> <p>If you want to get rid of using a password every time, see login to the Pelle console environment with an SSH key.</p>","tags":["login","log in","Pelle","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_pelle_console_password/#procedure","title":"Procedure","text":"Prefer a video? <p>Watch the YouTube video Login to the Pelle console environment with a password</p>","tags":["login","log in","Pelle","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_pelle_console_password/#1-use-ssh-to-log-in","title":"1. Use <code>ssh</code> to log in","text":"<p>From a terminal, use <code>ssh</code> to log in:</p> <pre><code>ssh [username]@pelle.uppmax.uu.se\n</code></pre> <p><code>[username]</code> is your UPPMAX username, for example, <code>sven</code>, resulting in:</p> <pre><code>ssh sven@pelle.uppmax.uu.se\n</code></pre> <p>Using this login, graphics (i.e. images) on Pelle cannot be displayed.</p> How does this look like the first time? <pre><code>sven@svens_computer:~$ ssh -X sven@pelle.uppmax.uu.se\nThe authenticity of host 'pelle.uppmax.uu.se (89.44.250.8&lt;X&gt;)' can't be established.\nECDSA key fingerprint is SHA256:W/MazH3WrH0wKrHBOJpPbDaU4qeYGqiv3FRPsdXIsb4.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <ul> <li> <p>Type <code>yes</code>!</p> </li> <li> <p>Other valid fingerprints are:</p> <ul> <li><code>SHA256:y241gg8SExSktACnpD+OvROrMPTJcXYYdT/zYReef+k</code></li> <li><code>SHA256:hkhuV+0mUDL7N4Jpr8/OWInrORSAL5ZRpvAqfjyg7Jg</code></li> </ul> </li> </ul> How can I display graphics on Pelle? <p>To display graphics (i.e. images) on Pelle, use <code>-X</code>:</p> <pre><code>ssh -X username@pelle.uppmax.uu.se\n</code></pre> <p>This option enable so-called X forwarding, which allows you to run programs that require light graphics, such as eog to display an image.</p> Why no <code>-A</code>? <p>On Pelle, one can use <code>-A</code>:</p> <pre><code>ssh -A username@pelle.uppmax.uu.se\n</code></pre> <p>This option is only useful when you want to log in to Pelle via the console using an SSH key. As we here use passwords (i.e. no SSH keys) to access Pelle, <code>-A</code> is unused and hence we simplify this documentation by omitting it.</p>","tags":["login","log in","Pelle","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_pelle_console_password/#2-type-your-uppmax-password","title":"2. Type your UPPMAX password","text":"<p>Type your UPPMAX password and press enter. You will see no asterisks to indicate how many characters you've typed in.</p> <p>If you are outside the university networks you will be asked for your UPPMAX 2-factor authentication number.</p>","tags":["login","log in","Pelle","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_pelle_console_password/#3-you-are-in","title":"3. You are in","text":"<p>Enjoy! You are in! Or, to be precise, you are in your home folder on a Pelle login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p>","tags":["login","log in","Pelle","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_pelle_console_ssh_key/","title":"Login to the Pelle console environment using SSH keys","text":"","tags":["login","log in","Pelle","console","terminal","SSH"]},{"location":"getting_started/login_pelle_console_ssh_key/#login-to-the-pelle-console-environment-using-ssh-keys","title":"Login to the Pelle console environment using SSH keys","text":"<p>There are multiple ways to log in to Pelle.</p> <p>This page describes how to log in to Pelle using a terminal and an SSH key pair.</p> How do I create an SSH key pair? <p>See the UPPMAX guide Create and use an SSH key pair for Pelle</p>","tags":["login","log in","Pelle","console","terminal","SSH"]},{"location":"getting_started/login_pelle_console_ssh_key/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>When inside SUNET, one can access a Pelle console environment using SSH and SSH keys.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p> <p>This is considered a bit harder to setup, but one only needs to type one password to login to Pelle. If you don't mind typing your UPPMAX password twice, an easier setup is log in to the Pelle console environment with a password.</p>","tags":["login","log in","Pelle","console","terminal","SSH"]},{"location":"getting_started/login_pelle_console_ssh_key/#2-use-ssh-to-log-in","title":"2. Use <code>ssh</code> to log in","text":"<p>From a terminal, use <code>ssh</code> to log in:</p> <pre><code>ssh -A [user]@pelle.uppmax.uu.se\n</code></pre> <p>For example:</p> <pre><code>ssh -A sven@pelle.uppmax.uu.se\n</code></pre>","tags":["login","log in","Pelle","console","terminal","SSH"]},{"location":"getting_started/login_pelle_console_ssh_key/#3-type-your-uppmax-password","title":"3. Type your UPPMAX password","text":"<p>Type your UPPMAX password.</p>","tags":["login","log in","Pelle","console","terminal","SSH"]},{"location":"getting_started/login_pelle_console_ssh_key/#4-you-are-in","title":"4. You are in","text":"<p>Enjoy! You are in! To be precise, you are on a Pelle login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> <p>In a Pelle console environment:</p> <ul> <li>Text display is limited to 50kBit/s.   This means that if you create a lot of text output,   you will have to wait some time before you get your prompt back.</li> <li>Cut, copy and paste work as usual.   Be careful to not copy-paste sensitive data!</li> </ul>","tags":["login","log in","Pelle","console","terminal","SSH"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/","title":"Log in to Pelle's remote desktop environment using a local ThinLinc client","text":"","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#log-in-to-pelles-remote-desktop-environment-using-a-local-thinlinc-client","title":"Log in to Pelle's remote desktop environment using a local ThinLinc client","text":"<p>There are multiple ways to log in to Pelle. This page described how to log in to its remote desktop environment via a local ThinLinc client.</p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#procedure","title":"Procedure","text":"Prefer a video? <p>Watch the YouTube video Log in to Pelle's remote desktop environment using a local ThinLinc client</p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#1-install-thinlinc","title":"1. Install ThinLinc","text":"<p>Install ThinLinc. For help, see the UPPMAX page on ThinLinc.</p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#2-start-thinlinc","title":"2. Start ThinLinc","text":"<p>In the ThinLinc login dialog,</p> <ul> <li>set the 'Server' name to <code>pelle-gui.uppmax.uu.se</code></li> <li>set the 'Username' to your UPPMAX username, e.g. <code>sven</code></li> <li>set the 'Password' to your UPPMAX password</li> </ul> What does that look like? <p></p> <p>The ThinLinc login dialog</p> Why not use <code>https://www.pelle-gui.uppmax.uu.se</code>? <p>Because that does not work :-)</p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#3-first-time-only-connect-to-pelle","title":"3. (first time only) Connect to Pelle","text":"<p>If you are warned about a key, click on 'Continue' if the key matches the one in the dialog below:</p> <p></p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#4-fill-in-your-2fa-totp","title":"4. Fill in your 2FA TOTP","text":"<p>When asked for a TOTP ('Time-based one-time password'), get it from the <code>UPPMAX</code> 2-factor authentication (i.e. not SUPR!)</p> What does that look like? <p></p> <p>Pelle asking for a TOTP</p> How do I setup 2-factor authentication? <p>See the guide at 2-factor authentication to setup an UPPMAX 2-factor authentication method.</p> <p>You really need to use the UPPMAX 2-factor authentication, i.e not the SUPR one, to login to Pelle.</p> <p></p> <p>Screenshot of a two-factor authentication app. Use the 2-factor authentication called 'UPPMAX' to access Pelle</p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#5-forward-the-thinlinc-welcome-dialog","title":"5. Forward the ThinLinc Welcome dialog","text":"<p>On the ThinLinc 'Welcome' dialog, click 'Forward'</p> What does that look like? <p></p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#6-select-a-thinlinc-profile","title":"6. Select a ThinLinc profile","text":"<p>On the ThinLinc 'Select profile' dialog, select a profile:</p> Profile Description MATE Fancier XFCE Simpler What does that look like? <p></p> <p>Here you are asked to pick a remote desktop flavor, with MATE as the default. Pick any.</p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_local_thinlinc_client/#7-you-are-in","title":"7. You are in","text":"<p>You are in! Well done!</p> <p>For tips on how to work with this environment, see the UPPMAX ThinLinc page.</p> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> What does that look like? <p></p>","tags":["login","log in","Pelle","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_pelle_remote_desktop_website/","title":"Log in to Pelle's remote desktop via a webbrowser","text":"","tags":["login","log in","Pelle","remote desktop","website","URL"]},{"location":"getting_started/login_pelle_remote_desktop_website/#log-in-to-pelles-remote-desktop-via-a-webbrowser","title":"Log in to Pelle's remote desktop via a webbrowser","text":"<p>This does not work yet</p> <p>This does not work yet.</p> <p>There are multiple ways to log in to Pelle. This page describes how to log in to its remote desktop environment via a web browser.</p>","tags":["login","log in","Pelle","remote desktop","website","URL"]},{"location":"getting_started/login_pelle_remote_desktop_website/#procedure","title":"Procedure","text":"Prefer a video? <p>TODO</p> <p>This is a procedure with one step. Most work will be to fulfill all Pelle usage prerequisites.</p>","tags":["login","log in","Pelle","remote desktop","website","URL"]},{"location":"getting_started/login_pelle_remote_desktop_website/#1-go-to-httpspelle-guiuppmaxuuse","title":"1. Go to https://pelle-gui.uppmax.uu.se","text":"<p>This does not work yet</p> <p>This does not work yet.</p> <p>In a webbrowser, go to https://pelle-gui.uppmax.uu.se.</p> <ul> <li>In the first field, fill in your UPPMAX username, e.g. <code>sven</code></li> <li>In the second field, fill in your UPPMAX password (e.g. <code>password</code>   and your UPPMAX 2FA (e.g. <code>123456</code>)   together, without a space (e.g. `password123456)</li> </ul> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p>","tags":["login","log in","Pelle","remote desktop","website","URL"]},{"location":"getting_started/login_pelle_remote_desktop_website/#usage","title":"Usage","text":"<p>For tips on how to work with this environment, see the UPPMAX ThinLinc page (as that software is used to do the heavy lifting for that website).</p>","tags":["login","log in","Pelle","remote desktop","website","URL"]},{"location":"getting_started/login_rackham/","title":"Log in to Rackham","text":"","tags":["login","log in","Rackham"]},{"location":"getting_started/login_rackham/#log-in-to-rackham","title":"Log in to Rackham","text":"<p>There are multiple UPPMAX clusters one can log in to. Here we describe how to log in to Rackham.</p> <ul> <li>Prerequisites describes what is needed before one can access Rackham</li> <li>Which way to login?<ul> <li>Website</li> <li>Terminal</li> <li>Local ThinLinc client</li> </ul> </li> </ul>","tags":["login","log in","Rackham"]},{"location":"getting_started/login_rackham/#which-way-to-login","title":"Which way to login?","text":"<p>There are multiple ways to log in to Rackham:</p> Login Description Screenshot Website Remote desktop, no installation needed, slow Terminal Console environment, recommended Local ThinLinc client Remote desktop, recommended, need installation <p>Here is a decision tree, to determine which way to log in:</p> <pre><code>flowchart TD\n  need_gui(Need to run a graphical program?)\n  use_terminal[Use a terminal]\n  use_website[Use the remote desktop website]\n  need_easy_or_speedy(Need easiest or fastest?)\n  use_local[Use a local ThinLinc client]\n\n  need_gui --&gt; |no| use_terminal\n  need_gui --&gt; |yes| need_easy_or_speedy\n  need_easy_or_speedy --&gt; |easiest| use_website\n  need_easy_or_speedy --&gt; |fastest| use_local\n\n  how_login(How to log in?)\n\n  use_password[Use password. Start here]\n  use_ssh_keys[Use SSH keys. No more password needed]\n\n  use_terminal --&gt; how_login\n  how_login --&gt; use_password\n  how_login --&gt; use_ssh_keys</code></pre> <p>The procedures can be found at:</p> <ul> <li>Login to the Rackham remote desktop environment using the website</li> <li>Login to the Rackham console environment with a password.   If you want to get rid of using a password every time, see login to the Rackham console environment with an SSH key</li> <li>Login to the Rackham remote desktop environment using a local ThinLinc client</li> </ul> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> <p>About Rackham hostname</p> <p>The hostname used to login via SSH is:</p> <p>rackham.uppmax.uu.se</p> <p>Note: This is a \"round robin\" address which will direct you to one of the physical login nodes, rackham1.uppmax.uu.se or rackham2.uppmax.uu.se, etc. (If needed, you can also login directly to one of these, by using their respective hostnames.)</p>","tags":["login","log in","Rackham"]},{"location":"getting_started/login_rackham_console_password/","title":"Login to the Rackham console environment with a password","text":"","tags":["login","log in","Rackham","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_rackham_console_password/#login-to-the-rackham-console-environment-with-a-password","title":"Login to the Rackham console environment with a password","text":"<p>There are multiple ways to log in to Rackham. This page describes how to do so using a terminal and a password.</p> <p>If you want to get rid of using a password every time, see login to the Rackham console environment with an SSH key.</p>","tags":["login","log in","Rackham","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_rackham_console_password/#procedure","title":"Procedure","text":"Prefer a video? <p>This procedure is also shown by this YouTube video.</p>","tags":["login","log in","Rackham","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_rackham_console_password/#1-use-ssh-to-log-in","title":"1. Use <code>ssh</code> to log in","text":"<p>From a terminal, use <code>ssh</code> to log in:</p> <pre><code>ssh [username]@rackham.uppmax.uu.se\n</code></pre> <p><code>[username]</code> is your UPPMAX username, for example, <code>sven</code>, resulting in:</p> <pre><code>ssh sven@rackham.uppmax.uu.se\n</code></pre> What does this look like the first time? <pre><code>sven@svens_computer:~$ ssh sven@rackham.uppmax.uu.se\nThe authenticity of host 'rackham.uppmax.uu.se (89.44.250.8&lt;X&gt;)' can't be established.\nECDSA key fingerprint is SHA256:W/MazH3WrH0wKrHBOJpPbDaU4qeYGqiv3FRPsdXIsb4.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <ul> <li> <p>Type <code>yes</code>!</p> </li> <li> <p>Other valid fingerprints are:</p> <ul> <li><code>SHA256:y241gg8SExSktACnpD+OvROrMPTJcXYYdT/zYReef+k</code></li> <li><code>SHA256:hkhuV+0mUDL7N4Jpr8/OWInrORSAL5ZRpvAqfjyg7Jg</code></li> </ul> </li> </ul> Why no <code>-X</code>? <p>On Rackham, one can use <code>-X</code>:</p> <pre><code>ssh -X username@rackham.uppmax.uu.se\n</code></pre> <p>This option is only useful when you want to enable so-called X forwarding: which allows you to run programs that require light graphics, such as eog to display an image.</p> Why no <code>-A</code>? <p>On Rackham, one can use <code>-A</code>:</p> <pre><code>ssh -A username@rackham.uppmax.uu.se\n</code></pre> <p>This option is only useful when you want to log in to Rackham via the console using an SSH key. As we here use passwords (i.e. no SSH keys) to access Rackham, <code>-A</code> is unused and hence we simplify this documentation by omitting it.</p>","tags":["login","log in","Rackham","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_rackham_console_password/#2-type-your-uppmax-password","title":"2. Type your UPPMAX password","text":"<p>Type your UPPMAX password and press enter. You will see no asterisks to indicate how many characters you've typed in.</p> <p>If you are outside the university networks you will be asked for your UPPMAX 2-factor authentication number.</p>","tags":["login","log in","Rackham","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_rackham_console_password/#3-you-are-in","title":"3. You are in","text":"<p>Enjoy! You are in! Or, to be precise, you are in your home folder on a Rackham login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p>","tags":["login","log in","Rackham","console","terminal","password","ssh","SSH"]},{"location":"getting_started/login_rackham_console_ssh_key/","title":"Login to the Rackham console environment using SSH keys","text":"","tags":["login","log in","Rackham","console","terminal","SSH"]},{"location":"getting_started/login_rackham_console_ssh_key/#login-to-the-rackham-console-environment-using-ssh-keys","title":"Login to the Rackham console environment using SSH keys","text":"<p>There are multiple ways to log in to Rackham.</p> <p>This page describes how to log in to Rackham using a terminal and an SSH key pair.</p>","tags":["login","log in","Rackham","console","terminal","SSH"]},{"location":"getting_started/login_rackham_console_ssh_key/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>When inside SUNET, one can access a Rackham console environment using SSH and SSH keys.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p> <p>This is considered a bit harder to setup, but one only needs to type one password to login to Rackham. If you don't mind typing your UPPMAX password twice, an easier setup is log in to the Rackham console environment with a password.</p>","tags":["login","log in","Rackham","console","terminal","SSH"]},{"location":"getting_started/login_rackham_console_ssh_key/#2-use-ssh-to-log-in","title":"2. Use <code>ssh</code> to log in","text":"<p>From a terminal, use <code>ssh</code> to log in:</p> <pre><code>ssh -A [user]@rackham.uppmax.uu.se\n</code></pre> <p>For example:</p> <pre><code>ssh -A sven@rackham.uppmax.uu.se\n</code></pre>","tags":["login","log in","Rackham","console","terminal","SSH"]},{"location":"getting_started/login_rackham_console_ssh_key/#3-type-your-uppmax-password","title":"3. Type your UPPMAX password","text":"<p>Type your UPPMAX password.</p>","tags":["login","log in","Rackham","console","terminal","SSH"]},{"location":"getting_started/login_rackham_console_ssh_key/#4-you-are-in","title":"4. You are in","text":"<p>Enjoy! You are in! To be precise, you are on a Rackham login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> <p>In a Rackham console environment:</p> <ul> <li>Text display is limited to 50kBit/s.   This means that if you create a lot of text output,   you will have to wait some time before you get your prompt back.</li> <li>Cut, copy and paste work as usual.   Be careful to not copy-paste sensitive data!</li> </ul>","tags":["login","log in","Rackham","console","terminal","SSH"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/","title":"Log in to Rackham's remote desktop environment using a local ThinLinc client","text":"","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#log-in-to-rackhams-remote-desktop-environment-using-a-local-thinlinc-client","title":"Log in to Rackham's remote desktop environment using a local ThinLinc client","text":"<p>There are multiple ways to log in to Rackham. This page described how to log in to its remote desktop environment via a local ThinLinc client.</p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#procedure","title":"Procedure","text":"Prefer a video? <p>This procedure is also shown by this YouTube video.</p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#1-install-thinlinc","title":"1. Install ThinLinc","text":"<p>Install ThinLinc. For help, see the UPPMAX page on ThinLinc.</p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#2-start-thinlinc","title":"2. Start ThinLinc","text":"<p>In the ThinLinc login dialog, set the server name to <code>rackham-gui.uppmax.uu.se</code>.</p> How does that look like? <p></p> <p>The ThinLinc login dialog</p> Why not use <code>https://www.rackham-gui.uppmax.uu.se</code>? <p>Because that does not work :-)</p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#3-forward-the-thinlinc-welcome-dialog","title":"3. Forward the ThinLinc Welcome dialog","text":"<p>On the ThinLinc 'Welcome' dialog, click 'Forward'</p> How does that look like? <p></p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#4-select-a-thinlinc-profile","title":"4. Select a ThinLinc profile","text":"<p>On the ThinLinc 'Select profile' dialog, select a profile:</p> Profile Recommendation GNOME Recommended KDE Avoid XFCE Recommended <p>Avoid choosing KDE</p> <p>Avoid choosing the KDE desktop, as it gives problems when running interactive sessions.</p> <p>Instead, we recommend GNOME or XFCE.</p> How does that look like? <p></p> <p>Here you are asked to pick a remote desktop flavor, with Xfce as the default. Pick any, except KDE.</p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#5-you-are-in","title":"5. You are in","text":"<p>You are in! Well done!</p> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> How does that look like? <p></p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_local_thinlinc_client/#usage","title":"Usage","text":"<p>For tips on how to work with this environment, see the UPPMAX ThinLinc page.</p>","tags":["login","log in","Rackham","remote desktop","local","ThinLinc","ThinLinc client","program","tool"]},{"location":"getting_started/login_rackham_remote_desktop_website/","title":"Log in to Rackham's remote desktop via a webbrowser","text":"","tags":["login","log in","Rackham","remote desktop","website","URL"]},{"location":"getting_started/login_rackham_remote_desktop_website/#log-in-to-rackhams-remote-desktop-via-a-webbrowser","title":"Log in to Rackham's remote desktop via a webbrowser","text":"<p>Rackham's remote desktop environment via a webbrowser</p> <p>There are multiple ways to log in to Rackham. This page describes how to log in to its remote desktop environment via a web browser.</p>","tags":["login","log in","Rackham","remote desktop","website","URL"]},{"location":"getting_started/login_rackham_remote_desktop_website/#procedure","title":"Procedure","text":"Prefer a video? <p>This procedure is also shown by this YouTube video.</p> <p>This is a procedure with one step. Most work will be to fulfill all Rackham usage prerequisites.</p>","tags":["login","log in","Rackham","remote desktop","website","URL"]},{"location":"getting_started/login_rackham_remote_desktop_website/#1-go-to-httpsrackham-guiuppmaxuuse","title":"1. Go to https://rackham-gui.uppmax.uu.se","text":"<p>In a webbrowser, go to https://rackham-gui.uppmax.uu.se.</p> <ul> <li>In the first field, fill in your UPPMAX username, e.g. <code>sven</code></li> <li>In the second field, fill in your UPPMAX password (e.g. <code>password</code>   and your UPPMAX 2FA (e.g. <code>123456</code>)   together, without a space (e.g. `password123456)</li> </ul> How does that page look like? <p></p> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p>","tags":["login","log in","Rackham","remote desktop","website","URL"]},{"location":"getting_started/login_rackham_remote_desktop_website/#usage","title":"Usage","text":"<p>For tips on how to work with this environment, see the UPPMAX ThinLinc page (as that software is used to do the heavy lifting for that website).</p>","tags":["login","log in","Rackham","remote desktop","website","URL"]},{"location":"getting_started/login_snowy/","title":"Log in to Snowy","text":"","tags":["login","log in","Snowy"]},{"location":"getting_started/login_snowy/#log-in-to-snowy","title":"Log in to Snowy","text":"<p>There are multiple UPPMAX clusters one can log in to. Here we describe how to log in to Snowy</p> <p>One needs to be allowed to use Snowy. These prerequisites describes what is needed before one can use Snowy.</p> <p>To make Snowy do a calculation, log in to a Rackham login node.</p> <p>After login, you will be on a login node.</p> <p>How to behave on a login node</p> <p>On a login node, one can and should do simple things only: it is a resource shared with all other users on that node.</p> <p>If you need to do more intense calculations, use the Slurm job scheduler.</p> <p>If you need to do more intense calculations interactively, use an interactive session.</p> <p>After logging in, one can</p> <ul> <li>Start a batch job using Snowy resources</li> <li>Start an interactive job</li> </ul> <pre><code>graph LR\n\n  subgraph \"Snowy\"\n    snowy_calculation_node[Calculation nodes]\n  end\n\n\n  subgraph \"Rackham\"\n    login_node[Login node]\n  end\n\n  login_node --&gt; |interactive or sbatch| snowy_calculation_node</code></pre>","tags":["login","log in","Snowy"]},{"location":"getting_started/pelle_usage_prerequisites/","title":"Prerequisites for using Pelle","text":""},{"location":"getting_started/pelle_usage_prerequisites/#prerequisites-for-using-pelle","title":"Prerequisites for using Pelle","text":"<p>To be allowed to log in to Pelle, one needs all of these:</p> <ul> <li>An active research project</li> <li>An UPPMAX account</li> <li>An UPPMAX password</li> <li>(for the Pelle remote desktop website) An UPPMAX 2FA</li> </ul> <p>These prerequisites are discussed in detail below.</p>"},{"location":"getting_started/pelle_usage_prerequisites/#an-active-research-project","title":"An active research project","text":"<p>One prerequisite for using Pelle is that you need to be a member of an active SNIC or SIMPLER research project (these can have many names such as <code>uppmax[number]</code>, <code>snic[number]</code> or<code>naiss[number]</code>), where <code>[number]</code> represent a number, for example <code>uppmax2021-2-1</code>, <code>snic2022-6-230</code> or <code>naiss2023-6-382</code>).</p> Forgot your Pelle projects? <p>How to see you research projects is described at research projects.</p> <p>Spoiler: go to https://supr.naiss.se</p> <p>SUPR (the 'Swedish User and Project Repository') is the website that allows one to request access to Pelle and to get an overview of the requested resources.</p> How does the SUPR website look like? <p></p> <p>First SUPR page</p> <p></p> <p>SUPR 2FA login. Use the SUPR 2FA (i.e. not UPPMAX)</p> <p>After logging in, the SUPR website will show all projects you are a member of, under the 'Projects' tab.</p> How does the 'Projects' tab of the SUPR website look like? <p></p> <p>Example overview of SUPR projects</p> <p>To see if a project has access to Pelle, click on the project and scroll to the 'Resources' section. In the 'Compute' subsection, there is a table. Under 'Resource' it should state 'Pelle @ UPPMAX'.</p> How does the 'Resources' page of an example project look like? <p></p> <p>The 'Resources' page of an example project. This project has two compute resources and two storage resources.</p> <p>Note that the 'Accounts' tab can be useful to verify your username.</p> How does the 'Accounts' tab help me find my username? <p></p> <p>An example of a SUPR 'Accounts' tab. The example user has username <code>sven-sens2023598</code>, which means his/her UPPMAX username is <code>sven</code></p> <p>You can become a member of an active SNIC SENS by:</p> <ul> <li>request membership to an existing project in SUPR</li> <li>Apply for an UPPMAX project</li> </ul>"},{"location":"getting_started/pelle_usage_prerequisites/#an-uppmax-user-account","title":"An UPPMAX user account","text":"<p>Another prerequisite for using Pelle is that you must have a personal UPPMAX user account.</p>"},{"location":"getting_started/pelle_usage_prerequisites/#an-uppmax-password","title":"An UPPMAX password","text":"<p>Another prerequisite for using Pelle is that you need to know your UPPMAX password. See how to reset and set your UPPMAX password to do so.</p>"},{"location":"getting_started/pelle_usage_prerequisites/#an-uppmax-2fa","title":"An UPPMAX 2FA","text":"<p>Another prerequisite for using Pelle, but only for the Pelle remote desktop website) is to have an UPPMAX 2FA. See how to get an UPPMAX 2FA</p>"},{"location":"getting_started/project/","title":"UPPMAX project","text":"","tags":["project","UPPMAX"]},{"location":"getting_started/project/#uppmax-project","title":"UPPMAX project","text":"<p>To use UPPMAX resources, one needs:</p> <ul> <li>an active research project</li> <li>an UPPMAX user account</li> </ul> <p>This page is about UPPMAX projects:</p> <ul> <li>View your existing UPPMAX projects</li> <li>Type of UPPMAX projects</li> <li>Apply to an UPPMAX project</li> </ul>","tags":["project","UPPMAX"]},{"location":"getting_started/project/#view-your-uppmax-projects","title":"View your UPPMAX projects","text":"<p>SUPR (the 'Swedish User and Project Repository') is the website that allows one to request access to Swedish computational resources and to get an overview of the requested resources.</p> How does the SUPR website look like? <p></p> <p>First SUPR page</p> <p></p> <p>SUPR 2FA login. Use the SUPR 2FA (i.e. not UPPMAX)</p> <p>After logging in, the SUPR website will show all projects you are a member of, under the 'Projects' tab.</p> How does the 'Projects' tab of the SUPR website look like? <p></p> <p>An example overview of SUPR projects</p>","tags":["project","UPPMAX"]},{"location":"getting_started/project/#how-to-convert-my-project-name-to-an-account-name-for-the-job-scheduler","title":"How to convert my project name to an account name for the job scheduler?","text":"<p>Here is a simple conversion table:</p> Project name Account name for the job scheduler? NAISS 2024/22-49 <code>naiss2024-22-49</code> sens2017625 <code>sens2017625</code> <p>Else, on an UPPMAX cluster do:</p> <pre><code>cd /proj\nls\n</code></pre> <p>and look for a project folder that resembles the name of your project. The name of that folder is the name of your account.</p> How does that look like? <p>Here is part of the output:</p> <pre><code>naiss2023-22-57           naiss2024-22-227  snic2015-10-19           snic2018-8-136  snic2020-15-16   snic2021-22-513   snic2022-22-1164  snic2022-5-333    uppstore2019112\nnaiss2023-22-570          naiss2024-22-24   snic2015-10-25           snic2018-8-139  snic2020-15-161  snic2021-22-517   snic2022-22-117   snic2022-5-334    uppstore2019113\nnaiss2023-22-574          naiss2024-22-244  snic2015-10-8            snic2018-8-14   snic2020-15-162  snic2021-22-521   snic2022-22-1172  snic2022-5-339    uppstore2019114\nnaiss2023-22-577          naiss2024-22-247  snic2015-1-142           snic2018-8-141  snic2020-15-163  snic2021-22-522   snic2022-22-1173  snic2022-5-34     uppstore2019115\nnaiss2023-22-578          naiss2024-22-253  snic2015-1-164           snic2018-8-143  snic2020-15-164  snic2021-22-525   snic2022-22-1178  snic2022-5-343    uppstore2019117\nnaiss2023-22-58           naiss2024-22-257  snic2015-1-176           snic2018-8-144  snic2020-15-165  snic2021-22-526   snic2022-22-1179  snic2022-5-364    uppstore2019118\nnaiss2023-22-580          naiss2024-22-26   snic2015-1-177           snic2018-8-145  snic2020-15-17   snic2021-22-529   snic2022-22-1180  snic2022-5-373    uppstore2019119\nnaiss2023-22-582          naiss2024-22-270  snic2015-1-201           snic2018-8-146  snic2020-15-172  snic2021-22-530   snic2022-22-1181  snic2022-5-376    uppstore2019120\nnaiss2023-22-583          naiss2024-22-275  snic2015-1-204           snic2018-8-147  snic2020-15-173  snic2021-22-535   snic2022-22-1184  snic2022-5-392    uppstore2019121\nnaiss2023-22-586          naiss2024-22-281  snic2015-1-228           snic2018-8-148  snic2020-15-175  snic2021-22-537   snic2022-22-1186  snic2022-5-403    uppstore2019123\nnaiss2023-22-590          naiss2024-22-282  snic2015-1-242           snic2018-8-149  snic2020-15-177  snic2021-22-538   snic2022-22-1194  snic2022-5-407    uppstore2021-23-134\nnaiss2023-22-598          naiss2024-22-295  snic2015-1-259           snic2018-8-15   snic2020-15-178  snic2021-22-541   snic2022-22-1195  snic2022-5-408    uu_1dl550_2021\nnaiss2023-22-600          naiss2024-22-299  snic2015-1-268           snic2018-8-150  snic2020-15-179  snic2021-22-544   snic2022-22-1197  snic2022-5-415    uucompbiochem\nnaiss2023-22-608          naiss2024-22-3    snic2015-1-281           snic2018-8-151  snic2020-15-18   snic2021-22-546   snic2022-22-1198  snic2022-5-42     var_inf_sim_alex\nnaiss2023-22-62           naiss2024-22-301  snic2015-1-315           snic2018-8-152  snic2020-15-182  snic2021-22-547   snic2022-22-12    snic2022-5-423    viher_snic2022\nnaiss2023-22-620          naiss2024-22-303  snic2015-1-33            snic2018-8-153  snic2020-15-183  snic2021-22-550   snic2022-22-1200  snic2022-5-428    viscaria_pilot\nnaiss2023-22-621          naiss2024-22-305  snic2015-1-345           snic2018-8-154  snic2020-15-185  snic2021-22-554   snic2022-22-1207  snic2022-5-432    vrognas\nnaiss2023-22-623          naiss2024-22-307  snic2015-1-364           snic2018-8-155  snic2020-15-186  snic2021-22-555   snic2022-22-1208  snic2022-5-443    wamr\nnaiss2023-22-624          naiss2024-22-308  snic2015-1-37            snic2018-8-156  snic2020-15-188  snic2021-22-557   snic2022-22-121   snic2022-5-451    wave_energy_parks\nnaiss2023-22-627          naiss2024-22-310  snic2015-1-398           snic2018-8-157  snic2020-15-189  snic2021-22-559   snic2022-22-1214  snic2022-5-454    wheatrnaseq\nnaiss2023-22-632          naiss2024-22-319  snic2015-1-399           snic2018-8-158  snic2020-15-19   snic2021-22-56    snic2022-22-1216  snic2022-5-461    wheatrnaseq2\nnaiss2023-22-633          naiss2024-22-322  snic2015-1-410           snic2018-8-159  snic2020-15-190  snic2021-22-562   snic2022-22-1224  snic2022-5-466    wiosym\nnaiss2023-22-634          naiss2024-22-324  snic2015-1-451           snic2018-8-16   snic2020-15-191  snic2021-22-563   snic2022-22-1227  snic2022-5-484    xfooli\nnaiss2023-22-64           naiss2024-22-326  snic2015-1-466           snic2018-8-161  snic2020-15-192  snic2021-22-564   snic2022-22-1228  snic2022-5-503    yeast1000storage\nnaiss2023-22-640          naiss2024-22-330  snic2015-1-475           snic2018-8-162  snic2020-15-193  snic2021-22-565   snic2022-22-123   snic2022-5-506    yeast-genomics\nnaiss2023-22-648          naiss2024-22-332  snic2015-1-52            snic2018-8-163  snic2020-15-195  snic2021-22-569   snic2022-22-1231  snic2022-5-51     yeast_hybrid_barcode\nnaiss2023-22-652          naiss2024-22-339  snic2015-16-12           snic2018-8-164  snic2020-15-196  snic2021-22-570   snic2022-22-1233  snic2022-5-52     zengkun\nnaiss2023-22-654          naiss2024-22-341  snic2015-16-27           snic2018-8-165  snic2020-15-197  snic2021-22-571   snic2022-22-1234  snic2022-5-528    zinc22\nnaiss2023-22-655          naiss2024-22-345  snic2015-16-34           snic2018-8-166  snic2020-15-198  snic2021-22-572   snic2022-22-1236  snic2022-5-530\nnaiss2023-22-658          naiss2024-22-347  snic2015-1-72            snic2018-8-167  snic2020-15-199  snic2021-22-573   snic2022-22-1237  snic2022-5-544\nnaiss2023-22-659          naiss2024-22-351  snic2015-1-92            snic2018-8-168  snic2020-15-2    snic2021-22-574   snic2022-22-1238  snic2022-5-548\nnaiss2023-22-660          naiss2024-22-354  snic2015-6-101           snic2018-8-169  snic2020-15-20   snic2021-22-579   snic2022-22-1247  snic2022-5-552\nnaiss2023-22-662          naiss2024-22-358  snic2015-6-102           snic2018-8-170  snic2020-15-201  snic2021-22-580   snic2022-22-125   snic2022-5-555\nnaiss2023-22-665          naiss2024-22-362  snic2015-6-104           snic2018-8-171  snic2020-15-202  snic2021-22-582   snic2022-22-1250  snic2022-5-560\nnaiss2023-22-667          naiss2024-22-363  snic2015-6-107           snic2018-8-173  snic2020-15-203  snic2021-22-583   snic2022-22-1253  snic2022-5-568\nnaiss2023-22-67           naiss2024-22-375  snic2015-6-109           snic2018-8-175  snic2020-15-204  snic2021-22-584   snic2022-22-1254  snic2022-5-582\n</code></pre>","tags":["project","UPPMAX"]},{"location":"getting_started/project/#type-of-uppmax-projects","title":"Type of UPPMAX projects","text":"<ul> <li>NAISS projects</li> <li>UPPMAX projects</li> <li>NGI Delivery projects</li> <li>Course projects</li> </ul>","tags":["project","UPPMAX"]},{"location":"getting_started/project/#apply-to-an-uppmax-project","title":"Apply to an UPPMAX project","text":"<p>See the UPPMAX page 'Apply to an UPPMAX project'.</p>","tags":["project","UPPMAX"]},{"location":"getting_started/project_apply/","title":"Applying for a Project","text":"","tags":["project","apply","application"]},{"location":"getting_started/project_apply/#project-application","title":"Project application","text":"<p>To use UPPMAX resources, one needs:</p> <ul> <li>an active research project</li> <li>an UPPMAX user account</li> </ul> <p>Your user account is a personal log-in to our systems. Computer resources like CPU-hours and disk storage are allocated to projects.</p> <p>The workflow is like this:</p> <ul> <li>Register in SUPR</li> <li>Accept SUPR user agreement</li> <li>Become a member of a project</li> <li>Apply for a user account at UPPMAX (or other resources)</li> </ul> <p>Warning</p> <p>Note that you can only get a user account on a resource if you belong to a project with allocations there!</p> <p>Tip</p> <p>UPPMAX rules</p>","tags":["project","apply","application"]},{"location":"getting_started/project_apply/#supr-account","title":"SUPR account","text":"<ul> <li>Register at SUPR</li> <li>Accept the user agreement in SUPR</li> </ul>","tags":["project","apply","application"]},{"location":"getting_started/project_apply/#applying-for-an-uppmax-project-pi","title":"Applying for an UPPMAX project (PI)","text":"<p>If you are a PI: apply for a project in SUPR.</p> <ul> <li>Apply for a Bianca project, i.e. a project   using sensitive data</li> <li>Apply for a Pelle project, i.e. a project   for the new Uppsala-local general-purpose cluster</li> <li>Apply for a Swedish Science Cloud project,   i.e. a project that provides an online service, e.g. a website</li> <li>Apply for a SIMPLER project,   i.e. a project for the Swedish Infrastructure for Medical Population-based Life-course and Environmental Research</li> <li>Undergraduate course project</li> </ul>","tags":["project","apply","application"]},{"location":"getting_started/project_apply/#become-a-member-of-a-project","title":"Become a member of a project","text":"<p>If you are not a PI: Apply for membership in a project you want to join in SUPR, Wait for the PI to accept your application. Alternatively, the PI can add you directly.</p> <ul> <li>Join an existing project.</li> </ul> I just got an UPPMAX project, yet I cannot login to UPPMAX? <p>It tends to be a matter of minutes to less than hours before the changes propagate from SUPR to UPPMAX.</p> <p>If after one night you cannot login, please contact support.</p>","tags":["project","apply","application"]},{"location":"getting_started/project_apply/#apply-for-an-account-at-uppmax","title":"Apply for an account at UPPMAX","text":"<p>If you don't already have an account at UPPMAX you are ready by now!</p> <ul> <li>Apply for an UPPMAX user account</li> </ul>","tags":["project","apply","application"]},{"location":"getting_started/project_apply_bianca/","title":"Project application for Bianca","text":"","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#project-application-for-bianca","title":"Project application for Bianca","text":"<p>To use an UPPMAX cluster, one needs to apply to a project.</p> <p>This page describes how to apply to a Bianca/Sens project.</p>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#pre-preparations-read-this-first","title":"Pre-preparations, read this first","text":"<ul> <li>Applying for NAISS SENS project</li> </ul>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#procedure-for-project-application","title":"Procedure for project application","text":"","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#1-go-to-httpssuprnaissse","title":"1. Go to https://supr.naiss.se/","text":"<p>Example SUPR NAISS main page</p>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#2-click-on-rounds","title":"2. Click on 'Rounds'","text":"<p>On the main page, click on 'Rounds'</p> <p></p> <p>On the main page, click on 'Rounds'</p>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#3-click-on-go-to-naiss-sens","title":"3. Click on 'Go to NAISS SENS'","text":"<p>In the 'Rounds' menu, click on 'Go to NAISS SENS'</p> <p></p> <p>In the 'Rounds' menu, click on 'Go to NAISS SENS'</p>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#4-click-on-go-to-naiss-sens-small-for-the-current-year","title":"4. Click on 'Go to NAISS SENS Small' for the current year","text":"<p>In the 'NAISS SENS Rounds' menu, click on 'Go to NAISS SENS Small' for the current year:</p> <p></p> <p>In the 'NAISS SENS Rounds' menu, click on 'Go to NAISS SENS Small' for the current year</p>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#5-click-create-new-proposal-for-naiss-sens-small-for-the-current-year","title":"5. Click 'Create New Proposal for NAISS SENS Small' for the current year","text":"<p>In the 'Open for Proposals' screen, click 'Create New Proposal for NAISS SENS Small' for the current year</p> <p></p> <p>In the 'Open for Proposals' screen, click 'Create New Proposal for NAISS SENS Small' for the current year</p>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_bianca/#6-add-a-project-title-and-click-create-new-proposal","title":"6. Add a project title and click 'Create new proposal'","text":"<p>In the 'Create New Proposal for NAISS SENS Small 2024', add a project title and click 'Create new proposal'</p> <p></p> <p>In the 'Create New Proposal for NAISS SENS Small 2024', add a project title and click 'Create new proposal'</p> <p>After this, the procedure is straightforward.</p> <p></p> <p>Resource available for a NAISS SENS Small project</p>","tags":["project","apply","application","Bianca"]},{"location":"getting_started/project_apply_pelle/","title":"Project application for Pelle","text":"","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#project-application-for-pelle","title":"Project application for Pelle","text":"<p>To use an UPPMAX cluster, one needs to apply to a project. This page describes how to apply to a Pelle project.</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#procedure","title":"Procedure","text":"Prefer a video? <p>See the YouTube video 'Apply for an UPPMAX Pelle project'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#1-go-to-httpssuprnaissse","title":"1. Go to https://supr.naiss.se/","text":"How does that look like? <p>Example SUPR NAISS main page</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#2-click-on-rounds","title":"2. Click on 'Rounds'","text":"<p>On the main page, click on 'Rounds'</p> How does that look like? <p></p> <p>On the main page, click on 'Rounds'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#3-click-on-go-to-compute-rounds","title":"3. Click on 'Go to Compute Rounds'","text":"<p>In the 'Rounds' menu, click on 'Go to Compute Rounds'</p> How does that look like? <p></p> <p>In the 'Rounds' menu, click on 'Go to Compute Rounds'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#4-click-on-go-to-centre-local-compute","title":"4. Click on 'Go to Centre Local Compute'","text":"<p>In the 'Compute Rounds' menu, click on 'Go to Centre Local Compute'</p> How does that look like? <p></p> <p>In the 'Compute Rounds' menu, click on 'Go to Centre Local Compute'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#5-click-on-go-to-uppmax-local","title":"5. Click on 'Go to UPPMAX Local'","text":"<p>In the 'Centre Local Compute Rounds' menu, click on 'Go to UPPMAX Local'</p> How does that look like? <p></p> <p>In the 'Centre Local Compute Rounds' menu, click on 'Go to UPPMAX Local'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#6-click-on-create-new-proposal","title":"6. Click on 'Create new proposal'","text":"<p>In the 'UPPMAX Local' menu, click on 'Create new proposal'</p> How does that look like? <p></p> <p>In the 'UPPMAX Local' menu, click on 'Create new proposal'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#7-fill-in-a-title-and-click-on-create-new-proposal","title":"7. Fill in a title and click on 'Create new proposal'","text":"<p>In the 'Create new proposal for UPPMAX local' menu, fill in a title and click on 'Create new proposal'</p> How does that look like? <p></p> <p>In the 'Create new proposal for UPPMAX local' menu, fill in a title and click on 'Create new proposal'</p> <p>You have just created an UPPMAX local compute project!</p> How does that look like? <p></p> <p>An UPPMAX local compute project</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#8-scroll-down-and-add-pelle","title":"8. Scroll down and add Pelle","text":"<p>In your UPPMAX local compute project, scroll down to 'Resources' and add Pelle.</p> How does that look like? <p></p> <p>In your UPPMAX local compute project, scroll down to 'Resources' and add Pelle</p> <p>Click on 'Add resource to proposal' to add Pelle as a resource.</p> How does that look like? <p></p> <p>In your UPPMAX local compute project, click 'Add resource to proposal'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#9-click-on-create-new-proposal","title":"9. Click on 'Create new proposal'","text":"<p>In the 'Add resource Pelle' menu, set the number of core hours and click 'Add resource'.</p> How does that look like? <p></p> <p>In the 'Add resource Pelle' menu, set the number of core hours and click 'Add resource'</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_pelle/#10-done","title":"10. Done","text":"<p>Now, Pelle is added to your proposal. Well done!</p> How does that look like? <p></p> <p>In your UPPMAX local compute project, Pelle is added</p>","tags":["project","apply","application","Pelle"]},{"location":"getting_started/project_apply_scc/","title":"Project application for SCC","text":"","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#project-application-for-scc","title":"Project application for SCC","text":"<p>To use an UPPMAX cluster, one needs to apply to a project. This page describes how to apply to a SCC project.</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#procedure","title":"Procedure","text":"Prefer a video? <p>...</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#1-go-to-httpssuprnaissse","title":"1. Go to https://supr.naiss.se/","text":"How does that look like? <p>Example SUPR NAISS main page</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#2-click-on-rounds","title":"2. Click on 'Rounds'","text":"<p>On the main page, click on 'Rounds'</p> How does that look like? <p></p> <p>On the main page, click on 'Rounds'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#3-click-on-go-to-swedish-science-cloud","title":"3. Click on 'Go to Swedish Science Cloud'","text":"<p>In the 'Rounds' menu, click on 'Go to Swedish Science Cloud'</p> How does that look like? <p></p> <p>In the 'Rounds' menu, click on 'Go to Swedish Science Cloud'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#4-click-on-go-to-swedish-science-cloud","title":"4. Click on 'Go to Swedish Science Cloud'","text":"<p>In the 'Cloud resource' menu, click on 'Go to NAISS Small Compute 2025'.</p> How does that look like? <p></p> <p>In the 'Cloud resource' menu, click on 'Go to NAISS Small Compute 2025'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#5-click-on-create-new-proposal-for-naiss-small-compute","title":"5. Click on 'Create new proposal for NAISS Small Compute'","text":"<p>In the 'Small Compute Rounds' menu, click on 'Create new proposal for NAISS Small Compute'.</p> How does that look like? <p></p> <p>In the 'Small Compute Rounds' menu, click on 'Create new proposal for NAISS Small Compute'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#6-add-a-project-title-and-click-on-create-new-proposal","title":"6. Add a project title and click on 'Create new proposal'","text":"<p>In the 'Create new proposal' menu, add a project title and click on 'Create new proposal'</p> How does that look like? <p></p> <p>In the 'Create new proposal' menu, add a project title and click on 'Create new proposal'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#7-scroll-down-to-resources","title":"7. Scroll down to 'Resources'","text":"<p>In this NAISS project proposal page, scroll down to 'Resources'.</p> How does that look like? <p></p> <p>In this NAISS project proposal page, scroll down to 'Resources'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#8-select-cloud-scc","title":"8. Select 'Cloud @ SCC'","text":"<p>In the 'Resources' dropbox, Select 'Cloud @ SCC'.</p> How does that look like? <p></p> <p>In the 'Resources' dropbox, Select 'Cloud @ SCC'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#9-set-the-amount-of-coins-and-click-add-resource","title":"9. Set the amount of coins and click 'Add Resource'","text":"<p>At the 'Add resource Cloud' page, set the amount of coins and click 'Add Resource'.</p> How does that look like? <p></p> <p>At the 'Add resource Cloud' page, set the amount of coins and click 'Add Resource'.</p> <p>The resource is now added to your project.</p> How does that look like? <p></p> <p>'Resource Cloud added to proposal'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_scc/#10-click-submit-proposal","title":"10. Click 'Submit proposal'","text":"<p>In this NAISS project proposal page, after all other details are filled in, scroll down and click on 'Submit proposal'</p> How does that look like? <p></p> <p>In this NAISS project proposal page, scroll down and click on 'Submit proposal'</p>","tags":["project","apply","application","SCC","Swedish Science Cloud"]},{"location":"getting_started/project_apply_simpler/","title":"SIMPLER project application","text":"","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#simpler-project-application","title":"SIMPLER project application","text":"<p>SIMPLER is an abbreviation</p> <p>SIMPLER is an abbreviation of 'Swedish Infrastructure for Medical Population-based Life-course and Environmental Research'. It does not meant to indicate that this is easier.</p> <p>To use an UPPMAX cluster, one needs to apply to a project. This page describes how to apply for a SIMPLER project.</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#read-this-first","title":"Read this first","text":"<p>The SIMPLER infrastructure offers resources on the SNIC NAISS platform, Bianca, for users who have been granted access to SIMPLER data. This page contains instructions to approved SIMPLER users for gaining access to their data on the analysis platform.</p> <p>Some background: This page is only for SIMPLER users (for more information see SIMPLER4Health). To gain access to your SIMPLER data, simply follow the instructions on this page.</p> <p>These instructions will help you:</p> <ul> <li>Create an account on SUPR, the user and project management portal for the Swedish National Infrastructure for Computing.</li> <li>Create a project for your SIMPLER data.</li> <li>Create a user account for the Bianca system, where your data will be accessible.</li> <li>Log in to your project on Bianca.</li> </ul> <p>Old page: Applying for a SIMPLER project</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#procedure","title":"Procedure","text":"Prefer a video? <p>View the YouTube video that shows this procedure</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#1-go-to-httpssuprnaissse","title":"1. Go to https://supr.naiss.se/","text":"<p>Example SUPR NAISS main page</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#2-click-on-rounds","title":"2. Click on 'Rounds'","text":"<p>On the main page, click on 'Rounds'</p> <p></p> <p>On the main page, click on 'Rounds'</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#3-click-on-go-to-naiss-sens","title":"3. Click on 'Go to NAISS SENS'","text":"<p>In the 'Rounds' menu, click on 'Go to NAISS SENS'</p> <p></p> <p>In the 'Rounds' menu, click on 'Go to NAISS SENS'</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#4-click-on-go-to-simpler-for-the-current-year","title":"4. Click on 'Go to SIMPLER' for the current year","text":"<p>In the 'Rounds' menu, click on 'Go to SIMPLER' for the current year.</p> <p></p> <p>In the 'Rounds' menu, click on 'Go to SIMPLER' for the current year.</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#5-click-create-new-proposal-for-simpler-for-the-current-year","title":"5. Click 'Create New Proposal for SIMPLER' for the current year","text":"<p>In the 'Open for Proposals' screen, click 'Create New Proposal for SIMPLER' for the current year</p> <p></p> <p>In the 'Open for Proposals' screen, click 'Create New Proposal for SIMPLER' for the current year</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#6-add-a-project-title-and-click-create-new-proposal","title":"6. Add a project title and click 'Create new proposal'","text":"<p>In the 'Create New Proposal for SIMPLER 2024', add a project title and click 'Create new proposal'</p> <p></p> <p>In the 'Create New Proposal for SIMPLER 2025', add a project title and click 'Create new proposal'</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#7-create-an-uppmax-user-account","title":"7. Create an UPPMAX user account","text":"<p>Once you have a SUPR-account and an approved project, then you can apply for an Uppmax account. This is described in Applying for a user account.</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#8-use-bianca","title":"8. Use Bianca","text":"<p>In order to log in to Bianca, you will need to activate two factor authentication according to the instructions.</p> <p>Before you try logging in to Bianca, we recommend that you change temporary password you received by email. This is done on the server Rackham. You reach Rackham e.g. via the software Mobaxterm which you must download and install. Instructions are here.</p> <p>The Bianca User Guide has more information about how the system is used.</p> <p>The data you have requested for SIMPLER will be placed by SIMPLER staff in the special directory \"dataset-simpler\" which is located in, and unique to, your project.</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/project_apply_simpler/#where-to-turn-for-support","title":"Where to turn for support","text":"<p>Problems with the Bianca system should be sent to UPPMAX via the SUPR support form, and issues with the data itself should be sent to <code>simpler@surgsci.uu.se</code>.</p>","tags":["project","apply","application","SIMPLER"]},{"location":"getting_started/rackham_usage_prerequisites/","title":"Prerequisites for using Rackham","text":""},{"location":"getting_started/rackham_usage_prerequisites/#prerequisites-for-using-rackham","title":"Prerequisites for using Rackham","text":"<p>To be allowed to log in to Rackham, one needs all of these:</p> <ul> <li>An active research project</li> <li>An UPPMAX account</li> <li>An UPPMAX password</li> <li>(for the Rackham remote desktop website) An UPPMAX 2FA</li> </ul> <p>These prerequisites are discussed in detail below.</p>"},{"location":"getting_started/rackham_usage_prerequisites/#an-active-research-project","title":"An active research project","text":"<p>One prerequisite for using Rackham is that you need to be a member of an active SNIC or SIMPLER research project (these can have many names such as <code>uppmax[number]</code>, <code>snic[number]</code> or<code>naiss[number]</code>), where <code>[number]</code> represent a number, for example <code>uppmax2021-2-1</code>, <code>snic2022-6-230</code> or <code>naiss2023-6-382</code>).</p> Forgot your Rackham projects? <p>How to see you research projects is described at research projects.</p> <p>Spoiler: go to https://supr.naiss.se</p> <p>SUPR (the 'Swedish User and Project Repository') is the website that allows one to request access to Rackham and to get an overview of the requested resources.</p> How does the SUPR website look like? <p></p> <p>First SUPR page</p> <p></p> <p>SUPR 2FA login. Use the SUPR 2FA (i.e. not UPPMAX)</p> <p>After logging in, the SUPR website will show all projects you are a member of, under the 'Projects' tab.</p> How does the 'Projects' tab of the SUPR website look like? <p></p> <p>Example overview of SUPR projects</p> <p>To see if a project has access to Rackham, click on the project and scroll to the 'Resources' section. In the 'Compute' subsection, there is a table. Under 'Resource' it should state 'Rackham @ UPPMAX'.</p> How does the 'Resources' page of an example project look like? <p></p> <p>The 'Resources' page of an example project. This project has two compute resources and two storage resources.</p> <p>Note that the 'Accounts' tab can be useful to verify your username.</p> How does the 'Accounts' tab help me find my username? <p></p> <p>An example of a SUPR 'Accounts' tab. The example user has username <code>sven-sens2023598</code>, which means his/her UPPMAX username is <code>sven</code></p> <p>You can become a member of an active SNIC SENS by:</p> <ul> <li>request membership to an existing project in SUPR</li> <li>Apply for an UPPMAX project</li> </ul>"},{"location":"getting_started/rackham_usage_prerequisites/#an-uppmax-user-account","title":"An UPPMAX user account","text":"<p>Another prerequisite for using Rackham is that you must have a personal UPPMAX user account.</p>"},{"location":"getting_started/rackham_usage_prerequisites/#an-uppmax-password","title":"An UPPMAX password","text":"<p>Another prerequisite for using Rackham is that you need to know your UPPMAX password. See how to reset and set your UPPMAX password to do so.</p>"},{"location":"getting_started/rackham_usage_prerequisites/#an-uppmax-2fa","title":"An UPPMAX 2FA","text":"<p>Another prerequisite for using Rackham, but only for the Rackham remote desktop website) is to have an UPPMAX 2FA. See how to get an UPPMAX 2FA</p>"},{"location":"getting_started/reset_uppmax_password/","title":"Reset your UPPMAX password","text":"","tags":["UPPMAX","password","reset","set","passwd"]},{"location":"getting_started/reset_uppmax_password/#reset-your-uppmax-password","title":"Reset your UPPMAX password","text":"Prefer a video? <p>See the YouTube video 'How to reset your UPPMAX password'</p>","tags":["UPPMAX","password","reset","set","passwd"]},{"location":"getting_started/reset_uppmax_password/#procedure","title":"Procedure","text":"","tags":["UPPMAX","password","reset","set","passwd"]},{"location":"getting_started/reset_uppmax_password/#1-go-to-httpssuprintegrationuppmaxuusegetpasswd","title":"1. Go to https://suprintegration.uppmax.uu.se/getpasswd","text":"<p>Go to https://suprintegration.uppmax.uu.se/getpasswd. After authenticating yourself, you're password is reset immediately.</p> <p>You will be sent an email in around 5 minutes.</p>","tags":["UPPMAX","password","reset","set","passwd"]},{"location":"getting_started/reset_uppmax_password/#2-open-email","title":"2. Open email","text":"<p>Open the email and click on the link that it suggests you to click.</p> How does that email look like? <p>Your email will look similar to this:</p> <pre><code>Greetings,\n\na new password has been generated for your account at UPPMAX.\n\nYou can fetch it by visiting the link below.\nNote though that the link is only valid for 7 days and one (1) visit.\n\nYou can retrieve the password at the following link:\n\nhttps://content.uppmax.uu.se/get-password2.php?sum=hvs.CAESIGOczS0[more_letters]\n\nIf the password has expired, you can request a new password from our homepage\nhttps://www.uppmax.uu.se and the link \"Lost your password?\".\n\nNote that if you requested a new password because your account was locked,\nit may take some additional time (up to an hour) before that change is\nreflected everywhere.\n\nIf you are unsure about what your user name is, this information is available\nin SUPR (https://supr.snic.se/) under Accounts.\n\nFor general information about how to login, change your password and\nso on, please see our getting started guide at\n\nhttp://www.uppmax.uu.se/support/user-guides/guide--first-login-to-uppmax/\n\nregards, UPPMAX Support\n\n\nVARNING: Klicka inte p\u00e5 l\u00e4nkar och \u00f6ppna inte bilagor om du inte k\u00e4nner igen avs\u00e4ndaren och vet att inneh\u00e5llet \u00e4r s\u00e4kert.\nCAUTION: Do not click on links or open attachments unless you recognise the sender and know the content is safe.\n</code></pre> <p>In this example, <code>https://content.uppmax.uu.se/get-password2.php?sum=hvs.CAESIGOczS0[more_letters]</code> is the link you should click</p> <p>This will take you to a page with your new password.</p>","tags":["UPPMAX","password","reset","set","passwd"]},{"location":"getting_started/reset_uppmax_password/#3-log-in-with-your-new-password","title":"3. Log in with your new password","text":"<p>At the page with your new password, you use that password to log in.</p>","tags":["UPPMAX","password","reset","set","passwd"]},{"location":"getting_started/reset_uppmax_password/#4-optional-change-your-password","title":"4. (optional) Change your password","text":"<p>If you want to change your password, see How to change your UPPMAX password.</p>","tags":["UPPMAX","password","reset","set","passwd"]},{"location":"getting_started/setup_vpn/","title":"Setup a VPN","text":"","tags":["setup","set up","VPN","Windows","Mac"]},{"location":"getting_started/setup_vpn/#setup-a-vpn","title":"Setup a VPN","text":"<p>Some UPPMAX clusters require you to have an IP address inside of SUNET. A virtual private network (VPN) allows one to do so: it will use the Swedish university networks to connect to the UPPMAX clusters.</p> <p>To be able to use a VPN to get inside of SUNET:</p> <ul> <li>For Uppsala University:<ul> <li>Mac and Windows users: go to this page</li> <li>Linux users: go to this page</li> </ul> </li> <li>For Lund University: go to this page</li> <li>For other Swedish universities, search their websites to get the required VPN credentials.</li> </ul> Where do I go if I am no longer affiliated with a Swedish university? <p> </p> <p>In this case, one cannot use a VPN. Instead, log in to.</p> <p>This is yet unknown. Please contact support.</p> Want a video to see how the UU VPN is used? <ul> <li>Use the UU VPN with 2FA</li> <li>Use the UU VPN (yet without 2FA) to access the Bianca remote desktop website</li> </ul>","tags":["setup","set up","VPN","Windows","Mac"]},{"location":"getting_started/setup_vpn_uu_linux/","title":"Setup a VPN from Uppsala University for Linux","text":"","tags":["setup","set up","VPN","Linux","UU","Uppsala"]},{"location":"getting_started/setup_vpn_uu_linux/#setup-a-vpn-from-uppsala-university-for-linux","title":"Setup a VPN from Uppsala University for Linux","text":"<p>How to set up a VPN differs between universities and differs between operating systems. This page describes how to set up a VPN from Uppsala University for Linux.</p>","tags":["setup","set up","VPN","Linux","UU","Uppsala"]},{"location":"getting_started/setup_vpn_uu_linux/#procedure","title":"Procedure","text":"","tags":["setup","set up","VPN","Linux","UU","Uppsala"]},{"location":"getting_started/setup_vpn_uu_linux/#1-install-the-needed-packages","title":"1. Install the needed packages","text":"<p>In a terminal, do:</p> <pre><code>sudo apt-get install openconnect network-manager-openconnect network-manager-openconnect-gnome\n</code></pre> How does that look like? <p>You output look similar to this:</p> <pre><code>sven@svens_computer:~$ sudo apt-get install openconnect network-manager-openconnect network-manager-openconnect-gnome\n[sudo] password for sven:\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages were automatically installed and are no longer required:\n libavcodec58 libavdevice58 libavfilter7 libavformat58 libavutil56 libcodec2-1.0\n libdav1d5 libdcmtk16 libilmbase25 libmfx1 libopenexr25 libpostproc55 libsrt1.4-gnutls\n libswresample3 libswscale5 libtiff5 libvpx7 libx264-163 mailcap\nUse 'sudo apt autoremove' to remove them.\nThe following additional packages will be installed:\n libopenconnect5 libpskc0t64 libstoken1t64 libtomcrypt1 libxmlsec1t64-openssl\n python3-asn1crypto python3-mechanize vpnc-scripts\nSuggested packages:\n dnsmasq\nThe following NEW packages will be installed:\n libopenconnect5 libpskc0t64 libstoken1t64 libtomcrypt1 libxmlsec1t64-openssl\n network-manager-openconnect network-manager-openconnect-gnome openconnect\n python3-asn1crypto python3-mechanize vpnc-scripts\n0 upgraded, 11 newly installed, 0 to remove and 22 not upgraded.\nNeed to get 2\u202f187 kB of archives.\nAfter this operation, 10,1 MB of additional disk space will be used.\nDo you want to continue? [Y/n]\nGet:1 http://archive.ubuntu.com/ubuntu noble/main amd64 libxmlsec1t64-openssl amd64 1.2.39-5build2 [84,1 kB]\nGet:2 http://archive.ubuntu.com/ubuntu noble-updates/main amd64 libpskc0t64 amd64 2.6.11-2.1ubuntu0.1 [27,9 kB]\nGet:3 http://archive.ubuntu.com/ubuntu noble/universe amd64 libtomcrypt1 amd64 1.18.2+dfsg-7build1 [384 kB]\nGet:4 http://archive.ubuntu.com/ubuntu noble/universe amd64 libstoken1t64 amd64 0.92-1.1build2 [27,6 kB]\nGet:5 http://archive.ubuntu.com/ubuntu noble/universe amd64 libopenconnect5 amd64 9.12-1build5 [217 kB]\nGet:6 http://archive.ubuntu.com/ubuntu noble/universe amd64 vpnc-scripts all 0.1~git20220510-1 [16,2 kB]\nGet:7 http://archive.ubuntu.com/ubuntu noble/universe amd64 openconnect amd64 9.12-1build5 [667 kB]\nGet:8 http://archive.ubuntu.com/ubuntu noble/universe amd64 network-manager-openconnect amd64 1.2.10-3build2 [556 kB]\nGet:9 http://archive.ubuntu.com/ubuntu noble/universe amd64 network-manager-openconnect-gnome amd64 1.2.10-3build2 [33,6 kB]\nGet:10 http://archive.ubuntu.com/ubuntu noble/universe amd64 python3-asn1crypto all 1.5.1-3 [79,7 kB]\nGet:11 http://archive.ubuntu.com/ubuntu noble/universe amd64 python3-mechanize all 1:0.4.9+ds-2 [93,9 kB]\nFetched 2\u202f187 kB in 1s (2\u202f434 kB/s)\nSelecting previously unselected package libxmlsec1t64-openssl:amd64.\n(Reading database ... 481882 files and directories currently installed.)\nPreparing to unpack .../00-libxmlsec1t64-openssl_1.2.39-5build2_amd64.deb ...\nUnpacking libxmlsec1t64-openssl:amd64 (1.2.39-5build2) ...\nSelecting previously unselected package libpskc0t64:amd64.\nPreparing to unpack .../01-libpskc0t64_2.6.11-2.1ubuntu0.1_amd64.deb ...\nUnpacking libpskc0t64:amd64 (2.6.11-2.1ubuntu0.1) ...\nSelecting previously unselected package libtomcrypt1:amd64.\nPreparing to unpack .../02-libtomcrypt1_1.18.2+dfsg-7build1_amd64.deb ...\nUnpacking libtomcrypt1:amd64 (1.18.2+dfsg-7build1) ...\nSelecting previously unselected package libstoken1t64:amd64.\nPreparing to unpack .../03-libstoken1t64_0.92-1.1build2_amd64.deb ...\nUnpacking libstoken1t64:amd64 (0.92-1.1build2) ...\nSelecting previously unselected package libopenconnect5:amd64.\nPreparing to unpack .../04-libopenconnect5_9.12-1build5_amd64.deb ...\nUnpacking libopenconnect5:amd64 (9.12-1build5) ...\nSelecting previously unselected package vpnc-scripts.\nPreparing to unpack .../05-vpnc-scripts_0.1~git20220510-1_all.deb ...\nUnpacking vpnc-scripts (0.1~git20220510-1) ...\nSelecting previously unselected package openconnect.\nPreparing to unpack .../06-openconnect_9.12-1build5_amd64.deb ...\nUnpacking openconnect (9.12-1build5) ...\nSelecting previously unselected package network-manager-openconnect.\nPreparing to unpack .../07-network-manager-openconnect_1.2.10-3build2_amd64.deb ...\nUnpacking network-manager-openconnect (1.2.10-3build2) ...\nSelecting previously unselected package network-manager-openconnect-gnome.\nPreparing to unpack .../08-network-manager-openconnect-gnome_1.2.10-3build2_amd64.deb ...\nUnpacking network-manager-openconnect-gnome (1.2.10-3build2) ...\nSelecting previously unselected package python3-asn1crypto.\nPreparing to unpack .../09-python3-asn1crypto_1.5.1-3_all.deb ...\nUnpacking python3-asn1crypto (1.5.1-3) ...\nSelecting previously unselected package python3-mechanize.\nPreparing to unpack .../10-python3-mechanize_1%3a0.4.9+ds-2_all.deb ...\nUnpacking python3-mechanize (1:0.4.9+ds-2) ...\nSetting up libtomcrypt1:amd64 (1.18.2+dfsg-7build1) ...\nSetting up python3-mechanize (1:0.4.9+ds-2) ...\nSetting up libstoken1t64:amd64 (0.92-1.1build2) ...\nSetting up libxmlsec1t64-openssl:amd64 (1.2.39-5build2) ...\nSetting up vpnc-scripts (0.1~git20220510-1) ...\nSetting up python3-asn1crypto (1.5.1-3) ...\nSetting up libpskc0t64:amd64 (2.6.11-2.1ubuntu0.1) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nProcessing triggers for man-db (2.12.0-4build2) ...\nProcessing triggers for dbus (1.14.10-4ubuntu4.1) ...\nProcessing triggers for sgml-base (1.31) ...\nSetting up libopenconnect5:amd64 (9.12-1build5) ...\nSetting up openconnect (9.12-1build5) ...\nSetting up network-manager-openconnect (1.2.10-3build2) ...\nSetting up network-manager-openconnect-gnome (1.2.10-3build2) ...\nProcessing triggers for libc-bin (2.39-0ubuntu8.3) ...\nsven@svens_computer:~$\n</code></pre>","tags":["setup","set up","VPN","Linux","UU","Uppsala"]},{"location":"getting_started/setup_vpn_uu_linux/#2-add-the-vpn","title":"2. Add the VPN","text":"<p>Go to the 'Settings'</p> How do I get there? <p>Press the meta/windows/command key (the one between <code>CTRL</code> and <code>ALT</code> that is not called <code>FN</code>).</p> <p>When typing <code>settings</code>, it will pop up. Click it. Or move there with the arrow keys and press enter.</p> <p></p> <p>Go to 'Network'.</p> How does that look like? <p></p> <p>In the VPN section, click on the plus to add a VPN.</p> How does that look like? <p></p> <p>In the 'Add VPN' dialog, click on 'Multi-protocol VPN client (openconnect)'.</p> How does that look like? <p></p> <p>In the 'Add VPN' dialog, click on the 'Identity' tab.</p> How does that look like? <p></p> <p>In the 'Identity' tab, set up the following values:</p> Field Value Name Any name, e.g. <code>My Uppasala University VPN</code> Gateway <code>vpn.uu.se</code> User Agent <code>AnyConnect Linux_64 4.7.00136</code> How does that look like? <p></p> <p>Now you have added the VPN.</p>","tags":["setup","set up","VPN","Linux","UU","Uppsala"]},{"location":"getting_started/setup_vpn_uu_linux/#3-activate-the-vpn","title":"3. Activate the VPN","text":"<p>In the Ubuntu network settings, we can see our inactive VPN.</p> <p>Click on the slider right of our UU VPN.</p> How does that look like? <p></p> <p>In the popup, click login.</p> How does that look like? <p></p> <p>You are now asked for you UU Login:</p> How does that look like? <p></p> <p>Fill in your UU login name and password.</p> How does that look like? <p></p> <p>Fill in your 2FA. The 2FA should be called <code>[akka_id]</code>, e.g. <code>svesv314</code> (and not <code>sven.svensson@icm.uu.se</code>).</p> Forgot how to set up 2FA for your UU user account? <p>See the UU page on how to setup 2FA for your user account.</p> <p>Pick the options for 'user account' (i.e. not for SharePoint).</p> How does that look like? <p></p> <p>You are now connected!</p> How can I see that? <p>You are connected to the UU VPN according to network settings:</p> <p></p> <p>You are connected to the UU VPN according to the main menu: </p> For staff: UIT documentation <p>This page is based on this procedure suggested by UIT:</p> <p></p> <p></p> <p></p>","tags":["setup","set up","VPN","Linux","UU","Uppsala"]},{"location":"getting_started/snowy_usage_prerequisites/","title":"Prerequisites for using Snowy","text":""},{"location":"getting_started/snowy_usage_prerequisites/#prerequisites-for-using-snowy","title":"Prerequisites for using Snowy","text":"<p>To be allowed to log in to Snowy, one needs all of these:</p> <ul> <li>An active research project</li> <li>An UPPMAX account</li> <li>An UPPMAX password</li> </ul> <p>These prerequisites are discussed in detail below.</p>"},{"location":"getting_started/snowy_usage_prerequisites/#an-active-research-project","title":"An active research project","text":"<p>One prerequisite for using Snowy is that you need to be a member of an active SNIC or SIMPLER research project (these can have many names such as <code>uppmax[number]</code>, <code>snic[number]</code> or<code>naiss[number]</code>), where <code>[number]</code> represent a number, for example <code>uppmax2021-2-1</code>, <code>snic2022-6-230</code> or <code>naiss2023-6-382</code>).</p> Forgot your Snowy projects? <p>How to see you research projects is described at research projects.</p> <p>Spoiler: go to https://supr.naiss.se</p> <p>SUPR (the 'Swedish User and Project Repository') is the website that allows one to request access to Snowy and to get an overview of the requested resources.</p> How does the SUPR website look like? <p></p> <p>First SUPR page</p> <p></p> <p>SUPR 2FA login. Use the SUPR 2FA (i.e. not UPPMAX)</p> <p>After logging in, the SUPR website will show all projects you are a member of, under the 'Projects' tab.</p> How does the 'Projects' tab of the SUPR website look like? <p></p> <p>Example overview of SUPR projects</p> <p>To see if a project has access to Snowy, click on the project and scroll to the 'Resources' section. In the 'Compute' subsection, there is a table. Under 'Resource' it should state 'Snowy @ UPPMAX'.</p> How does the 'Resources' page of an example project look like? <p></p> <p>The 'Resources' page of an example project. This project has two compute resources and two storage resources. A Snowy project would show the word 'Snowy' somewhere, so this is not a Snowy project.</p> <p>Note that the 'Accounts' tab can be useful to verify your username.</p> How does the 'Accounts' tab help me find my username? <p></p> <p>An example of a SUPR 'Accounts' tab. The example user has username <code>sven-sens2023598</code>, which means his/her UPPMAX username is <code>sven</code></p> <p>You can become a member of an active SNIC SENS by:</p> <ul> <li>request membership to an existing project in SUPR</li> <li>Apply for an UPPMAX project</li> </ul>"},{"location":"getting_started/snowy_usage_prerequisites/#an-uppmax-user-account","title":"An UPPMAX user account","text":"<p>Another prerequisite for using Snowy is that you must have a personal UPPMAX user account.</p>"},{"location":"getting_started/snowy_usage_prerequisites/#an-uppmax-password","title":"An UPPMAX password","text":"<p>Another prerequisite for using Snowy is that you need to know your UPPMAX password. If you change it, it may take up to an hour before changes are reflected in Snowy.</p> <p>For advice on handling sensitive personal data correctly on Snowy, see our FAQ page.</p>"},{"location":"getting_started/storage/","title":"Data storage","text":""},{"location":"getting_started/storage/#data-storage","title":"Data storage","text":""},{"location":"getting_started/storage/#links","title":"Links","text":"<ul> <li>UU guide on data storage <code>https://www.uu.se/en/staff/gateway/research/research-handbook/research-data/store-data-and-cooperate</code> (broken link)</li> </ul>"},{"location":"getting_started/supr/","title":"SUPR","text":"","tags":["SUPR"]},{"location":"getting_started/supr/#supr","title":"SUPR","text":"<p>SUPR ('Swedish User and Project Repository') is a website at https://supr.naiss.se/ that manages HPC accounts and projects.</p>","tags":["SUPR"]},{"location":"getting_started/supr/#apply-for-an-uppmax-project","title":"Apply for an UPPMAX project","text":"<p>See the UPPMAX page on how to apply for an UPPMAX project.</p>","tags":["SUPR"]},{"location":"getting_started/supr/#setting-up-an-uppmax-2fa-token","title":"Setting up an UPPMAX 2FA token","text":"<p>Go to https://suprintegration.uppmax.uu.se/bootstrapotp/ to initiate the token creation. This should take you to a landing page with some initial information and let you know that you will be sent to SUPR to log in.</p> How does that look like? <p></p> <p>Once you click \"Continue\", you'll be sent to SUPR where you should log in. Once you've done so SUPR will let you know that you will be sent back and the identity you are logged in with.</p> For staff only <p>SUPR API documentation, requires the same certificate as RT</p>","tags":["SUPR"]},{"location":"getting_started/supr_register/","title":"Register at SUPR","text":""},{"location":"getting_started/supr_register/#register-at-supr","title":"Register at SUPR","text":"<ul> <li>Go to https://supr.naiss.se/person/register/</li> <li>If you already have an account you must use that account, otherwise you can register with or without SWAMID by clicking on the appropriate button.</li> </ul> <ul> <li>If you clicked on register via SWAMID you will have to choose the university that you belong to. Here as an example I choose Uppsala University:</li> </ul> <ul> <li>Click on the University and then choose if you want to login via this SWAMID once</li> <li>or if your browser should remember your choice and use every time you visit SUPR</li> <li>Here again as an example is Uppsala University:</li> </ul> <ul> <li>You can now check the information your university has sent to SUPR and accept it to create a SUPR account.</li> <li>If you instead click on Register without SWAMID you will have to fill in this:</li> </ul> <ul> <li>Fill in the form and click the button to create your SUPR account.</li> <li>You have to confirm your account by answering an email sent to the address you registered.</li> </ul>"},{"location":"getting_started/supr_register/#accept-the-user-agreement","title":"Accept the User Agreement","text":"<ul> <li>After logging into your SUPR account you must accept the user agreement. Click on Handle User Agreement</li> </ul> <ul> <li>Depending on how you take care of the User Agreement, it may be approved automatically or it may require manual checking (for example if you choose to use the paper form). You will get an email from SUPR when it has been approved.</li> </ul>"},{"location":"getting_started/user_account/","title":"UPPMAX User Accounts","text":""},{"location":"getting_started/user_account/#uppmax-user-accounts","title":"UPPMAX User Accounts","text":"<p>An UPPMAX user account is needed to use UPPMAX resources (together with an active UPPMAX research project) and allows you to log in into the UPPMAX clusters.</p>"},{"location":"getting_started/user_account/#apply-to-an-uppmax-user-account","title":"Apply to an UPPMAX user account","text":"<p>To apply for an UPPMAX user account, you (the user) and the PI of the project (the researcher in charge of the research project) must complete the following steps:</p> <ul> <li>Register at SUPR.</li> <li>Make sure that you don't already have an account at SUPR.   You must not have more than one account in SUPR.</li> <li>Accept the user agreement in SUPR.</li> <li>Become a member of a project:<ul> <li>If you are a PI: apply for a project in SUPR.</li> <li>If you are not a PI: Apply for membership in a project you want to join in SUPR, Wait for the PI to accept your application. Alternatively, the PI can add you directly. Join an existing project</li> </ul> </li> <li>You must apply for an account at UPPMAX in SUPR.</li> </ul>"},{"location":"getting_started/user_account/#apply-for-an-account-at-uppmax","title":"Apply for an account at UPPMAX","text":"<ul> <li>When the PI has accepted your membership application. You will receive an email.</li> <li> <p>Log in to SUPR and click on Accounts in the list to the left.</p> </li> <li> <p>You will see the login accounts you already have at other NAISS centres if you have any. Under the \"Possible Resource Account Requests\" headings you find the UPPMAX resources you can apply for login account on. Just use the \"Request Account on UPPMAX\" button.</p> </li> </ul> <p></p> <ul> <li>You can then request a username. Then click Request Account</li> </ul> <p></p> <ul> <li> <p>After applying it might take up to 2 working days before you receive 2 emails with information on how to login to UPPMAX.</p> </li> <li> <p>If you have any questions please contact us through the Support Form on how to access the UPPMAX resources.</p> </li> </ul> <p>Note</p> <p>After the 4 steps are completed your account will be created at UPPMAX within 2 working days and you will receive two emails with information</p>"},{"location":"getting_started/why_2fa/","title":"Why is 2FA important?","text":"","tags":["2FA","MFA","UPPMAX","Why"]},{"location":"getting_started/why_2fa/#why-is-2fa-important","title":"Why is 2FA important?","text":"<p>By requiring a second factor, typically a cell phone or other physical device, it becomes much harder for an attacker to gain access to your account if they somehow have gotten hold of your password (which in this case is the 1:st factor).</p> <p>For security reasons you will have to use a two factor authentication system if you are a) connecting to UPPMAX from outside of Sweden, or b) connecting from a network within Sweden that does not support forward and reverse DNS lookups (due to a misconfiguration in your network, you can ask your internet service provider about this). More information about why can be found below.</p>","tags":["2FA","MFA","UPPMAX","Why"]},{"location":"getting_started/why_2fa/#from-outside-sweden","title":"From outside Sweden","text":"<p>If you try to connect directly to our resources from computers outside Sweden you will most likely be required to set up and use two factor authentication (you will be asked for a code from your second factor automatically if required).</p> <p>Another alternative, if you need to access UPPMAX from outside Sweden, may be to use a Swedish VPN service. For example, if you're employed at Uppsala University, then you can connect using the university's VPN service.</p>","tags":["2FA","MFA","UPPMAX","Why"]},{"location":"getting_started/why_2fa/#from-within-sweden","title":"From within Sweden","text":"<p>If you are required to use two factor authentication, and are connecting from a computer in Sweden, this is typically caused by your computer not having a proper DNS name, or the forward and reverse name resolution do not match.</p> <p>If this is the case, please contact your ISP and ask them to correct this.</p>","tags":["2FA","MFA","UPPMAX","Why"]},{"location":"getting_started/why_2fa/#note","title":"Note","text":"<p>You can check forward and reverse name resolution on this webpage:</p> <ul> <li>http://www.whatismyip.com/reverse-dns-lookup</li> </ul> <p>To see what address the other side thinks you come from (which will likely be what our systems see), services like</p> <ul> <li>https://ifconfig.co/</li> </ul> <p>can be helpful.</p> <p>On Linux, you can also use these commands:</p> <ul> <li>Forward resolution: <code>host mycomputername.domain.tld</code>.   You have to replace <code>mycomputername.domain.tld</code>   with your computers actual name, for example:</li> </ul> <pre><code>host rackham2.uppmax.uu.se\n</code></pre> <p>will give:</p> <pre><code>rackham2.uppmax.uu.se has address 89.44.250.83\n</code></pre> <ul> <li>Reverse resolution: <code>host my_ipnumber</code>.   You have to replace <code>my_ipnumber</code> with your computers actual IP number,   for example:</li> </ul> <pre><code>host 89.44.250.83\n</code></pre> <p>which should give something similar to:</p> <pre><code>89.44.250.83.in-addr.arpa domain name pointer rackham2.uppmax.uu.se\n</code></pre>","tags":["2FA","MFA","UPPMAX","Why"]},{"location":"hardware/overview/","title":"Hardware overview","text":"","tags":["overview","hardware","specifications","specs"]},{"location":"hardware/overview/#hardware-overview","title":"Hardware overview","text":"<p>This page describes the hardware architecture of the different compute clusters at UPPMAX as well as their storage systems.</p> <p>UPPMAX is part of the National Academic Infrastructure for Supercomputing in Sweden (NAISS).</p> Parameter Rackham Snowy Bianca UPPMAX Cloud Purpose General-purpose General-purpose Sensitive data IaaS Reserved for NAISS projects Uppsala researchers and course projects See Bianca NAISS and local projects Nodes (Intel) 486+144 228 + 50 N vidia T4 GPUs See Bianca 40 + 20 A2 and 4 T4 Nvidia GPUs Cores per node 20/16 16 See Bianca 16 Memory per node 128GB 128GB See Bianca 128/256GB Fat nodes 256GB &amp; 1TB 256, 512 GB &amp; 4TB See Bianca N/A Local disk (scratch) 2/3TB 4TB See Bianca N/A Network InfiniBand FDR 56Gbit/s InfiniBand FDR 40Gbit/ s See Bianca 10GbE Operating System CentOS 7 CentOS 7 See Bianca Linux cloud image Login nodes Yes No (reached from Rackham) See Bianca N/A \"Home\" storage Domus Domus See Bianca N/A \"Project\" Storage Crex, Lutra Crex, Lutra See Bianca N/A <p>The storage systems we have provide a total volume of about 20 PB, the equivalent of nearly 15\u00a0billion 3.5-inch floppy disks or 40,000 years of 128-bit encoded\u00a0music.</p>","tags":["overview","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/","title":"Bianca hardware","text":"","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#bianca-hardware","title":"Bianca hardware","text":"","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#technical-summary","title":"Technical Summary","text":"<ul> <li>204 compute nodes with single or dual CPUs and one 4TB mechanical drive or 1TB SSD</li> <li>Each CPU has 16 cores</li> <li>75 compute nodes, 256 GB memory each.</li> <li>15 compute nodes, 512 GB memory each</li> <li>10 compute nodes each equipped with 2xNVIDIA A100 (40GB) GPUs</li> <li>Total number of CPU cores is 4800</li> <li>Login nodes have 2vCPU each and 16GB memory</li> <li>Dual 10 Gigabit Ethernet for all nodes</li> </ul>","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#parameters","title":"Parameters","text":"Parameter Bianca Purpose Sensitive data Reserved for NAISS-SENS projects Nodes (Intel) 272 + 4 nodes of 2 NVIDIA A100 GPUs Cores per node 16/64 Memory per node 128GB Fat nodes 256 &amp; 512GB Local disk (scratch) 4TB Network Dual 10Gbit/s Operating System CentOS 7 Login nodes Yes (2 cores and 15 GB) \"Home\" storage Castor/Cygnus \"Project\" Storage Castor/Cygnus","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#cpu","title":"CPU","text":"","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#gpu","title":"GPU","text":"","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#network","title":"Network","text":"","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#storage","title":"Storage","text":"","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/bianca/#security","title":"Security","text":"<p>Since Bianca is designed to handle sensitive personal data security is a key aspect of the configuration. In order to ensure that the data is safe we have implemented a series of security measures including, but not limited to:</p> <ul> <li>One virtualized cluster per project, no resources are shared between projects.</li> <li>Separate storage volumes per project.</li> <li>Detailed logging of file transfers in and out of the cluster.</li> <li>Two factor authentication</li> <li>No internet access inside the clusters.</li> <li>Locked racks for the hardware</li> <li>Destruction of broken hard drives</li> </ul> <p>Uppsala University has decided on the following KRT classifications for Bianca:</p> <ul> <li>321 for project directories</li> <li>322 for home directories</li> </ul>","tags":["Bianca","hardware","specifications","specs"]},{"location":"hardware/clusters/decomissioned_clusters/","title":"Decomissioned clusters","text":"","tags":["cluster","decomissioned","history","old"]},{"location":"hardware/clusters/decomissioned_clusters/#decomissioned-clusters","title":"Decomissioned clusters","text":"<p>These are HPC cluster that no longer exists, yet are part of UPPMAX's history.</p> HPC cluster Years Nodes Total cores RAM/Node Description Source Kalkyl 2010-2014 350 2,800 24 GiB; 16\u00d7 48 GiB; 16\u00d7 72 GiB Half for NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Halvan 2011-2015 1 64 2 TiB NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Tintin 2012-2017 160 2,560 64 GiB; 16\u00d7 128 GiB Primarily non-NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Milou 2013-2017 208 3,328 128 GiB; 17\u00d7 256 GiB; 17\u00d7 512 GiB NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Fysast1 2013-2017 40 640 128 GiB Non-NGS projects in physics and astronomy <code>[Dahl\u00f6 et al., 2018]</code>","tags":["cluster","decomissioned","history","old"]},{"location":"hardware/clusters/decomissioned_clusters/#references","title":"References","text":"<ul> <li><code>[Dahl\u00f6 et al., 2018]</code> Dahl\u00f6, Martin, et al. \"Tracking the NGS revolution: managing life science research on shared high-performance computing clusters.\" GigaScience 7.5 (2018): giy028.</li> </ul>","tags":["cluster","decomissioned","history","old"]},{"location":"hardware/clusters/miarka/","title":"Miarka","text":""},{"location":"hardware/clusters/miarka/#miarka","title":"Miarka","text":"<p>Page under construction</p>"},{"location":"hardware/clusters/pelle/","title":"Pelle/Maja hardware","text":""},{"location":"hardware/clusters/pelle/#pellemaja-hardware","title":"Pelle/Maja hardware","text":"No of nodes CPUs CoresThreads Memory Scratch GPUs Name 116 AMD EPYC 9454P (Zen4)  2.75 GHz 4896 768 GiB 6? TB N/A p[1-115] 2 AMD EPYC 9454P (Zen4)  2.75 GHz 4896 2 / 3 TiB 6? TB N/A p[251-252] 4 2 x AMD EPYC 9124 (Zen4)  3 GHz 2 x 162 x 32 384 GiB 6? TB 10 x L40 p[201-204] 2 2 x AMD EPYC 9124 (Zen4)  3 GHz 2 x 162 x 32 384 GiB 6? TB 2 x H100 p[205-206]"},{"location":"hardware/clusters/pelle/#cpus","title":"CPUs","text":"<ul> <li>AMD EPYC 9454P (Zen4) 48-Core Processor 2.75 GHz</li> <li>AMD EPYC 9124 (Zen4) 16-Core Processor 3 GHz</li> </ul>"},{"location":"hardware/clusters/pelle/#gpus","title":"GPUs","text":"<ul> <li> <p>L40</p> <ul> <li>Unprecedented visual computing performance for the data center.</li> <li>GPU memory 48 GB</li> <li>GPU Memory Bandwidth 864GB/s</li> </ul> </li> <li> <p>H100 tensor core</p> <ul> <li>Extraordinary performance, scalability, and security for every data center.</li> <li>GPU memory 94 GB</li> <li>GPU Memory Bandwidth 3.9 TB/s</li> </ul> </li> </ul>"},{"location":"hardware/clusters/pelle/#network","title":"Network","text":"<p>100 Gbit/s</p>"},{"location":"hardware/clusters/pelle/#storage","title":"Storage","text":"<ul> <li>Gorilla</li> <li>Domus/Crex in an transition period</li> </ul>"},{"location":"hardware/clusters/pelle/#os","title":"OS","text":"<p>Rocky 9</p>"},{"location":"hardware/clusters/rackham/","title":"Rackham hardware","text":""},{"location":"hardware/clusters/rackham/#rackham-hardware","title":"Rackham hardware","text":"Nodes CPUs Cores Memory Scratch GPUs Name Comment 272 2x Xeon E5-2630 V4 2.2 GHz (turbo 3.1 GHz) 20 (2 x 10) 128GB 3/4TB N/A r33-r304 . 32 2x Xeon E5-2630 V4 2.2 GHz (turbo 3.1 GHz) 20 (2 x 10) 256GB 3/4TB N/A r1-r32 . 4 2x Xeon E5-2630 V4 2.2 GHz (turbo 3.1 GHz) 20 (2 x 10) 1TB 3/4TB N/A ? . 4 2x Xeon E5-2630 V4 2.2 GHz (turbo 3.1 GHz) 20 (2 x 10) 256GB 3/4TB Nvidia Quatro K2200 rackham1-rackham3 Login nodes <p>The Rackham cluster was introduced in February 2017. Rackham is a NAISS resource and is estimated to be in production until first of January 2023. The major features of Rackham and its storage system Crex is found below. For more technical data please see the end of this article.</p>"},{"location":"hardware/clusters/rackham/#cpu","title":"CPU","text":""},{"location":"hardware/clusters/rackham/#network","title":"Network","text":""},{"location":"hardware/clusters/rackham/#storage","title":"Storage","text":""},{"location":"hardware/clusters/snowy/","title":"Snowy hardware","text":""},{"location":"hardware/clusters/snowy/#snowy-hardware","title":"Snowy hardware","text":"Nodes CPUs Cores Memory Scratch GPUs Name Comment 122 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 128GB 3/4TB N/A s1-s12, s14-s40, s42-s120, s201-s204 . 49 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 128GB 3/4TB Tesla T4 s151-s163, s164-s200 . 15 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 512GB 3/4TB N/A s121-s129, s131, s133-s137 . 12 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 256GB 3/4TB N/A s139-s150 . 1 2x Xeon E5-2660 2.2 GHz 80 (10 x 8) 4TB 3/4TB N/A s229 . 1 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 256GB 3/4TB Tesla T4 s138 ."},{"location":"hardware/clusters/snowy/#cpu","title":"CPU","text":""},{"location":"hardware/clusters/snowy/#gpu","title":"GPU","text":""},{"location":"hardware/clusters/snowy/#network","title":"Network","text":""},{"location":"hardware/clusters/snowy/#storage","title":"Storage","text":""},{"location":"hardware/clusters/uppmax_cloud/","title":"UPPMAX cloud","text":""},{"location":"hardware/clusters/uppmax_cloud/#uppmax-cloud","title":"UPPMAX cloud","text":"<p>Page under construction</p>"},{"location":"hardware/storage/castor/","title":"Castor","text":""},{"location":"hardware/storage/castor/#castor","title":"Castor","text":"<p>UPPMAX has many storage systems. This page describes the Castor storage system.</p> <p>Castor is a custom built storage system running GlusterFS dedicated to Bianca. The system consists of 54 Huawei 5288 V3 servers, each server is equipped with 36 x 3TB SATA disks working as one logical volume (with redundancy) and providing 109TB raw disk space per one server. This gives about 5,7 PB raw disk space in total. Each storage server is connected to network with 2 x 40 Gbit/s Ethernet links working as one aggregated link at 80 Gbit/s.</p>"},{"location":"hardware/storage/crex/","title":"Crex","text":""},{"location":"hardware/storage/crex/#crex","title":"Crex","text":"<p>UPPMAX has many storage systems. This page describes the Crex storage system.</p> <p>Rackham and Snowy's storage system\u00a0is a DDN (DataDirect Networks) EXAScaler filesystem based on the ES14KX platform. Crex uses 840 10TB NL-SAS drives and 24 300GB SAS drives for metadata storage. The total volume is 6 PB, with 1 PB reserved for SciLifeLab, 4.5 PB\u00a0reserved for SNIC projects, and 0.5\u00a0PB for UPPMAX use. The filesystem is Lustre, a highly scalable filesystem common in HPC.</p>"},{"location":"hardware/storage/cygnus/","title":"Cygnus","text":""},{"location":"hardware/storage/cygnus/#cygnus","title":"Cygnus","text":"<p>UPPMAX has many storage systems. This page describes the Cygnus storage system.</p> <p>Cygnus is a DDN Secure Lustre file system for Bianca.</p>"},{"location":"hardware/storage/decomissioned_storage_systems/","title":"Decomissioned storage systems","text":"","tags":["storage systems","decomissioned","history","old"]},{"location":"hardware/storage/decomissioned_storage_systems/#decomissioned-storage-systems","title":"Decomissioned storage systems","text":"<p>These are HPC storage systems that no longer exists, yet are part of UPPMAX's history.</p> HPC storage Year Capacity, PiB Format Description Description Source Bubo 2009-2013 0.7 PanFS Shared storage Half for NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Lynx 2012-2015 0.5 PanFS Shared storage NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Gulo 2012-2016 1.0 Lustre Global scratch storage Primarily non-NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Pica 2013-2017 5.5 NFS Shared storage NGS projects <code>[Dahl\u00f6 et al., 2018]</code> Fysast1 2013-2017 40 640 128 GiB Non-NGS projects in physics and astronomy <code>[Dahl\u00f6 et al., 2018]</code>","tags":["storage systems","decomissioned","history","old"]},{"location":"hardware/storage/decomissioned_storage_systems/#references","title":"References","text":"<ul> <li><code>[Dahl\u00f6 et al., 2018]</code> Dahl\u00f6, Martin, et al. \"Tracking the NGS revolution: managing life science research on shared high-performance computing clusters.\" GigaScience 7.5 (2018): giy028.</li> </ul>","tags":["storage systems","decomissioned","history","old"]},{"location":"hardware/storage/domus/","title":"Domus","text":""},{"location":"hardware/storage/domus/#domus","title":"Domus","text":"<p>UPPMAX has many storage systems. This page describes the Domus storage system.</p> <p>Domus hosts the home directories and some common system directories, e.g. the software catalogue. The system is a NetApp totalling\u00a0100 TB on 96 SAS 10K disks, supports snapshots, and has off-site backup.</p>"},{"location":"hardware/storage/lutra/","title":"Lutra","text":""},{"location":"hardware/storage/lutra/#lutra","title":"Lutra","text":"<p>UPPMAX has many storage systems. This page describes the Lutra storage system.</p> <p>Lutra is a custom built storage system running GlusterFS. The system consist of 6 Huawei 5288 V5 servers with a total of 6x38 10TB SATA-drives for a capacity of 2.2 PB. The usable disk space is 1.8PB. Lutra is meant for \"offload\" or archive storage and available for all users at a cost of (at this moment) 500 SEK/TB/year, for a commitment of four years and a minimum 50TB. The design and filesystem choice makes Lutra very scalable, cost efficient while retaining moderate read/write performance. Lutra is connected to Rackham and Snowy for general availability.</p> <p>If you are interested in this type of storage please contact support.</p>"},{"location":"hardware/storage/spirula/","title":"Spirula","text":""},{"location":"hardware/storage/spirula/#spirula","title":"Spirula","text":"<p>Spirula is a DDLS-funded SciLifeLab FAIR Data Storage system which exposes 10 PB of raw S3 compatible object storage (or blob storage) through Ceph Object Gateway, also known as RADOS Gateway (RGW).</p> <ul> <li>Spirula is not in production.</li> <li>There is no backup of any data stored on Spirula.</li> <li>You should not store any primary data on Spirula.</li> <li>Spirula does not expose any public endpoints.</li> </ul> <p>See storage systems for other storage systems at UPPMAX.</p>"},{"location":"hardware/storage/spirula/#support","title":"Support","text":"<p>The responsibility for Spirula is shared between UPPMAX and SciLifeLab.</p> <p>Spirula is accessible from within UPPMAX and from certain whitelisted SciLifeLab addresses. Access to Rackham/Pelle is not granted for projects with Spirula allocations, in cases where you don't have access to Rackham/Pelle through some other means please refer to FAIRstorage@scilifelab.se for more information.</p> <p>In general follow these guidelines when requesting support.</p> <p>Contact SciLifeLab DC at FAIRstorage@scilifelab.se for issues regarding:</p> <ul> <li>Project grants, allocations and quota in SUPR.</li> <li>Access to Spirula through SciLifeLab addresses.</li> <li>FAIR Storage and the intended use-cases for Spirula.</li> </ul> <p>Contact UPPMAX Support at support@uppmax.uu.se for issues regarding:</p> <ul> <li>Retrieving access tokens and/or quota from the SSH service.</li> <li>Technical implementation and/or limitations of the system.</li> </ul>"},{"location":"hardware/storage/spirula/#usage","title":"Usage","text":""},{"location":"hardware/storage/spirula/#access-tokens-and-displaying-usage-quota","title":"Access tokens and displaying usage quota","text":"<p>If you are able to <code>ping</code> and <code>ssh</code> to <code>s3.spirula.uppmax.uu.se</code> you can interact with UPPMAX's SSH to S3 credential service. You will login the service using your UPPMAX user account and second factor authentication.</p> <pre><code>ssh s3.spirula.uppmax.uu.se\nPassword: ********\nTwo factor (UPPMAX): 123456\n _   ________________  ___  ___  __   __      _____       _            _\n| | | | ___ \\ ___ \\  \\/  | / _ \\ \\ \\ / /     /  ___|     (_)          | |\n| | | | |_/ / |_/ / .  . |/ /_\\ \\ \\ V /______\\ `--. _ __  _ _ __ _   _| | __ _\n| | | |  __/|  __/| |\\/| ||  _  | /   \\______|`--. \\ '_ \\| | '__| | | | |/ _` |\n| |_| | |   | |   | |  | || | | |/ /^\\ \\     /\\__/ / |_) | | |  | |_| | | (_| |\n \\___/\\_|   \\_|   \\_|  |_/\\_| |_/\\/   \\/     \\____/| .__/|_|_|   \\__,_|_|\\__,_|\n                                                   | |\n                                                   |_|\nYou have 2 available credentials. Please select which credential to use:\n0: User tintin in sll1234001\n1: Project user for sll1234001\n</code></pre> <p>After selecting which credentials you want to use you can either retrieve your access credentials or view your current usage and quota.</p> <pre><code>SSH to S3 credentials service - v0.2\n-------------------------------------------\nYou have several options. Please select what you want to do.\n1: Retrieve S3 Access Credentials\n2: View Current Quota\n</code></pre>"},{"location":"hardware/storage/spirula/#configuring-a-s3-client","title":"Configuring a S3 client","text":"<p>Spirula through the Ceph Object Gateway supports multiple S3 compliant clients such as <code>aws cli</code> or <code>s3cmd</code>. Setup differs between tools, adopt the configuration below to your particular needs.</p>"},{"location":"hardware/storage/spirula/#aws-cli","title":"AWS CLI","text":"<p>For setting up AWS command line interface <code>aws cli</code> to use Spirula you can create or update the <code>~/.aws/config</code> configuration file with the following options:</p> <pre><code># ~/.aws/config\n[default]\nregion = None\nendpoint_url = https://s3.spirula.uppmax.uu.se:8443\n</code></pre> <p>Full usage instructions and configuration options for <code>aws cli</code>  are available through AWS documentation.</p>"},{"location":"hardware/storage/spirula/#example-of-using-aws-cli","title":"Example of using AWS CLI","text":"<p>After retrieving your credentials using the SSH to S3 credential service, you can store them e.g. in a credentials file in your AWS CLI configuration directory, as per the following instructions in the AWS documentation.</p> <p>With that done, you can access your storage area on Spirula using the AWS CLI. To see which areas you have access to, run <code>aws s3 ls</code> and you should see something like:</p> <pre><code>aws s3 ls\n2023-11-29 13:07:57 testuser-nobackup-insecure\n2025-03-06 10:44:15 sll2021001-testuser-nobackup\n</code></pre> <p>To view the contents of an area, run e.g. <code>aws s3 ls sll2021001-testuser-nobackup</code>.</p> <p>Suppose you have a file called <code>testfile</code> in your current working directory. To upload it, run:</p> <pre><code>aws s3 cp ./testfile s3://sll2021001-testuser-nobackup/\nupload: ./testfile to s3://sll2021001-testuser-nobackup/testfile \n</code></pre> <p>For more information about basic <code>aws s3</code> commands, see the AWS documentation.</p>"},{"location":"hardware/storage/vulpes/","title":"Vulpes","text":""},{"location":"hardware/storage/vulpes/#vulpes","title":"Vulpes","text":"<p>UPPMAX has many storage systems. This page describes the Vulpes storage system.</p> <p>Lupus provided storage for Miarka.</p>"},{"location":"naiss/","title":"NAISS","text":""},{"location":"naiss/#naiss","title":"NAISS","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>NAISS does stuff.</p>"},{"location":"naiss/alvis/","title":"Alvis","text":"","tags":["Alvis","HPC","center","centre"]},{"location":"naiss/alvis/#alvis","title":"Alvis","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>Alvis is an HPC cluster at Chalmers.</p> <ul> <li>Log in to Alvis</li> </ul>","tags":["Alvis","HPC","center","centre"]},{"location":"naiss/alvis_file_transfer_using_filezilla/","title":"File transfer to/from Alvis using FileZilla","text":"","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#file-transfer-tofrom-alvis-using-filezilla","title":"File transfer to/from Alvis using FileZilla","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the C3SE documentation. However, it has not been suggested to be added to their documentaton yet.</p> <p>HPC clusters have different ways to do file transfer using FileZilla.</p> <p>This page shows how to do so for Alvis.</p> <p></p> <p>FileZilla connected to Alvis</p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to Alvis using FileZilla.</p> <p>FileZilla is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from Alvis using FileZilla, do the following steps:</p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>Get inside of SUNET.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#2-install-putty-tools","title":"2. Install <code>putty-tools</code>","text":"What if I don't use Linux? <p>Instead of following steps 2 and 3, follow the procedure at the puttygen page, section 'Create SSH key files'.</p> <pre><code>sudo apt install putty-tools\n</code></pre>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#3-create-the-needed-files-in-the-ssh-folder","title":"3. Create the needed files in the <code>.ssh</code> folder","text":"<p>Navigate into the <code>.ssh</code> folder.</p> <pre><code>cd .ssh\n</code></pre>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#31-create-the-ppk-file","title":"3.1 Create the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -t rsa -b 2048 -C \"[alvis_username]@alvis.c3se.chalmers.se\" -o [filename].ppk\n</code></pre> <p>For example:</p> <p></p><pre><code>puttygen -t rsa -b 2048 -C \"svens@alvis.c3se.chalmers.se\" -o alvis_filezilla.ppk\n</code></pre> alvis_file_transfer_using_filezilla_select_file_site_manager","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#32-extract-the-private-ssh-key-from-the-ppk-file","title":"3.2 Extract the private SSH key from the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -O private-openssh-new [filename].ppk -o [filename]\n</code></pre> <p>For example:</p> <pre><code>puttygen -O private-openssh-new alvis_filezilla.ppk -o alvis_filezilla\n</code></pre>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#33-extract-the-public-ssh-key-from-the-ppk-file","title":"3.3 Extract the public SSH key from the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -O public-openssh [filename].ppk -o [filename].pub\n</code></pre> <p>For example:</p> <pre><code>puttygen -O public-openssh alvis_filezilla.ppk -o alvis_filezilla.pub\n</code></pre>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#4-add-the-public-ssh-key-to-sshauthorized_keys","title":"4. Add the public SSH key to <code>~/.ssh/authorized_keys</code>","text":"<p>Copy the public SSH key (in the <code>.pub</code> file) to the <code>~/.ssh/authorized_keys</code> file on Alvis.</p> <p>If that file does not exist yet, create it and set the right permissions as such:</p> <pre><code>mkdir .ssh\ntouch .ssh/authorized_keys\nchmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#5-start-filezilla","title":"5. Start FileZilla","text":"<p>Start FileZilla.</p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#6-start-filezillas-site-manager","title":"6. Start FileZilla's site manager","text":"<p>From the menu, select 'File | Site manager'</p> Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#7-add-a-new-site-in-filezillas-site-manager","title":"7. Add a new site in FileZilla's site manager","text":"<p>In FileZilla's site manager, click 'New site'</p> Where is that? <p>It is here:</p> <p></p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#8-setup-the-site","title":"8. Setup the site","text":"<p>In FileZilla's site manager:</p> <ul> <li>create a name for the site, e.g. <code>Alvis</code>.</li> <li>for that site, use all standards, except:<ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>alvis1.c3se.chalmers.se</code> or <code>alvis2.c3se.chalmers.se</code></li> <li>Set user to <code>[username]</code>, e.g. <code>svens</code></li> <li>Set logon type: Key file</li> <li>Upload the key file at <code>/.ssh/alvis_filezilla.ppk</code>   from you local's computer</li> </ul> </li> </ul> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#9-connect-to-the-site","title":"9. Connect to the site","text":"<p>Click 'Connect'.</p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_filezilla/#10-ready-to-transfer-files","title":"10. Ready to transfer files","text":"<p>Now you can transfer files between your local computer and Alvis.</p> How does that look like? <p>It looks like this:</p> <p></p>","tags":["FileZilla","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/","title":"File transfer to/from Alvis using rsync","text":"","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#file-transfer-tofrom-alvis-using-rsync","title":"File transfer to/from Alvis using rsync","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the C3SE documentation. However, it has not been suggested to be added to their documentaton yet.</p> <p>HPC clusters have different ways to do file transfer using rsync.</p> <p>This page shows how to do so for Alvis.</p>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to Alvis using rsync.</p> <p><code>rsync</code> is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from Alvis using <code>rsync</code>, do the following steps:</p>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>Get inside of SUNET.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#2-start-a-terminal-on-your-local-computer","title":"2. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#3-transfer-files-to-alvis","title":"3. Transfer files to Alvis","text":"<p>You can transfer files to Alvis by:</p> <ul> <li>3a. Transfer individual files to Alvis</li> <li>3b. Transfer all files in a folder to Alvis</li> </ul>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#3a-transfer-individual-files-to-alvis","title":"3a. Transfer individual files to Alvis","text":"<p>On local computer, do:</p> <pre><code>rsync [my_local_file] [username]@alvis1.c3se.chalmers.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your file to</li> <li><code>[username]</code> is your C3SE username</li> </ul> <p>for example:</p> <pre><code>rsync my_file.txt svens@alvis1.c3se.chalmers.se:/cephyr/users/svens/Alvis/\n                                                              ^\n                                                              |\n                                                              +--- username\n</code></pre> <p>If asked, give your C3SE password.</p>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#3b-transfer-all-files-in-a-folder-to-alvis","title":"3b. Transfer all files in a folder to Alvis","text":"<p>On local computer, do:</p> <pre><code>rsync --recursive my_folder [username]@alvis1.c3se.chalmers.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[target_folder]</code> is the target folder</li> <li><code>[username]</code> is your C3SE username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive my_folder svens@alvis1.c3se.chalmers.se:/cephyr/users/svens/Alvis/\n</code></pre> <p>If asked, give your C3SE password.</p> <p>Note that in <code>rsync</code>, a slash (<code>/</code>) matters:</p> Command Effect <code>rsync --recursive my_folder svens@alvis1.c3se.chalmers.se:/cephyr/users/svens/Alvis</code> Will put the files in <code>my_folder</code> in the Alvis home folder <code>rsync --recursive my_folder svens@alvis1.c3se.chalmers.se:/cephyr/users/svens/Alvis/</code> Will put the folder <code>my_folder</code> in the Alvis home folder","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#4-transfer-files-from-alvis-to-you-local-computer","title":"4. Transfer files from Alvis to you local computer","text":"<p>You can transfer files from Alvis to your local computer by:</p> <ul> <li>4a. Transfer individual files from Alvis to your local computer</li> <li>4b. Transfer all folders from Alvis to you local computer</li> </ul>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#4a-transfer-individual-files-from-alvis-to-your-local-computer","title":"4a. Transfer individual files from Alvis to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync [username]@alvis1.c3se.chalmers.se:[path_to_file] .\n</code></pre> <p>where</p> <ul> <li><code>[path_to_file]</code> is the path to the file you want to download</li> <li><code>[username]</code> is your C3SE username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync svens@alvis1.c3se.chalmers.se:/cephyr/users/svens/Alvis/my_file.txt .\n</code></pre> <p>If asked, give your C3SE password.</p>","tags":["rsync","Alvis"]},{"location":"naiss/alvis_file_transfer_using_rsync/#4b-transfer-all-folders-from-alvis-to-you-local-computer","title":"4b. Transfer all folders from Alvis to you local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync --recursive [username]@alvis1.c3se.chalmers.se:/[source_folder] .\n</code></pre> <p>where</p> <ul> <li><code>[source_folder]</code> is the path to the folder you want to download</li> <li><code>[username]</code> is your C3SE username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive svens@alvis1.c3se.chalmers.se:/cephyr/users/svens/Alvis/my_folder .\n</code></pre> <p>If asked, give your C3SE password.</p>","tags":["rsync","Alvis"]},{"location":"naiss/arrhenius/","title":"Arrhenius","text":"","tags":["Dardel","NAISS","HPC","cluster"]},{"location":"naiss/arrhenius/#arrhenius","title":"Arrhenius","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>Arrhenius is the next Sweden-wide HPC cluster.</p> <p>From this NAISS news item:</p> <ul> <li>424 AMD Turin 128-core CPUs</li> <li>GPU:<ul> <li>382 GPU nodes, each with four Grace Hopper Superchips from Nvidia</li> <li>HPL (High-Performance Linpack) performance of more than 60PFLOPS, approximately seven times that of the current most powerful NAISS system, Dardel.</li> </ul> </li> <li>A partition for cloud computing</li> <li>A partition dedicated to sensitive data</li> <li>Storage is provided by a fast parallel file system of 29 PB</li> </ul>","tags":["Dardel","NAISS","HPC","cluster"]},{"location":"naiss/cosmos/","title":"COSMOS","text":"","tags":["COSMOS","LUNARC"]},{"location":"naiss/cosmos/#cosmos","title":"COSMOS","text":"<p>COSMOS is an HPC cluster maintained by LUNARC.</p> <p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p>","tags":["COSMOS","LUNARC"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/","title":"File transfer to/from COSMOS using FileZilla","text":"","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#file-transfer-tofrom-cosmos-using-filezilla","title":"File transfer to/from COSMOS using FileZilla","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the LUNARC documentation. However, it has not been suggested to be added to their documentaton yet.</p> <p>HPC clusters have different ways to do file transfer using FileZilla.</p> <p>This page shows how to do so for COSMOS.</p> <p></p> <p>FileZilla connected to COSMOS</p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to COSMOS using FileZilla</p> <p>FileZilla is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from COSMOS using FileZilla, do the following steps:</p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#1-start-filezilla","title":"1. Start FileZilla","text":"<p>Start FileZilla.</p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#2-start-filezillas-site-manager","title":"2. Start FileZilla's site manager","text":"<p>From the menu, select 'File | Site manager'</p> Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#3-add-a-new-site-in-filezillas-site-manager","title":"3. Add a new site in FileZilla's site manager","text":"<p>In FileZilla's site manager, click 'New site'</p> Where is that? <p>It is here:</p> <p></p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#4-setup-the-site","title":"4. Setup the site","text":"<p>In FileZilla's site manager:</p> <ul> <li>create a name for the site, e.g. <code>COSMOS</code>.</li> <li>for that site, use all standards, except:<ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>cosmos.lunarc.lu.se</code></li> <li>Set user to <code>[username]</code>, e.g. <code>sven</code></li> <li>Set logon type to 'Interactive'</li> </ul> </li> </ul> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#8-connect-to-the-site","title":"8. Connect to the site","text":"<p>Click 'Connect'.</p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_filezilla/#9-ready-to-transfer-files","title":"9. Ready to transfer files","text":"<p>Now you can transfer files between your local computer and COSMOS.</p> How does that look like? <p>It looks like this:</p> <p></p>","tags":["FileZilla","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/","title":"Data transfer to/from COSMOS using rsync","text":"","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#data-transfer-tofrom-cosmos-using-rsync","title":"Data transfer to/from COSMOS using rsync","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the LUNARC documentation. However, it has not been suggested to be added to their documentaton yet.</p> <p>There are multiple ways to transfer files to or from COSMOS. Here it is described how to do file transfer to/from COSMOS using rsync.</p>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#procedure","title":"Procedure","text":"Prefer a video? <p>Watch the YouTube video Data transfer to/from COSMOS using rsync.</p>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#2-transfer-files-to-cosmos","title":"2. Transfer files to COSMOS","text":"<p>You can transfer files to COSMOS by:</p> <ul> <li>2a. Transfer individual files to COSMOS</li> <li>2b. Transfer all files in a folder to COSMOS</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#2a-transfer-individual-files-to-cosmos","title":"2a. Transfer individual files to COSMOS","text":"<p>On local computer, do:</p> <pre><code>rsync [my_local_file] [username]@cosmos.lunarc.lu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your file to</li> <li><code>[username]</code> is your LUNARC username</li> </ul> <p>for example:</p> <pre><code>rsync my_local_file.txt sven@cosmos.lunarc.lu.se:/home/sven/\n</code></pre> <p>If asked, give your LUNARC password.</p>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#2b-transfer-all-files-in-a-folder-to-cosmos","title":"2b. Transfer all files in a folder to COSMOS","text":"<p>On local computer, do:</p> <pre><code>rsync --recursive my_folder [username]@cosmos.lunarc.lu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[target_folder]</code> is the target folder</li> <li><code>[username]</code> is your LUNARC username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive my_folder sven@cosmos.lunarc.lu.se:/home/sven/\n</code></pre> <p>If asked, give your LUNARC password.</p> <p>Note that in <code>rsync</code>, a slash (<code>/</code>) matters:</p> Command Effect <code>rsync --recursive my_folder sven@cosmos.lunarc.lu.se:/home/sven</code> Will put the files in <code>my_folder</code> in the COSMOS home folder <code>rsync --recursive my_folder sven@cosmos.lunarc.lu.se:/home/sven/</code> Will put the folder <code>my_folder</code> in the COSMOS home folder","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#3-transfer-files-from-cosmos-to-you-local-computer","title":"3. Transfer files from COSMOS to you local computer","text":"<p>You can transfer files from COSMOS to your local computer by:</p> <ul> <li>3a. Transfer individual files from COSMOS to your local computer</li> <li>3b. Transfer all folders from COSMOS to you local computer</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#3a-transfer-individual-files-from-cosmos-to-your-local-computer","title":"3a. Transfer individual files from COSMOS to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync [username]@cosmos.lunarc.lu.se:[path_to_file] .\n</code></pre> <p>where</p> <ul> <li><code>[path_to_file]</code> is the path to the file you want to download</li> <li><code>[username]</code> is your LUNARC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync sven@cosmos.lunarc.lu.se:/home/sven/my_file.txt .\n</code></pre> <p>If asked, give your LUNARC password.</p>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/cosmos_file_transfer_using_rsync/#3b-transfer-all-folders-from-cosmos-to-you-local-computer","title":"3b. Transfer all folders from COSMOS to you local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync --recursive [username]@cosmos.lunarc.lu.se:/[source_folder] .\n</code></pre> <p>where</p> <ul> <li><code>[source_folder]</code> is the path to the folder you want to download</li> <li><code>[username]</code> is your LUNARC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive sven@cosmos.lunarc.lu.se:/home/sven/my_folder .\n</code></pre> <p>If asked, give your LUNARC password.</p>","tags":["transfer","data transfer","file transfer","rsync","COSMOS"]},{"location":"naiss/dardel/","title":"Dardel","text":"","tags":["Dardel","PDC","HPC","cluster"]},{"location":"naiss/dardel/#dardel","title":"Dardel","text":"<p>Dardel is an HPC cluster maintained by PDC.</p> <p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <ul> <li>Video: apply for a Dardel 'Small Compute' project</li> <li>File transfer to/from Dardel using FileZilla](dardel_file_transfer_using_filezilla.md)</li> </ul>","tags":["Dardel","PDC","HPC","cluster"]},{"location":"naiss/dardel_file_transfer_using_filezilla/","title":"File transfer to/from Dardel using FileZilla","text":"","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#file-transfer-tofrom-dardel-using-filezilla","title":"File transfer to/from Dardel using FileZilla","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the PDC documentation. However, it has not been suggested to be added to their documentaton yet.</p> <p>HPC clusters have different ways to do file transfer using FileZilla.</p> <p>This page shows how to do so for Dardel.</p> <p></p> <p>FileZilla connected to Dardel</p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to Dardel using FileZilla.</p> <p>FileZilla is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from Dardel using FileZilla, do the following steps:</p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#1-install-putty-tools","title":"1. Install <code>putty-tools</code>","text":"What if I don't use Linux? <p>Instead of following steps 1 and 2, follow the procedure at the puttygen page, section 'Create SSH key files'.</p> <p>On Linux, do:</p> <pre><code>sudo apt install putty-tools\n</code></pre>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#2-create-the-needed-files-in-the-ssh-folder","title":"2. Create the needed files in the <code>.ssh</code> folder","text":"<p>Navigate into the <code>.ssh</code> folder.</p> <pre><code>cd .ssh\n</code></pre>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#21-create-the-ppk-file","title":"2.1 Create the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -t rsa -b 2048 -C \"[dardel_username]@dardel.pdc.kth.se\" -o [filename].ppk\n</code></pre> <p>For example:</p> <pre><code>puttygen -t rsa -b 2048 -C \"svensv@dardel.pdc.kth.se\" -o dardel_filezilla.ppk\n</code></pre>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#22-extract-the-private-ssh-key-from-the-ppk-file","title":"2.2 Extract the private SSH key from the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -O private-openssh-new [filename].ppk -o [filename]\n</code></pre> <p>For example:</p> <pre><code>puttygen -O private-openssh-new dardel_filezilla.ppk -o dardel_filezilla\n</code></pre>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#22-extract-the-public-ssh-key-from-the-ppk-file","title":"2.2 Extract the public SSH key from the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -O public-openssh [filename].ppk -o [filename].pub\n</code></pre> <p>For example:</p> <pre><code>puttygen -O public-openssh dardel_filezilla.ppk -o dardel_filezilla.pub\n</code></pre>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#3-upload-the-public-ssh-key-into-the-pdc-login-portal","title":"3. Upload the public SSH key into the PDC Login Portal","text":"<p>Upload the public SSH key into the PDC Login Portal.</p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#4-start-filezilla","title":"4. Start FileZilla","text":"<p>Start FileZilla.</p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#5-start-filezillas-site-manager","title":"5. Start FileZilla's site manager","text":"<p>From the menu, select 'File | Site manager'</p> Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#6-add-a-new-site-in-filezillas-site-manager","title":"6. Add a new site in FileZilla's site manager","text":"<p>In FileZilla's site manager, click 'New site'</p> Where is that? <p>It is here:</p> <p></p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#7-setup-the-site","title":"7. Setup the site","text":"<p>In FileZilla's site manager:</p> <ul> <li>create a name for the site, e.g. <code>Dardel</code>.</li> <li>for that site, use all standards, except:<ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>dardel.pdc.kth.se</code></li> <li>Set user to <code>[username]</code>, e.g. <code>svensv</code></li> <li>Set logon type: Key file</li> <li>Upload the key file at <code>/.ssh/dardel_filezilla.ppk</code>   from you local's computer</li> </ul> </li> </ul> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#8-connect-to-the-site","title":"8. Connect to the site","text":"<p>Click 'Connect'.</p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_filezilla/#9-ready-to-transfer-files","title":"9. Ready to transfer files","text":"<p>Now you can transfer files between your local computer and Dardel.</p> How does that look like? <p>It looks like this:</p> <p></p>","tags":["FileZilla","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/","title":"File transfer to/from Dardel using rsync","text":"","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#file-transfer-tofrom-dardel-using-rsync","title":"File transfer to/from Dardel using <code>rsync</code>","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the PDC documentation. However, it has not been suggested to be added to their documentaton yet.</p> <p>HPC clusters have different ways to do file transfer using <code>rsync</code>.</p> <p>This page shows how to do so for Dardel.</p>","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to Dardel using <code>rsync</code>.</p> <p><code>rsync</code> is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from Dardel using <code>rsync</code>, do the following steps:</p>","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#2-transfer-files-to-dardel","title":"2. Transfer files to Dardel","text":"<p>You can transfer files to Dardel by:</p> <ul> <li>2a. Transfer individual files to Dardel</li> <li>2b. Transfer all files in a folder to Dardel</li> </ul>","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#2a-transfer-individual-files-to-dardel","title":"2a. Transfer individual files to Dardel","text":"<p>On local computer, do:</p> <pre><code>rsync [my_local_file] [username]@dardel.pdc.kth.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your file to</li> <li><code>[username]</code> is your PDC username</li> </ul> <p>for example:</p> <pre><code>rsync my_file.txt svensv@dardel.pdc.kth.se:~\n</code></pre> <p>If asked, give your PDC password.</p>","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#2b-transfer-all-files-in-a-folder-to-dardel","title":"2b. Transfer all files in a folder to Dardel","text":"<p>On local computer, do:</p> <pre><code>rsync --recursive my_folder [username]@dardel.pdc.kth.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[target_folder]</code> is the target folder</li> <li><code>[username]</code> is your PDC username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive my_folder svensv@dardel.pdc.kth.se:~/\n</code></pre> <p>If asked, give your PDC password.</p> <p>Note that in <code>rsync</code>, a slash (<code>/</code>) matters:</p> Command Effect <code>rsync --recursive my_folder svensv@dardel.pdc.kth.se:~/</code> Will put the files in <code>my_folder</code> in the Dardel home folder <code>rsync --recursive my_folder svensv@dardel.pdc.kth.se:~/</code> Will put the folder <code>my_folder</code> in the Dardel home folder","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#3-transfer-files-from-dardel-to-you-local-computer","title":"3. Transfer files from Dardel to you local computer","text":"<p>You can transfer files from Dardel to your local computer by:</p> <ul> <li>3a. Transfer individual files from Dardel to your local computer</li> <li>3b. Transfer all folders from Dardel to you local computer</li> </ul>","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#3a-transfer-individual-files-from-dardel-to-your-local-computer","title":"3a. Transfer individual files from Dardel to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync [username]@dardel.pdc.kth.se:[path_to_file] .\n</code></pre> <p>where</p> <ul> <li><code>[path_to_file]</code> is the path to the file you want to download</li> <li><code>[username]</code> is your PDC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync svensv@dardel.pdc.kth.se:~/my_file.txt .\n</code></pre> <p>If asked, give your PDC password.</p>","tags":["rsync","Dardel"]},{"location":"naiss/dardel_file_transfer_using_rsync/#3b-transfer-all-folders-from-dardel-to-you-local-computer","title":"3b. Transfer all folders from Dardel to you local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync --recursive [username]@dardel.pdc.kth.se:/[source_folder] .\n</code></pre> <p>where</p> <ul> <li><code>[source_folder]</code> is the path to the folder you want to download</li> <li><code>[username]</code> is your PDC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive svensv@dardel.pdc.kth.se:~/my_folder .\n</code></pre> <p>If asked, give your PDC password.</p>","tags":["rsync","Dardel"]},{"location":"naiss/file_transfer_using_filezilla/","title":"File transfer using FileZilla","text":"","tags":["FileZilla"]},{"location":"naiss/file_transfer_using_filezilla/#file-transfer-using-filezilla","title":"File transfer using FileZilla","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>Here is an overview of how to transfer file to/home the multiple NAISS and local clusters. The videos contain a link to the documentation of that HPC center.</p> HPC Cluster Videos Alvis Video Bianca Video COSMOS Video Dardel Video Kebnekaise Cannot: is not a NAISS cluster Rackham Video Tetralith Does not work, created support ticket :-/","tags":["FileZilla"]},{"location":"naiss/file_transfer_using_rsync/","title":"File transfer using rsync","text":"","tags":["rsync"]},{"location":"naiss/file_transfer_using_rsync/#file-transfer-using-rsync","title":"File transfer using rsync","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>Here is an overview of how to transfer files to/from the multiple NAISS and local clusters. The videos contain a link to the documentation of that HPC center.</p> HPC Cluster Videos Alvis . Berzelius . Bianca Video COSMOS . Dardel . Kebnekaise . LUMI . Rackham Video Tetralith . Vera .","tags":["rsync"]},{"location":"naiss/hpc2n/","title":"HPC2N","text":"","tags":["HPC2n","Ume\u00e5","HPC","center","centre"]},{"location":"naiss/hpc2n/#hpc2n","title":"HPC2N","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>NAISS has many HPC centers. HPC2N is one of those.</p> <ul> <li>HPC2N documentation main page</li> </ul>","tags":["HPC2n","Ume\u00e5","HPC","center","centre"]},{"location":"naiss/kebnekaise/","title":"Kebnekaise","text":"","tags":["Kebnekaise","HPC2N"]},{"location":"naiss/kebnekaise/#kebnekaise","title":"Kebnekaise","text":"<p>Kebnekaise is an HPC cluster maintained by HPC2N.</p> <p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p>","tags":["Kebnekaise","HPC2N"]},{"location":"naiss/login/","title":"Login to NAISS or local HPC cluster","text":"","tags":["login","log in","NAISS","local","cluster","HPC"]},{"location":"naiss/login/#login-to-naiss-or-local-hpc-cluster","title":"Login to NAISS or local HPC cluster","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place, for example, a NAISS-wide documentation website.</p> <p>Here is an overview of how to login to multiple NAISS and local clusters. The videos contain a link to the documentation of that HPC center.</p> HPC cluster Login method Documentation Video Alvis SSH Documentation Documentation Bianca SSH Documentation Documentation Bianca Website Documentation Documentation COSMOS SSH Documentation Documentation COSMOS Local ThinLinc client Documentation Documentation Dardel SSH Documentation Documentation Dardel Local ThinLinc client Documentation Documentation Kebnekaise SSH Documentation Documentation Kebnekaise Local ThinLinc client Documentation Documentation Kebnekaise Remote desktop website Documentation Documentation LUMI SSH Documentation Documentation Rackham SSH Documentation Documentation Rackham Local ThinLinc client Documentation Documentation Rackham Remote desktop website Documentation Documentation Tetralith SSH Documentation Documentation Tetralith Local ThinLinc client Documentation, at 'Running graphical applications using ThinLinc' Documentation","tags":["login","log in","NAISS","local","cluster","HPC"]},{"location":"naiss/login_alvis/","title":"Login Alvis","text":""},{"location":"naiss/login_alvis/#login-alvis","title":"Login Alvis","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>NAISS HPC cluster have different ways to login.</p> <p>This page is about how to log in to Alvis.</p> <pre><code>ssh [chambers_id]@[login_cluster].c3se.chalmers.se\n</code></pre> <p>For example:</p> <ul> <li><code>ssh svens@alvis1.c3se.chalmers.se</code></li> <li><code>ssh svens@alvis2.c3se.chalmers.se</code></li> </ul>"},{"location":"naiss/login_alvis/#videos","title":"Videos","text":"<ul> <li>Using SSH</li> </ul>"},{"location":"naiss/login_cosmos/","title":"Login COSMOS","text":""},{"location":"naiss/login_cosmos/#login-cosmos","title":"Login COSMOS","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>NAISS HPC cluster have different ways to login.</p> <p>This page is about how to log in to COSMOS.</p>"},{"location":"naiss/login_cosmos/#videos","title":"Videos","text":"<ul> <li>Using SSH, including password and Pocket Pass reset</li> <li>Using a local ThinLinc client</li> </ul>"},{"location":"naiss/login_kebnekaise/","title":"Login Kebnekaise","text":""},{"location":"naiss/login_kebnekaise/#login-kebnekaise","title":"Login Kebnekaise","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>NAISS HPC cluster have different ways to login.</p> <p>This page is about how to log in to Kebnekaise.</p>"},{"location":"naiss/login_kebnekaise/#videos","title":"Videos","text":"<ul> <li>Using SSH and a password</li> <li>Using a website to access the remote desktop environment</li> <li>Using a local ThinLinc client to access the remote desktop environment</li> </ul>"},{"location":"naiss/lumi_file_transfer_using_filezilla/","title":"File transfer to/from LUMI using FileZilla","text":"","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#file-transfer-tofrom-lumi-using-filezilla","title":"File transfer to/from LUMI using FileZilla","text":"Why is this page at UPPMAX? <p>It was the intention that this guide would be moved to the CSC documentation. However, contacting CSC regarding this, made it clear that CSC does not (yet) intend to document this.</p> <p>HPC clusters have different ways to do file transfer using FileZilla.</p> <p>This page shows how to do so for LUMI.</p> <p></p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"Would you like a video? <p>Watch the YouTube video File transfer to/from LUMI using FileZilla</p> <p>FileZilla is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from LUMI using FileZilla, do the following steps:</p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#1-install-putty-tools","title":"1. Install <code>putty-tools</code>","text":"What if I don't use Linux? <p>Instead of following steps 2 and 3, follow the procedure at the puttygen page, section 'Create SSH key files'.</p> <pre><code>sudo apt install putty-tools\n</code></pre>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#2-create-the-needed-files-in-the-ssh-folder","title":"2. Create the needed files in the <code>.ssh</code> folder","text":"<p>Navigate into the <code>.ssh</code> folder.</p> <pre><code>cd .ssh\n</code></pre>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#3-create-the-ppk-file","title":"3. Create the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -t rsa -b 4096 -C \"[lumi_username]@lumi.csc.fi\" -o [filename].ppk\n</code></pre> <p>For example:</p> <pre><code>puttygen -t rsa -b 4096 -C \"svens@lumi.csc.fi\" -o lumi_filezilla.ppk\n</code></pre>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#4-extract-the-private-ssh-key-from-the-ppk-file","title":"4. Extract the private SSH key from the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -O private-openssh-new [filename].ppk -o [filename]\n</code></pre> <p>For example:</p> <pre><code>puttygen -O private-openssh-new lumi_filezilla.ppk -o lumi_filezilla\n</code></pre>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#5-extract-the-public-ssh-key-from-the-ppk-file","title":"5. Extract the public SSH key from the <code>.ppk</code> file","text":"<p>In the <code>.ssh</code> folder, from a terminal do:</p> <pre><code>puttygen -O public-openssh [filename].ppk -o [filename].pub\n</code></pre> <p>For example:</p> <pre><code>puttygen -O public-openssh lumi_filezilla.ppk -o lumi_filezilla.pub\n</code></pre>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#6-add-the-public-ssh-key-to-sshauthorized_keys","title":"6. Add the public SSH key to <code>~/.ssh/authorized_keys</code>","text":"<p>Copy the public SSH key (in the <code>.pub</code> file) to the <code>~/.ssh/authorized_keys</code> file on LUMI.</p> <p>If that file does not exist yet, create it and set the right permissions as such:</p> <pre><code>mkdir .ssh\ntouch .ssh/authorized_keys\nchmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#7-start-filezilla","title":"7. Start FileZilla","text":"<p>Start FileZilla.</p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#8-start-filezillas-site-manager","title":"8. Start FileZilla's site manager","text":"<p>From the menu, select 'File | Site manager'</p> Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#9-add-a-new-site-in-filezillas-site-manager","title":"9. Add a new site in FileZilla's site manager","text":"<p>In FileZilla's site manager, click 'New site'</p> Where is that? <p>It is here:</p> <p></p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#10-setup-the-site","title":"10. Setup the site","text":"<p>In FileZilla's site manager:</p> <ul> <li>create a name for the site, e.g. <code>LUMI</code>.</li> <li>for that site, use all standards, except:<ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>lumi.csc.fi</code></li> <li>Set user to <code>[username]</code>, e.g. <code>svensson</code></li> <li>Set logon type: Key file</li> <li>Upload the key file at <code>/.ssh/lumi_filezilla.ppk</code>   from you local's computer</li> </ul> </li> </ul> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#11-connect-to-the-site","title":"11. Connect to the site","text":"<p>Click 'Connect'.</p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_filezilla/#12-ready-to-transfer-files","title":"12. Ready to transfer files","text":"<p>Now you can transfer files between your local computer and LUMI.</p> <p></p>","tags":["FileZilla","LUMI","file transfer","data transfer","upload","download"]},{"location":"naiss/lumi_file_transfer_using_rsync/","title":"Data transfer to/from LUMI using rsync","text":"","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#data-transfer-tofrom-lumi-using-rsync","title":"Data transfer to/from LUMI using rsync","text":"Why is this page at UPPMAX? <p>It was the intention that this guide would be moved to the CSC documentation, or that the CSC documentation would be improved.</p> <p>However, CSC refuses to update the documentation, hence this page is the only place where this is documented.</p> <p>There are multiple ways to transfer files to or from LUMI. Here it is described how to do file transfer to/from LUMI using rsync.</p>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#procedure","title":"Procedure","text":"Prefer a video? <p>Watch the YouTube video Data transfer to/from LUMI using rsync.</p>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#2-transfer-files-to-lumi","title":"2. Transfer files to LUMI","text":"<p>You can transfer files to LUMI by:</p> <ul> <li>2a. Transfer individual files to LUMI</li> <li>2b. Transfer all files in a folder to LUMI</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#2a-transfer-individual-files-to-lumi","title":"2a. Transfer individual files to LUMI","text":"<p>On local computer, do:</p> <pre><code>rsync -e \"ssh -i [private_ssh_key_path]\" [my_local_file] [username]@lumi.csc.fi:[target_folder]/.\n</code></pre> <p>where</p> <ul> <li><code>[private_ssh_key_path]</code> is the path to a LUMI SSH key</li> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your file to</li> <li><code>[username]</code> is your CSC username</li> </ul> <p>for example:</p> <pre><code>rsync -e \"ssh -i ~/.ssh/id_rsa_lumi\" local_file.txt sven@lumi.csc.fi:/users/sven/.\n</code></pre>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#2b-transfer-all-files-in-a-folder-to-lumi","title":"2b. Transfer all files in a folder to LUMI","text":"<p>On local computer, do:</p> <pre><code>rsync -e \"ssh -i [private_ssh_key_path]\" --recursive [my_local_folder] [username]@lumi.csc.fi:[target_folder]/.\n</code></pre> <p>where</p> <ul> <li><code>[private_ssh_key_path]</code> is the path to a LUMI SSH key</li> <li><code>[my_local_folder]</code> is the path to your local folder</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your folder into</li> <li><code>[username]</code> is your CSC username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive -e \"ssh -i ~/.ssh/id_rsa_lumi\" my_local_folder/ sven@lumi.csc.fi:/users/sven/.\n</code></pre> <p>If asked, give your CSC password.</p> <p>Note that in <code>rsync</code>, a slash (<code>/</code>) matters:</p> Command Effect <code>rsync --recursive my_folder sven@lumi.csc.fi:/users/sven</code> Will put the files in <code>my_folder</code> in the LUMI home folder <code>rsync --recursive my_folder sven@lumi.csc.fi:/users/sven/</code> Will put the folder <code>my_folder</code> in the LUMI home folder","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#3-transfer-files-from-lumi-to-you-local-computer","title":"3. Transfer files from LUMI to you local computer","text":"<p>You can transfer files from LUMI to your local computer by:</p> <ul> <li>3a. Transfer individual files from LUMI to your local computer</li> <li>3b. Transfer all folders from LUMI to you local computer</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#3a-transfer-individual-files-from-lumi-to-your-local-computer","title":"3a. Transfer individual files from LUMI to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync -e \"ssh -i [private_ssh_key_path]\" [username]@lumi.csc.fi:[path_to_file] .\n</code></pre> <p>where</p> <ul> <li><code>[private_ssh_key_path]</code> is the path to a LUMI SSH key</li> <li><code>[path_to_file]</code> is the path to the file you want to download</li> <li><code>[username]</code> is your CSC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync -e \"ssh -i ~/.ssh/id_rsa_lumi\" sven@lumi.csc.fi:/users/sven/my_file.txt .\n</code></pre> <p>If asked, give your CSC password.</p>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lumi_file_transfer_using_rsync/#3b-transfer-all-folders-from-lumi-to-you-local-computer","title":"3b. Transfer all folders from LUMI to you local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync --recursive -e \"ssh -i [private_ssh_key_path]\" [username]@lumi.csc.fi:/[source_folder] .\n</code></pre> <p>where</p> <ul> <li><code>[private_ssh_key_path]</code> is the path to a LUMI SSH key</li> <li><code>[source_folder]</code> is the path to the folder you want to download</li> <li><code>[username]</code> is your CSC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive -e \"ssh -i ~/.ssh/id_rsa_lumi\" sven@lumi.csc.fi:/users/sven/my_folder .\n</code></pre> <p>If asked, give your CSC password.</p>","tags":["transfer","data transfer","file transfer","rsync","LUMI","CSC"]},{"location":"naiss/lunarc/","title":"LUNARC","text":"","tags":["LUNARC","Link\u00f6ping","HPC","center","centre"]},{"location":"naiss/lunarc/#lunarc","title":"LUNARC","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>NAISS has many HPC centers. LUNARC is one of those.</p> <ul> <li>LUNARC documentation main page</li> <li>LUNARC documentation GitHub repository</li> </ul> <p>Pages to be merged with their documentation:</p> <ul> <li>Log in to COSMOS</li> </ul>","tags":["LUNARC","Link\u00f6ping","HPC","center","centre"]},{"location":"naiss/request_tracker/","title":"Request Tracker","text":"","tags":["RT","Request Tracker"]},{"location":"naiss/request_tracker/#request-tracker","title":"Request Tracker","text":"<p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <p>Request Tracker, commonly abbreviated to 'RT' is the software used by the NAISS ticket system.</p>","tags":["RT","Request Tracker"]},{"location":"naiss/request_tracker/#workflow","title":"Workflow","text":"","tags":["RT","Request Tracker"]},{"location":"naiss/request_tracker/#as-presented","title":"As presented","text":"<p>As presented by Henric Zazzi on 2024-10-03 at the NAISS All-Hands:</p> <pre><code>flowchart TD\n  new_ticket[New ticket]\n  owned_ticket[Owned ticket]\n  stalled_ticket[Stalled ticket]\n  resolved_ticket[Resolved ticket]\n\n  new_ticket --&gt; |time and knowledge| owned_ticket\n  owned_ticket --&gt; |when solution has been sent| resolved_ticket\n  owned_ticket --&gt; |When ticket cannot be solved yet| stalled_ticket\n  stalled_ticket --&gt; |When ticket can be solved| owned_ticket</code></pre>","tags":["RT","Request Tracker"]},{"location":"naiss/request_tracker/#alternative","title":"Alternative","text":"<p>As discussed at the whiteboard discussion:</p> <pre><code>flowchart TD\n  new_ticket[New ticket]\n  owned_ticket[Owned ticket]\n  stalled_ticket[Stalled ticket]\n  resolved_ticket[Resolved ticket]\n\n  new_ticket --&gt; |time and knowledge| owned_ticket\n  owned_ticket --&gt; |When ticket cannot be solved yet| stalled_ticket\n  owned_ticket --&gt; |When use has not confirmed the ticket is solved yet| stalled_ticket\n  stalled_ticket --&gt; |When ticket can be solved| owned_ticket\n  stalled_ticket --&gt; |When the user has confirmed the ticket has been solved| resolved_ticket</code></pre>","tags":["RT","Request Tracker"]},{"location":"naiss/rstudio_on_tetralith/","title":"RStudio on Tetralith","text":"","tags":["RStudio","Tetralith"]},{"location":"naiss/rstudio_on_tetralith/#rstudio-on-tetralith","title":"RStudio on Tetralith","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the C3SE documentation. However, it has not been suggested to be added to their documentaton yet.</p>","tags":["RStudio","Tetralith"]},{"location":"naiss/rstudio_on_tetralith/#introduction","title":"Introduction","text":"<p>RStudio is an IDE specialized for the R programming language.</p> What is an IDE? <p>See the page on IDEs.</p> <p>In this session, we show how to use RStudio on Tetralith, using Tetralith's remote desktop environment.</p> <p>As RStudio is a resource-heavy program, it must be run on an interactive session.</p>","tags":["RStudio","Tetralith"]},{"location":"naiss/rstudio_on_tetralith/#procedure-to-start-rstudio","title":"Procedure to start RStudio","text":"<p>Below is a step-by-step procedure to start RStudio on Tetralith.</p> Prefer a video? <p>This procedure is also demonstrated in <code>TODO</code>.</p>","tags":["RStudio","Tetralith"]},{"location":"naiss/rstudio_on_tetralith/#1-start-a-tetralith-remote-desktop-environment","title":"1. Start a Tetralith remote desktop environment","text":"<p>...</p>","tags":["RStudio","Tetralith"]},{"location":"naiss/rstudio_on_tetralith/#2-start-an-interactive-session","title":"2. Start an interactive session","text":"<p>Within the Tetralith remote desktop environment, start a terminal. Within that terminal, start an interactive session with 2 cores:</p> <pre><code>interactive -A [naiss_project_id] -n 2 -t [duration]\n</code></pre> <p>Where:</p> <ul> <li><code>[naiss_project_id]</code> is your UPPMAX project code</li> <li><code>[duration]</code> is the duration of the interactive session</li> </ul> <p>Resulting in, For example:</p> <pre><code>interactive -A naiss2024-22-310 -n 2 -t 8:00:00\n</code></pre> <p>Why two cores?</p> <p>RStudio is a resource-heavy program. Due to this, we recommend using at least two cores for a more pleasant user experience.</p>","tags":["RStudio","Tetralith"]},{"location":"naiss/rstudio_on_tetralith/#3-load-the-modules-needed","title":"3. Load the modules needed","text":"<p>In the terminal of the interactive session, do:</p> <pre><code># Something like this\nmodule load R/4.3.1 R_packages/4.3.1 RStudio/2023.12.1-402\n</code></pre> How does that look like? <p>Your output will be similar to:</p> <pre><code>[sven@r210 sven]$ module load R/4.3.1 R_packages/4.3.1 RStudio/2023.06.2-561\nR/4.3.1: Nearly all CRAN and BioConductor packages are installed and available by loading\nthe module R_packages/4.3.1\nR_packages/4.3.1: Note that loading some spatial analysis packages, especially geo-related packages, might\nR_packages/4.3.1: require you to load additional modules prior to use. monocle3 is such a package. See\nR_packages/4.3.1: 'module help R_packages/4.3.1'\n\nR_packages/4.3.1: The RStudio packages pane is disabled when loading this module, due to RStudio slowdowns\nR_packages/4.3.1: because there are &gt;20000 available packages. *All packages are still available.*  For\nR_packages/4.3.1: more information and instructions to re-enable the packages pane (not recommended) see\nR_packages/4.3.1: 'module help R_packages/4.3.1'\n\nRStudio/2023.12.1-402: Sandboxing is not enabled for RStudio at UPPMAX. See 'module help RStudio/2023.12.1-402' for more information\n</code></pre>","tags":["RStudio","Tetralith"]},{"location":"naiss/rstudio_on_tetralith/#4-start-rstudio","title":"4. Start RStudio","text":"<p>With the modules loaded, start RStudio from the terminal (on the interactive session):</p> <pre><code>rstudio\n</code></pre> <p>RStudio can be slow to startup, as R has thousands (!) of packages. Additionally, at startup and if enabled, your saved RStudio workspace (with potentially a lot of data!) is read.</p>","tags":["RStudio","Tetralith"]},{"location":"naiss/swestore/","title":"Swestore","text":"","tags":["Swestore","dCache","HPC","center","centre"]},{"location":"naiss/swestore/#swestore","title":"Swestore","text":"<p>Swestore is a NAISS HPC center hosting the storage system called 'dCache'.</p> <p>This is the information from SUPR:</p> <pre><code>dCache is a resource at Swestore. The total capacity allocated in this round is 100 TiB. The round upper limit is 10 TiB.\n\nSwestore is a Research Data Storage Infrastructure, intended for active research data and operated by the National Academic Infrastructure for Supercomputing in Sweden, NAISS,\n\nThe storage resources provided by Swestore are made available for free for academic research funded by VR and Formas through open calls such that the best Swedish research is supported and new research is facilitated.\n\nThe purpose of Swestore allocations, granted by National Allocations Committee (NAC), is to provide large scale data storage for \u201clive\u201d or \u201cworking\u201d research data, also known as active research data.\nSee the documentation at: https://docs.swestore.se\n</code></pre> <p>Times are changing</p> <p>The following information appears at application rounds and in decision mails from first of January 2025:</p> <pre><code>Please note: NAISS can currently only approve storage on dCache at Swestore until 2026-01-01. Storage solutions for non-hot data, such as Swestore, is being investigated in accelerated form by NAISS very early 2025, and we hope to communicate the plan for long-term services before the large allocation rounds in spring 2025 are opened.\n</code></pre>","tags":["Swestore","dCache","HPC","center","centre"]},{"location":"naiss/tetralith/","title":"Tetralith","text":"","tags":["Tetralith","NSC"]},{"location":"naiss/tetralith/#tetralith","title":"Tetralith","text":"<p>Tetralith is an HPC cluster maintained by NSC.</p> <p>Info</p> <p>This page is here temporarily, until its content is moved to a better place.</p> <ul> <li>NSC documentation about setting up 2FA</li> <li>NSC documentation about connecting to its clusters</li> <li>Video: apply for a Tetralith 'Small Compute' project</li> <li>Video: Using SSH and a password</li> <li>Video: Using a local ThinLinc client to access the remote desktop environment</li> </ul>","tags":["Tetralith","NSC"]},{"location":"naiss/tetralith_file_transfer_using_rsync/","title":"File transfer to/from Tetralith using rsync","text":"","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#file-transfer-tofrom-tetralith-using-rsync","title":"File transfer to/from Tetralith using <code>rsync</code>","text":"Why is this page at UPPMAX? <p>It is the intention that this guide is moved to the NSC documentation. However, it has not been suggested to be added to their documentaton yet.</p> <p>HPC clusters have different ways to do file transfer using <code>rsync</code>.</p> <p>This page shows how to do so for Tetralith.</p>","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to Tetralith using rsync.</p> <p><code>rsync</code> is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from Tetralith using <code>rsync</code>, do the following steps:</p>","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#2-transfer-files-to-tetralith","title":"2. Transfer files to Tetralith","text":"<p>You can transfer files to Tetralith by:</p> <ul> <li>2a. Transfer individual files to Tetralith</li> <li>2b. Transfer all files in a folder to Tetralith</li> </ul>","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#2a-transfer-individual-files-to-tetralith","title":"2a. Transfer individual files to Tetralith","text":"<p>On local computer, do:</p> <pre><code>rsync [my_local_file] [username]@tetralith.nsc.liu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your file to</li> <li><code>[username]</code> is your NSC username</li> </ul> <p>for example:</p> <pre><code>rsync my_file.txt x_svesv@tetralith.nsc.liu.se:~\n</code></pre> <p>If asked, give your NSC password.</p>","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#2b-transfer-all-files-in-a-folder-to-tetralith","title":"2b. Transfer all files in a folder to Tetralith","text":"<p>On local computer, do:</p> <pre><code>rsync --recursive my_folder [username]@tetralith.nsc.liu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[target_folder]</code> is the target folder</li> <li><code>[username]</code> is your NSC username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive my_folder x_svesv@tetralith.nsc.liu.se:~/\n</code></pre> <p>If asked, give your NSC password.</p> <p>Note that in <code>rsync</code>, a slash (<code>/</code>) matters:</p> Command Effect <code>rsync --recursive my_folder x_svesv@tetralith.nsc.liu.se:~/</code> Will put the files in <code>my_folder</code> in the Tetralith home folder <code>rsync --recursive my_folder x_svesv@tetralith.nsc.liu.se:~/</code> Will put the folder <code>my_folder</code> in the Tetralith home folder","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#3-transfer-files-from-tetralith-to-you-local-computer","title":"3. Transfer files from Tetralith to you local computer","text":"<p>You can transfer files from Tetralith to your local computer by:</p> <ul> <li>3a. Transfer individual files from Tetralith to your local computer</li> <li>3b. Transfer all folders from Tetralith to you local computer</li> </ul>","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#3a-transfer-individual-files-from-tetralith-to-your-local-computer","title":"3a. Transfer individual files from Tetralith to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync [username]@tetralith.nsc.liu.se:[path_to_file] .\n</code></pre> <p>where</p> <ul> <li><code>[path_to_file]</code> is the path to the file you want to download</li> <li><code>[username]</code> is your NSC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync x_svesv@tetralith.nsc.liu.se:~/my_file.txt .\n</code></pre> <p>If asked, give your NSC password.</p>","tags":["rsync","Tetralith"]},{"location":"naiss/tetralith_file_transfer_using_rsync/#3b-transfer-all-folders-from-tetralith-to-you-local-computer","title":"3b. Transfer all folders from Tetralith to you local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync --recursive [username]@tetralith.nsc.liu.se:/[source_folder] .\n</code></pre> <p>where</p> <ul> <li><code>[source_folder]</code> is the path to the folder you want to download</li> <li><code>[username]</code> is your NSC username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive x_svesv@tetralith.nsc.liu.se:~/my_folder .\n</code></pre> <p>If asked, give your NSC password.</p>","tags":["rsync","Tetralith"]},{"location":"software/allinea-ddt/","title":"Allinea DDT","text":""},{"location":"software/allinea-ddt/#allinea-ddt","title":"Allinea DDT","text":"<p>UPPMAX has many debuggers installed. This page describes Allinea DDT ('Distributed Debugging Tool').</p> <p>UPPMAX has 96 licenses (one license per (MPI) process) that allows you to debug programs running in parallel with up to 6 nodes on 16 cores or any other combination. The licenses are shared between all users that are in active debugging session.</p> <p>To use the graphical user interface use ssh with forwarding or ThinLinc.</p> <p>To use the program load the <code>ddt</code> module from you command line:</p> <pre><code>module load ddt\n</code></pre> <p>To start the program run:</p> <pre><code>ddt\n</code></pre> <p>or</p> <pre><code>ddt ./myprogram\n</code></pre> <p><code>ddt</code> can only do debugging if you have compiled your code with debugging flag options.</p>"},{"location":"software/allinea-ddt/#debugging-multithreaded-programs","title":"Debugging Multithreaded programs","text":"<p>Start an interactive job with multiple cores (e.g. <code>interactive -p core -n 20 -A snicXYZ -t 04:00:00</code>) before starting DDT. In the run window, select the OpenMP box. You can change the number of OpenMP threads directly in the DDT window before running.</p>"},{"location":"software/allinea-ddt/#debugging-mpi-programs","title":"Debugging MPI programs","text":"<p>To be able to debug MPI program select MPI option as well as the 'Submit to Queue' option, and then click on 'Change' to select submission script configuration for Rackham and provide the job specific options:</p> <ul> <li>'System | MPI/UPC Implementation | check Auto-Detect'</li> <li>'System | MPI/UPC Implementation | tick Create Root and Workers group automatically'</li> <li>Select a template file depending on the partition you want to use:<ul> <li><code>core</code>: 'Job Submission | Submission template file | Browse and select <code>/sw/comp/ddt/7.0/templates/rackham-core.qtf</code></li> <li><code>node</code>: 'Job Submission | Submission template file | Browse and select <code>/sw/comp/ddt/7.0/templates/rackham-node.qtf</code></li> <li><code>devcore</code>: 'Job Submission | Submission template file | Browse and select <code>/sw/comp/ddt/7.0/templates/rackham-node.qtf</code></li> </ul> </li> <li>'Job Submission | tick Quick Restart':   allows you to restart your program without cancelling   the allocated time and allocating it again.</li> <li>Edit Queue Submission Parameters to specify Partition, Project and requested time.   Failing to provide project number will cause failures in the submission process</li> </ul> <p>On the main configuration window the button \"Run\" will change to \"Submit\". Click on this button to submit your debugging session to the queue manager.</p> <p>If you enable \"Memory debugging\", click the \"Details\" button and tick 'Preload the memory debugging library' and select \"C/Fortran threads\" in the \"Language:\" field. Read the manual for more detail on the other options in this panel.</p>"},{"location":"software/allinea-ddt/#links","title":"Links","text":"<ul> <li>DDT home page (formerly: Allinea, now Linaroforge)</li> </ul>"},{"location":"software/bash/","title":"bash","text":""},{"location":"software/bash/#bash","title":"bash","text":"<p>Bash is the default Unix shell, a command-line interpreter and script host that provides a traditional user interface for the linux operating system at UPPMAX. Users direct the operation of the computer by entering command input as text for a command line interpreter to execute or by creating text scripts of one or more such commands.</p>"},{"location":"software/bash/#special-bash-files","title":"Special bash files","text":"<ul> <li><code>.bash_profile</code>: is run whenever you login or when you start a login   shell (as in starting a job in the queue).</li> <li><code>.bashrc</code>: is run when an interactive shell that is not a login shell   is started, or if it is called from the <code>.bash_profile</code>   (as it is in the default configuration).</li> <li><code>.bash_logout</code>: is run when you log out.</li> </ul>"},{"location":"software/beast/","title":"BEAST","text":""},{"location":"software/beast/#beast","title":"BEAST","text":"<p>BEAST is a tool for Bayesian phylogenetic analysis.</p> Is BEAST2 a new version of BEAST? <p>No.</p> <p>Although BEAST and BEAST2 achieve a similar goal, BEAST and BEAST2 are developed independently.</p> <p>Hence,</p> <ul> <li>there are things BEAST can do that BEAST2 cannot, and vice versa</li> <li>one cannot create a BEAST XML file and expect BEAST2 to be able to run it, and vice versa</li> </ul>"},{"location":"software/beast/#run-tracer","title":"Run Tracer","text":"<p>Tracer is a tool to analyse the results of a BEAST (or BEAST2) run.</p> <p>See Tracer how to use Tracer.</p> <p></p>"},{"location":"software/beast2/","title":"BEAST2","text":""},{"location":"software/beast2/#beast2","title":"BEAST2","text":"<p>BEAST2 is a tool for Bayesian phylogenetic analysis.</p> Is BEAST2 a new version of BEAST? <p>No.</p> <p>Although BEAST and BEAST2 achieve a similar goal, BEAST and BEAST2 are developed independently.</p> <p>Hence:</p> <ul> <li>there are things BEAST can do that BEAST2 cannot, and vice versa</li> <li>one cannot create a BEAST XML file and expect BEAST2 to be able to run it, and vice versa</li> </ul>"},{"location":"software/beast2/#using-beast2","title":"Using BEAST2","text":"<p>Here is how to use BEAST2 on the UPPMAX clusters.</p> Prefer a video? <p>This YouTube video shows how to use BEAST2 on the UPPMAX clusters.</p>"},{"location":"software/beast2/#1-load-a-beast2-module","title":"1. Load a <code>beast2</code> module","text":"<p>First step is to load a BEAST2 module.</p> <p>Here is how to find the BEAST2 versions on the UPPMAX clusters:</p> <pre><code>module spider beast2\n</code></pre> <p>When loading a BEAST2 module, also load <code>bioinfo-tools</code>:</p> <pre><code>module load bioinfo-tools beast2/2.7.4\n</code></pre> How does that look like? <pre><code>$ module load bioinfo-tools beast2/2.7.4\nbeast2/2.7.4: Also loaded beagle/4.0.0\nbeast2/2.7.4: Many Beast packages are available, to see the list, 'packagemanager -list'\nbeast2/2.7.4: Use BEAST_XMX to specify the amount of RAM (default 5g), 'export BEAST_XMX=15g'. Do not exceed RAM available to your job.\n</code></pre>"},{"location":"software/beast2/#2-run-beauti","title":"2. Run <code>BEAUti</code>","text":"<p>Next step is to create a BEAST2 configuration file using <code>BEAUti</code>. This graphical tool can be started using:</p> <pre><code>beauti\n</code></pre> <p>As <code>BEAUti</code> is a graphical program, it needs SSH with X forwarding enabled enabled.</p> How does that look like? <p>Starting <code>BEAUti</code> results in the following pop-up window:</p> <p></p> <p>After using <code>BEAUti</code>, save the file with your BEAST2 model.</p>"},{"location":"software/beast2/#3-run","title":"3. Run","text":"<p>A BEAST2 run takes a lot of computing power, hence do not run it on a login node. Instead, run it on an interactive session or use a script.</p> How to start an interactive session? <p>View the UPPMAX documentation 'How to start an interactive session on Rackham'.</p> <p>On an interactive session, run BEAST2 on the saved BEAST2 model:</p> <pre><code>beast beast2_setup.xml\n</code></pre> <p>When using a script, put that line in a script. Below is an example script, called <code>run_beast2.sh</code>:</p> run_beast2.sh<pre><code>#!/bin/bash\n#SBATCH -A uppmax2023-2-25\nmodule load bioinfo-tools beast2/2.7.4\nbeast beast2_setup.xml\n</code></pre> <ul> <li>In line 2, replace <code>uppmax2023-2-25</code> with your UPPMAX project.</li> <li>In line 3, you may want to replace <code>beast2/2.7.4</code> with your favorite BEAST2 version</li> </ul> <p>Then run this script using <code>sbatch run_beast2.sh</code>.</p> <p>Note that this is a minimal script. See the UPPMAX documentation on Slurm for ways to improve this script.</p>"},{"location":"software/beast2/#view-the-trees-using-densitree","title":"View the trees using DensiTree","text":"<p>DensiTree is a tool that allows one to display the posterior tree distribution of a BEAST2 run.</p> <p>Run:</p> <pre><code>densitree [trees_filename]\n</code></pre> <p>where <code>[trees_filename]</code> is the name of the file containing the posterior trees, resulting in, for example, <code>densitree my_file.trees</code>.</p> <p></p>"},{"location":"software/beast2/#run-tracer","title":"Run Tracer","text":"<p>Tracer is a tool to analyse the results of a (BEAST or) BEAST2 run.</p> <p>See Tracer how to use Tracer.</p> <p></p>"},{"location":"software/beast2/#show-info","title":"Show info","text":"<pre><code>beast -beagle_info\n</code></pre> How does that look like? <p>Here the command is run on a Rackham compute node, using an interactive session.</p> <p>Here an interactive session with 1 node:</p> <pre><code>interactive -A uppmax2023-2-25 -M snowy -N 1 -n 16 --exclusive -t 1-00:00:00\n</code></pre> <pre><code>[sven@s93 ~]$ beast -beagle_info\n\n                        BEAST v2.7.4, 2002-2023\n             Bayesian Evolutionary Analysis Sampling Trees\n                       Designed and developed by\n Remco Bouckaert, Alexei J. Drummond, Andrew Rambaut &amp; Marc A. Suchard\n\n                   Centre for Computational Evolution\n                         University of Auckland\n                       r.bouckaert@auckland.ac.nz\n                        alexei@cs.auckland.ac.nz\n\n                   Institute of Evolutionary Biology\n                        University of Edinburgh\n                           a.rambaut@ed.ac.uk\n\n                    David Geffen School of Medicine\n                 University of California, Los Angeles\n                           msuchard@ucla.edu\n\n                      Downloads, Help &amp; Resources:\n                           http://beast2.org/\n\n  Source code distributed under the GNU Lesser General Public License:\n                   http://github.com/CompEvol/beast2\n\n                           BEAST developers:\n   Alex Alekseyenko, Trevor Bedford, Erik Bloomquist, Joseph Heled,\n Sebastian Hoehna, Denise Kuehnert, Philippe Lemey, Wai Lok Sibon Li,\nGerton Lunter, Sidney Markowitz, Vladimir Minin, Michael Defoin Platel,\n          Oliver Pybus, Tim Vaughan, Chieh-Hsi Wu, Walter Xie\n\n                               Thanks to:\n          Roald Forsberg, Beth Shapiro and Korbinian Strimmer\n\n\n--- BEAGLE RESOURCES ---\n\n0 : CPU (x86_64)\n    Flags: PRECISION_SINGLE PRECISION_DOUBLE COMPUTATION_SYNCH EIGEN_REAL EIGEN_COMPLEX SCALING_MANUAL SCALING_AUTO SCALING_ALWAYS SCALERS_RAW SCALERS_LOG VECTOR_SSE VECTOR_NONE THREADING_CPP THREADING_NONE PROCESSOR_CPU FRAMEWORK_CPU\n</code></pre> <p>Here an interactive session with 2 nodes:</p> <pre><code>interactive -A uppmax2023-2-25 -M snowy -N 2 -n 32 --exclusive -t 1-00:00:00\n</code></pre> <pre><code>[sven@s106 ~]$ beast -beagle_info\n\n                        BEAST v2.7.4, 2002-2023\n             Bayesian Evolutionary Analysis Sampling Trees\n                       Designed and developed by\n Remco Bouckaert, Alexei J. Drummond, Andrew Rambaut &amp; Marc A. Suchard\n\n                   Centre for Computational Evolution\n                         University of Auckland\n                       r.bouckaert@auckland.ac.nz\n                        alexei@cs.auckland.ac.nz\n\n                   Institute of Evolutionary Biology\n                        University of Edinburgh\n                           a.rambaut@ed.ac.uk\n\n                    David Geffen School of Medicine\n                 University of California, Los Angeles\n                           msuchard@ucla.edu\n\n                      Downloads, Help &amp; Resources:\n                           http://beast2.org/\n\n  Source code distributed under the GNU Lesser General Public License:\n                   http://github.com/CompEvol/beast2\n\n                           BEAST developers:\n   Alex Alekseyenko, Trevor Bedford, Erik Bloomquist, Joseph Heled,\n Sebastian Hoehna, Denise Kuehnert, Philippe Lemey, Wai Lok Sibon Li,\nGerton Lunter, Sidney Markowitz, Vladimir Minin, Michael Defoin Platel,\n          Oliver Pybus, Tim Vaughan, Chieh-Hsi Wu, Walter Xie\n\n                               Thanks to:\n          Roald Forsberg, Beth Shapiro and Korbinian Strimmer\n\n\n--- BEAGLE RESOURCES ---\n\n0 : CPU (x86_64)\n    Flags: PRECISION_SINGLE PRECISION_DOUBLE COMPUTATION_SYNCH EIGEN_REAL EIGEN_COMPLEX SCALING_MANUAL SCALING_AUTO SCALING_ALWAYS SCALERS_RAW SCALERS_LOG VECTOR_SSE VECTOR_NONE THREADING_CPP THREADING_NONE PROCESSOR_CPU FRAMEWORK_CPU\n</code></pre>"},{"location":"software/beast2/#troubleshooting","title":"Troubleshooting","text":""},{"location":"software/beast2/#beauti-gives-badalloc","title":"BEAUti gives <code>BadAlloc</code>","text":"<ul> <li>Platform(s): MacOS</li> </ul> <p>This problem seems to be related to not having a proper X server installed. In this case, SSH X forwarding works to the extent that SSH is able to show <code>xeyes</code>, yet fails to show BEAUti. Also, using the remote desktop via a ThinLinc client fails.</p> <p>A solution may be to use the remote desktop via the web</p> How does that look like? <p>Here is how it looks like:</p> <pre><code>[kayakhi@rackham2 ~]$ xeyes\n\n[kayakhi@rackham2 ~]$ module load bioinfo-tools beast2/2.7.4\n\nbeast2/2.7.4: Also loaded beagle/4.0.0\n\nbeast2/2.7.4: Many Beast packages are available, to see the list, 'packagemanager -list'\n\nbeast2/2.7.4: Use BEAST_XMX to specify the amount of RAM (default 5g), 'export BEAST_XMX=15g'. Do not exceed RAM available to your job.\n\n[kayakhi@rackham2 ~]$ beauti\n\nX Error of failed request:  BadAlloc (insufficient resources for operation)\n\n  Major opcode of failed request:  149 (GLX)\n\n  Minor opcode of failed request:  5 (X_GLXMakeCurrent)\n\n  Serial number of failed request:  0\n\n  Current serial number in output stream:  32\n</code></pre> <p>Note that this user has enabled SSH X forwarding, as is proven by calling <code>xeyes</code> without problems.</p>"},{"location":"software/beast2/#optimize-performance","title":"Optimize performance","text":"<ul> <li>BEAST2 performance suggestions</li> <li>BEAST2 and BEAGLE</li> </ul>"},{"location":"software/beast2/#links","title":"Links","text":"<ul> <li>DensiTree GitHub repository</li> <li>CIPRES: cyberinfrastructure for phylogenetics research</li> </ul>"},{"location":"software/bianca_file_transfer_using_filezilla/","title":"File transfer to/from Bianca using FileZilla","text":"","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#file-transfer-tofrom-bianca-using-filezilla","title":"File transfer to/from Bianca using FileZilla","text":"<p>FileZilla connected to Bianca</p> <p>There are multiple ways to transfer data to/from Bianca.</p> <p>Here, we show how to transfer files using a graphical tool called \"FileZilla\". See the UPPMAX page on 'FileZilla' on how to download and install it.</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to Bianca using FileZilla.</p> <p>To transfer files to/from Bianca using FileZilla, do the following steps:</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>Check how to get inside university networks : Get inside of SUNET. If you are part of a Swedish university, you can use the VPN provided by your university.</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#2-start-filezilla","title":"2. Start FileZilla","text":"<p>Open FileZilla application.</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#3-select-file-site-manager","title":"3. Select 'File | Site manager'","text":"<p>In FileZilla, from the menu, select 'File | Site manager'.</p> Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#4-click-new-site","title":"4. Click 'New site'","text":"<p>In the 'Site Manager' dialog, click 'New site'</p> Where is that? <p>It is here:</p> <p></p> <p>'New site' can be found at the bottom-left</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#5-create-a-name-for-the-site","title":"5. Create a name for the site","text":"<p>In the 'New Site' dialog, create a name for the site, e.g. <code>bianca-sens2023613</code>.</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#6-configure-site","title":"6. Configure site","text":"<p>In the 'New Site' dialog, make the following changes:</p> <ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>bianca-sftp.uppmax.uu.se</code></li> <li>Set Logon Type to <code>Ask for password</code></li> <li>Set user to <code>[username]-[project]</code>, e.g. <code>sven-sens2023613</code></li> </ul> How does that look like? <p>It looks similar to these:</p> <p></p> <p></p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#7-click-connect","title":"7. Click 'Connect'","text":"<p>In FileZilla, click 'Connect'</p> <p>You will be asked for your password. Enter your UPPMAX password and two-factor authentication code together. Hence type <code>[your password][2FA code]</code>, e.g. <code>MyPassword123456</code>, where <code>MyPassword</code> is your UPPMAX password and <code>123456</code> is your 6 digit UPPMAX two-factor authentication code.</p> <p>Uncheck \"Remember password until FileZilla is closed\".</p> <p>Remembering/storing a password is useless</p> <p>Because Bianca holds sensitive data, there is need to use the UPPMAX two-factor authentication code that changes every 30 seconds, every time you login. Due to this, storing a password is useless.</p> <p>Now you can transfer files between your local computer and your <code>wharf</code> folder.</p> How does that look like? <p>It looks like this:</p> <p></p> <p>FileZilla is ready to transfer files</p> <p>NOTE: Filezilla will ask for your password and two-factor for each file you transfer. To avoid that, go to Site Manager &gt; Transfer Settings &gt; Limit number of simultaneous connections to 1.</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#troubleshooting","title":"Troubleshooting","text":"","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_filezilla/#access-denied","title":"Access denied","text":"<p>Full error, in the FileZilla terminal:</p> <pre><code>Status: Connecting to bianca-sftp.uppmax.uu.se...\n\nStatus: Using username \"sven-sens2023613\".\n\nStatus: Access denied\n\nError: Authentication failed.\n\nError: Critical error: Could not connect to server\n</code></pre> <p>Hypotheses:</p> <ul> <li>The user is not within SUNET</li> </ul> How do I know if I am within the university networks? <p>If you login via <code>eduroam</code> you are within the university networks.</p> <p>When unsure, go to the Bianca remote desktop website at https://bianca.uppmax.uu.se: if this page does not load, you are outside of the university networks.</p> <p>See How to get inside of the university networks if you outside of the university networks.</p> <ul> <li>The account is not active</li> </ul> How do I know if the Bianca project is active? <p>A quick way to confirm your Bianca project is active: go to https://bianca.uppmax.uu.se and type your username. If the project is displayed, it is active.</p> <p>To confirm your project is active or inactive, use the SUPR NAISS website. See the UPPMAX documentation on projects how to see if your project is active?</p> <ul> <li>The user is not a member of the Bianca project</li> </ul> How do I know if I am a member of the Bianca project? <p>A quick way to confirm you are a member of the Bianca project: go to https://bianca.uppmax.uu.se and type your username. If the project is displayed, you are a member of the Bianca project.</p> <p>To confirm your project is active or inactive, use the SUPR NAISS website. See the UPPMAX documentation on projects how to see which projects you are a member of.</p> <p>See the UPPMAX page on contacting support on how to contact us.</p>","tags":["FileZilla","Bianca","data transfer","file transfer","transfer","visual","graphical","GUI"]},{"location":"software/bianca_file_transfer_using_gui/","title":"File transfer to/from Bianca using a graphical tool","text":""},{"location":"software/bianca_file_transfer_using_gui/#file-transfer-tofrom-bianca-using-a-graphical-tool","title":"File transfer to/from Bianca using a graphical tool","text":"<p>FileZilla connected to Bianca</p>"},{"location":"software/bianca_file_transfer_using_gui/#overview","title":"Overview","text":"<p>As a user, we need to transfer files between our local computer and Bianca. There are many ways to transfer files to/from Bianca. On this page, we learn how to transfer files to Bianca using a graphical tool/program.</p> <p>There are constraints on which programs we can use, due to Bianca being an HPC cluster for sensitive data. Details are described in 'Bianca's constraints', here are graphical tools that work:</p> Tool Description FileZilla All operating systems WinSCP Windows-only <p>When using such a graphical tool, one needs to be inside of SUNET.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p> <p>When a tool is setup, one can only transfer files between you local computer and your Bianca <code>wharf</code> folder.</p>"},{"location":"software/bianca_file_transfer_using_gui/#biancas-constraints","title":"Bianca's constraints","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#faf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#aaf,color:#000,stroke:#00f\n\n    subgraph sub_inside[IP inside SUNET]\n      subgraph sub_bianca_shared_env[Bianca shared network]\n        subgraph sub_bianca_private_env[The project's private virtual project cluster]\n          login_node(login/calculation/interactive session):::calculation_node\n          files_in_wharf(Files in wharf):::file_node\n          files_in_bianca_project(Files in Bianca project folder):::file_node\n        end\n      end\n      user(User)\n      user_local_files(Files on user computer):::file_node\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#fcc,color:#000,stroke:#fcc\n    style sub_bianca_shared_env fill:#ffc,color:#000,stroke:#ffc\n    style sub_bianca_private_env fill:#cfc,color:#000,stroke:#cfc\n\n    user --&gt; |logs in |login_node\n    user --&gt; |uses| user_local_files\n\n    %% As of 2023-12-22, using `**text**` for bold face, does not render correctly\n    %% user_local_files &lt;== \"`**transfer files**`\" ==&gt; files_in_wharf\n    user_local_files &lt;== \"transfer files\" ==&gt; files_in_wharf\n\n    login_node --&gt; |can use|files_in_bianca_project\n    login_node --&gt; |can use|files_in_wharf\n    files_in_wharf &lt;--&gt; |transfer files| files_in_bianca_project</code></pre> <p>Overview of file transfer on Bianca, when using a graphical tool. The purple nodes are about file transfer, the blue nodes are about 'doing other things'. In this session, we will transfer files between 'Files on user computer' and 'Files in wharf' using a graphical tool, e.g. FileZilla</p> <p>Bianca is an HPC cluster for sensitive data. To protect that sensitive data, Bianca has no direct internet connection. This means that files cannot be downloaded directly.</p> What is an HPC cluster again? <p>See the UPPMAX page on HPC clusters.</p> <p>Instead, one needs to learn one of the many ways to do secure file transfer.</p> <p>Here, we show how to transfer files using a graphical tool called FileZilla.</p> <p>In general, one can pick any graphical tools with these constraints:</p> <ul> <li>the tool must support SFTP</li> <li>the tool must not store a password</li> </ul> <p>Whatever tool one picks, it must do secure file transfer. For secure file transfer, Bianca supports the SFTP protocol. So, for secure file transfer to Bianca, one needs a tool that supports SFTP.</p> Use SFTP ... and why users think incorrectly that SCP will work <p>Only SFTP will work. SCP will never work.</p> <p>However, some users use tools that support SFTP, yet that have 'SCP' in the name, for example, 'WinSCP'. As users hear from colleagues that the tool 'WinSCP' works, they may incorrectly conclude that SCP will work.</p> <p>SCP will never work. Only SFTP will work.</p> <p>Whatever tool one picks, additionally, the tool must not store a password. Due to security reasons, one needs to connect to Bianca using a password and a two-factor authentication number (e.g. <code>VerySecret123456</code>). If a tool stores a password, that password will be valid for only one session.</p> <p>One tool that can be used for file transfer to Bianca is FileZilla, which is described in detail below. The extra materials at the bottom of this page contain other tools.</p>"},{"location":"software/bianca_file_transfer_using_gui/#file-transfer-overview","title":"File transfer overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[IP inside SUNET]\n      subgraph sub_bianca_shared_env[Bianca shared network]\n        subgraph sub_bianca_private_env[The project's private virtual project cluster]\n          login_node(login/calculation/interactive session):::calculation_node\n          files_in_wharf(Files in wharf):::file_node\n          files_in_bianca_project(Files in Bianca project folder):::file_node\n        end\n      end\n      user(User)\n      user_local_files(Files on user computer):::file_node\n      files_on_transit(Files posted to Transit):::transit_node\n      files_on_other_clusters(Files on other HPC clusters):::file_node\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#fcc,color:#000,stroke:#fcc\n    style sub_bianca_shared_env fill:#ffc,color:#000,stroke:#ffc\n    style sub_bianca_private_env fill:#cfc,color:#000,stroke:#cfc\n\n    user --&gt; |logs in |login_node\n    user --&gt; |uses| user_local_files\n    user_local_files &lt;--&gt; |transfer files|files_in_wharf\n    user_local_files &lt;--&gt; |transfer files|files_on_transit\n    files_on_transit &lt;--&gt; |transfer files|files_in_wharf\n    files_on_transit &lt;--&gt; |transfer files|files_on_other_clusters\n    login_node --&gt; |can use|files_in_bianca_project\n    login_node --&gt; |can use|files_in_wharf\n    files_in_wharf &lt;--&gt; |transfer files| files_in_bianca_project</code></pre> <p>Overview of file transfer on Bianca The purple nodes are about file transfer, the blue nodes are about 'doing other things'.</p>"},{"location":"software/bianca_file_transfer_using_lftp/","title":"Using lftp with Bianca","text":"","tags":["lftp","Bianca","data transfer","file transfer","transfer","terminal","command-line"]},{"location":"software/bianca_file_transfer_using_lftp/#using-lftp-with-bianca","title":"Using <code>lftp</code> with Bianca","text":"<p><code>lftp</code> is a command-line program to transfer files to/from Bianca.</p> <p>Here, the procedure for file transfer is shown, as well as some troubleshooting.</p>","tags":["lftp","Bianca","data transfer","file transfer","transfer","terminal","command-line"]},{"location":"software/bianca_file_transfer_using_lftp/#procedure","title":"Procedure","text":"","tags":["lftp","Bianca","data transfer","file transfer","transfer","terminal","command-line"]},{"location":"software/bianca_file_transfer_using_lftp/#1-get-inside-of-sunet","title":"1. Get inside of SUNET","text":"<p>Get inside of SUNET.</p>","tags":["lftp","Bianca","data transfer","file transfer","transfer","terminal","command-line"]},{"location":"software/bianca_file_transfer_using_lftp/#2-start-lftp","title":"2. Start <code>lftp</code>","text":"<pre><code>lftp sftp://[user_name]-[project_id]@bianca-sftp.uppmax.uu.se/[user_name]-[project_id]/\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[user_name]</code> is the name of your UPPMAX user account</li> </ul> <p>For example:</p> <pre><code>lftp sftp://sven-sens2025560@bianca-sftp.uppmax.uu.se/sven-sens2025560/\n</code></pre> <p>You'll be asked to type a password.</p>","tags":["lftp","Bianca","data transfer","file transfer","transfer","terminal","command-line"]},{"location":"software/bianca_file_transfer_using_lftp/#3-enter-password-with-totp","title":"3. Enter password with TOTP","text":"<p>When asked for a password, type the password followed with a one-time password from your UPPMAX two-factor authenticator.</p> I don't see asterisks appear when I type <p>Well spotted!</p> <p>Indeed, you will not see the characters you type, which is common for Linux systems.</p> <p>Just type the password and TOTP and press enter :-)</p> <p>For example, if your password is <code>VerySecret</code> and the 2FA app gives you <code>123456</code>, then type <code>VerySecret123456</code>.</p>","tags":["lftp","Bianca","data transfer","file transfer","transfer","terminal","command-line"]},{"location":"software/bianca_file_transfer_using_lftp/#troubleshooting","title":"Troubleshooting","text":"<p>You need to \"set net:connection_limit 1\". <code>lftp</code> may also defer the actual connection until it's really required unless you end your connect URL with a path.</p>","tags":["lftp","Bianca","data transfer","file transfer","transfer","terminal","command-line"]},{"location":"software/bianca_file_transfer_using_rsync/","title":"File transfer to/from Bianca using rsync","text":"","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#file-transfer-tofrom-bianca-using-rsync","title":"File transfer to/from Bianca using rsync","text":"<p>rsync is a tool to do file transfer to/from Bianca, that works under Linux, Mac and Windows.</p> Prefer a video? <p>Watch this video to see the procedure below as a video.</p> <p>To transfer files to/from Bianca using rsync, do the following steps:</p> <pre><code>flowchart TD\n  local_computer_ourside_sunet[Local computer outside of SUNET]\n  local_computer[Local computer]\n  transit[Transit]\n  bianca[Bianca]\n  local_computer_ourside_sunet --&gt; |1.Get inside SUNET|local_computer\n  local_computer --&gt; |2.login| transit\n  local_computer --&gt; |4.rsync| bianca\n  bianca --&gt; |5.rsync| local_computer\n  transit --&gt; |3.mount| bianca</code></pre>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>Get inside SUNET.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#2-log-in-to-transit","title":"2. Log in to Transit","text":"<p>On your local computer, start a terminal and use <code>ssh</code> to login to Transit:</p> <pre><code>ssh [username]@transit.uppmax.uu.se\n</code></pre> <p>where</p> <ul> <li><code>[username]</code> is your UPPMAX username</li> </ul> <p>For example:</p> <pre><code>ssh sven@transit.uppmax.uu.se\n</code></pre> <p>When asked for a password, use your UPPMAX password (without 2FA).</p> <p>See Log in to transit for more details on how to log in to Transit.</p>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#3-mount-a-bianca-project","title":"3. Mount a Bianca project","text":"<p>On transit, mount the wharf of your Bianca project:</p> <pre><code>mount_wharf [project_id]\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> </ul> What about the <code>[path]</code> argument? <p>Well spotted!</p> <p>Indeed, the Transit server gives these arguments:</p> <pre><code>mount_wharf [project_id] [path]\n</code></pre> <p>However, the <code>[path]</code> argument is optional: if not given, a default will be used.</p> <p>To simplify matters, here we use the default.</p> <p>for example:</p> <pre><code>mount_wharf sens2016001\n</code></pre> <p>The password is your normal UPPMAX password directly followed by the six digits from the the <code>UPPMAX</code> 2-factor authentication. For example, if your password is <code>VerySecret</code> and the second factor code is <code>123456</code> you would type <code>VerySecret123456</code> as the password in this step.</p> <p>Now a folder called <code>sens2016001</code> is created.</p>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#4-transfer-files-to-bianca","title":"4. Transfer files to Bianca","text":"<p>You can transfer files to Bianca by:</p> <ul> <li>4a. Transfer individual files to Bianca</li> <li>4b. Transfer all files in a folder to Bianca</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#4a-transfer-individual-files-to-bianca","title":"4a. Transfer individual files to Bianca","text":"<p>On local computer, do:</p> <pre><code>rsync [my_local_file] [username]@transit.uppmax.uu.se:[project_id]\n</code></pre> <p>where</p> <ul> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[username]</code> is your UPPMAX username</li> </ul> <p>for example:</p> <pre><code>rsync my_local_file.txt sven@transit.uppmax.uu.se:sens2016001\n</code></pre> <p>No need to specify the path to the mounted folder, if defaults are used.</p> <p>The files can now be found in your wharf folder.</p>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#4b-transfer-all-files-in-a-folder-to-bianca","title":"4b. Transfer all files in a folder to Bianca","text":"<p>On local computer, do:</p> <pre><code>rsync --recursive my_folder [username]@transit.uppmax.uu.se:[project_id]\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[username]</code> is your UPPMAX username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive my_folder sven@transit.uppmax.uu.se:sens2016001\n</code></pre> <p>No need to specify the path to the mounted folder, if defaults are used.</p> <p>The files can now be found in your wharf folder.</p>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#5-transfer-files-from-bianca-to-you-local-computer","title":"5. Transfer files from Bianca to you local computer","text":"<p>Be responsible with sensitive data</p> <p>This command below will copy data from Bianca to your local computer.</p> <p>You can transfer files from Bianca to your local computer by:</p> <ul> <li>5a. Transfer individual files from Bianca to your local computer</li> <li>5b. Transfer all folders from Bianca to you local computer</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#5a-transfer-individual-files-from-bianca-to-your-local-computer","title":"5a. Transfer individual files from Bianca to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync [username]@transit.uppmax.uu.se:[project_id]/[file_in_wharf] .\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[username]</code> is your UPPMAX username</li> <li><code>[file_in_wharf]</code> is the name of the file in <code>wharf</code></li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync sven@transit.uppmax.uu.se:sens2016001/my_file_in_wharf.txt .\n</code></pre> <p>To copy the individual files in your wharf to your local computer.</p>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_rsync/#5b-transfer-all-folders-from-bianca-to-you-local-computer","title":"5b. Transfer all folders from Bianca to you local computer","text":"<p>This will copy all folders in your wharf</p> <p>This command below will copy all folders in your wharf folder to your local computer.</p> <p>This assumes that there is few data in your wharf folder.</p> <p>We assume your follow good wharf hygiene, i.e. your wharf folder is mostly empty most of the time.</p> <p>On your local computer, do:</p> <pre><code>rsync --recursive [username]@transit.uppmax.uu.se:[project_id] .\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[username]</code> is your UPPMAX username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive sven@transit.uppmax.uu.se:sens2016001 .\n</code></pre> <p>To your wharf folder to your local computer. The folder created on your local computer will be called <code>[project_id]</code>, for example, <code>sens2016001</code>.</p>","tags":["transfer","data transfer","file transfer","rsync","Bianca"]},{"location":"software/bianca_file_transfer_using_sftp/","title":"Using sftp with Bianca","text":"","tags":["sftp","SFTP","Bianca","data transfer","file transfer","transfer","terminal","command-line","console"]},{"location":"software/bianca_file_transfer_using_sftp/#using-sftp-with-bianca","title":"Using <code>sftp</code> with Bianca","text":"<p><code>sftp</code> is a command-line program to transfer files to/from Bianca.</p>","tags":["sftp","SFTP","Bianca","data transfer","file transfer","transfer","terminal","command-line","console"]},{"location":"software/bianca_file_transfer_using_sftp/#procedure","title":"Procedure","text":"Would you enjoy a video? <p>See a video showing how to <code>sftp</code> with Bianca.</p>","tags":["sftp","SFTP","Bianca","data transfer","file transfer","transfer","terminal","command-line","console"]},{"location":"software/bianca_file_transfer_using_sftp/#1-get-inside-of-sunet","title":"1. Get inside of SUNET","text":"<p>Get inside of SUNET.</p> <p>If needed, start the grace period.</p>","tags":["sftp","SFTP","Bianca","data transfer","file transfer","transfer","terminal","command-line","console"]},{"location":"software/bianca_file_transfer_using_sftp/#2-start-sftp","title":"2. Start <code>sftp</code>","text":"<pre><code>sftp [user_name]-[project_id]@bianca-sftp.uppmax.uu.se:/[user_name]-[project_id]\n</code></pre> <p>where</p> <ul> <li><code>[project_id]</code> is the ID of your NAISS project</li> <li><code>[user_name]</code> is the name of your UPPMAX user account</li> </ul> <p>For example:</p> <pre><code>sftp sven-sens2016001@bianca-sftp.uppmax.uu.se:/sven-sens2016001\n</code></pre>","tags":["sftp","SFTP","Bianca","data transfer","file transfer","transfer","terminal","command-line","console"]},{"location":"software/bianca_file_transfer_using_sftp/#3-supply-password","title":"3. Supply password","text":"<p><code>sftp</code> will ask for a password:</p> <pre><code>sven-sens2016001@bianca-sftp.uppmax.uu.se's password:\n</code></pre> <p>The password is your normal UPPMAX password directly followed by the six digits from the the <code>UPPMAX</code> 2-factor authentication. For example, if your password is <code>VerySecret</code> and the second factor code is <code>123456</code> you would type <code>VerySecret123456</code> as the password in this step.</p> <p>After typing in the password and 2FA one sees a welcome message and the <code>sftp</code> prompt.</p> How does that look like? <p>This is the welcome message:</p> <pre><code>Hi!\n\nYou are connected to the bianca wharf (sftp service) at\nbianca-sftp.uppmax.uu.se.\n\nNote that we only support SFTP, which is not exactly the\nsame as SSH (rsync and scp will not work).\n\nPlease see our homepage and the Bianca User Guide\nfor more information:\n\nhttps://www.uppmax.uu.se/support/user-guides/bianca-user-guide/\n\nIf you have any questions not covered by the User Guide, you are\nwelcome to contact us at support@uppmax.uu.se.\n\nBest regards,\nUPPMAX\n\nsven-sens2016001@bianca-sftp.uppmax.uu.se's password:\nConnected to bianca-sftp.uppmax.uu.se.\nsftp&gt;\n</code></pre> How do I get rid of the welcome message? <p>Use <code>sftp</code>'s <code>-q</code> (which is short for 'quiet') flag:</p> <pre><code>sftp -q sven-sens2016001@bianca-sftp.uppmax.uu.se\n</code></pre> <p>The last line, <code>sftp&gt;</code> is the <code>sftp</code> prompt.</p> <p>Once connected you will have to type the <code>sftp</code> commands to upload/download files. See the UPPMAX page on <code>sftp</code> how to do so.</p> <p>With <code>sftp</code> you only have access to your wharf folder.</p>","tags":["sftp","SFTP","Bianca","data transfer","file transfer","transfer","terminal","command-line","console"]},{"location":"software/bianca_file_transfer_using_winscp/","title":"File transfer to/from Bianca using WinSCP","text":"","tags":["transfer","data transfer","file transfer","Bianca","WinSCP"]},{"location":"software/bianca_file_transfer_using_winscp/#file-transfer-tofrom-bianca-using-winscp","title":"File transfer to/from Bianca using WinSCP","text":"<p>Download and install from UU Software Center.</p> <p>You can also download the software from WinSCP website.</p> <p></p> <p>WinSCP is a secure file transfer tool that works under Windows.</p> <p>To transfer files to/from Bianca using WinSCP, do the following steps:</p>","tags":["transfer","data transfer","file transfer","Bianca","WinSCP"]},{"location":"software/bianca_file_transfer_using_winscp/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video how to do file transfer from/to Bianca using FileZilla</p> <p>To transfer files to/from Bianca using FileZilla, do the following steps:</p>","tags":["transfer","data transfer","file transfer","Bianca","WinSCP"]},{"location":"software/bianca_file_transfer_using_winscp/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>Get inside of SUNET.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","data transfer","file transfer","Bianca","WinSCP"]},{"location":"software/bianca_file_transfer_using_winscp/#2-start-winscp","title":"2. Start WinSCP","text":"<p>Start WinSCP.</p>","tags":["transfer","data transfer","file transfer","Bianca","WinSCP"]},{"location":"software/bianca_file_transfer_using_winscp/#3-create-a-new-site","title":"3. Create a new site","text":"<p>In WinSCP, click on 'Create new site'.</p> <p>For that site, use all standards, except:</p> <ul> <li>Set file protocol to 'SFTP'</li> <li>Set host name to <code>bianca-sftp.uppmax.uu.se</code></li> <li>Set user name to <code>[username]-[project]</code>, e.g. <code>sven-sens123456</code></li> <li>Do not set password! Provide your UPPMAX password followed immediately by the UPPMAX 2FA  when asked by the interactive login.</li> </ul> How does that look like? <p>It looks like this:</p> <p></p>","tags":["transfer","data transfer","file transfer","Bianca","WinSCP"]},{"location":"software/bianca_file_transfer_using_winscp/#4-click-connect","title":"4. Click 'Connect'","text":"<p>In WinSCP, click 'Connect'</p> <p>You will be asked for your password with two-factor identification, hence type <code>[your password][2FA code]</code>, e.g. <code>VerySecret123456</code>.</p> <p>Now you can transfer files between your local computer and your <code>wharf</code> folder.</p>","tags":["transfer","data transfer","file transfer","Bianca","WinSCP"]},{"location":"software/cellranger/","title":"Cell Ranger","text":"","tags":["cellranger","cell ranger","Cellranger","Cell Ranger","10XGenomics","10x Genomics"]},{"location":"software/cellranger/#cell-ranger","title":"Cell Ranger","text":"<p>According to the Cell Ranger GitHub repository:</p> <p>Cell Ranger is a set of analysis pipelines that perform sample demultiplexing, barcode processing, single cell 3' and 5' gene counting, V(D)J transcript sequence assembly and annotation, and Feature Barcode analysis from 10x Genomics Chromium Single Cell data.</p> <p>Cell Ranger (the tool) is part of the <code>cellranger</code> module.</p> <p>Finding the module that has <code>cowsay</code> installed:</p> <pre><code>module spider cellranger\n</code></pre> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ module spider cellranger\n\n----------------------------------------------------------------------------\n  cellranger:\n----------------------------------------------------------------------------\n     Versions:\n        cellranger/1.1.0\n        cellranger/1.3.0\n        cellranger/2.0.2\n        cellranger/2.2.0\n        cellranger/3.0.1\n        cellranger/4.0.0\n        cellranger/5.0.1\n        cellranger/6.0.2\n        cellranger/6.1.2\n        cellranger/7.0.0\n        cellranger/7.0.1\n        cellranger/7.1.0\n        cellranger/8.0.1\n     Other possible modules matches:\n        cellranger-ARC  cellranger-ARC-data  cellranger-ATAC  cellranger-ATAC-da\nta  ...\n\n----------------------------------------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*cellranger.*'\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"cellranger\" package (including how\nto load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider cellranger/8.0.1\n----------------------------------------------------------------------------\n</code></pre> How to see the tools similar to <code>cellranger</code>? <p>In case you want to search for similar tools, add a dash at the end of the search term:</p> <pre><code>module spider cellranger-\n</code></pre> <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ module spider cellranger-\n\n----------------------------------------------------------------------------\n  cellranger-ARC:\n----------------------------------------------------------------------------\n     Versions:\n        cellranger-ARC/1.0.0\n        cellranger-ARC/2.0.2\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"cellranger-ARC\" package (including\nhow to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider cellranger-ARC/2.0.2\n----------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n  cellranger-ARC-data:\n----------------------------------------------------------------------------\n     Versions:\n        cellranger-ARC-data/2020-A\n        cellranger-ARC-data/2020-A-2.0.0\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"cellranger-ARC-data\" package (inclu\nding how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider cellranger-ARC-data/2020-A-2.0.0\n----------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n  cellranger-ATAC:\n----------------------------------------------------------------------------\n     Versions:\n        cellranger-ATAC/1.2.0\n        cellranger-ATAC/2.0.0\n        cellranger-ATAC/2.1.0\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"cellranger-ATAC\" package (including\n how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider cellranger-ATAC/2.1.0\n----------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n  cellranger-ATAC-data:\n----------------------------------------------------------------------------\n     Versions:\n        cellranger-ATAC-data/1.2.0\n        cellranger-ATAC-data/2.0.0\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"cellranger-ATAC-data\" package (incl\nuding how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider cellranger-ATAC-data/2.0.0\n----------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n  cellranger-DNA: cellranger-DNA/1.1.0\n----------------------------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the\n \"cellranger-DNA/1.1.0\" module is available to load.\n\n      bioinfo-tools\n\n    Help:\n       cellranger-DNA - use cellranger-DNA 1.1.0\n\n\n      The cellranger-DNA-data/1.0.0 module is loaded as a prerequisite.\n\n\n\n\n----------------------------------------------------------------------------\n  cellranger-DNA-data: cellranger-DNA-data/1.0.0\n----------------------------------------------------------------------------\n\n    This module can be loaded directly: module load cellranger-DNA-data/1.0.0\n\n    Help:\n       cellranger-DNA-data - use cellranger-DNA-data 1.0.0\n\n\n      10X Genomics Chromium Cell Ranger DNA data\n      Version 1.0.0\n      https://support.10xgenomics.com/single-cell-dna/software/downloads/latest\n\n      NOTE: This is a data module. The software that uses this data is the cellr\nanger-DNA module, which loads this.\n\n\n      Default data for GRCh38, GRCh38 and GRCm38 references can be found in $CEL\nLRANGER_DNA_DATA.\n      To see the top-level directories:\n\n       ls -l $CELLRANGER_DNA_DATA\n\n      Genome assembly    Subdirectory\n      ---------------    ------------\n      GRCh38             refdata-GRCh38-1.0.0\n      GRCh37             refdata-GRCh37-1.0.0\n      GRCm38             refdata-GRCm38-1.0.0\n\n      Sample Index Set Sequences (both CSV and JSON formats)\n      ------------------------------------------------------\n      Chromium DNA     chromium-shared-sample-indexes-plate.csv\n                       chromium-shared-sample-indexes-plate.json\n\n      For information on how each dataset was produced, see the References secti\non of\n      https://support.10xgenomics.com/single-cell-dna/software/downloads/latest\n\n\n\n\n----------------------------------------------------------------------------\n  cellranger-VDJ-data:\n----------------------------------------------------------------------------\n     Versions:\n        cellranger-VDJ-data/4.0.0\n        cellranger-VDJ-data/5.0.0\n        cellranger-VDJ-data/7.0.0\n        cellranger-VDJ-data/7.1.0\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"cellranger-VDJ-data\" package (inclu\nding how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider cellranger-VDJ-data/7.1.0\n----------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n  cellranger-data:\n----------------------------------------------------------------------------\n     Versions:\n        cellranger-data/1.1.0\n        cellranger-data/1.2.0\n        cellranger-data/3.0.0\n        cellranger-data/2020-A\n        cellranger-data/2024-A\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"cellranger-data\" package (including\n how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider cellranger-data/2024-A\n----------------------------------------------------------------------------\n</code></pre> <p>Loading the latest version of the <code>cellranger</code> module:</p> <pre><code>module load bioinfo-tools cellranger/8.0.1\n</code></pre> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ module load bioinfo-tools cellranger/8.0.1\nDefault data for several references are available at $CELLRANGER_DATA; see 'module help cellranger-data/2024-A' for more information\nDefault data for GRCh38 and GRCm38 immune profiling references are available at $CELLRANGER_VDJ_DATA; see 'module help cellranger-VDJ-data/7.1.0' for more information\n</code></pre> <p>Now you can run Cell Ranger:</p> <pre><code>cellranger\n</code></pre> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ cellranger\ncellranger cellranger-8.0.1\n\nProcess 10x Genomics Gene Expression, Feature Barcode, and Immune Profiling data\n\nUsage: cellranger &lt;COMMAND&gt;\n\nCommands:\n  count           Count gene expression and/or feature barcode reads from a\n                      single sample and GEM well\n  multi           Analyze multiplexed data or combined gene\n                      expression/immune profiling/feature barcode data\n  multi-template  Output a multi config CSV template\n  vdj             Assembles single-cell VDJ receptor sequences from 10x\n                      Immune Profiling libraries\n  aggr            Aggregate data from multiple Cell Ranger runs\n  reanalyze       Re-run secondary analysis (dimensionality reduction,\n                      clustering, etc)\n  mkvdjref        Prepare a reference for use with CellRanger VDJ\n  mkfastq         Run Illumina demultiplexer on sample sheets that contain\n                      10x-specific sample index sets\n  testrun         Execute the 'count' pipeline on a small test dataset\n  mat2csv         Convert a feature-barcode matrix to CSV format\n  mkref           Prepare a reference for use with 10x analysis software.\n                      Requires a GTF and FASTA\n  mkgtf           Filter a GTF file by attribute prior to creating a 10x\n                      reference\n  upload          Upload analysis logs to 10x Genomics support\n  sitecheck       Collect Linux system configuration information\n  help            Print this message or the help of the given subcommand(s)\n\nOptions:\n  -h, --help     Print help\n  -V, --version  Print version\n</code></pre>","tags":["cellranger","cell ranger","Cellranger","Cell Ranger","10XGenomics","10x Genomics"]},{"location":"software/cellranger/#using-cell-ranger-from-python","title":"Using Cell Ranger from Python","text":"For staff <p>Related to ticket 297240</p>","tags":["cellranger","cell ranger","Cellranger","Cell Ranger","10XGenomics","10x Genomics"]},{"location":"software/cellranger/#links","title":"Links","text":"<ul> <li>Cell Ranger GitHub repository</li> </ul>","tags":["cellranger","cell ranger","Cellranger","Cell Ranger","10XGenomics","10x Genomics"]},{"location":"software/chmod/","title":"chmod","text":""},{"location":"software/chmod/#chmod","title":"<code>chmod</code>","text":"<p><code>chmod</code> is a Linux command to change the ownership of a folder</p>"},{"location":"software/chmod/#how-to-make-a-script-executable","title":"How to make a script executable?","text":"<p>Use (when the script is called <code>my_script.sh</code>):</p> <pre><code>chmod +x my_script.sh\n</code></pre> <p>You can now run the script using:</p> <pre><code>./my_script.sh\n</code></pre>"},{"location":"software/chmod/#how-to-create-a-folder-in-the-shared-project-folder-that-only-i-can-access","title":"How to create a folder in the shared project folder that only I can access?","text":"<p>Your project folders at <code>/proj/[naiss_project]</code> are shared by members of that NAISS project.</p> <p>If you need a folder that only you can access, assuming that folder is called <code>my_private_folder</code>, do the following:</p> <pre><code>chmod 700 my_private_folder\n</code></pre> How can I confirm it worked? <p>Use <code>ll</code>:</p> <pre><code>$ ll\ndrwxrwsr-x 2 sven my_group 4096 Aug 14 09:07 a_shared_folder/\ndrwx--S--- 2 sven my_group 4096 Aug 14 09:06 my_private_folder/\n</code></pre> <p>The first characters is what it is about:</p> <ul> <li><code>drwxrwsr-x</code>: accessible with group</li> <li><code>drwx--S---</code>: only accessible by you</li> </ul> <p>Now, you can enter that folder:</p> <pre><code>cd my_private_folder\n</code></pre> <p>However, others cannot and get this error message:</p> <pre><code>bash: cd: my_private_folder/: Permission denied\n</code></pre>"},{"location":"software/compilers/","title":"Compilers","text":""},{"location":"software/compilers/#compilers","title":"Compilers","text":"<p>UPPMAX supports multiple compilers:</p> Compiler Language(s) Description GCC C, C++, Fortran The GNU compiler collection icc C Older Intel C compiler icpc C++ Intel C++ compiler icx C Newer Intel C compiler ifort Fortran Older Intel Fortran compiler ifx Fortran Newer Intel Fortran compiler javac Java Java compiler <p>Different compilers are association with different debuggers and different profiling tools.</p> How to make sure you have only the right compiler loaded? <p>Use</p> <pre><code>module list\n</code></pre> <p>to get a list of modules.</p> <p>This may look like this:</p> <pre><code>Currently Loaded Modules:\n  1)  uppmax    2) intel/19.5\n</code></pre> <p>If there are modules connected to the incorrect compiler, unload the module, for example:</p> <pre><code>module unload intel\n</code></pre> <p>This scenario is valid if you want to use tools that use the GCC compiler.</p>"},{"location":"software/compiling_parallel/","title":"Compiling parallel code","text":""},{"location":"software/compiling_parallel/#mpi-and-openmp-user-guide","title":"MPI and OpenMP user guide","text":"<p>Table of contents:</p> <ul> <li>Compiling and running parallel programs on UPPMAX clusters.<ul> <li>Introduction</li> </ul> </li> <li>Overview of available compilers from GCC and Intel and compatible MPI libraries</li> <li>Running serial programs on execution nodes</li> <li>MPI using the OpenMPI library<ul> <li>C programs -Fortran programs</li> </ul> </li> <li>OpenMP<ul> <li>C programs</li> <li>Fortran programs</li> </ul> </li> <li>Pthreads</li> </ul> <p>This is a short tutorial about how to use the queuing system, and how to compile and run MPI and OpenMP jobs.</p> <p>For serial programs, see a short version of this page at Compiling source code.</p>"},{"location":"software/compiling_parallel/#compiling-and-running-parallel-programs-on-uppmax-clusters","title":"Compiling and running parallel programs on UPPMAX clusters","text":""},{"location":"software/compiling_parallel/#introduction","title":"Introduction","text":"<p>These notes show by brief examples how to compile and run serial and parallel programs on the clusters at UPPMAX.</p> <p>All programs are of the trivial \"hello, world\" type. The point is to demonstrate how to compile and execute the programs, not how to write parallel programs!</p>"},{"location":"software/compiling_parallel/#running-serial-programs-on-execution-nodes","title":"Running serial programs on execution nodes","text":""},{"location":"software/compiling_parallel/#standard-compatibility","title":"Standard compatibility","text":"<ul> <li>c11            gcc/4.8      intel/16+</li> <li>c17 (bug-fix)  gcc/8        intel/17+ 19 full</li> <li>Fortran2008    gcc/9        intel/15+ 18 full</li> <li>Fortran2018    gcc/9        intel/19+</li> </ul>"},{"location":"software/compiling_parallel/#examples","title":"Examples","text":"<p>Jobs are submitted to execution nodes through the resource manager. We use Slurm on our clusters.</p> <p>We will use the hello program we wrote in the section Compiling source code. The program language should not matter here when we deal with serial programs.</p> <p>To run the serial program hello as a batch job using Slurm, enter the following shell script in the file <code>hello.sh</code>:</p> <pre><code>#!/bin/bash -l\n# hello.sh :  execute hello serially in Slurm\n# command: $ sbatch hello.sh\n# sbatch options use the sentinel #SBATCH\n# You must specify a project\n#SBATCH -A your_project_name\n#SBATCH -J serialtest\n# Put all output in the file hello.out\n#SBATCH -o hello.out\n# request 5 seconds of run time\n#SBATCH -t 0:0:5\n# request one core\n#SBATCH -p core -n 1\n./hello\n</code></pre> <p>The last line in the script is the command used to start the program.</p> <p>Submit the job to the batch queue:</p> <pre><code>sbatch hello.sh\n</code></pre> <p>The program's output to stdout is saved in the file named at the -o flag.</p> <pre><code>$ cat hello.out\nhello, world\n</code></pre>"},{"location":"software/compiling_parallel/#mpi-using-the-openmpi-library","title":"MPI using the OpenMPI library","text":"<p>Before compiling a program for MPI we must choose, in addition to the compiler, which version of MPI we want to use. At UPPMAX there are two, openmpi and intelmpi. These, with their versions, are compatible only to a subset of the gcc and intel compiler versions.</p> <p>Tip</p> <p>Check this compatibility page for a more complete picture of compatible versions.</p>"},{"location":"software/compiling_parallel/#c-programs-using-openmpi","title":"C programs using OpenMPI","text":"<p>Enter the following mpi program in c and save in the file hello.c</p> <pre><code>/* hello-mpi.c :  mpi program in c printing a message from each process */\n#include &lt;stdio.h&gt;\n#include &lt;mpi.h&gt;\nint main(int argc, char *argv[])\n{\n    int npes, myrank;\n    MPI_Init(&amp;argc, &amp;argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &amp;npes);\n    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myrank);\n    printf(\"From process %d out of %d, Hello World!\\n\", myrank, npes);\n    MPI_Finalize();\n    return 0;\n}\n</code></pre> <p>Before compiling a program for MPI we must choose which version of MPI. At UPPMAX there are two, openmpi and intelmpi. For this example we will use openmpi. To load the openmpi module, enter the command below or choose other versions according to the lists above.</p> <pre><code>module load gcc/10.3.0 openmpi/3.1.6\n</code></pre> <p>To check that the openmpi modules is loaded, use the command:</p> <pre><code>module list\n</code></pre> <p>The command to compile a c program for mpi is mpicc. Which compiler is used when this command is issued depends on what compiler module was loaded before openmpi</p> <p>To compile, enter the command:</p> <pre><code>mpicc -o hello-mpi hello-mpi.c\n</code></pre> <p>You should add optimization and other flags to the mpicc command, just as you would to the compiler used. So if the gcc compiler is used and you wish to compile an mpi program written in C with good, fast optimization you should use a command similar to the following:</p> <pre><code>mpicc -fast -o hello-mpi hello-mpi.c\n</code></pre> <p>To run the mpi program hello using the batch system, we make a batch script with name <code>hello-mpi.sh</code></p> <pre><code>#!/bin/bash -l\n# hello.sh :  execute parallel mpi program hello on Slurm\n# use openmpi\n# command: $ sbatch hello.sh\n# Slurm options use the sentinel #SBATCH\n#SBATCH -A your_project_name\n#SBATCH -J mpitest\n#SBATCH -o hello.out\n#\n# request 5 seconds of run time\n#SBATCH -t 00:00:05\n#SBATCH -p node -n 8\nmodule load gcc/10.3 openmpi/3.1.3\nmpirun ./hello-mpi\n</code></pre> <p>The last line in the script is the command used to start the program. The last word on the last line is the program name hello.</p> <p>Submit the job to the batch queue:</p> <pre><code>sbatch hello-mpi.sh\n</code></pre> <p>The program's output to stdout is saved in the file named at the -o flag. A test run of the above program yelds the following output file:</p> <pre><code>$ cat hello-mpi.out\nFrom process 4 out of 8, Hello World!\nFrom process 5 out of 8, Hello World!\nFrom process 2 out of 8, Hello World!\nFrom process 7 out of 8, Hello World!\nFrom process 6 out of 8, Hello World!\nFrom process 3 out of 8, Hello World!\nFrom process 1 out of 8, Hello World!\nFrom process 0 out of 8, Hello World!\n</code></pre>"},{"location":"software/compiling_parallel/#fortran-programs-using-openmpi","title":"Fortran programs using OpenMPI","text":"<p>The following example program does numerical integration to find Pi (inefficiently, but it is just an example):</p> <pre><code>program testampi\n    implicit none\n    include 'mpif.h'\n    double precision :: h,x0,x1,v0,v1\n    double precision :: a,amaster\n    integer :: i,intlen,rank,size,ierr,istart,iend\n    call MPI_Init(ierr)\n    call MPI_Comm_size(MPI_COMM_WORLD,size,ierr)\n    call MPI_Comm_rank(MPI_COMM_WORLD,rank,ierr)\n    intlen=100000000\n    write (*,*) 'I am node ',rank+1,' out of ',size,' nodes.'\n\n    h=1.d0/intlen\n    istart=(intlen-1)*rank/size\n    iend=(intlen-1)*(rank+1)/size\n    write (*,*) 'start is ', istart\n    write (*,*) 'end is ', iend\n    a=0.d0\n    do i=istart,iend\n           x0=i*h\n           x1=(i+1)*h\n           v0=sqrt(1.d0-x0*x0)\n           v1=sqrt(1.d0-x1*x1)\n           a=a+0.5*(v0+v1)*h\n    enddo\n    write (*,*) 'Result from node ',rank+1,' is ',a\n    call MPI_Reduce(a,amaster,1, &amp;\n             MPI_DOUBLE_PRECISION,MPI_SUM,0,MPI_COMM_WORLD,ierr)\n    if (rank.eq.0) then\n           write (*,*) 'Result of integration is ',amaster\n           write (*,*) 'Estimate of Pi is ',amaster*4.d0\n    endif\n    call MPI_Finalize(ierr)\n    stop\nend program testampi\n</code></pre> <p>The program can be compiled by this procedure, using mpif90:</p> <pre><code>module load intel/20.4 openmpi/3.1.6\nmpif90 -Ofast -o testampi testampi.f90\n</code></pre> <p>The program can be run by creating a submit script sub.sh:</p> <pre><code>#!/bin/bash -l\n# execute parallel mpi program in Slurm\n# command: $ sbatch sub.sh\n# Slurm options use the sentinel #SBATCH\n#SBATCH -J mpitest\n#SBATCH -A your_project_name\n#SBATCH -o pi\n#\n# request 5 seconds of run time\n#SBATCH -t 00:00:05\n#\n#SBATCH -p node -n 8\nmodule load intel/20.4 openmpi/3.1.6\n\nmpirun ./testampi\n</code></pre> <p>Submit it:</p> <pre><code>sbatch sub.sh\n</code></pre> <p>Output from the program on Rackham:</p> <pre><code>I am node             8  out of             8  nodes.\nstart is      87499999\nend is      99999999\nI am node             3  out of             8  nodes.\nstart is      24999999\nend is      37499999\nI am node             5  out of             8  nodes.\nstart is      49999999\nend is      62499999\nI am node             2  out of             8  nodes.\nstart is      12499999\nend is      24999999\nI am node             7  out of             8  nodes.\nstart is      74999999\nend is      87499999\nI am node             6  out of             8  nodes.\nstart is      62499999\nend is      74999999\nI am node             1  out of             8  nodes.\nstart is             0\nend is      12499999\nI am node             4  out of             8  nodes.\nstart is      37499999\nend is      49999999\nResult from node             8  is    4.0876483237300587E-002\nResult from node             5  is    0.1032052706959522\nResult from node             2  is    0.1226971551244773\nResult from node             3  is    0.1186446918315650\nResult from node             7  is    7.2451466712425514E-002\nResult from node             6  is    9.0559231928350928E-002\nResult from node             1  is    0.1246737119371059\nResult from node             4  is    0.1122902087263801\nResult of integration is    0.7853982201935574\nEstimate of Pi is     3.141592880774230\n</code></pre>"},{"location":"software/compiling_parallel/#openmp","title":"OpenMP","text":"<p>OpenMP uses threads that use shared memory. OpenMP is supported by both the gcc and intel compilers and in the c/c++ and Fortran languages. Don't mix with OpenMPI whis is an open source library for MPI. OpenMP is built in in all modern compiler libraries.</p> <p>Depending on your preferences load the chosen compiler:</p> <pre><code>module load gcc/12.1.0\n</code></pre> <p>or</p> <pre><code>module load intel/20.4\n</code></pre>"},{"location":"software/compiling_parallel/#c-programs-using-openmp","title":"C programs using OpenMP","text":"<p>Enter the following openmp program in c and save in the file hello_omp.c</p> <pre><code>/* hello.c :  openmp program in c printing a message from each thread */\n#include &lt;stdio.h&gt;\n#include &lt;omp.h&gt;\nint main()\n{\n      int nthreads, tid;\n      #pragma omp parallel private(nthreads, tid)\n      {\n            nthreads = omp_get_num_threads();\n            tid = omp_get_thread_num();\n           printf(\"From thread %d out of %d, hello, world\\n\", tid, nthreads);\n    }\n    return 0;\n}\n</code></pre> <p>To compile, enter the command (note the -fopenmp or -qopenmp flag depending on compiler):</p> <pre><code>gcc -fopenmp -o hello_omp hello_omp.c\n</code></pre> <p>or</p> <pre><code>icc qfopenmp -o hello_omp hello_omp.c\n</code></pre> <p>Also here you should add optimization flags such as -fast as appropriate.</p> <p>To run the OpenMP program hello using the batch system, enter the following shell script in the file hello.sh:</p> <pre><code>#!/bin/bash -l\n# hello.sh :  execute parallel openmp program hello on Slurm\n# use openmp\n# command: $ sbatch hello.sh\n# Slurm options use the sentinel #SBATCH\n#SBATCH -J omptest\n#SBATCH -A your_project_name\n#SBATCH -o hello.out\n#\n# request 5 seconds of run time\n#SBATCH -t 00:00:05\n#SBATCH -p node -n 8\nuname -n\n#Tell the openmp program to use 8 threads\nexport OMP_NUM_THREADS=8\nmodule load intel/20.4\n# or gcc...\nulimit -s  $STACKLIMIT\n./hello_omp\n</code></pre> <p>The last line in the script is the command used to start the program.</p> <p>Submit the job to the batch queue:</p> <pre><code>sbatch hello.sh\n</code></pre> <p>The program's output to stdout is saved in the file named at the -o flag. A test run of the above program yelds the following output file:</p> <pre><code>$ cat hello.out\nr483.uppmax.uu.se\nunlimited\nFrom thread 0 out of 8, hello, world\nFrom thread 1 out of 8, hello, world\nFrom thread 2 out of 8, hello, world\nFrom thread 3 out of 8, hello, world\nFrom thread 4 out of 8, hello, world\nFrom thread 6 out of 8, hello, world\nFrom thread 7 out of 8, hello, world\nFrom thread 5 out of 8, hello, world\n</code></pre>"},{"location":"software/compiling_parallel/#fortran-programs-using-openmp","title":"Fortran programs using OpenMP","text":"<p>Enter the following openmp program in Fortran and save in the file hello_omp.f90</p> <pre><code>PROGRAM HELLO\nINTEGER NTHREADS, TID, OMP_GET_NUM_THREADS, OMP_GET_THREAD_NUM\n! Fork a team of threads giving them their own copies of variables\n!$OMP PARALLEL PRIVATE(NTHREADS, TID)\n! Obtain thread number\nTID = OMP_GET_THREAD_NUM()\nPRINT *, 'Hello World from thread = ', TID\n! Only master thread does this\nIF (TID .EQ. 0) THEN\n NTHREADS = OMP_GET_NUM_THREADS()\nPRINT *, 'Number of threads = ', NTHREADS\nEND IF\n! All threads join master thread and disband\n!$OMP END PARALLEL\nEND\n</code></pre> <p>With gcc compiler:</p> <pre><code>gfortran hello_omp.f90 -o hello_omp -fopenmp\n</code></pre> <p>and with Intel compiler:</p> <pre><code>ifort hello_omp.f90 -o hello_omp -qopenmp\n</code></pre> <p>Run with:</p> <pre><code>$ ./hello_omp\n\n Hello World from thread =            1\n Hello World from thread =            2\n Hello World from thread =            0\n Hello World from thread =            3\n Number of threads =            4\n</code></pre> <p>A batch file would look similar to the C version, above.</p>"},{"location":"software/compiling_parallel/#pthreads","title":"Pthreads","text":"<p>Pthreads (Posix threads) are more low-level than OpenMP. That means that for a beginner it is easier to get rather expected gain only with a few lines with OpenMP. On the other hand it may be possible to gain more efficiency from your code with pthreads, though with quite some effort. Pthreads is native in c/c++. With additional installation of a POSIX library for Fortran it is possible to run it in there as well.</p> <p>Enter the following program in c and save in the file hello_pthreads.c</p> <pre><code>/* hello.c :  create system pthreads and print a message from each thread */\n#include &lt;stdio.h&gt;\n#include &lt;pthread.h&gt;\n// does not work for setting array length of \"tid\": const int NTHR = 8;\n// Instead use \"#define\"\n#define NTHR 8\nint nt = NTHR, tid[NTHR];\npthread_attr_t attr;\nvoid *hello(void *id)\n{\n     printf(\"From thread %d out of %d: hello, world\\n\", *((int *) id), nt);\n     pthread_exit(0);\n}\nint main()\n{\n    int i, arg1;\n    pthread_t thread[NTHR];\n    /* system threads */\n    pthread_attr_init(&amp;attr);\n    pthread_attr_setscope(&amp;attr, PTHREAD_SCOPE_SYSTEM);\n    /* create threads */\n    for (i = 0; i &lt; nt; i++) {\n          tid[i] = i;\n          pthread_create(&amp;thread[i], &amp;attr, hello, (void *) &amp;tid[i]);\n     }\n    /* wait for threads to complete */\n    for (i = 0; i &lt; nt; i++)\n            pthread_join(thread[i], NULL);\n      return 0;\n}\n</code></pre> <p>To compile, enter the commands</p> <pre><code>module load gcc/10.2.0\ngcc -pthread -o hello_pthread hello_pthread.c\n</code></pre> <p>To run the pthread program hello using the batch system, enter the following shell script in the file hello.sh:</p> <pre><code>#!/bin/bash -l\n# hello.sh :  execute parallel pthreaded program hello on Slurm\n# command: $ sbatch hello.sh\n# Slurm options use the sentinel #SBATCH\n#SBATCH -J pthread\n#SBATCH -A your_project_name\n#SBATCH -o hello.out\n#\n# request 5 seconds of run time\n#SBATCH -t 00:00:05\n# use openmp programming environment\n# to ensure all processors on the same node\n#SBATCH -p node -n 8\nuname -n\n./hello_pthread\n</code></pre> <p>The last line in the script is the command used to start the program. Submit the job to the batch queue:</p> <pre><code>sbatch hello.sh\n</code></pre> <p>The program's output to stdout is saved in the file named at the -o flag. A test run of the above program yelds the following output file:</p> <pre><code>$ cat hello.out\nr483.uppmax.uu.se\nFrom thread 0 out of 8: hello, world\nFrom thread 4 out of 8: hello, world\nFrom thread 5 out of 8: hello, world\nFrom thread 6 out of 8: hello, world\nFrom thread 7 out of 8: hello, world\nFrom thread 1 out of 8: hello, world\nFrom thread 2 out of 8: hello, world\nFrom thread 3 out of 8: hello, world\n</code></pre>"},{"location":"software/compiling_serial/","title":"Compiling serial code","text":""},{"location":"software/compiling_serial/#compiling-serial-source-code","title":"Compiling serial source code","text":"<p>For parallel programs, see MPI and OpenMP user guide.</p>"},{"location":"software/compiling_serial/#overview","title":"Overview","text":"Language Compiler Find guide at ... C GCC Compile C using GCC C Intel, <code>icc</code> Compile C using icc C Intel, <code>icx</code> Compile C using icx C++ GCC Compile C++ using GCC C++ Intel, <code>icpc</code> Compile C++ using icpc Fortran GCC Compile Fortran using GCC Fortran Intel, <code>ifort</code> Compile Fortran using ifort Fortran Intel, <code>ifx</code> Compile Fortran using ifx Java <code>javac</code> Compile Java using javac"},{"location":"software/conda/","title":"Installing with conda","text":""},{"location":"software/conda/#conda","title":"Conda","text":"Want to see the video 'How to use Conda on Rackham'? <p>See the YouTube video how to use Conda on Rackham</p>"},{"location":"software/conda/#install-packages-or-not-check-it","title":"Install packages or not? Check it","text":""},{"location":"software/conda/#python","title":"Python","text":"<ul> <li>Check python versions: <code>module spider python</code></li> </ul> How does that look like? <p>It will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ module spider python\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  python:\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        python/2.7.6\n        python/2.7.9\n        python/2.7.11\n        python/2.7.15\n        python/3.3\n        python/3.3.1\n        python/3.4.3\n        python/3.5.0\n        python/3.6.0\n        python/3.6.8\n        python/3.7.2\n        python/3.8.7\n        python/3.9.5\n        python/3.10.8\n        python/3.11.4\n        python/3.11.8\n        python/3.12.1\n     Other possible modules matches:\n        Biopython  Boost.Python  GitPython  IPython  Python  biopython  flatbuffers-python  netcdf4-python  protobuf-python  python-parasail  python3  python_GIS_packages  ...\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*python.*'\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"python\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider python/3.12.1\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <ul> <li>load a python version like: <code>module load python/3.10.8</code></li> <li>from the Python shell with the <code>import</code> command</li> <li>from BASH shell with the<ul> <li><code>pip list</code> command</li> <li><code>module help python/3.9.5</code> (or other version) at UPPMAX</li> </ul> </li> </ul>"},{"location":"software/conda/#is-it-not-there-or-is-it-a-stand-alone-tool-then-proceed","title":"Is it not there, or is it a stand-alone tool? Then proceed!**","text":"<p>Tip Python packages</p> <ul> <li>Try Conda first directly on Bianca.</li> <li>Otherwise, on Rackham, in first case use Pip.</li> <li>We have mirrored all major Conda repositories directly on UPPMAX, on both Rackham and Bianca. These are updated every third day.</li> <li>If you want to keep number of files down, use PyPI (pip).</li> </ul>"},{"location":"software/conda/#python-packages-with-pip","title":"Python packages with pip","text":"Want to see the video 'Load and use Python packages on UPPMAX'? <p>See the YouTube video how to load and use Python packages</p> <p>See the Python user guide</p>"},{"location":"software/conda/#conda-repositories","title":"Conda repositories","text":"<p>We have mirrored all major non-proprietary Conda repositories (not <code>main</code>, <code>anaconda</code> and <code>r</code>) directly on UPPMAX, on both Rackham and Bianca. These are updated every third day.</p> <p>Available Conda channels</p> <ul> <li>bioconda</li> <li>biocore</li> <li>conda-forge</li> <li>dranew</li> <li>free</li> <li>main</li> <li>pro</li> <li>qiime2</li> <li>r</li> <li>r2018.11</li> <li>scilifelab-lts</li> <li>nvidia</li> <li>pytorch</li> </ul> <p>More info</p> <ul> <li>Installing Conda packages on Bianca</li> </ul>"},{"location":"software/conda/#using-conda","title":"Using Conda","text":"<p>Conda cheat sheet</p> <ul> <li> <p>List all environments: <code>conda info -e</code> or <code>conda env list</code></p> </li> <li> <p>Create a conda environment (it is good to directly define the packages included AND channels do not need to be explicitly mentioned)</p> <pre><code>conda create --prefix /some/path/to/env &lt;package1&gt; [&lt;package2&gt; ... ]\n</code></pre> <ul> <li>On our systems the above should replace <code>conda create --name myenvironment ...</code></li> </ul> </li> <li> <p>Create a new environment from requirements.txt:</p> <ul> <li><code>conda create --prefix /some/path/to/env --file requirements.txt</code></li> </ul> </li> <li> <p>Activate a specific environment: <code>conda activate myenvironment</code></p> </li> <li> <p>List packages in present environment: <code>conda list</code></p> <ul> <li>Also pip list will work</li> </ul> </li> <li> <p>Install additional package from an active environment:</p> <ul> <li><code>conda install somepackage</code></li> </ul> </li> <li> <p>Install from certain channel (conda-forge):</p> <ul> <li><code>conda install -c conda-forge somepackage</code></li> </ul> </li> <li> <p>Install a specific version: <code>conda install somepackage=1.2.3</code></p> <ul> <li>Install a specific version: <code>conda install somepackage=1.2.3</code></li> </ul> </li> <li> <p>Deactivate current environment: <code>conda deactivate</code></p> </li> <li> <p>Conda cheat sheet</p> </li> </ul>"},{"location":"software/conda/#your-conda-settings-on-rackham-and-bianca","title":"Your conda settings on Rackham and Bianca","text":"<ul> <li><code>export CONDA_ENVS_PATH=/a/path/to/a/place/in/your/project/</code></li> </ul> <p>Tip</p> <ul> <li>You may want to have the same path for all conda environments in the present project</li> <li><code>echo \"export CONDA_ENVS_PATH=/a/path/to/a/place/in/your/project/\" &gt;&gt; ~/.bashrc</code><ul> <li>Example: <code>echo \"export CONDA_ENVS_PATH=/proj/&lt;project&gt;/conda\" &gt;&gt; ~/.bashrc</code></li> </ul> </li> </ul> <p>Warning</p> <ul> <li>It seems you are required to use this path, ending with the name of your environment, together with <code>--prefix</code> when you install new envronments AND packages also after activating the conda environment!   Like: <code>conda install --prefix $CONDA_ENVS_PATH/&lt;your-environment&gt; ...</code></li> </ul> <p>Tip</p> <ul> <li>REMEMBER TO <code>conda clean -a</code> once in a while to remove unused and unnecessary files</li> </ul> By choice <ul> <li> <p>Run <code>source conda_init.sh</code> to initialise your shell (bash) to be able to run <code>conda activate</code> and <code>conda deactivate</code> etcetera instead of <code>source activate</code>. It will modify (append) your <code>.bashrc</code> file.</p> </li> <li> <p>When conda is loaded you will by default be in the <code>base</code> environment, which works in the same way as other Conda environments. It is a \u201cbest practice\u201d to avoid installing additional packages into your base software environment unless it is very general packages</p> </li> </ul>"},{"location":"software/conda/#installing-using-conda","title":"Installing using Conda","text":"<p>We have mirrored all major Conda repositories directly on UPPMAX, on both Rackham and Bianca. These are updated every third day. See above for these conda channels.</p> <ul> <li>You reach them all by loading the <code>conda</code> module.</li> <li>You don't have to state the specific channel when using UPPMAX.</li> <li>Also, you are offline on Bianca which means that the default is <code>--offline</code>, which you can specify if you want to simulate the experience on Rackham.</li> </ul> <p>Tip</p> <p>If you need a channel that isn't in our repository, we can easily add it. Just send us a message and we will do it.</p>"},{"location":"software/conda/#make-a-new-conda-environment","title":"Make a new conda environment","text":"<p>Tip</p> <ul> <li>Since python or other packages are dependent on each-other expect solving the versions takes some time.</li> <li>use an interactive session!</li> </ul> <ol> <li> <p>Do <code>module load conda</code></p> <ul> <li>This grants you access to the latest version of Conda and all major repositories on all UPPMAX systems.</li> <li>Check the text output as <code>conda</code> is loaded, especially the first time, see below</li> </ul> </li> <li> <p>Create the Conda environment</p> <ul> <li> <p>Example:</p> <pre><code>conda create --prefix  $CONDA_ENVS_PATH/python36-env python=3.6 numpy=1.13.1 matplotlib=2.2.2\n</code></pre> </li> </ul> <p>!!! info \"The <code>mamba</code> alternative is not needed in newer versions of Conda!</p> <ul> <li> <p>It all worked if you get something like this:</p> <pre><code># To activate this environment, use\n#\n#     $ conda activate python36-env\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n</code></pre> </li> </ul> </li> <li> <p>Activate the conda environment by <code>source activate</code> if you have not enabled <code>conda activate</code>, see above:</p> <pre><code>source activate python36-env\n</code></pre> <ul> <li> <p>You will see that your prompt is changing to start with <code>(python-36-env)</code> to show that you are within an environment.</p> </li> <li> <p>You can also see the installed packages by:</p> </li> </ul> <pre><code>conda list\npip list\n</code></pre> <ul> <li>you can also add more packages within the environment by exact version (use <code>=</code>) or latest (?) compatible version:</li> </ul> <pre><code>conda install --prefix   $CONDA_ENVS_PATH/python36-env pandas\n</code></pre> <ul> <li>that may have given you <code>pandas=1.1.5</code> which would be the newest version compatible with <code>python3.6</code> and <code>numpy=1.13.1</code></li> </ul> </li> <li> <p>Now do your work!</p> </li> <li> <p>Deactivate with <code>conda deactivate</code> (this will work in any case!)</p> <pre><code>(python-36-env) $ conda deactivate\n</code></pre> </li> </ol> <p>Warning</p> <ul> <li>Conda is known to create many small files.   Your diskspace is not only limited in gigabytes,   but also in number of files (typically 300000 in <code>$HOME</code>).</li> <li>Check your disk usage and quota limit with <code>uquota</code></li> <li>Do a <code>conda clean -a</code> once in a while to remove unused and unnecessary files</li> </ul>"},{"location":"software/conda/#working-with-conda-environments-defined-by-files","title":"Working with Conda environments defined by files","text":"<ul> <li> <p>Create an environment based on dependencies given in an environment   file:</p> <pre><code>conda env create --file environment.yml\n</code></pre> </li> <li> <p>Create file from present conda environment:</p> <pre><code>conda env export &gt; environment.yml\n</code></pre> </li> </ul> <p><code>environments.yml</code> (for conda) is a yaml-file which looks like this:</p> <pre><code>name: my-environment\nchannels:        # not needed on bianca\n- defaults\ndependencies:\n- numpy\n- matplotlib\n- pandas\n- scipy\n</code></pre> <p><code>environments.yml</code> with versions:</p> <pre><code>name: my-environment\nchannels:            #not needed on bianca\n- defaults\ndependencies:\n- python=3.7\n- numpy=1.18.1\n- matplotlib=3.1.3\n- pandas=1.1.2\n- scipy=1.6.2\n</code></pre> <p>More on dependencies</p> <ul> <li>Dependency management from course Python for Scientific computing</li> </ul> <p>keypoints</p> <ul> <li> <p>Conda is an installer of packages but also bigger toolkits</p> </li> <li> <p>Conda on Bianca is easy since the repos in the most used channels are local.</p> </li> <li> <p>Conda creates isolated environments not clashing with other installations of python and other versions of packages</p> </li> <li> <p>Conda environment requires that you install all packages needed by yourself, although automatically.</p> </li> <li> <p>That is, you cannot load the python module and use the packages therein inside your Conda environment.</p> </li> </ul>"},{"location":"software/conda/#conda-in-batch-scripts","title":"Conda in batch scripts","text":"<p>If you already have setup the CONDA_ENVS_PATH path and run 'conda init bash' a batch script containing a conda environment shall include</p> <pre><code>module load conda\nconda activate &lt;name of environment&gt;\n</code></pre>"},{"location":"software/conda/#packages-on-bianca","title":"Packages on Bianca","text":"<p>Since we have mirrored conda repositories locally conda will work also on Bianca!</p> <p>First try Conda! There is a mirrored repository with many available packages.</p> <p>If your desired package is not there but available as pip follow the guide below, perhaps , while looking at Bianca user guide  and Transit user guide.</p> <p>Make an installation on Rackham and then use the wharf to copy it over to your directory on Bianca.</p> <p>Path on Rackham and Bianca could be <code>~/.local/lib/python&lt;version&gt;/site-packages/</code>.</p> <p>You may have to:</p> <p>in source directory:</p> <pre><code>cp \u2013a &lt;package_dir&gt; &lt;wharf_mnt_path&gt;\n</code></pre> <p>you may want to tar before copying to include all possible symbolic links:</p> <pre><code>$ tar cfz &lt;tarfile.tar.gz&gt; &lt;package&gt;\nand in target directory (wharf_mnt) on Bianca:\n$ tar xfz &lt;tarfile.tar.gz&gt; #if there is a tar file!\n$ mv \u2013a  &lt;file(s)&gt; ~/.local/lib/python&lt;version&gt;/site-packages/\n</code></pre> <p>If problems arise, send an email to <code>support@uppmax.uu.se</code> and we'll help you.</p>"},{"location":"software/containers/","title":"Containers","text":""},{"location":"software/containers/#containers","title":"Containers","text":"<p>Containers allow one to bundle installed software into a file, with the goal to run software on any platform.</p> <ul> <li>Docker containers: cannot be run on UPPMAX clusters</li> <li>Singularity containers</li> </ul>"},{"location":"software/cowsay/","title":"cowsay","text":""},{"location":"software/cowsay/#cowsay","title":"cowsay","text":"<p><code>cowsay</code> is a tool that commonly use as a toy example.</p> <p>Because <code>cowsay</code> is not part of the Linux kernel, users commonly need to install it. Or in our case: load a module to use it.</p> <p><code>cowsay</code> (the tool) is part of the identically-named <code>cowsay</code> module.</p> <p>Finding the module that has <code>cowsay</code> installed:</p> <pre><code>module spider cowsay\n</code></pre> How does that look like? <p>You output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ module spider cowsay\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  cowsay: cowsay/3.03\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n    This module can be loaded directly: module load cowsay/3.03\n\n    Help:\n       cowsay - use cowsay\n</code></pre> <p>Loading the latest version of the <code>cowsay</code> module:</p> <pre><code>module load cowsay/3.03\n</code></pre> <p>Now you can run <code>cowsay</code>:</p> <pre><code>cowsay hello\n</code></pre> <p>results in:</p> <pre><code> _______\n&lt; hello &gt;\n -------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"software/cram/","title":"Cram","text":""},{"location":"software/cram/#using-cram-to-compress-bam-files","title":"Using CRAM to compress BAM files","text":""},{"location":"software/cram/#introduction","title":"Introduction","text":"<p>Biological data is being produced at a higher rate each day, and it is a challenge to store it all somewhere</p> <p>The bioinformatics community is trying to keep up with the growing data amounts, and new file formats is part of this evolution. The BAM format was a huge success due to its ability to compress aligned reads by ~50-80% of their original size, but even that is not sustainable in the long run.</p> <p>CRAM is a new program that can compress SAM/BAM files even more, which makes it suitable for long-term storage. We think this format will become more common, and that it will be supported by most tools, like the BAM format is today.</p> <p>There are a couple of options you can give to CRAM that will make it behave differently. Even more about the different options on the developers homepage.</p> <p>Lossless compression: When converting BAM -&gt; CRAM -&gt; BAM, the final BAM file will look identical to the initial BAM file.</p> <p>Lossy compression: You can specify how to deal with the quality scores in a multitude of different way. To cite the creators of CRAM:</p> <p>\"Bam2Cram allows to specify lossy model via a string which can be composed of one or more words separated by '-'. Each word is read or base selector and quality score treatment, which can be binning (Illumina 8 bins) or full scale (40 values).</p> <p>Here are some examples:</p> <ul> <li>N40-D8 - preserve quality scores for non-matching bases with full precision, and bin quality scores for positions flanking deletions.</li> <li>m5 - preserve quality scores for reads with mapping quality score lower than 5</li> <li>R40X10-N40 - preserve non-matching quality scores and those matching with coverage lower than 10</li> <li><code>*8</code> - bin all quality scores</li> </ul> <p>Selectors:</p> <ul> <li><code>R</code> - bases matching the reference sequence N aligned bases mismatching the reference, this only applies to 'M', '=' (EQ) or 'X' BAM cigar elements.</li> <li><code>U</code> - unmapped read</li> <li><code>Pn</code> - pileup: capture all bases at a given position on the reference if there are at least n mismatches D read positions flanking a deletion</li> <li><code>Mn</code> - reads with mapping quality score higher than n</li> <li><code>mn</code> - reads with mapping quality score lower than n</li> <li><code>I</code> - insertions</li> <li><code>*</code> - all</li> </ul> <p>By default no quality scores will be preserved.</p> <p>Illumina 8-binning scheme:</p> <pre><code>0, 1, 6, 6, 6, 6, 6, 6, 6, 6, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n15, 22, 22, 22, 22, 22, 27, 27, 27, 27, 27, 33, 33, 33, 33, 33, 37,\n37, 37, 37, 37, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n40, 40, 40, 40, 40, 40\"\n</code></pre> <p>Illumina's white paper on the matter</p>"},{"location":"software/cram/#compression-rate","title":"Compression rate","text":"<p>So, how much compression are we talking about here? Here are the results of a test with a 1.9 GB BAM file (7.4 GB SAM format).</p> <p>CRAM COMPRESSION RATE</p> File format File size (GB) SAM 7.4 BAM 1.9 CRAM lossless 1.4 CRAM 8 bins 0.8 CRAM no quality scores 0.26 <p></p>"},{"location":"software/cram/#examples","title":"Examples","text":""},{"location":"software/cram/#lossless-compression-of-a-bam-file","title":"Lossless compression of a BAM file","text":"<p>Lossless compression means that the BAM file will be identical before and after compression/decompression The downside of this is that the produced CRAM file will be larger since if has to save each and every quality score. To make a lossless compression, use the following command (can also be written as a single line by removing the backslashes):</p> <pre><code>$ module load bioinfo-tools cramtools\n$ java -jar $CRAM_HOME/cram.jar cram \\\n-I file.bam \\\n-O file.cram \\\n-R ref.fa \\\n--capture-all-tags \\\n--lossless-quality-score\n</code></pre> <p>The important parts here are:</p> <ul> <li><code>-I</code> which means the input file (name of the BAM file to be compressed).</li> <li><code>-O</code> which means the output file (name of the new compressed CRAM file).</li> <li><code>-R</code> which means the reference file (the FASTA reference to be used. Must be the same when decompressing).</li> <li><code>--capture-all-tags which</code> means that all the tags in the BAM file will be saved.</li> <li><code>--lossless-quality-score</code> which means the quality scores will be preserved.</li> </ul> <p>CRAM assumed you have indexed your reference genome using e.g. samtools faidx, i.e. that you will have both ref.fa and ref.fa.fai (Note the index name: ref.fa.fai, NOT ref.fai)</p> <p>To decompress the CRAM file to a BAM file again, use this command (can also be written as a single line by removing the backslashes):</p> <pre><code>$ module load bioinfo-tools cramtools\n$ java -jar $CRAM_HOME/cram.jar bam \\\n-I file.cram \\\n-O file.bam \\\n-R ref.fa\n</code></pre> <p>If you had NM or MD tags in your original BAM file, you have to specify that they should be added in the BAM file that is to be created by adding</p> <pre><code>--calculate-md-tag\nand/or\n--calculate-nm-tag\n</code></pre> <p>to the command.</p>"},{"location":"software/cram/#lossy-compression-of-a-bam-file","title":"Lossy compression of a BAM file","text":"<p>The motivation to use a lossy compression is that the compression ratio will be much larger, i.e. the cram file will be much smaller. The best compression ratio is reached, naturally, when the quality scores are removed all together. This does have an impact on future analysis such as SNP calling, so the trick is, as usual, to find a good balance.</p> <p>Illumina has started with a practice called binning. That means that instead of having 40 unique quality scores, you put similar values into bins. Illumina thought 8 bins would get the job done, and that is what CRAM recommends. See this page's introduction for more details about the bins.</p> <p>To compress your BAM file and binning the quality scores in the same way as Illumina, use this command (can also be written as a single line by removing the backslashes):</p> <pre><code>$ module load bioinfo-tools cramtools\n$ java -jar $CRAM_HOME/cram.jar cram \\\n-I file.bam \\\n-O file.cram \\\n-R ref.fa \\\n--capture-all-tags \\\n--lossy-quality-score-spec \\*8\n</code></pre> <p>The important parts here are:</p> <ul> <li>-I which means the input file (name of the BAM file to be compressed).</li> <li>-O which means the output file (name of the new compressed BRAM file).</li> <li>-R which means the reference file (the FASTA reference to be used. Must be the same when decompressing.).</li> <li>--capture-all-tags which means that all the tags in the BAM file will be saved.</li> <li>--lossy-quality-score-spec *8 which means the quality scores will be binned into 8 bins the Illumina way. (Notice that we need to apply a \"\\\" before the \"8\" as your shell Bash will otherwise expand this expression if you'd happen to have any filenames ending with eights in the current directory.)</li> </ul> <p>To decompress the CRAM file to a BAM file again, use this command (can also be written as a single line by removing the backslashes):</p> <pre><code>$ module load bioinfo-tools cramtools\n$ java -jar $CRAM_HOME/cram.jar bam \\\n-I file.cram \\\n-O file.bam \\\n-R ref.fa\n</code></pre>"},{"location":"software/create_singularity_container/","title":"Creating a Singularity container","text":"","tags":["Singularity","Singularity container","create","build"]},{"location":"software/create_singularity_container/#creating-a-singularity-container","title":"Creating a Singularity container","text":"<p>There are many ways to create a Singularity container.</p>","tags":["Singularity","Singularity container","create","build"]},{"location":"software/create_singularity_container/#how-and-where-to-build","title":"How and where to build?","text":"<p>Here is a decision tree on how and where to build a Singularity container.</p> <pre><code>flowchart TD\n  where_to_build[Where to build my Singularity container?]\n  where_to_build --&gt; have_linux\n  have_linux[Do you have Linux with sudo rights and Singularity installed?]\n  build_short[Is the build short?]\n  use_linux(Build on Linux computer with sudo rights)\n  use_remote_builder_website(Build using Sylabs remote builder website)\n  use_remote_builder_rackham(Build using Sylabs remote builder from Rackham)\n\n  have_linux --&gt; |yes| use_linux\n  have_linux --&gt; |no| build_short\n  build_short --&gt; |yes| use_remote_builder_website\n  build_short --&gt; |yes| use_remote_builder_rackham</code></pre> How and where Features Local Linux Easiest for Linux users, can do longer builds Remote builder from website Easiest for non-Linux users, short builds only Remote builder from Rackham Can do longer builds","tags":["Singularity","Singularity container","create","build"]},{"location":"software/create_singularity_container_for_r_package/","title":"Create a Singularity container for an R package","text":"","tags":["Singularity","Singularity script","create","build","R package","R"]},{"location":"software/create_singularity_container_for_r_package/#create-a-singularity-container-for-an-r-package","title":"Create a Singularity container for an R package","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to create a Singularity container for an R package.</p> <p>Although the <code>R_Packages</code> module has thousands of packages, sometimes you need a package from GitHub.</p>","tags":["Singularity","Singularity script","create","build","R package","R"]},{"location":"software/create_singularity_container_for_r_package/#procedure","title":"Procedure","text":"Prefer a video? <p>See the video 'Create a Singularity container for an R package on GitHub'</p> <p>The hardest part of this procedure may be to have Linux with Singularity installed on a computer where you have super-user rights.</p> <p>The most important things for creating a Singularity container is to start with a good container.</p>","tags":["Singularity","Singularity script","create","build","R package","R"]},{"location":"software/create_singularity_container_for_r_package/#1-create-a-singularity-script","title":"1. Create a Singularity script","text":"<p>Create a file called <code>Singularity</code> (this is the recommended filename for Singularity scripts) with the following content:</p> <pre><code>Bootstrap: docker\nFrom: rocker/tidyverse\n\n%post\n    # From https://github.com/brucemoran/Singularity/blob/8eb44591284ffb29056d234c47bf8b1473637805/shub/bases/recipe.CentOs7-R_3.5.2#L21\n    echo 'export LANG=en_US.UTF-8 LANGUAGE=C LC_ALL=C LC_CTYPE=C LC_COLLATE=C  LC_TIME=C LC_MONETARY=C LC_PAPER=C LC_MEASUREMENT=C' &gt;&gt; $SINGULARITY_ENVIRONMENT\n\n    Rscript -e 'install.packages(c(\"remotes\", \"devtools\"))'\n    Rscript -e 'remotes::install_github(\"bmbolstad/preprocessCore\")'\n\n%runscript\nRscript \"$@\"\n</code></pre> <p>This example script installs the R package hosted on GitHub at https://github.com/bmbolstad/preprocessCore. Replace the R package to suit your needs.</p>","tags":["Singularity","Singularity script","create","build","R package","R"]},{"location":"software/create_singularity_container_for_r_package/#2-build-the-singularity-container","title":"2. Build the Singularity container","text":"<p>Here is how you create a Singularity container called <code>my_container.sif</code> from the Singularity script:</p> <pre><code>sudo singularity build my_container.sif Singularity\n</code></pre> <p>Which will build a Singularity container called <code>my_container.sif</code>.</p> How does that look like? <p>You output will be similar to this:</p> <pre><code>sven@sven-N141CU:~/temp$ sudo singularity build my_container.sif Singularity\nINFO:    Starting build...\nINFO:    Fetching OCI image...\n307.6MiB / 307.6MiB [================================================================================================================================================] 100 % 0.0 b/s 0s\n30.9MiB / 30.9MiB [==================================================================================================================================================] 100 % 0.0 b/s 0s\n28.2MiB / 28.2MiB [==================================================================================================================================================] 100 % 0.0 b/s 0s\n261.1MiB / 261.1MiB [================================================================================================================================================] 100 % 0.0 b/s 0s\n193.7MiB / 193.7MiB [================================================================================================================================================] 100 % 0.0 b/s 0s\n26.3MiB / 26.3MiB [==================================================================================================================================================] 100 % 0.0 b/s 0s\n288.7KiB / 288.7KiB [================================================================================================================================================] 100 % 0.0 b/s 0s\nINFO:    Extracting OCI image...\nINFO:    Inserting Singularity configuration...\nINFO:    Running post scriptlet\n+ echo export LANG=en_US.UTF-8 LANGUAGE=C LC_ALL=C LC_CTYPE=C LC_COLLATE=C  LC_TIME=C LC_MONETARY=C LC_PAPER=C LC_MEASUREMENT=C\n+ Rscript -e install.packages(c(\"remotes\", \"devtools\"))\nInstalling packages into \u2018/usr/local/lib/R/site-library\u2019\n(as \u2018lib\u2019 is unspecified)\ntrying URL 'https://p3m.dev/cran/__linux__/jammy/latest/src/contrib/remotes_2.5.0.tar.gz'\nContent type 'binary/octet-stream' length 436043 bytes (425 KB)\n==================================================\ndownloaded 425 KB\n\ntrying URL 'https://p3m.dev/cran/__linux__/jammy/latest/src/contrib/devtools_2.4.5.tar.gz'\nContent type 'binary/octet-stream' length 435688 bytes (425 KB)\n==================================================\ndownloaded 425 KB\n\n* installing *binary* package \u2018remotes\u2019 ...\n* DONE (remotes)\n* installing *binary* package \u2018devtools\u2019 ...\n* DONE (devtools)\n\nThe downloaded source packages are in\n \u2018/tmp/Rtmpow1CFQ/downloaded_packages\u2019\n+ Rscript -e remotes::install_github(\"bmbolstad/preprocessCore\")\nDownloading GitHub repo bmbolstad/preprocessCore@HEAD\n\u2500\u2500 R CMD build \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2714  checking for file \u2018/tmp/Rtmpx5C1XE/remotes13a1238df5bce/bmbolstad-preprocessCore-33ccbd9/DESCRIPTION\u2019 (337ms)\n\u2500  preparing \u2018preprocessCore\u2019:\n\u2714  checking DESCRIPTION meta-information ...\n\u2500  cleaning src\n\u2500  running \u2018cleanup\u2019\n\u2500  checking for LF line-endings in source and make files and shell scripts\n\u2500  checking for empty or unneeded directories\n\u2500  building \u2018preprocessCore_1.61.0.tar.gz\u2019\n\nInstalling package into \u2018/usr/local/lib/R/site-library\u2019\n(as \u2018lib\u2019 is unspecified)\n* installing *source* package \u2018preprocessCore\u2019 ...\n** using staged installation\n'config' variable 'CPP' is defunct\nchecking for gcc... gcc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\nchecking for suffix of executables...\nchecking whether we are cross compiling... no\nchecking for suffix of object files... o\nchecking whether we are using the GNU C compiler... yes\nchecking whether gcc accepts -g... yes\nchecking for gcc option to accept ISO C89... none needed\nchecking how to run the C preprocessor... gcc -E\nchecking for library containing pthread_create... none required\nchecking for grep that handles long lines and -e... /usr/bin/grep\nchecking for egrep... /usr/bin/grep -E\nchecking for ANSI C header files... yes\nchecking for sys/types.h... yes\nchecking for sys/stat.h... yes\nchecking for stdlib.h... yes\nchecking for string.h... yes\nchecking for memory.h... yes\nchecking for strings.h... yes\nchecking for inttypes.h... yes\nchecking for stdint.h... yes\nchecking for unistd.h... yes\nchecking for stdlib.h... (cached) yes\nchecking if PTHREAD_STACK_MIN is defined... yes\nchecking if R is using flexiblas... flexiblas not found. preprocessCore threading will not be disabled\nconfigure: Enabling threading for preprocessCore\nconfigure: creating ./config.status\nconfig.status: creating src/Makevars\n** libs\nusing C compiler: \u2018gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\u2019\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_colSummarize.c -o R_colSummarize.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_plmd_interfaces.c -o R_plmd_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_plmr_interfaces.c -o R_plmr_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_rlm_interfaces.c -o R_rlm_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_subColSummarize.c -o R_subColSummarize.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_subrcModel_interfaces.c -o R_subrcModel_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c avg.c -o avg.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c avg_log.c -o avg_log.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c biweight.c -o biweight.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c init_package.c -o init_package.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c lm.c -o lm.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c log_avg.c -o log_avg.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c log_median.c -o log_median.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c matrix_functions.c -o matrix_functions.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c median.c -o median.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c median_log.c -o median_log.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c medianpolish.c -o medianpolish.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c plmd.c -o plmd.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c plmr.c -o plmr.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c psi_fns.c -o psi_fns.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c qnorm.c -o qnorm.o\nqnorm.c: In function \u2018qnorm_c_l\u2019:\nqnorm.c:595:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n  595 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n  596 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\nqnorm.c:616:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n  616 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n  617 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\nqnorm.c: In function \u2018qnorm_c_determine_target_l\u2019:\nqnorm.c:2004:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n 2004 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n 2005 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\nqnorm.c: In function \u2018qnorm_c_determine_target_via_subset_l\u2019:\nqnorm.c:2604:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n 2604 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n 2605 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rlm.c -o rlm.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rlm_anova.c -o rlm_anova.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rlm_se.c -o rlm_se.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rma_background4.c -o rma_background4.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rma_common.c -o rma_common.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c weightedkerneldensity.c -o weightedkerneldensity.o\ngcc -shared -L/usr/local/lib/R/lib -L/usr/local/lib -o preprocessCore.so R_colSummarize.o R_plmd_interfaces.o R_plmr_interfaces.o R_rlm_interfaces.o R_subColSummarize.o R_subrcModel_interfaces.o avg.o avg_log.o biweight.o init_package.o lm.o log_avg.o log_median.o matrix_functions.o median.o median_log.o medianpolish.o plmd.o plmr.o psi_fns.o qnorm.o rlm.o rlm_anova.o rlm_se.o rma_background4.o rma_common.o weightedkerneldensity.o -llapack -lblas -lgfortran -lm -lquadmath -L/usr/local/lib/R/lib -lR\ninstalling to /usr/local/lib/R/site-library/00LOCK-preprocessCore/00new/preprocessCore/libs\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (preprocessCore)\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: my_container.sif\n</code></pre>","tags":["Singularity","Singularity script","create","build","R package","R"]},{"location":"software/create_singularity_container_for_r_package/#3-create-an-r-script-for-the-container-to-use","title":"3. Create an R script for the container to use","text":"<p>Here we create an R script of the container to use.</p> <p>Here is such an example R script, that prints the contents of the <code>preprocessCore::colSummarizeAvgLog</code> function:</p> <pre><code>preprocessCore::colSummarizeAvgLog\n</code></pre> <p>Save this R script, for example, as <code>my_r_script.R</code>.</p>","tags":["Singularity","Singularity script","create","build","R package","R"]},{"location":"software/create_singularity_container_for_r_package/#4-use-the-singularity-container-on-an-r-script","title":"4. Use the Singularity container on an R script","text":"<p>Run the container on the R script:</p> <pre><code>./my_container.sif my_r_script.R\n</code></pre> How does that look like? <p>You output will be similar to this:</p> <pre><code>sven@sven-N141CU:~/temp$ ./my_container.sif my_r_script.R\nfunction (y)\n{\n    if (!is.matrix(y))\n        stop(\"argument should be matrix\")\n    if (!is.double(y) &amp; is.numeric(y))\n        y &lt;- matrix(as.double(y), dim(y)[1], dim(y)[2])\n    else if (!is.numeric(y))\n        stop(\"argument should be numeric matrix\")\n    .Call(\"R_colSummarize_avg_log\", y, PACKAGE = \"preprocessCore\")\n}\n&lt;bytecode: 0x62d460a4d470&gt;\n&lt;environment: namespace:preprocessCore&gt;\n</code></pre>","tags":["Singularity","Singularity script","create","build","R package","R"]},{"location":"software/create_singularity_container_from_a_singularity_script/","title":"Create a Singularity container from a Singularity script","text":"","tags":["Singularity","Singularity script","create","build"]},{"location":"software/create_singularity_container_from_a_singularity_script/#create-a-singularity-container-from-a-singularity-script","title":"Create a Singularity container from a Singularity script","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to create a Singularity container from a Singularity script.</p> <p>These are the procedures:</p> Procedure Description using a website Easiest for Mac and Windows users using a computer with Linux where you have super-user right Harder for Mac and Windows users <p>Note that users have no super-user rights on our UPPMAX clusters.</p>","tags":["Singularity","Singularity script","create","build"]},{"location":"software/create_singularity_container_from_a_singularity_script_on_linux/","title":"Create a Singularity container from a Singularity script on a computer with Linux where you have super-user rights","text":"","tags":["Singularity","Singularity script","create","build","Linux","local computer"]},{"location":"software/create_singularity_container_from_a_singularity_script_on_linux/#create-a-singularity-container-from-a-singularity-script-on-a-computer-with-linux-where-you-have-super-user-rights","title":"Create a Singularity container from a Singularity script on a computer with Linux where you have super-user rights","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to do so using a computer with Linux where you have super-user rights.</p> <p>Note that users have no super-user rights on our UPPMAX clusters.</p>","tags":["Singularity","Singularity script","create","build","Linux","local computer"]},{"location":"software/create_singularity_container_from_a_singularity_script_on_linux/#procedure","title":"Procedure","text":"","tags":["Singularity","Singularity script","create","build","Linux","local computer"]},{"location":"software/create_singularity_container_from_a_singularity_script_on_linux/#1-save-the-script-to-a-singularity-file","title":"1. Save the script to a Singularity file","text":"<p>Save the script as a file called <code>Singularity</code> (this is the recommended filename for Singularity scripts).</p> Do you have an example Singularity script? <p>Yes! Here is an example Singularity script:</p> <pre><code>Bootstrap: docker\nFrom: rocker/tidyverse\n\n%post\n    # From https://github.com/brucemoran/Singularity/blob/8eb44591284ffb29056d234c47bf8b1473637805/shub/bases/recipe.CentOs7-R_3.5.2#L21\n    echo 'export LANG=en_US.UTF-8 LANGUAGE=C LC_ALL=C LC_CTYPE=C LC_COLLATE=C  LC_TIME=C LC_MONETARY=C LC_PAPER=C LC_MEASUREMENT=C' &gt;&gt; $SINGULARITY_ENVIRONMENT\n\n    Rscript -e 'install.packages(c(\"remotes\", \"devtools\"))'\n    Rscript -e 'remotes::install_github(\"bmbolstad/preprocessCore\")'\n\n%runscript\nRscript \"$@\"\n</code></pre>","tags":["Singularity","Singularity script","create","build","Linux","local computer"]},{"location":"software/create_singularity_container_from_a_singularity_script_on_linux/#2-build-the-singularity-container","title":"2. Build the Singularity container","text":"<p>Here is how you create a Singularity container called <code>my_container.sif</code> from the Singularity script:</p> <pre><code>sudo singularity build my_container.sif Singularity\n</code></pre> <p>Which will build a Singularity container called <code>my_container.sif</code>.</p> How does that look like? <p>You output will be similar to this:</p> <pre><code>sven@sven-N141CU:~/temp$ sudo singularity build my_container.sif Singularity\nINFO:    Starting build...\nINFO:    Fetching OCI image...\n307.6MiB / 307.6MiB [================================================================================================================================================] 100 % 0.0 b/s 0s\n30.9MiB / 30.9MiB [==================================================================================================================================================] 100 % 0.0 b/s 0s\n28.2MiB / 28.2MiB [==================================================================================================================================================] 100 % 0.0 b/s 0s\n261.1MiB / 261.1MiB [================================================================================================================================================] 100 % 0.0 b/s 0s\n193.7MiB / 193.7MiB [================================================================================================================================================] 100 % 0.0 b/s 0s\n26.3MiB / 26.3MiB [==================================================================================================================================================] 100 % 0.0 b/s 0s\n288.7KiB / 288.7KiB [================================================================================================================================================] 100 % 0.0 b/s 0s\nINFO:    Extracting OCI image...\nINFO:    Inserting Singularity configuration...\nINFO:    Running post scriptlet\n+ echo export LANG=en_US.UTF-8 LANGUAGE=C LC_ALL=C LC_CTYPE=C LC_COLLATE=C  LC_TIME=C LC_MONETARY=C LC_PAPER=C LC_MEASUREMENT=C\n+ Rscript -e install.packages(c(\"remotes\", \"devtools\"))\nInstalling packages into \u2018/usr/local/lib/R/site-library\u2019\n(as \u2018lib\u2019 is unspecified)\ntrying URL 'https://p3m.dev/cran/__linux__/jammy/latest/src/contrib/remotes_2.5.0.tar.gz'\nContent type 'binary/octet-stream' length 436043 bytes (425 KB)\n==================================================\ndownloaded 425 KB\n\ntrying URL 'https://p3m.dev/cran/__linux__/jammy/latest/src/contrib/devtools_2.4.5.tar.gz'\nContent type 'binary/octet-stream' length 435688 bytes (425 KB)\n==================================================\ndownloaded 425 KB\n\n* installing *binary* package \u2018remotes\u2019 ...\n* DONE (remotes)\n* installing *binary* package \u2018devtools\u2019 ...\n* DONE (devtools)\n\nThe downloaded source packages are in\n \u2018/tmp/Rtmpow1CFQ/downloaded_packages\u2019\n+ Rscript -e remotes::install_github(\"bmbolstad/preprocessCore\")\nDownloading GitHub repo bmbolstad/preprocessCore@HEAD\n\u2500\u2500 R CMD build \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2714  checking for file \u2018/tmp/Rtmpx5C1XE/remotes13a1238df5bce/bmbolstad-preprocessCore-33ccbd9/DESCRIPTION\u2019 (337ms)\n\u2500  preparing \u2018preprocessCore\u2019:\n\u2714  checking DESCRIPTION meta-information ...\n\u2500  cleaning src\n\u2500  running \u2018cleanup\u2019\n\u2500  checking for LF line-endings in source and make files and shell scripts\n\u2500  checking for empty or unneeded directories\n\u2500  building \u2018preprocessCore_1.61.0.tar.gz\u2019\n\nInstalling package into \u2018/usr/local/lib/R/site-library\u2019\n(as \u2018lib\u2019 is unspecified)\n* installing *source* package \u2018preprocessCore\u2019 ...\n** using staged installation\n'config' variable 'CPP' is defunct\nchecking for gcc... gcc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\nchecking for suffix of executables...\nchecking whether we are cross compiling... no\nchecking for suffix of object files... o\nchecking whether we are using the GNU C compiler... yes\nchecking whether gcc accepts -g... yes\nchecking for gcc option to accept ISO C89... none needed\nchecking how to run the C preprocessor... gcc -E\nchecking for library containing pthread_create... none required\nchecking for grep that handles long lines and -e... /usr/bin/grep\nchecking for egrep... /usr/bin/grep -E\nchecking for ANSI C header files... yes\nchecking for sys/types.h... yes\nchecking for sys/stat.h... yes\nchecking for stdlib.h... yes\nchecking for string.h... yes\nchecking for memory.h... yes\nchecking for strings.h... yes\nchecking for inttypes.h... yes\nchecking for stdint.h... yes\nchecking for unistd.h... yes\nchecking for stdlib.h... (cached) yes\nchecking if PTHREAD_STACK_MIN is defined... yes\nchecking if R is using flexiblas... flexiblas not found. preprocessCore threading will not be disabled\nconfigure: Enabling threading for preprocessCore\nconfigure: creating ./config.status\nconfig.status: creating src/Makevars\n** libs\nusing C compiler: \u2018gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\u2019\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_colSummarize.c -o R_colSummarize.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_plmd_interfaces.c -o R_plmd_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_plmr_interfaces.c -o R_plmr_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_rlm_interfaces.c -o R_rlm_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_subColSummarize.c -o R_subColSummarize.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c R_subrcModel_interfaces.c -o R_subrcModel_interfaces.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c avg.c -o avg.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c avg_log.c -o avg_log.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c biweight.c -o biweight.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c init_package.c -o init_package.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c lm.c -o lm.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c log_avg.c -o log_avg.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c log_median.c -o log_median.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c matrix_functions.c -o matrix_functions.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c median.c -o median.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c median_log.c -o median_log.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c medianpolish.c -o medianpolish.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c plmd.c -o plmd.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c plmr.c -o plmr.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c psi_fns.c -o psi_fns.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c qnorm.c -o qnorm.o\nqnorm.c: In function \u2018qnorm_c_l\u2019:\nqnorm.c:595:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n  595 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n  596 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\nqnorm.c:616:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n  616 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n  617 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\nqnorm.c: In function \u2018qnorm_c_determine_target_l\u2019:\nqnorm.c:2004:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n 2004 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n 2005 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\nqnorm.c: In function \u2018qnorm_c_determine_target_via_subset_l\u2019:\nqnorm.c:2604:63: warning: format \u2018%d\u2019 expects argument of type \u2018int\u2019, but argument 2 has type \u2018size_t\u2019 {aka \u2018long unsigned int\u2019} [-Wformat=]\n 2604 |          error(\"ERROR; return code from pthread_join(thread #%d) is %d, exit status for thread was %d\\n\",\n      |                                                              ~^\n      |                                                               |\n      |                                                               int\n      |                                                              %ld\n 2605 |                i, returnCode, *((int *) status));\n      |                ~\n      |                |\n      |                size_t {aka long unsigned int}\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rlm.c -o rlm.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rlm_anova.c -o rlm_anova.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rlm_se.c -o rlm_se.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rma_background4.c -o rma_background4.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rma_common.c -o rma_common.o\ngcc -I\"/usr/local/lib/R/include\" -DNDEBUG -I/usr/local/include  -I/usr/local/include   -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -DPACKAGE_NAME=\\\"\\\" -DPACKAGE_TARNAME=\\\"\\\" -DPACKAGE_VERSION=\\\"\\\" -DPACKAGE_STRING=\\\"\\\" -DPACKAGE_BUGREPORT=\\\"\\\" -DPACKAGE_URL=\\\"\\\" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_STDLIB_H=1 -DUSE_PTHREADS=1 -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c weightedkerneldensity.c -o weightedkerneldensity.o\ngcc -shared -L/usr/local/lib/R/lib -L/usr/local/lib -o preprocessCore.so R_colSummarize.o R_plmd_interfaces.o R_plmr_interfaces.o R_rlm_interfaces.o R_subColSummarize.o R_subrcModel_interfaces.o avg.o avg_log.o biweight.o init_package.o lm.o log_avg.o log_median.o matrix_functions.o median.o median_log.o medianpolish.o plmd.o plmr.o psi_fns.o qnorm.o rlm.o rlm_anova.o rlm_se.o rma_background4.o rma_common.o weightedkerneldensity.o -llapack -lblas -lgfortran -lm -lquadmath -L/usr/local/lib/R/lib -lR\ninstalling to /usr/local/lib/R/site-library/00LOCK-preprocessCore/00new/preprocessCore/libs\n** R\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (preprocessCore)\nINFO:    Adding runscript\nINFO:    Creating SIF file...\nINFO:    Build complete: my_container.sif\n</code></pre>","tags":["Singularity","Singularity script","create","build","Linux","local computer"]},{"location":"software/create_singularity_container_from_a_singularity_script_on_linux/#3-use-the-container","title":"3. Use the container","text":"<p>How to use a container, depends on what it does.</p> <p>Here are some thing to try:</p> <p>Run the container without arguments, in the hope of getting a clear error message with instructions:</p> <pre><code>./my_container.sif\n</code></pre> <p>Run the container in the hope of seeing its documentation:</p> <pre><code>./my_container.sif --help\n</code></pre> <p>Run the container on the local folder, in the hope of getting a clear error message with instructions:</p> <pre><code>./my_container.sif .\n</code></pre>","tags":["Singularity","Singularity script","create","build","Linux","local computer"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/","title":"Create a Singularity container from a Singularity script using a website","text":"","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#create-a-singularity-container-from-a-singularity-script-using-a-website","title":"Create a Singularity container from a Singularity script using a website","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to do so using a website</p>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#procedure","title":"Procedure","text":"","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#1-go-to-to-sylabs-website","title":"1. Go to to Sylabs website","text":"<p>Go to the Sylabs website</p> How does that look like? <p>The Sylabs website looks similar to this:</p> <p></p>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#2-got-to-the-sylabs-singularity-container-services-website","title":"2. Got to the Sylabs Singularity Container Services website","text":"<p>On the Sylabs website, click 'Products | Singularity Container Services'</p> Where to click? <p>Click here:</p> <p></p> <p>You will be takes to the 'Singularity Container Services'.</p> How does that look like? <p>The Singularity Container Services website looks similar to this:</p> <p></p>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#3-sign-in-or-sign-up","title":"3. Sign in or sign up","text":"<p>At the 'Singularity Container Services' website, click 'Sign Up' or 'Sign In'</p> How does signing in look like? <p>Signing in looks similar to this:</p> <p></p> <p>You are now logged in at the 'Singularity Container Services':</p> How does that look like? <p>The Singularity Container Services looks similar to this after logging in:</p> <p></p>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#4-go-to-the-remote-builder","title":"4. Go to the remote builder","text":"<p>Click on 'Remote builder'.</p> Where to click? <p>Click here:</p> <p></p>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#5-setup-the-remote-builder","title":"5. Setup the remote builder","text":"<p>The remote builder shows a Singularity script and some default settings.</p> How does that look like? <p>The remote builder's default settings look similar to this:</p> <p></p> <p>Make the following changes:</p> <ul> <li>paste your Singularity script in the text box</li> <li>change <code>Repository</code> to a valid name (as indicated), for example, as <code>default/my_container</code></li> </ul> How does that look like? <p>The remote builder with modified values looks similar to this:</p> <p></p>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#6-let-the-container-be-built","title":"6. Let the container be built","text":"<p>Click 'Submit Build'.</p> Where to click? <p>Click here:</p> <p></p> <p>The building will start.</p> How does that look like? <p>A build that has just started looks similar to this:</p> <p></p> <p>After a while the building will be done.</p> How does that look like? <p>A build that has finished looks similar to this:</p> <p></p>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#7-download-the-container","title":"7. Download the container","text":"<p>There are multiple ways to download your Singularity container:</p> <ul> <li>Download from the website: click on 'View image',   then scroll down and click 'Download'</li> </ul> How does that look like? <p>Click on 'View image' here:</p> <p></p> <p>The 'View image' page looks similar to this:</p> <p></p> <p>At the 'View image' page, scroll down to find the 'Download' button:</p> <p></p> <ul> <li>Use a <code>singularity pull</code></li> </ul> <p>For example:</p> <pre><code>singularity pull library://sven/default/my_container\n</code></pre> How does that look like? <p>For example:</p> <pre><code>$ singularity pull library://pontus/default/sortmerna:3.0.3\nWARNING: Authentication token file not found : Only pulls of public images will succeed\nINFO:    Downloading library image\n 65.02 MiB / 65.02 MiB [=========================================================================================================================================] 100.00% 30.61 MiB/s 2s\n</code></pre>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder/#8-use-the-container","title":"8. Use the container","text":"<p>How to use a container, depends on what it does.</p> <p>Here are some thing to try:</p> <p>Run the container without arguments, in the hope of getting a clear error message with instructions:</p> <pre><code>./my_container.sif\n</code></pre> <p>Run the container in the hope of seeing its documentation:</p> <pre><code>./my_container.sif --help\n</code></pre> <p>Run the container on the local folder, in the hope of getting a clear error message with instructions:</p> <pre><code>./my_container.sif .\n</code></pre>","tags":["Singularity","Singularity script","create","build","website","Sylabs","remote builder"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder_from_rackham/","title":"Create a Singularity container from a Singularity script using a remote build from Rackham","text":"","tags":["Singularity","Singularity script","create","build","remote builder","Rackham"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder_from_rackham/#create-a-singularity-container-from-a-singularity-script-using-a-remote-build-from-rackham","title":"Create a Singularity container from a Singularity script using a remote build from Rackham","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to do so using a remote build from Rackham.</p>","tags":["Singularity","Singularity script","create","build","remote builder","Rackham"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder_from_rackham/#building-images-on-rackham","title":"Building images on Rackham","text":"<p>On Rackham, the singularity capabilities are instead provided by Apptainer. The differences are beyond the scope of this material, but you can safely assume you are working with Singularity. Apptainer, also allows you to build containers without sudo/administrative rights. In most of the cases, you can simply start building directly without sudo i.e. <code>singularity build myimage.img examples/ubuntu.def</code>. Here are some precautions that will allow you to safely build images on Rackham.</p> <pre><code># Change to fit your account\nPRJ_DIR=/crex/uppmax2022-0-00\n\n# Singularity\nexport SINGULARITY_CACHEDIR=${PRJ_DIR}/nobackup/SINGULARITY_CACHEDIR\nexport SINGULARITY_TMPDIR=${PRJ_DIR}/nobackup/SINGULARITY_TMPDIR\nmkdir -p $SINGULARITY_CACHEDIR $SINGULARITY_TMPDIR\n\n# Apptainer\nexport APPTAINER_CACHEDIR=${PRJ_DIR}/nobackup/SINGULARITY_CACHEDIR\nexport APPTAINER_TMPDIR=${PRJ_DIR}/nobackup/SINGULARITY_TMPDIR\nmkdir -p $APPTAINER_CACHEDIR $APPTAINER_TMPDIR\n\n# Disabling cache completelly - perfect when you only need to pull containers\n# export SINGULARITY_DISABLE_CACHE=true\n# export APPTAINER_DISABLE_CACHE=true\n</code></pre>","tags":["Singularity","Singularity script","create","build","remote builder","Rackham"]},{"location":"software/create_singularity_container_from_a_singularity_script_using_remote_builder_from_rackham/#procedure","title":"Procedure","text":"<p>The remote builder service provided by Sylabs also supports remote builds through an API. This means you can call on it from the shell at UPPMAX.</p> <p>Using this service also requires you to register/log in to the Sylabs cloud service. To use this, simply run</p> <pre><code>singularity remote login SylabsCloud\n</code></pre> <p>and you should see</p> <pre><code>Generate an API Key at https://cloud.sylabs.io/auth/tokens, and paste here:\nAPI Key:\n</code></pre> <p>if you visit that link and give a name, a text-token will be created for you. Copy and paste this to the prompt at UPPMAX. You should see</p> <pre><code>INFO: API Key Verified!\n</code></pre> <p>once you've done this, you can go on and build images almost as normal, using commands like</p> <pre><code>singularity build --remote testcontainer.sif testdefinition.def\n</code></pre> <p>which will build the container from <code>testdefinition.def</code> remotely and transfer it to your directory, storing it as <code>testcontainer.sif</code>.</p> Could you give an example script? <p>A sample job script for running a tool provided in a container may look like</p> <pre><code>#!/bin/bash -l\n#SBATCH -N 1\n#SBATCH -n 1\n#SBATCH -t 0:30:00\n#SBATCH -A your-project\n#SBATCH -p core\ncd /proj/something/containers\n\nsingularity exec ./ubuntu.img echo \"Hey, I'm running ubuntu\"\nsingularity exec ./ubuntu.img lsb_release -a\nsingularity run ./anotherimage some parameters here\n./yetanotherimage parameters\n</code></pre>","tags":["Singularity","Singularity script","create","build","remote builder","Rackham"]},{"location":"software/create_singularity_container_from_conda/","title":"Create a Singularity container from conda","text":"","tags":["Singularity","Singularity script","create","build","conda"]},{"location":"software/create_singularity_container_from_conda/#create-a-singularity-container-from-conda","title":"Create a Singularity container from conda","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to create a Singularity container from a Singularity script that uses conda.</p> <p>As an example we use a script that build qiime2:</p> <pre><code>BootStrap: library\nFrom: centos:7\n\n%runscript\n  . /miniconda/etc/profile.d/conda.sh\n  PATH=$PATH:/miniconda/bin\n  conda activate qiime2-2019.7\n  qiime \"$@\"\n\n%post\n  yum clean all\n  yum -y update\n  yum -y install wget python-devel\n  cd /tmp\n  wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh\n  bash ./Miniconda2-latest-Linux-x86_64.sh -b -p /miniconda\n  /miniconda/bin/conda update -y conda\n  wget https://data.qiime2.org/distro/core/qiime2-2019.7-py36-linux-conda.yml\n  /miniconda/bin/conda env create -n qiime2-2019.7 --file qiime2-2019.7-py36-linux-conda.yml\n  # OPTIONAL CLEANUP\n  rm qiime2-2019.7-py36-linux-conda.yml\n  /miniconda/bin/conda clean -a\n</code></pre>","tags":["Singularity","Singularity script","create","build","conda"]},{"location":"software/create_singularity_container_from_docker_pull/","title":"Create a Singularity container from a Docker pull","text":"","tags":["Singularity","Singularity script","create","build","Docker","docker pull","pull"]},{"location":"software/create_singularity_container_from_docker_pull/#create-a-singularity-container-from-a-docker-pull","title":"Create a Singularity container from a Docker pull","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to create a Singularity container from a Docker pull, such as the 'lycheeverse/lychee' Docker container)</p> <pre><code>docker pull lycheeverse/lychee\n</code></pre>","tags":["Singularity","Singularity script","create","build","Docker","docker pull","pull"]},{"location":"software/create_singularity_container_from_docker_pull/#procedure","title":"Procedure","text":"Prefer a video? <p>You can see the procedure below in the video Create a Singularity container from <code>docker pull</code>.</p> <p>The hardest part of this procedure may be to have Linux with Singularity installed on a computer where you have super-user rights.</p> <p>In this example, we create a Singularity container for lychee, a tool to check for broken links in text files.</p>","tags":["Singularity","Singularity script","create","build","Docker","docker pull","pull"]},{"location":"software/create_singularity_container_from_docker_pull/#1-create-the-singularity-container","title":"1. Create the Singularity container","text":"<p>Here we build a Singularity container from a Docker file:</p> <pre><code>sudo singularity build my_container.sif [location to Docker file]\n</code></pre> <p>The magic is in <code>[location to Docker file]</code>.</p> <p>In our case, we have seen the documentation state the command <code>docker pull lycheeverse/lychee</code> to install this Docker container. Using a <code>docker pull</code> like this, means that the Docker script is on Docker Hub. And yes, our Docker script is on Docker Hub!</p> <p>To build a Singularity container from a Docker file on Docker Hub, do:</p> <pre><code>sudo singularity build my_container.sif docker:lycheeverse/lychee\n</code></pre>","tags":["Singularity","Singularity script","create","build","Docker","docker pull","pull"]},{"location":"software/create_singularity_container_from_docker_pull/#2-use-the-singularity-container","title":"2. Use the Singularity container","text":"<pre><code>./my_container.sif [your command-line arguments]\n</code></pre> <p>For example, in this case:</p> <pre><code>./my_container.sif .\n</code></pre> <p>The <code>.</code> means 'in this folder'.</p> <p>As, in this example, we have created a Singularity container for lychee, a tool to check for broken links in text files. Hence, the full command can be read as 'Check all files in this folder for broken links'.</p>","tags":["Singularity","Singularity script","create","build","Docker","docker pull","pull"]},{"location":"software/create_singularity_container_from_dockerhub/","title":"Create a Singularity container from Docker Hub","text":"","tags":["Singularity","Singularity script","create","build","Docker Hub"]},{"location":"software/create_singularity_container_from_dockerhub/#create-a-singularity-container-from-docker-hub","title":"Create a Singularity container from Docker Hub","text":"<p>There are multiple ways how to create a Singularity container.</p> <p>This page shows how to create a Singularity container from a Docker script on Docker Hub.</p>","tags":["Singularity","Singularity script","create","build","Docker Hub"]},{"location":"software/create_singularity_container_from_dockerhub/#procedure","title":"Procedure","text":"<p>The hardest part of this procedure may be to have Linux with Singularity installed on a computer where you have super-user rights.</p> <p>In this example, we create a Singularity container for https://github.com/lindenb/jvarkit with a Docker Hub script at https://hub.docker.com/r/lindenb/jvarkit.</p>","tags":["Singularity","Singularity script","create","build","Docker Hub"]},{"location":"software/create_singularity_container_from_dockerhub/#1-create-the-singularity-container","title":"1. Create the Singularity container","text":"<p>Here we build a Singularity container from a Docker file:</p> <pre><code>sudo singularity build my_container.sif docker:[owner/file]\n</code></pre> <p>The magic is in <code>docker:[owner/file]</code>, which for us becomes <code>docker:lindenb/jvarkit</code>:</p> <pre><code>sudo singularity build my_container.sif docker:lindenb/jvarkit\n</code></pre> <p>In some case, the Singularity container is now created.</p> How does that look like? <pre><code>$ sudo singularity build my_container.sif docker:lindenb/jvarkit\nINFO:    Starting build...\nINFO:    Fetching OCI image...\n28.2MiB / 28.2MiB [================================================================================================================================================] 100 % 2.5 MiB/s 0s\n1.0GiB / 1.0GiB [==================================================================================================================================================] 100 % 2.5 MiB/s 0s\nINFO:    Extracting OCI image...\nINFO:    Inserting Singularity configuration...\nINFO:    Creating SIF file...\nINFO:    Build complete: my_container.sif\n</code></pre>","tags":["Singularity","Singularity script","create","build","Docker Hub"]},{"location":"software/create_singularity_container_from_dockerhub/#11-troubleshooting","title":"1.1 Troubleshooting","text":"<p>In our case, however, we get the <code>MANIFEST_UNKNOWN</code> error:</p> <pre><code>[sudo] password for sven:\nINFO:    Starting build...\nINFO:    Fetching OCI image...\nFATAL:   While performing build: conveyor failed to get: GET https://index.docker.io/v2/lindenb/jvarkit/manifests/latest: MANIFEST_UNKNOWN: manifest unknown; unknown tag=latest\n</code></pre> <p>This means that Docker Hub cannot conclude with Docker script we want to use exactly. To solve this, we need to find a tag that allows us to find an exact script. On Docker Hub, we can find the tags for our Docker script ar https://hub.docker.com/r/lindenb/jvarkit/tags.</p> How does that page look like? <p>Here is how https://hub.docker.com/r/lindenb/jvarkit/tags looks like:</p> <p></p> <p>We can see there that <code>1b2aedf24</code> is the tag for the latest version.</p> <pre><code>sudo singularity build my_container.sif docker:lindenb/jvarkit:1b2aedf24\n</code></pre> How does that look like? <pre><code>$ sudo singularity build my_container.sif docker:lindenb/jvarkit\nINFO:    Starting build...\nINFO:    Fetching OCI image...\n28.2MiB / 28.2MiB [================================================================================================================================================] 100 % 2.5 MiB/s 0s\n1.0GiB / 1.0GiB [==================================================================================================================================================] 100 % 2.5 MiB/s 0s\nINFO:    Extracting OCI image...\nINFO:    Inserting Singularity configuration...\nINFO:    Creating SIF file...\nINFO:    Build complete: my_container.sif\n</code></pre> <p>Works!</p>","tags":["Singularity","Singularity script","create","build","Docker Hub"]},{"location":"software/create_singularity_container_from_dockerhub/#2-use-the-singularity-container","title":"2. Use the Singularity container","text":"<pre><code>./my_container.sif [your command-line arguments]\n</code></pre> <p>For example, in this case:</p> <pre><code>./my_container.sif --help\n</code></pre> <p>However, this container is setup differently. From the documentation, one find that this container is used as such:</p> <pre><code>./jvarkit.sif java -jar /opt/jvarkit/dist/jvarkit.jar --help\n</code></pre>","tags":["Singularity","Singularity script","create","build","Docker Hub"]},{"location":"software/darsync/","title":"darsync","text":""},{"location":"software/darsync/#darsync","title":"Darsync","text":"<p>Darsync is a tool used to prepare your project for transfer to Dardel. It has two modes; check mode where it goes through your files and looks for uncompressed file formats and counts the number of files, and gen mode where it generates a script file you can submit to Slurm to do the actual data transfer.</p> <p>The idea is to</p> <ol> <li>Run the check mode and mitigate any problems problems it finds.</li> <li>Run the gen mode.</li> <li>Submit the generated script as a job.</li> </ol> <pre><code>flowchart TD\n  check[Check files]\n  generate[Generate script for transferring files safely]\n  submit[Submit script]\n\n  check --&gt; |no errors| generate\n  check --&gt; |errors that need fixing| check\n  generate --&gt; |no errors| submit</code></pre> <p>The Darsync workflow</p> <p>Temporarily add a <code>PATH</code></p> <p>Until the darsync script is added to the <code>/sw/uppmax/bin</code> folder you will have to add its location to your <code>PATH</code> variable manually:</p> <pre><code>export PATH=$PATH:/proj/staff/dahlo/testarea/darsync\n</code></pre>"},{"location":"software/darsync/#tldr","title":"TLDR","text":"<p>If you know your way around Linux, here is the short version.</p> <pre><code># run check\ndarsync check -l /path/to/dir\n\n# fix warnings on your own\n\n# book a 30 day single core job on Snowy and run the rsync command\nrsync -e \"ssh -i ~/.ssh/id_rsa\" -acPuv /local/path/to/files/ username@dardel.pdc.kth.se:/remote/path/to/files/\n</code></pre> How does that look like? <p>Running the temporary export gives no output:</p> <pre><code>[sven@rackham4 ~]$ export PATH=$PATH:/proj/staff/dahlo/testarea/darsync\n</code></pre> <p>The folder <code>GitHubs</code> is a folder containing multiple GitHub repositories and is chosen as the test subject:</p> <pre><code>[sven@rackham4 ~]$ darsync check -l GitHubs/\n\n\n   ____ _   _ _____ ____ _  __\n  / ___| | | | ____/ ___| |/ /\n | |   | |_| |  _|| |   | ' /\n | |___|  _  | |__| |___| . \\\n  \\____|_| |_|_____\\____|_|\\_\\\n\nThe check module of this script will recursivly go through\nall the files in, and under, the folder you specify to see if there\nare any improvments you can to do save space and speed up the data transfer.\n\nIt will look for file formats that are uncompressed, like fasta and vcf files\n(most uncompressed file formats have compressed variants of them that only\ntake up 25% of the space of the uncompressed file).\n\nIf you have many small files, e.g. folders with 100 000 or more files,\nit will slow down the data transfer since there is an overhead cost per file\nyou want to transfer. Large folders like this can be archived/packed into\na single file to speed things up.\nGitHubs/git/scripts\n\n\nChecking completed. Unless you got any warning messages above you should be good to go.\n\nGenerate a Slurm script file to do the transfer by running this script again, but use the 'gen' option this time.\nSee the help message for details, or continue reading the user guide for examples on how to run it.\nhttps://\n\ndarsync gen -h\n\nA file containing file ownership information,\ndarsync_GitHubs.ownership.gz\nhas been created. This file can be used to make sure that the\nfile ownership (user/group) will look the same on Dardel as it does here. See https:// for more info about this.\n</code></pre> NBIS staff test project code <p>Follow the UPPMAX project application procedure, then request permission to join project <code>NAISS 2023/22-1027</code></p>"},{"location":"software/darsync/#check-mode","title":"Check mode","text":"<p>To initiate the check mode you run Darsync with the check argument. If you run it without any other arguments it will ask you interactive questions to get the information it needs.</p> <pre><code># interactive mode\ndarsync check\n\n# or give it the path to the directory to check directly\ndarsync check -l /path/to/dir\n</code></pre> <p>The warnings you can get are:</p>"},{"location":"software/darsync/#too-many-uncompressed-files","title":"Too many uncompressed files","text":"<p>It looks for files with file endings matching common uncompressed file formats, like <code>.fq</code>, <code>.sam</code>, <code>.vcf</code>, <code>.txt</code>. If the combined file size of these files are above a threshold it will trigger the warning. Most programs that uses these formats can also read the compressed version of them.</p> <p>Examples of how to compress common formats:</p> <pre><code># fastq/fq/fasta/txt\ngzip file.fq\n\n# vcf\nbgzip file.vcf\n\n# sam\nsamtools view -b file.sam &gt; file.bam\n# when the above command is completed successfully:\n# rm file.sam\n</code></pre> <p>For examples on how to compress other file formats, use an internet search engine to look for</p> <pre><code>how to compress &lt;insert file format name&gt; file\n</code></pre>"},{"location":"software/darsync/#too-many-files","title":"Too many files","text":"<p>If a project consists of many small files it will decrease the data transfer speed, as there is an overhead cost to starting and stopping each file transfer. A way around this is to pack all the small files into a single <code>tar</code> archive, so that it only has to start and stop a single time.</p> <p>Example of how to pack a folder and all files in it into a single <code>tar</code> archive.</p> <pre><code># pack it\ntar -czvf folder.tar.gz /path/to/folder\n\n# unpack it after transfer\ntar -xzvf folder.tar.gz\n</code></pre> <p>Once you have mitigated any warnings you got you are ready to generate the Slurm script that will preform the data transfer.</p>"},{"location":"software/darsync/#gen-mode","title":"Gen mode","text":"<p>To generate a transfer script you will need to supply Darsync with some information. Make sure to have this readily available:</p> <ul> <li>ID of the UPPMAX project that will run the transfer job, e.g. <code>naiss2099-23-99</code><ul> <li>If you don't remember if, find the name of the project you want to transfer by looking in the list of active project in SUPR.</li> </ul> </li> <li>Path to the folder you want to transfer, .e.g. <code>/proj/naiss2099-23-999</code><ul> <li>Either transfer your whole project, or put the files and folder your want to transfer into a new folder in your project folder and transfer that folder.</li> <li>The project's folder on UPPMAX will be located in the <code>/proj/</code> folder, most likely a folder with the same name as the project's ID, <code>/proj/&lt;project id&gt;</code>, e.g. <code>/proj/naiss2024-23-999</code>. If your project has picked a custom directory name when it was created it will have that name instead of the project ID, e.g. <code>/proj/directory_name</code>. Check which directory name your project has by looking at the project's page in SUPR and look at the field called <code>Directory name:</code></li> </ul> </li> <li>Your Dardel username.<ul> <li>You can see your Dardel username in SUPR</li> </ul> </li> <li>The path on Dardel where you want to put your data, e.g. <code>/cfs/klemming/projects/snic/naiss2099-23-999</code><ul> <li>Check which project ID you have for your project on Dardel in the list of active project in SUPR.</li> </ul> </li> <li>The path to the SSH key you have prepared to be used to login from Rackham to Dardel, e.g. <code>~/.ssh/id_rsa</code><ul> <li>Check</li> </ul> </li> <li>The path to where you want to save the generated transfer script.</li> </ul> <p>To initiate the gen mode you run Darsync with the <code>gen</code> argument. If you run it without any other arguments it will ask you interactive questions to get the information it needs.</p> <pre><code># interactive mode\ndarsync gen\n\n\n# or give it any or all arguments directly\ndarsync check -l /path/to/dir/on/uppmax/ -r /path/to/dir/on/dardel/ -A naiss2099-23-99 -u dardel_username -s ~/.ssh/id_rsa -o ~/dardel_transfer_script.sh\n</code></pre>"},{"location":"software/darsync/#starting-the-transfer","title":"Starting the transfer","text":"<p>Before you submit the generated transfer script you should make sure everything is in order. You can try to run the transfer script directly on the UPPMAX login node and see if it starts or if you get any errors:</p> <pre><code>bash ~/dardel_transfer_script.sh\n</code></pre> <p>If you start see progress reports from <code>rsync</code> you know it works and you can press <code>ctrl+c</code> to stop.</p> <p>Example of how it can look when it works:</p> <pre><code>bash darsync_temp.slurm\nsending incremental file list\ntemp/\ntemp/counts\n             10 100%    0,51kB/s    0:00:00 (xfr#4, to-chk=72/77)\ntemp/export.sh\n             13 100%    0,67kB/s    0:00:00 (xfr#5, to-chk=71/77)\ntemp/my_stuff.py\n             70 100%    3,60kB/s    0:00:00 (xfr#7, to-chk=69/77)\ntemp/run.sh\n             52 100%    2,67kB/s    0:00:00 (xfr#8, to-chk=68/77)\ntemp/sequence_tools.py\n            345 100%   17,73kB/s    0:00:00 (xfr#9, to-chk=67/77)\ntemp/similar_sequences.txt\n             24 100%    1,23kB/s    0:00:00 (xfr#10, to-chk=66/77)\ntemp/t.py\n            328 100%   16,86kB/s    0:00:00 (xfr#11, to-chk=65/77)\n</code></pre> <p>Example of how it can look when it doesn't work:</p> <pre><code>bash darsync_temp.slurm\nuser@dardel.pdc.kth.se: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).\nrsync: connection unexpectedly closed (0 bytes received so far) [sender]\nrsync error: unexplained error (code 255) at io.c(231) [sender=3.2.7]\n</code></pre>"},{"location":"software/darsync/#troubleshooting","title":"Troubleshooting","text":"<p>Apart from getting the username or paths wrong, we foresee that the most common problem will be to get the SSH keys generated, added to the PDC login portal, and adding the UPPMAX ip/hostname as authorized for that SSH key. Please see the PDC user guide on how to set up SSH keys. Once you have your key created and added to the login portal, go to the login portal again and add the address <code>*.uppmax.uu.se</code> to your key to make it work from Rackham.</p>"},{"location":"software/darsync/#links","title":"Links","text":"<ul> <li>darsync GitHub repository</li> </ul>"},{"location":"software/debuggers/","title":"Debuggers","text":""},{"location":"software/debuggers/#debuggers","title":"Debuggers","text":"<p>There are debugging tools provided with each compiler.</p> <ul> <li>Allinea DDT: a C, C++ and Fortran debugger</li> <li><code>ddt</code>: the Allinea DDT C, C++ and Fortran debugger</li> <li><code>gdb</code>, the GNU debugger: works well with C, C++, and Fortran programs</li> <li><code>idb</code>, the Intel debugger (obsolete, use <code>gdb</code> instead)</li> <li><code>[some other compiler]</code>: works well with Fortran90/95 programs</li> </ul>"},{"location":"software/dnabert2/","title":"DNABERT 2","text":""},{"location":"software/dnabert2/#dnabert-2","title":"DNABERT 2","text":"<p>DNABERT 2 is 'a foundation model trained on large-scale multi-species genome that achieves the state-of-the-art performance on 28 tasks of the GUE benchmark', according to DNABERT 2</p> <p>DNABERT 2 is not part of the UPPMAX module system.</p> For UPPMAX staff <p>See this ticket. with our notes on installing and running DNABERT2 on Rackham and Snowy.</p>"},{"location":"software/dnabert2/#installing-dnabert-2","title":"Installing DNABERT 2","text":"<p>Run dnabert2_install_on_rackham.sh.</p>"},{"location":"software/dnabert2/#running-dnabert-2","title":"Running DNABERT 2","text":"<p>Run dnabert2_run_on_rackham.sh with the example Python script dnabert2_example.py.</p>"},{"location":"software/dnabert2/#links","title":"Links","text":"<ul> <li>DNABERT 2 GitHub repository</li> </ul>"},{"location":"software/doc/","title":"Software-specific documentation","text":""},{"location":"software/emacs/","title":"Emacs","text":""},{"location":"software/emacs/#emacs","title":"Emacs","text":"<p>UPPMAX has multiple text editors available. This page describes the Emacs text editor.</p> <p>Emacs is an advanced terminal editor that is fast fast and powerful, once you learn it.</p>"},{"location":"software/emacs/#examples-how-to-use-emacs","title":"Examples how to use Emacs","text":"<p>Start <code>emacs</code> on a terminal with:</p> <pre><code>emacs\n</code></pre> <p>Start <code>emacs</code> to edit a file:</p> <pre><code>emacs filename\n</code></pre> <p>Start Emacs keeping you in your terminal window:</p> <pre><code>emacs \u2013nw\n</code></pre> <p>Do the editing you want, then save with:</p> <pre><code>Control-x, Control-s\n</code></pre> <p>Exit emacs with:</p> <pre><code>Control-x, Control-c\n</code></pre> <p>You can read a tutorial in emacs by doing:</p> <pre><code>Control-h t\n</code></pre>"},{"location":"software/eog/","title":"eog","text":""},{"location":"software/eog/#eog","title":"<code>eog</code>","text":"<p><code>eog</code> is a tool to view images on an UPPMAX cluster.</p> <p>To be able to see the images, either use SSH with X-forwarding or login to a remote desktop</p> <p>Usage:</p> <pre><code>eog [filename]\n</code></pre> <p>for example:</p> <pre><code>eog my.png\n</code></pre> Need an example image to work with? <p>In the terminal, do:</p> <pre><code>convert -size 32x32 xc:transparent my.png\n</code></pre> <p>This will create an empty PNG image.</p> How does this look like? <p></p>"},{"location":"software/filezilla/","title":"FileZilla","text":"","tags":["FileZilla","transfer","software"]},{"location":"software/filezilla/#filezilla","title":"FileZilla","text":"<p>FileZilla connected to Bianca</p> <p>FileZilla is a free, open-source and cross-platform tool to transfer files.</p>","tags":["FileZilla","transfer","software"]},{"location":"software/filezilla/#install-filezilla","title":"Install FileZilla","text":"<p>You can download and install FileZilla for your operating system from one or more locations:</p> Operating system Location(s) Linux FileZilla website, Ubuntu App Center MacOS (intel) FileZilla website MacOS (Apple Silicon) FileZilla website Windows UU Software Center (Suggested!), FileZilla website <p>AI tool caution</p> <p>If you have a UU registered Windows or MacOS laptop, we suggest downloading from the UU Software Center, if FileZilla is available there.</p>","tags":["FileZilla","transfer","software"]},{"location":"software/filezilla/#transfer-files","title":"Transfer files","text":"<ul> <li>Transfer file to/from Bianca using FileZilla</li> <li>Transfer file to/from Rackham using FileZilla</li> <li>Transfer file to/from Transit using FileZilla</li> </ul>","tags":["FileZilla","transfer","software"]},{"location":"software/finishedjobinfo/","title":"finishedjobinfo","text":""},{"location":"software/finishedjobinfo/#finishedjobinfo","title":"<code>finishedjobinfo</code>","text":"<p><code>finishedjobinfo</code> shows information on jobs that have finished, which is useful to help optimize Slurm jobs.</p>"},{"location":"software/finishedjobinfo/#usage","title":"Usage","text":"<pre><code>finishedjobinfo\n</code></pre> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ finishedjobinfo\n2024-10-08 00:00:01 jobid=50661814 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r65 procs=1 partition=core qos=normal jobname=P8913_295.chr12 maxmemory_in_GiB=2.1 maxmemory_node=r65 timelimit=12:00:00 submit_time=2024-10-07T21:07:37 start_time=2024-10-07T21:15:52 end_time=2024-10-08T00:00:01 runtime=02:44:09 margin=09:15:51 queuetime=00:08:15\n2024-10-08 00:00:09 jobid=50661456 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r437 procs=1 partition=core qos=normal jobname=P8913_276.chr16 maxmemory_in_GiB=2.1 maxmemory_node=r437 timelimit=12:00:00 submit_time=2024-10-07T21:07:29 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:00:09 runtime=02:48:44 margin=09:11:16 queuetime=00:03:56\n2024-10-08 00:00:13 jobid=50661186 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r349 procs=1 partition=core qos=normal jobname=P8913_262.chr13 maxmemory_in_GiB=2.1 maxmemory_node=r349 timelimit=12:00:00 submit_time=2024-10-07T21:07:23 start_time=2024-10-07T21:11:23 end_time=2024-10-08T00:00:13 runtime=02:48:50 margin=09:11:10 queuetime=00:04:00\n2024-10-08 00:00:19 jobid=50661172 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r344 procs=1 partition=core qos=normal jobname=P8913_261.chr18 maxmemory_in_GiB=2.1 maxmemory_node=r344 timelimit=12:00:00 submit_time=2024-10-07T21:07:23 start_time=2024-10-07T21:11:23 end_time=2024-10-08T00:00:19 runtime=02:48:56 margin=09:11:04 queuetime=00:04:00\n2024-10-08 00:00:23 jobid=50661695 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r370 procs=1 partition=core qos=normal jobname=P8913_289.chr7 maxmemory_in_GiB=2.1 maxmemory_node=r370 timelimit=12:00:00 submit_time=2024-10-07T21:07:35 start_time=2024-10-07T21:15:49 end_time=2024-10-08T00:00:23 runtime=02:44:34 margin=09:15:26 queuetime=00:08:14\n2024-10-08 00:00:27 jobid=50661466 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r438 procs=1 partition=core qos=normal jobname=P8913_277.chr7 maxmemory_in_GiB=2.1 maxmemory_node=r438 timelimit=12:00:00 submit_time=2024-10-07T21:07:29 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:00:27 runtime=02:49:02 margin=09:10:58 queuetime=00:03:56\n2024-10-08 00:00:39 jobid=50661663 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r360 procs=1 partition=core qos=normal jobname=P8913_287.chr13 maxmemory_in_GiB=2.1 maxmemory_node=r360 timelimit=12:00:00 submit_time=2024-10-07T21:07:34 start_time=2024-10-07T21:15:49 end_time=2024-10-08T00:00:39 runtime=02:44:50 margin=09:15:10 queuetime=00:08:15\n2024-10-08 00:00:43 jobid=50661471 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r441 procs=1 partition=core qos=normal jobname=P8913_277.chr12 maxmemory_in_GiB=2.1 maxmemory_node=r441 timelimit=12:00:00 submit_time=2024-10-07T21:07:30 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:00:43 runtime=02:49:18 margin=09:10:42 queuetime=00:03:55\n2024-10-08 00:00:58 jobid=50661227 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r387 procs=1 partition=core qos=normal jobname=P8913_264.chr16 maxmemory_in_GiB=2.1 maxmemory_node=r387 timelimit=12:00:00 submit_time=2024-10-07T21:07:24 start_time=2024-10-07T21:11:23 end_time=2024-10-08T00:00:58 runtime=02:49:35 margin=09:10:25 queuetime=00:03:59\n2024-10-08 00:01:00 jobid=50661458 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r437 procs=1 partition=core qos=normal jobname=P8913_276.chr18 maxmemory_in_GiB=2.1 maxmemory_node=r437 timelimit=12:00:00 submit_time=2024-10-07T21:07:29 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:01:00 runtime=02:49:35 margin=09:10:25 queuetime=00:03:56\n</code></pre>"},{"location":"software/finishedjobinfo/#show-the-help","title":"Show the help","text":"<p>To show the help of <code>finishedjobinfo</code>, in a terminal , do:</p> <pre><code>finishedjobinfo -h\n</code></pre> How does that look like? <p>```bash [sven@rackham3 ~]$ finishedjobinfo -h Usage: finishedjobinfo  [-h] [-M cluster_name] [-j jobid[,jobid...]] [-m|-y|-s YYYY-MM-DD[/hhss]] [-e YYYY-MM-DD[/hhss]] [project_or_user]...     -h        Ask for help     -M        Request data from a named other cluster     -j        Request data for a specific jobid or jobids (comma-separated)     -q        Quiet, quick, abbreviated output (no QOS or memory information)     -v        Verbose, tells a little more     -m        Start time is start of this month     -y        Start time is start of this year     -s        Request a start time (default is a month back in time)     -e        Request an end time (default is now)     Time can also be specified as NOW, TODAY, YYYY, YYYY-MM, YYYY-w, w, hhss, or name of month</p> <p>Meaning of jobstate: CANCELLED    Job was cancelled, before or after it had started COMPLETED    Job run to finish, last command gave exit code 0 FAILED        Job crashed or at least ended with an exit code that was not 0 NODE_FAIL    One of your job nodes experienced a major problem, perhaps your job used all available memory TIMEOUT        Job exceeded the specified timelimit and was therefore terminated ````</p>"},{"location":"software/finishedjobinfo/#show-the-information-about-a-specific-job","title":"Show the information about a specific job","text":"<p>Use <code>finishedjobinfo -j [job_number]</code> to get information about a specific job, where <code>[job_number]</code> is the job number, for example <code>finishedjobinfo -j 44981366</code>.</p> How does that look like? <p>Here is an example output:</p> <pre><code>[sven@rackham3 ~]$ finishedjobinfo -j 44981366\n2024-02-09 12:30:37 jobid=44981366 jobstate=TIMEOUT username=sven account=staff nodes=r35 procs=1 partition=core qos=normal jobname=run_beast2.sh maxmemory_in_GiB=0.1 maxmemory_node=r35 timelimit=00:01:00 submit_time=2024-02-09T12:27:29 start_time=2024-02-09T12:29:18 end_time=2024-02-09T12:30:37 runtime=00:01:19 margin=-00:00:19 queuetime=00:01:49\n</code></pre> <p>2024-02-09 12:30:37 jobid=44981366 jobstate=TIMEOUT username=sven account=staff nodes=r35 procs=1 partition=core qos=normal jobname=run_beast2.sh maxmemory_in_GiB=0.1 maxmemory_node=r35 timelimit=00:01:00 submit_time=2024-02-09T12:27:29 start_time=2024-02-09T12:29:18 end_time=2024-02-09T12:30:37 runtime=00:01:19 margin=-00:00:19 queuetime=00:01:49</p>"},{"location":"software/finishedjobinfo/#how-do-i-find-jobs-that-have-finished-and-took-longer-than-an-hour-and-less-than-a-day","title":"How do I find jobs that have finished and took longer than an hour and less than a day?","text":"<pre><code>finishedjobinfo | grep \"runtime.[0-9][1-9]\"\n</code></pre> <p>Press <code>CTRL-C</code> to stop the process: it will take very long to finish.</p> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ finishedjobinfo | grep \"runtime.[0-9][1-9]\"\n2024-10-08 00:00:01 jobid=50661814 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r65 procs=1 partition=core qos=normal jobname=P8913_295.chr12 maxmemory_in_GiB=2.1 maxmemory_node=r65 timelimit=12:00:00 submit_time=2024-10-07T21:07:37 start_time=2024-10-07T21:15:52 end_time=2024-10-08T00:00:01 runtime=02:44:09 margin=09:15:51 queuetime=00:08:15\n2024-10-08 00:00:09 jobid=50661456 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r437 procs=1 partition=core qos=normal jobname=P8913_276.chr16 maxmemory_in_GiB=2.1 maxmemory_node=r437 timelimit=12:00:00 submit_time=2024-10-07T21:07:29 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:00:09 runtime=02:48:44 margin=09:11:16 queuetime=00:03:56\n2024-10-08 00:00:13 jobid=50661186 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r349 procs=1 partition=core qos=normal jobname=P8913_262.chr13 maxmemory_in_GiB=2.1 maxmemory_node=r349 timelimit=12:00:00 submit_time=2024-10-07T21:07:23 start_time=2024-10-07T21:11:23 end_time=2024-10-08T00:00:13 runtime=02:48:50 margin=09:11:10 queuetime=00:04:00\n2024-10-08 00:00:19 jobid=50661172 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r344 procs=1 partition=core qos=normal jobname=P8913_261.chr18 maxmemory_in_GiB=2.1 maxmemory_node=r344 timelimit=12:00:00 submit_time=2024-10-07T21:07:23 start_time=2024-10-07T21:11:23 end_time=2024-10-08T00:00:19 runtime=02:48:56 margin=09:11:04 queuetime=00:04:00\n2024-10-08 00:00:23 jobid=50661695 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r370 procs=1 partition=core qos=normal jobname=P8913_289.chr7 maxmemory_in_GiB=2.1 maxmemory_node=r370 timelimit=12:00:00 submit_time=2024-10-07T21:07:35 start_time=2024-10-07T21:15:49 end_time=2024-10-08T00:00:23 runtime=02:44:34 margin=09:15:26 queuetime=00:08:14\n2024-10-08 00:00:27 jobid=50661466 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r438 procs=1 partition=core qos=normal jobname=P8913_277.chr7 maxmemory_in_GiB=2.1 maxmemory_node=r438 timelimit=12:00:00 submit_time=2024-10-07T21:07:29 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:00:27 runtime=02:49:02 margin=09:10:58 queuetime=00:03:56\n2024-10-08 00:00:39 jobid=50661663 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r360 procs=1 partition=core qos=normal jobname=P8913_287.chr13 maxmemory_in_GiB=2.1 maxmemory_node=r360 timelimit=12:00:00 submit_time=2024-10-07T21:07:34 start_time=2024-10-07T21:15:49 end_time=2024-10-08T00:00:39 runtime=02:44:50 margin=09:15:10 queuetime=00:08:15\n</code></pre> <p>This output took around 1 second to produce.</p>"},{"location":"software/finishedjobinfo/#how-do-i-find-jobs-that-have-finished-and-took-longer-than-an-hour","title":"How do I find jobs that have finished and took longer than an hour?","text":"<pre><code>finishedjobinfo | grep -E \"runtime.([0-9]-)?[0-9][1-9]\"\n</code></pre> <p>Press <code>CTRL-C</code> to stop the process: it will take very long to finish.</p> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ finishedjobinfo | grep -E \"runtime.([0-9]-)?[0-9][1-9]\"\n2024-10-08 00:00:01 jobid=50661814 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r65 procs=1 partition=core qos=normal jobname=P8913_295.chr12 maxmemory_in_GiB=2.1 maxmemory_node=r65 timelimit=12:00:00 submit_time=2024-10-07T21:07:37 start_time=2024-10-07T21:15:52 end_time=2024-10-08T00:00:01 runtime=02:44:09 margin=09:15:51 queuetime=00:08:15\n2024-10-08 00:00:09 jobid=50661456 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r437 procs=1 partition=core qos=normal jobname=P8913_276.chr16 maxmemory_in_GiB=2.1 maxmemory_node=r437 timelimit=12:00:00 submit_time=2024-10-07T21:07:29 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:00:09 runtime=02:48:44 margin=09:11:16 queuetime=00:03:56\n2024-10-08 00:00:13 jobid=50661186 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r349 procs=1 partition=core qos=normal jobname=P8913_262.chr13 maxmemory_in_GiB=2.1 maxmemory_node=r349 timelimit=12:00:00 submit_time=2024-10-07T21:07:23 start_time=2024-10-07T21:11:23 end_time=2024-10-08T00:00:13 runtime=02:48:50 margin=09:11:10 queuetime=00:04:00\n2024-10-08 00:00:19 jobid=50661172 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r344 procs=1 partition=core qos=normal jobname=P8913_261.chr18 maxmemory_in_GiB=2.1 maxmemory_node=r344 timelimit=12:00:00 submit_time=2024-10-07T21:07:23 start_time=2024-10-07T21:11:23 end_time=2024-10-08T00:00:19 runtime=02:48:56 margin=09:11:04 queuetime=00:04:00\n2024-10-08 00:00:23 jobid=50661695 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r370 procs=1 partition=core qos=normal jobname=P8913_289.chr7 maxmemory_in_GiB=2.1 maxmemory_node=r370 timelimit=12:00:00 submit_time=2024-10-07T21:07:35 start_time=2024-10-07T21:15:49 end_time=2024-10-08T00:00:23 runtime=02:44:34 margin=09:15:26 queuetime=00:08:14\n2024-10-08 00:00:27 jobid=50661466 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r438 procs=1 partition=core qos=normal jobname=P8913_277.chr7 maxmemory_in_GiB=2.1 maxmemory_node=r438 timelimit=12:00:00 submit_time=2024-10-07T21:07:29 start_time=2024-10-07T21:11:25 end_time=2024-10-08T00:00:27 runtime=02:49:02 margin=09:10:58 queuetime=00:03:56\n2024-10-08 00:00:39 jobid=50661663 jobstate=COMPLETED username=mrendon account=naiss2023-5-478 nodes=r360 procs=1 partition=core qos=normal jobname=P8913_287.chr13 maxmemory_in_GiB=2.1 maxmemory_node=r360 timelimit=12:00:00 submit_time=2024-10-07T21:07:34 start_time=2024-10-07T21:15:49 end_time=2024-10-08T00:00:39 runtime=02:44:50 margin=09:15:10 queuetime=00:08:15\n</code></pre> <p>This output took around 1 second to produce.</p>"},{"location":"software/finishedjobinfo/#how-do-i-find-jobs-that-have-finished-and-took-longer-than-a-day","title":"How do I find jobs that have finished and took longer than a day?","text":"<pre><code>finishedjobinfo | grep \"runtime.[0-9]-\"\n</code></pre> <p>Press <code>CTRL-C</code> to stop the process: it will take very long to finish.</p> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham1 ~]$ finishedjobinfo | grep \"runtime.[0-9]-\"\n2024-10-08 00:01:18 jobid=50597318 jobstate=COMPLETED username=nikolay account=naiss2024-22-35 nodes=r356 procs=20 partition=node qos=normal jobname=168011 maxmemory_in_GiB=5.3 maxmemory_node=r356 timelimit=10-00:00:00 submit_time=2024-10-02T10:36:59 start_time=2024-10-06T21:05:31 end_time=2024-10-08T00:01:18 runtime=1-02:55:47 margin=8-21:04:13 queuetime=4-10:28:32\n2024-10-08 00:21:55 jobid=50597286 jobstate=COMPLETED username=nikolay account=naiss2024-22-35 nodes=r432 procs=20 partition=node qos=normal jobname=1578718 maxmemory_in_GiB=5.3 maxmemory_node=r432 timelimit=10-00:00:00 submit_time=2024-10-02T10:36:10 start_time=2024-10-06T14:32:36 end_time=2024-10-08T00:21:55 runtime=1-09:49:19 margin=8-14:10:41 queuetime=4-03:56:26\n</code></pre> <p>This output took 30 seconds to produce as there were few jobs at that time that took longer than a day to finish.</p>"},{"location":"software/games_us/","title":"GAMESS_US","text":""},{"location":"software/games_us/#gamess-us-user-guide","title":"GAMESS-US user guide","text":"<p>GAMESS-US versions 20170930 is installed on Rackham. Newer versions can be installed on request to UPPMAX support. Snowy currently lacks GAMESS-US.</p>"},{"location":"software/games_us/#running-gamess","title":"Running GAMESS","text":"<p>Load the module using</p> <pre><code>module load gamess/20170930\n</code></pre> <p>Below is an example submit script for Rackham, running on 40 cores (2 nodes with 20 cores each). It is essential to specify the project name:</p> <pre><code>#!/bin/bash -l\n#SBATCH -J jobname\n#SBATCH -p node -n 40\n#SBATCH -A PROJECT\n#SBATCH -t 03:00:00\n\nmodule load gamess/20170930\n\nrungms gms &gt;gms.out\n</code></pre>"},{"location":"software/games_us/#memory-specification","title":"Memory specification","text":"<p>GAMESS uses two kinds of memory: replicated memory and distributed memory. Both kinds of memory should be given in the $SYSTEM specification. Replicated memory is specified using the MWORDS keyword and distributed memory with the MEMDDI keyword. It is very important that you understand the uses of these keywords. Check the GAMESS documentation for further information.</p> <p>If your job requires 16MW (mega-words) of replicated memory and 800MW of distributed memory, as in the example below, the memory requirements per CPU core varies as 16+800/N where N is the number of cores. Each word is 8 bytes of memory, why the amount of memory per core is (16+800/N)*8. The amount of memory per node depends on the number of cores per node. Rackham has 20 cores per node, most nodes have 128 GB of memory, but 30 nodes have 512 GB and 4 nodes at 1 TB.</p>"},{"location":"software/games_us/#communication","title":"Communication","text":"<p>For intra-node communication shared memory is used. For inter-node communication MPI is used which uses the InfiniBand interconnect.</p>"},{"location":"software/games_us/#citing-gamess-papers","title":"Citing GAMESS papers","text":"<p>It is essential that you read the GAMESS manual thoroughly to properly reference the papers specified in the instructions. All publications using GAMESS should cite at least the following paper:</p> <pre><code>@Article{GAMESS,\nauthor={M.W.Schmidt and K.K.Baldridge and J.A.Boatz and S.T.Elbert and\nM.S.Gordon and J.J.Jensen and S.Koseki and N.Matsunaga and\nK.A.Nguyen and S.Su and T.L.Windus and M.Dupuis and J.A.Montgomery},\njournal={J.~Comput.~Chem.},\nvolume=14,\npages={1347},\nyear=1993,\ncomment={The GAMESS program}}\n</code></pre> <p>If you need to obtain GAMESS yourself, please visit the GAMESS website for further instructions.</p>"},{"location":"software/gaussian/","title":"Gaussian","text":""},{"location":"software/gaussian/#gaussian-09-user-guide","title":"Gaussian 09 user guide","text":"<p>A short guide on how to run g09 on UPPMAX.</p>"},{"location":"software/gaussian/#access-to-gaussian-09","title":"Access to Gaussian 09","text":"<p>Gaussian 09 is available at UPPMAX. Uppsala University has an university license for all employees. If you want to be able to run g09 email support@uppmax.uu.se and ask to be added to the g09 group.</p>"},{"location":"software/gaussian/#running-g09","title":"Running g09","text":"<p>In order to run g09 you must first set up the correct environment. You load this module with:</p> <pre><code>module load gaussian/g09.d01\n</code></pre>"},{"location":"software/gaussian/#running-single-core-jobs-in-slurm","title":"Running single core jobs in Slurm","text":"<p>Here is an example of a submit script for Slurm:</p> <pre><code>#!/bin/bash -l\n#SBATCH -J g09test\n#SBATCH -p core\n#SBATCH -n 1\n#If you ask for a single core in slurm on Rackham you get 6.4 Gb of memory\n#SBATCH -t 1:00:00\n#SBATCH -A your_project_name\n\nmodule load gaussian/g09.d01\ng09 mp2.inp mp2.out\n</code></pre> <p>If you run a single core job on Rackham you can't use more than 6.4GB of memory.</p> <p>When specifying the memory requirements, make sure that you ask for some more memory in the submit-script than in g09 to allow for some memory overhead for the program. As a general rule you should ask for 200MB more than you need in the calculation.</p> <p>The <code>mp2.inp</code> input file in the example above:</p> <pre><code>%Mem=800MB\n#P MP2 aug-cc-pVTZ OPT\n\ntest\n\n0 1\nLi\nF 1 1.0\n</code></pre>"},{"location":"software/gaussian/#scratch-space","title":"Scratch space","text":"<p>The g09 module sets the environment <code>GAUSS_SCRDIR</code> to <code>/scratch/$SLURM_JOBID</code> in slurm. These directories are removed after the job is finished.</p> <p>If you want to set <code>GAUSS_SCRDIR</code>, you must do it after module load <code>gaussian/g09.a02</code> in your script.</p> <p>If you set <code>GAUSS_SCRDIR</code> to something else in your submit script remember to remove all unwanted files after your job has finished.</p> <p>If you think you will use a large amount of scratch space, you might want to set maxdisk in your input file. You can either set maxdisk directly on the command line in your input file:</p> <pre><code>#P MP2 aug-cc-pVTZ SCF=Tight maxdisk=170GB\n</code></pre> <p>or you can put something like:</p> <pre><code>MAXDISK=$( df | awk '/scratch/ { print $4 }' )KB\nsed -i '/^#/ s/ maxdisk=[[:digit:]]*KB//' inputfile\nsed -i '/^#/ s/$/ maxdisk='$MAXDISK'/'; inputfile\n</code></pre> <p>in your scriptfile. This will set maxdisk to the currently available size of the /scratch disk on the node you will run on. Read more on maxdisk in the online manual.</p>"},{"location":"software/gaussian/#running-g09-in-parallel","title":"Running g09 in parallel","text":"<p>Gaussian can be run in parallel on a single node using shared memory. This is the input file for the slurm example below:</p> <p>The <code>dimer4.inp</code> input:</p> <pre><code>%Mem=3800MB\n%NProcShared=4\n#P MP2 aug-cc-pVTZ SCF=Tight\n\nmethanol dimer MP2\n\n0 1\n6 0.754746 -0.733607 -0.191063\n1 -0.033607 -1.456810 -0.395634\n1 1.007890 -0.778160 0.867678\n1 1.635910 -0.998198 -0.774627\n8 0.317192 0.576306 -0.534002\n1 1.033100 1.188210 -0.342355\n6 1.513038 3.469264 0.971885\n1 1.118398 2.910304 1.819367\n1 0.680743 3.818664 0.361783\n1 2.062618 4.333044 1.344537\n8 2.372298 2.640544 0.197416\n1 2.702458 3.161614 -0.539550\n</code></pre>"},{"location":"software/gaussian/#running-g09-in-parallel-in-slurm","title":"Running g09 in parallel in slurm","text":"<p>This can be done by asking for CPUs on the same node using the parallel node environments and telling Gaussian to use several CPUs using the <code>NProcShared</code> link 0 command.</p> <p>An example submit-script:</p> <pre><code>#!/bin/bash -l\n#SBATCH -J g09_4\n#SBATCH -p node -n 8\n#SBATCH -t 1:00:00\n#SBATCH -A your_project_name\n\nmodule load gaussian/g09.d01\nexport OMP_NUM_THREADS=1\nulimit -s $STACKLIMIT\n\ng09 dimer4.inp dimer4.out\n</code></pre> <p>Notice that 8 cores are requested from the queue-system using the line <code>#SLURM -p node -n 8</code> and that Gaussian is told to use 4 cores with the link 0 command <code>%NProcShared=4</code>. The example above runs about 1.7 times as fast on eight cores than on four, just change in the input file to <code>%NProcShared=8</code>. Please benchmark your own inputs as the speedup depends heavily on the method and size of system. In some cases Gaussian cannot use all the cpus you ask for. This is indicated in the output with lines looking like this:</p> <p>PrsmSu: requested number of processors reduced to: 1 ShMem 1 Linda.</p> <p>The reason for specifying <code>OMP_NUM_THREADS=1</code> is to not use the parts of OpenMP in the Gaussian code, but to use Gaussians own threads.</p>"},{"location":"software/gaussian/#running-g09-in-parallel-with-linda","title":"Running g09 in parallel with linda","text":"<p>In order to run g09 in parallel over several nodes we have acquired Linda TCP.</p>"},{"location":"software/gaussian/#running-g09-in-parallel-with-linda-in-slurm","title":"Running g09 in parallel with linda in slurm","text":"<p>This can be done by asking for CPUs on the same node using the parallel node environments and telling Gaussian to use several CPUs using the <code>NProcLinda</code> and <code>NProcShared</code> link 0 command.</p> <p>An example submit-script:</p> <pre><code>#!/bin/bash -l\n#SBATCH -J g09-linda\n#\n#SBATCH -t 2:00:0\n#\n#SBATCH -p node -n 40\n#SBATCH -A your_project_name\n\nmodule load gaussian/g09.d01\nulimit -s $STACKLIMIT\nexport OMP_NUM_THREADS=1\n\n#Next lines are there for linda to know what nodes to run on\nsrun hostname -s | sort -u &gt; tsnet.nodes.$SLURM_JOBID\nexport GAUSS_LFLAGS='-nodefile tsnet.nodes.$SLURM_JOBID -opt \"Tsnet.Node.lindarsharg: ssh\"'\n\n#export GAUSS_SCRDIR=\ntime g09 dimer20-2.inp dimer20-2.out\n\nrm tsnet.nodes.$SLURM_JOBID\n</code></pre> <p>Here is the input file:</p> <pre><code>%NProcLinda=2\n%NProcShared=20\n%Mem=2800MB\n#P MP2 aug-cc-pVTZ SCF=Tight\n\nmethanol dimer MP2\n\n0 1\n6 0.754746 -0.733607 -0.191063\n1 -0.033607 -1.456810 -0.395634\n1 1.007890 -0.778160 0.867678\n1 1.635910 -0.998198 -0.774627\n8 0.317192 0.576306 -0.534002\n1 1.033100 1.188210 -0.342355\n6 1.513038 3.469264 0.971885\n1 1.118398 2.910304 1.819367\n1 0.680743 3.818664 0.361783\n1 2.062618 4.333044 1.344537\n8 2.372298 2.640544 0.197416\n1 2.702458 3.161614 -0.539550\n</code></pre> <p>Notice that 40 cores are requested from the queue-system using the line <code>#SLURM -p node -n 40</code> and that g09 is told to use 2 nodes via linda with the <code>%NProcLinda=2</code> link 0 command and 20 cores on each node with the link 0 command <code>%NProcShared=20</code>.</p> <p>Please benchmark your own inputs as the speedup depends heavily on the method and size of system.</p> <p>In some cases Gaussian cannot use all the cpus you ask for. This is indicated in the output with lines looking like this:</p> <pre><code>_ PrsmSu: requested number of processors reduced to: 1 ShMem 1 Linda._\n</code></pre>"},{"location":"software/gaussian/#number-of-cpus-on-the-shared-memory-nodes","title":"Number of CPUs on the shared memory nodes","text":"<p>Use the information below as a guide to how many CPUs to request for your calculation:</p>"},{"location":"software/gaussian/#on-rackham","title":"On Rackham","text":"<ul> <li>272 nodes with two 10-core CPUs and 128GB memory</li> <li>32 nodes with two 10-core CPUs and 256GB memory</li> </ul>"},{"location":"software/gaussian/#on-milou","title":"On Milou","text":"<ul> <li>174 nodes with two 8-core CPUs and 128GB memory</li> <li>17 nodes with two 8-core CPUs and 256GB memory</li> <li>17 nodes with two 8-core CPUs and 512GB memory</li> </ul>"},{"location":"software/gaussian/#note-on-chk-files","title":"Note on chk-files","text":"<p>You may experience difficulties if you mix different versions (g09 and g03) or revisions of gaussian. If you use a checkpoint file (.chk file) from an older revision (say g03 e.01), in a new calculation with revision a.02, g09 may not run properly.</p> <p>We recommend using the same revision if you want to restart a calculation or reuse an older checkpoint file.</p>"},{"location":"software/gcc/","title":"GCC/gcc","text":""},{"location":"software/gcc/#gccgcc","title":"GCC/<code>gcc</code>","text":"<p>GCC is shorthand for 'GNU Compiler Collection', a collection of compilers, where <code>gcc</code> is the name of the actual program.</p> <p><code>gcc</code> is part of the <code>gcc</code> module.</p> How do I find the <code>gcc</code> module? <p>Like you'd find any module:</p> <pre><code>module spider gcc\n</code></pre> Which versions does the <code>gcc</code> module have? <p>Like you'd find the version of any module:</p> <pre><code>module spider gcc\n</code></pre> <p>This will look similar to this:</p> <pre><code>[sven@rackham2 ~]$ module spider gcc\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  gcc:\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        gcc/4.2.3\n        gcc/4.3.0\n        gcc/4.4\n        gcc/4.8.2\n        gcc/4.8.3\n        gcc/4.9.2\n        gcc/4.9.4\n        gcc/5.2.0\n        gcc/5.3.0\n        gcc/5.4.0\n        gcc/5.5.0\n        gcc/6.1.0\n        gcc/6.2.0\n        gcc/6.3.0\n        gcc/6.4.0\n        gcc/7.1.0\n        gcc/7.2.0\n        gcc/7.3.0\n        gcc/7.4.0\n        gcc/8.1.0\n        gcc/8.2.0\n        gcc/8.3.0\n        gcc/8.4.0\n        gcc/9.1.0\n        gcc/9.2.0\n        gcc/9.3.0\n        gcc/10.1.0\n        gcc/10.2.0\n        gcc/10.3.0\n        gcc/11.2.0\n        gcc/11.3.0\n        gcc/12.1.0\n        gcc/12.2.0\n        gcc/12.3.0\n        gcc/13.1.0\n        gcc/13.2.0\n        gcc/13.3.0\n        gcc/14.1.0\n     Other possible modules matches:\n        GCC  GCCcore  gcccuda\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*gcc.*'\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"gcc\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider gcc/14.1.0\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The GCC can be used to:</p> <ul> <li>Compile a C program</li> <li>Compile a C++ program</li> <li>Compile a Fortran program</li> </ul> <p>Working together with GCC are:</p> <ul> <li>A debugger called <code>gdb</code></li> <li>A profiler called <code>gprof</code></li> <li>A general profiler called Valgrind</li> </ul>"},{"location":"software/gcc_compile_c/","title":"Compile C using GCC","text":""},{"location":"software/gcc_compile_c/#compile-c-using-gcc","title":"Compile C using GCC","text":"<p>GCC (shorthand for 'GNU Compiler Collection') is a collection of compilers able to compile multiple different programming languages.</p> <p>This page describes how to compile C code using the GCC.</p>"},{"location":"software/gcc_compile_c/#procedure","title":"Procedure","text":""},{"location":"software/gcc_compile_c/#0-create-a-c-source-file","title":"0. Create a C source file","text":"<p>You will need C code to work on.</p> <p>In this optional step, a file with a minimal C program is created.</p> <p>Create and write a C source file called <code>hello_world.c</code>:</p> <pre><code>nano hello_world.c\n</code></pre> <p>In nano, write the C program as such:</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  printf(\"hello, world\\n\");\n}\n</code></pre>"},{"location":"software/gcc_compile_c/#1-load-a-gcc-module","title":"1. Load a GCC module","text":"<p>Load a recent GCC module:</p> <pre><code>module load gcc/13.2.0\n</code></pre> Do I really need to load a module? <p>No, as there is a system-installed GCC.</p> <p>For sake of doing reproducible research, always load a module of a specific version.</p> <p>If you need the C11 or C17 standards, use these module versions or newer:</p> Module version Description <code>gcc/4.8</code> Fully implemented C11 standard <code>gcc/8</code> Fully implemented C17 standard"},{"location":"software/gcc_compile_c/#2-compile-the-source-file","title":"2. Compile the source file","text":"<p>After saving and closing nano, compile as such:</p> <pre><code>gcc hello_world.c\n</code></pre> <p>This compiles the file <code>hello_world.c</code> using all defaults:</p> <ul> <li>default/no optimization</li> <li>the executable created is called <code>a.out</code></li> </ul> <p>To compiles the file <code>hello_world.c</code> with run-time speed optimization and creating an executable with a more sensible name, use:</p> <pre><code>gcc -O3 -o hello_world hello_world.c\n</code></pre> <ul> <li><code>-O3</code>: optimize for run-time speed</li> <li><code>-o hello_world</code>: the executable created is called <code>hello_world</code></li> </ul>"},{"location":"software/gcc_compile_c/#3-run","title":"3. Run","text":"<p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/gcc_compile_cpp/","title":"Compile C++ using GCC","text":""},{"location":"software/gcc_compile_cpp/#compile-c-using-gcc","title":"Compile C++ using GCC","text":"<p>GCC (shorthand for 'GNU Compiler Collection') is a collection of compilers able to compile multiple different programming languages.</p> <p>This page describes how to compile C++ code using the GCC.</p>"},{"location":"software/gcc_compile_cpp/#procedure","title":"Procedure","text":""},{"location":"software/gcc_compile_cpp/#0-create-a-c-source-file","title":"0. Create a C++ source file","text":"<p>You will need C++ code to work on.</p> <p>In this optional step, a file with a minimal C++ program is created.</p> <p>Create and write a C++ source file called <code>hello_world.cpp</code>:</p> <pre><code>nano hello_world.c\n</code></pre> <p>In nano, write the C++ program as such:</p> <pre><code>#include &lt;iostream&gt;\n\nint main()\n{\n  std::cout &lt;&lt; \"hello, world\\n\";\n}\n</code></pre>"},{"location":"software/gcc_compile_cpp/#1-load-a-gcc-module","title":"1. Load a GCC module","text":"<p>Load a recent GCC module:</p> <pre><code>module load gcc/13.2.0\n</code></pre> Do I really need to load a module? <p>No, as there is a system-installed GCC.</p> <p>For sake of doing reproducible research, always load a module of a specific version.</p>"},{"location":"software/gcc_compile_cpp/#2-compile-the-source-file","title":"2. Compile the source file","text":"<p>After saving and closing nano, compile as such:</p> <pre><code>g++ hello_world.cpp\n</code></pre> <p>This compiles the file <code>hello_world.cpp</code> using all defaults:</p> <ul> <li>default/no optimization</li> <li>the executable created is called <code>a.out</code></li> </ul> <p>To compiles the file <code>hello_world.cpp</code> with run-time speed optimization and creating an executable with a more sensible name, use:</p> <pre><code>g++ -O3 -o hello_world hello_world.cpp\n</code></pre> <ul> <li><code>-O3</code>: optimize for run-time speed</li> <li><code>-o hello_world</code>: the executable created is called <code>hello_world</code></li> </ul>"},{"location":"software/gcc_compile_cpp/#3-run","title":"3. Run","text":"<p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/gcc_compile_fortran/","title":"Compile Fortran using GCC","text":""},{"location":"software/gcc_compile_fortran/#compile-fortran-using-gcc","title":"Compile Fortran using GCC","text":"<p>GCC (shorthand for 'GNU Compiler Collection') is a collection of compilers able to compile multiple different programming languages.</p> <p>This page describes how to compile Fortran code using the GCC.</p>"},{"location":"software/gcc_compile_fortran/#procedure","title":"Procedure","text":""},{"location":"software/gcc_compile_fortran/#0-create-a-fortran-source-file","title":"0. Create a Fortran source file","text":"<p>You will need Fortran code to work on.</p> <p>In this optional step, a file with a minimal Fortran program is created.</p> <p>Create and write a Fortran source file called <code>hello_world.f</code>:</p> <pre><code>nano hello_world.f\n</code></pre> <p>In nano, write the Fortran program as such:</p> <pre><code>C     HELLO.F :  PRINT MESSAGE ON SCREEN\n      PROGRAM HELLO\n      WRITE(*,*) \"hello, world\";\n      END\n</code></pre>"},{"location":"software/gcc_compile_fortran/#1-load-a-gcc-module","title":"1. Load a GCC module","text":"<p>Load a recent GCC module:</p> <pre><code>module load gcc/13.2.0\n</code></pre> Do I really need to load a module? <p>No, as there is a system-installed GCC.</p> <p>For sake of doing reproducible research, always load a module of a specific version.</p>"},{"location":"software/gcc_compile_fortran/#2-compile-the-source-file","title":"2. Compile the source file","text":"<p>After saving and closing nano, compile as such:</p> <pre><code>gfortran hello_world.f\n</code></pre> <p>This compiles the file <code>hello_world.f</code> using all defaults:</p> <ul> <li>default/no optimization</li> <li>the executable created is called <code>a.out</code></li> </ul> <p>To compiles the file <code>hello_world.f</code> with run-time speed optimization and creating an executable with a more sensible name, use:</p> <pre><code>gfortran -Ofast -o hello_world hello_world.f\n</code></pre> <ul> <li><code>-Ofast</code>: optimize for run-time speed, similar to <code>-O3</code></li> <li><code>-o hello_world</code>: the executable created is called <code>hello_world</code></li> </ul>"},{"location":"software/gcc_compile_fortran/#3-run","title":"3. Run","text":"<p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/gdb/","title":"gdb","text":""},{"location":"software/gdb/#gdb","title":"<code>gdb</code>","text":"<p>There are many debuggers. This page described <code>gdb</code>, the GNU debugger.</p> <p><code>gdb</code> is a debugger provided with the GNU compilers. It works fine for C, C++ and Fortran. With older versions there were problems with fortran90/95.</p> <ul> <li>Debugging GCC-compiled programs</li> <li>Debugging Intel-compiled programs</li> </ul>"},{"location":"software/gdb/#debugging-gcc-compiled-programs","title":"Debugging GCC-compiled programs","text":"<p>In order to use gdb do the following. load a recent gcc module and a gdb module (system gdb is from 2013!).</p> <pre><code>module load gcc/10.3.0 gdb/11.2\n</code></pre> <p>compile your program with flags for debugging added, e.g. -ggdb</p> <pre><code>gcc -ggdb your-program.c -o your-program\n</code></pre> <p>run the gdb program:</p> <pre><code>gdb your-program\n</code></pre> <p>Then you can use the gdb commands, like run, break, step, help, ...</p> <p>Exit with <code>Ctrl+D</code>.</p>"},{"location":"software/gdb/#debugging-intel-compiled-programs","title":"Debugging Intel-compiled programs","text":"<p>In order to use <code>gdb</code> with Intel-compiled programs, do the following\"</p> <p>Load the <code>icc</code> module</p> <pre><code>module load intel/20.4\n</code></pre> <p>Compile your program with flags for debugging added, e.g. -g</p> <pre><code>icc -g your-program.c -o your-program\n</code></pre> <p>Run the gdb program:</p> <pre><code>gdb your-program\n</code></pre> <p>Then you can use the gdb commands, like run, break, step, help, ...</p> <p>Exit with <code>Ctrl+D</code>.</p>"},{"location":"software/gedit/","title":"gedit","text":""},{"location":"software/gedit/#gedit","title":"gedit","text":"<p>There are many text editors installed on the UPPMAX systems. <code>gedit</code> is one of these.</p> <p><code>gedit</code> has a graphical user interface and is included within MobaXterm.</p>"},{"location":"software/git_on_bianca/","title":"Git on Bianca","text":""},{"location":"software/git_on_bianca/#git-on-bianca","title":"Git on Bianca","text":"<p>NOTE: This guide assumes you know basic git commands and will not cover how to use git as a tool.</p> <ul> <li>One of the security features of Bianca is that there is no internet access from the cluster.</li> <li>This makes it a bit more complicated to use things like Git to collaborate on files.</li> <li> <p>In this guide we will cover two use-cases:</p> <ol> <li>collaborate with other users within the same Bianca project, and</li> <li>collaborate with other users using Github.</li> </ol> </li> </ul>"},{"location":"software/git_on_bianca/#within-the-same-bianca-project","title":"Within the same Bianca project","text":"<p>Usually an external service like GitHub is used to host a remote repository (repo) that everyone pushes and pulls from. Since we don\u2019t have an internet connection on Bianca we have to push and pull from a location within your Bianca project. Luckily that is simple to setup with git.</p> <p>To create your own remote repo that everyone will push and pull from, create an empty directory somewhere in your project folder, go into it and initialize the repo.</p> <pre><code># go to project dir\ncd /proj/nobackup/sens2023999/\n\n# create dir\nmkdir my_repo.git\n\n# go into dir\ncd my_repo.git\n\n# init repo\ngit init --bare --share=group\n</code></pre> <p>The name of the created directory doesn\u2019t have to end with .git but it is good for us humans to indicate that this is a repo people will use to push and pull from, and not where you will manually edit files.</p> <p>To start using this repo you will clone it just like you would clone a GitHub repo.</p> <pre><code># go to where you want to clone the repo, e.g. your home\ncd ~/\n\n# clone it\ngit clone /proj/nobackup/sens2023999/my_repo.git\n\n# add a file and make the first commit\necho \"# my_repo\" &gt;&gt; README.md\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit push -u origin main\n</code></pre> <p>Now you will have a new directory named my_repo that only has a README.md file, and you can start creating other files in there. From this point onwards git will work the same way as if you were using a GitHub hosted repo to collaborate. Once you have pushed your files the others in your project can clone the repo and start pushing and pulling their changes.</p>"},{"location":"software/git_on_bianca/#using-github-or-any-other-git-hosting-service","title":"Using Github (or any other git hosting service)","text":"<p>These instructions will work with any git hosting provider, like GitLab or Bitbucket, but we\u2019ll use GitHub in the examples.</p> <p>In the examples we use Rackham to mount the wharf directory. This is not the only way to do it. If you\u2019d rather use a sftp client to transfer your files from the outside of Bianca to and from the wharf it will work just as well.</p>"},{"location":"software/git_on_bianca/#cloning-and-pulling-only","title":"Cloning and pulling only","text":"<p>If you only want to run someone else's software that they have stored in a GitHub repo, you only need to clone the repo to be able to use it. Since you are only a user of the software there is no need to be able to push to the repo. If there are any updates to the repo you only need to pull the repo to get them.</p> <p>The way to do this on Bianca is to simply clone the repo on a computer with internet access, move it to the Bianca wharf, and then copy it to its final destination on Bianca. If there are any updates to the repo you want to get you move the repo back to the wharf, pull the updates to the mounted wharf directory on Rackham, then move the directory back to its final destination on Bianca.</p> <pre><code>### on rackham ###\n\n# set variables for readability\nPROJ=sens2023999\nUNAME=youruppmaxusername\n\n# mount the wharf directory\nmkdir -p ~/wharf_mnt\n/proj/staff/dahlo/bin/sshfs $UNAME-$PROJ@bianca-sftp.uppmax.uu.se:$UNAME-$PROJ ~/wharf_mnt\n\n# clone the repo to the wharf directory\ncd ~/wharf_mnt\ngit clone git@github.com:example/example.git\n\n### on Bianca ###\n\n# move the directory to its final destination on Bianca\n\nmv /proj/$PROJ/nobackup/wharf/$UNAME/$USER-$PROJ/example/ /proj/$PROJ/\n</code></pre> <p>If there are any updates to the software you might want to pull the changes from GitHub.</p> <pre><code>### on bianca ###\n\n# move the directory you cloned from GitHub back to the wharf\nmv /proj/$PROJ/example/ /proj/$PROJ/nobackup/wharf/$UNAME/$USER-$PROJ/\n\n### on rackham ###\n\n# mount the wharf directory\nmkdir -p ~/wharf_mnt\n/proj/staff/dahlo/bin/sshfs $UNAME-$PROJ@bianca-sftp.uppmax.uu.se:$UNAME-$PROJ ~/wharf_mnt\n\n# pull the updates\ncd ~/wharf_mnt/example\ngit pull\n\n### on bianca ###\n\n# move the directory to its final destination on Bianca\nmv /proj/$PROJ/nobackup/wharf/$UNAME/$USER-$PROJ/example/ /proj/$PROJ/\n</code></pre>"},{"location":"software/git_on_bianca/#pushing-and-pulling","title":"Pushing and pulling","text":"<p>If you are a collaborator on a software you will need to do both pulling and pushing to the repo</p> <p>The general approach to using git as a collaborator with GitHub on Bianca is:</p> <ol> <li>On Bianca: make a backup of your code directory.</li> <li>On Bianca: move the entire code directory to the wharf folder.</li> <li>On Rackham: mount the wharf directory.</li> <li>On Rackham: change the git remote URL to GitHub\u2019s URL.</li> <li>On Rackham: pull and push from GitHub.</li> <li>On Bianca: move the directory from the wharf back to your project.</li> <li>On Bianca: change the git remote URL back to your local Bianca repo.</li> <li>On Bianca: push any changes you got from GitHub to your local Bianca repo.</li> </ol> <p>Best way to show this is by an example:</p> <pre><code>### on bianca ###\n\n# set variables for readability\nPROJ=sens2023999\nUNAME=youruppmaxusername\n\n# make a copy of your code dir, delete this later if all goes well :)\ncp -ar /proj/$PROJ/code_dir /proj/$PROJ/code_dir.$(date +%Y-%m-%d)\n\n# move the directory with your code to the wharf\nmv /proj/$PROJ/code_dir/ /proj/$PROJ/nobackup/wharf/$UNAME/$UNAME-$PROJ/\n\n### on rackham ###\n\n# set variables for readability\nPROJ=sens2023999\nUNAME=youruppmaxusername\n\n# mount the wharf folder\nmkdir -p ~/wharf_mnt\n/proj/staff/dahlo/bin/sshfs $UNAME-$PROJ@bianca-sftp.uppmax.uu.se:$UNAME-$PROJ ~/wharf_mnt\n\n# update the remote repo's URL to your GitHub URL\ncd ~/wharf_mnt/code_dir\ngit remote set-url origin git@github.com:example/example.git\ngit pull\ngit push\n\n### on bianca ###\n\n# move the directory back from the wharf\nmv /proj/$PROJ/nobackup/wharf/$UNAME/$USER-$PROJ/code_dir/ /proj/$PROJ/\n\n# change the remote repo's URL back to your local repo on Bianca\ngit remote set-url origin /path/to/local/repo\n\n# push any changes you got from GitHub to your local repo\ngit push\n</code></pre>"},{"location":"software/globus/","title":"globus","text":""},{"location":"software/globus/#globus","title":"globus","text":"<p>Globus is a service to easily and safely transfer data.</p> <p></p> <p>However, Uppsala University does not have a subscription.</p> Does UU have a subscription now? <p>Please contribute by letting us know. Thanks!</p>"},{"location":"software/globus/#links","title":"Links","text":"<ul> <li>Globus homepage</li> </ul>"},{"location":"software/gprof/","title":"gprof","text":""},{"location":"software/gprof/#gprof","title":"<code>gprof</code>","text":"<p>There are multiple profilers available on UPPMAX. This page describes gprof.</p> <p>gprof is the GNU profiler, provided with the GNU compiler package.</p> <p>In order to use <code>gprof</code> do the following:</p> <p>Load a recent <code>gcc</code> module and a recent <code>binutils</code> module:</p> <pre><code>module load gcc\nmodule load binutils\n</code></pre> <p>Compile your program with the <code>-pg -g</code> flags added</p> <pre><code>gcc -O0 -pg -g your-program.c -o your-program\n</code></pre> <p>run it:</p> <pre><code>./your-program\n</code></pre> <p>then do:</p> <pre><code>gprof your-program gmon.out &gt; output-file\n</code></pre>"},{"location":"software/gromacs/","title":"GROMACS","text":""},{"location":"software/gromacs/#running-gromacs-at-uppmax","title":"Running Gromacs at UPPMAX","text":"<p>This page describes how to run the GROMACS molecular dynamics software on UPPMAX systems. See the gromacs web page for more information.</p> <p>Have a look on this page as well - best practices running GROMAC on HPC.</p> <p>Selected setups for benchmarking on HPC2N as examples.</p>"},{"location":"software/gromacs/#loading-the-gromac-module","title":"Loading the gromac module","text":"<pre><code>module load gromacs/2021.1.th\n</code></pre>"},{"location":"software/gromacs/#sbatch-script","title":"SBATCH script","text":"<p>adapted from HPC2N</p> <pre><code>#!/bin/bash -l\n#SBATCH -A SNIC_project\n#SBATCH -t 00:15:00\n#SBATCH -p node -n 10\n# Use 2 threads per task\n#SBATCH -c 2\n\nmodule load gromacs/2021.1.th\n\n# Automatic selection of single or multi node based GROMACS\nif [ $SLURM_JOB_NUM_NODES -gt 1 ]; then\n  GMX=\"gmx_mpi\"\n  MPIRUN=\"mpirun\"\n  ntmpi=\"\"\nelse\n  GMX=\"gmx\"\n  MPIRUN=\"\"\n  ntmpi=\"-ntmpi $SLURM_NTASKS\"\nfi\n\n# Automatic selection of ntomp argument based on \"-c\" argument to sbatch\nif [ -n \"$SLURM_CPUS_PER_TASK\" ]; then\n  ntomp=\"$SLURM_CPUS_PER_TASK\"\nelse\n  ntomp=\"1\"\nfi\n# Make sure to set OMP_NUM_THREADS equal to the value used for ntomp\n# to avoid complaints from GROMACS\nexport OMP_NUM_THREADS=$ntomp\n$MPIRUN $GMX mdrun $ntmpi -ntomp $ntomp -s MEM.tpr -nsteps 10000 -resethway\n</code></pre>"},{"location":"software/gromacs/#how-important-is-to-select-appropriate-options","title":"How important is to select appropriate options","text":"<p>Here is a simple benchmark ran on single interactive session with 20CPUs using the MEM example from this benchmark:</p> <pre><code>module load gromacs/2021.1.th\nmpirun -np XX gmx_mpi mdrun -ntomp YY -s MEM.tpr -nsteps 10000 -resethway\n</code></pre> <p>where XX * YY = 20</p> <pre><code>$ grep \"gmx_mpi\\|MPI ranks\\|Performance\" *\n\n#md.log.1#:  gmx_mpi mdrun -ntomp 1 -s MEM.tpr -nsteps 10000 -resethway\n#md.log.1#:On 12 MPI ranks doing PP, and\n#md.log.1#:on 8 MPI ranks doing PME\n#md.log.1#:Performance:       20.520        1.170\n\n#md.log.2#:  gmx_mpi mdrun -ntomp 2 -s MEM.tpr -nsteps 10000 -resethway\n#md.log.2#:On 10 MPI ranks, each using 2 OpenMP threads\n#md.log.2#:Performance:       25.037        0.959\n\n#md.log.3#:  gmx_mpi mdrun -ntomp 4 -s MEM.tpr -nsteps 10000 -resethway\n#md.log.3#:On 5 MPI ranks, each using 4 OpenMP threads\n#md.log.3#:Performance:        5.388        4.454\n\n#md.log.4#:  gmx_mpi mdrun -ntomp 5 -s MEM.tpr -nsteps 10000 -resethway\n#md.log.4#:On 4 MPI ranks, each using 5 OpenMP threads\n#md.log.4#:Performance:       24.090        0.996\n\n#md.log.5#:  gmx_mpi mdrun -ntomp 10 -s MEM.tpr -nsteps 10000 -resethway\n#md.log.5#:NOTE: Your choice of number of MPI ranks and amount of resources results in using 10 OpenMP threads per rank, which is most likely inefficient. The optimum is usually between 1 and 6 threads per rank.\n#md.log.5#:On 2 MPI ranks, each using 10 OpenMP threads\n#md.log.5#:Performance:        3.649        6.577\n\nmd.log:  gmx_mpi mdrun -ntomp 20 -s MEM.tpr -nsteps 10000 -resethway\nmd.log:Performance:        2.012       11.931\n</code></pre> <p>Notice how bad is the last run</p> <p><code>$ mpirun -np 1 gmx_mpi mdrun -ntomp 20 -s MEM.tpr -nsteps 10000 -resethway</code> (lines 25-26)</p> <p>According to this short test, this particular setup runs best on single Rackham node with</p> <p><code>$ mpirun -np 10 gmx_mpi mdrun -ntomp 2 -s MEM.tpr -nsteps 10000 -resethway</code> (lines 8-10)</p>"},{"location":"software/gromacs/#running-older-versions-of-gromacs","title":"Running older versions of gromacs","text":""},{"location":"software/gromacs/#versions-451-to-504","title":"Versions 4.5.1 to 5.0.4","text":"<p>The gromacs tools have been compiled serially. The mdrun program has also been compiled in parallel using MPI. The name of the parallel binary is mdrun_mpi.</p> <p>Run the parallelized program using:</p> <pre><code>mpirun -np XXX mdrun_mpi\n</code></pre> <p>... where XXX is the number of cores to run the program on.</p>"},{"location":"software/gromacs/#version-511","title":"Version 5.1.1","text":"<p>The binary is gmx_mpi and (e.g.) the mdrun command is issued like this:</p> <pre><code>mpirun -np XXX gmx_mpi mdrun\n</code></pre>"},{"location":"software/icc/","title":"icc","text":""},{"location":"software/icc/#icc","title":"<code>icc</code>","text":"<p>There are multiple compilers on the UPPMAX HPC clusters. This page describes <code>icc</code>, an Intel C compiler.</p> <p>The Intel compiler is part of the <code>intel</code> module and can be used to:</p> <ul> <li>Compile a C program</li> </ul> <p>Working together with this Intel compiler are:</p> <ul> <li>A debugger called <code>gdb</code></li> <li>An obsolete debugger called <code>idb</code></li> <li>Some general profiler called Intel VTune   and Intel Advisor</li> </ul>"},{"location":"software/icc_compile_c/","title":"Compile a C program using icc","text":""},{"location":"software/icc_compile_c/#compile-a-c-program-using-icc","title":"Compile a C program using <code>icc</code>","text":"<p>icc is the Intel C compiler. This page describes how to compile C code using <code>icc</code>.</p>"},{"location":"software/icc_compile_c/#procedure","title":"Procedure","text":""},{"location":"software/icc_compile_c/#1-load-an-intel-module","title":"1. Load an <code>intel</code> module","text":"<p>For version of the Intel compiler to and including 2020, load an <code>intel</code> module with a version having two digits, from 15 to and including 20:</p> <pre><code>module load intel/20.4\n</code></pre> <p>C11 and C17 (bug fix) standards have support from intel/17+ (fully from 19).</p>"},{"location":"software/icc_compile_c/#2","title":"2","text":"<p>Create and write a C source file called <code>hello_world.c</code>:</p> <pre><code>nano hello_world.c\n</code></pre> <p>In nano, write the C program as such:</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  printf(\"hello, world\\n\");\n}\n</code></pre> <p>After saving and closing nano, compile as such:</p> <pre><code>icc hello_world.c\n</code></pre> <p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/icpc/","title":"icpc","text":""},{"location":"software/icpc/#icpc","title":"<code>icpc</code>","text":"<p>There are multiple compilers on the UPPMAX HPC clusters. This page describes <code>icpc</code>, an Intel C++ compiler.</p> <p>The Intel compiler is part of the <code>intel</code> module and can be used to:</p> <ul> <li>Compile a C++ program</li> </ul> <p>Working together with this Intel compiler are:</p> <ul> <li>A debugger called <code>gdb</code></li> <li>An obsolete debugger called <code>idb</code></li> <li>Some general profiler called Intel VTune   and Intel Advisor</li> </ul>"},{"location":"software/icpc_compile_cpp/","title":"Compile a C++ program using icpc","text":""},{"location":"software/icpc_compile_cpp/#compile-a-c-program-using-icpc","title":"Compile a C++ program using <code>icpc</code>","text":"<p>icpc is an Intel C++ compiler. This page describes how to compile C++ code using <code>icpc</code>.</p>"},{"location":"software/icpc_compile_cpp/#procedure","title":"Procedure","text":""},{"location":"software/icpc_compile_cpp/#1-load-the-modules","title":"1. Load the modules","text":"<p>Load a recent <code>intel</code> module:</p> <pre><code>module load intel/20.4\n</code></pre>"},{"location":"software/icpc_compile_cpp/#2-write-the-c-program","title":"2. Write the C++ program","text":"<p>Create and write a C++ source file called <code>hello_world.cpp</code>:</p> <pre><code>nano hello_world.cpp\n</code></pre> <p>In nano, write the C++ program as such:</p> <pre><code>#include &lt;iostream&gt;\n\nint main()\n{\n  std::cout &lt;&lt; \"hello, world\\n\";\n}\n</code></pre>"},{"location":"software/icpc_compile_cpp/#3-compile-the-c-program","title":"3. Compile the C++ program","text":"<p>After saving and closing nano, compile as such:</p> <pre><code>icpc hello_world.cpp\n</code></pre>"},{"location":"software/icpc_compile_cpp/#4-run-the-executable","title":"4. Run the executable","text":"<p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/icx/","title":"icx","text":""},{"location":"software/icx/#icx","title":"<code>icx</code>","text":"<p>There are multiple compilers on the UPPMAX HPC clusters. This page describes <code>icx</code>, an Intel C compiler.</p> <p>The Intel compiler is part of the <code>intel</code> module and can be used to:</p> <ul> <li>Compile a C program</li> </ul> <p>Working together with this Intel compiler are:</p> <ul> <li>A debugger called <code>gdb</code></li> <li>An obsolete debugger called <code>idb</code></li> <li>Some general profiler called Intel VTune   and Intel Advisor</li> </ul>"},{"location":"software/icx_compile_c/","title":"Compile a C program using icx","text":""},{"location":"software/icx_compile_c/#compile-a-c-program-using-icx","title":"Compile a C program using <code>icx</code>","text":"<p>icx is an Intel C compiler. This page describes how to compile C code using <code>icx</code>.</p>"},{"location":"software/icx_compile_c/#procedure","title":"Procedure","text":""},{"location":"software/icx_compile_c/#1-load-the-modules","title":"1. Load the modules","text":"<p>load an these modules:</p> <pre><code>module load intel-oneapi\nmodule load compiler/2023.1.0\n</code></pre>"},{"location":"software/icx_compile_c/#2-write-the-c-program","title":"2. Write the C program","text":"<p>Create and write a C source file called <code>hello_world.c</code>:</p> <pre><code>nano hello_world.c\n</code></pre> <p>In nano, write the C program as such:</p> <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n  printf(\"hello, world\\n\");\n}\n</code></pre>"},{"location":"software/icx_compile_c/#3-compile-the-c-program","title":"3. Compile the C program","text":"<p>After saving and closing nano, compile as such:</p> <pre><code>icx hello_world.c\n</code></pre>"},{"location":"software/icx_compile_c/#4-run-the-executable","title":"4. Run the executable","text":"<p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/idb/","title":"idb","text":""},{"location":"software/idb/#idb","title":"<code>idb</code>","text":"<p>There are many debuggers. This page described <code>idb</code>, the Intel debugger.</p> <p><code>idb</code> was provided with the Intel compiler. Now it is deprecated and you are advised to use gdb. to debug programs compiled with the Intel compiler. See gdb how to do so.</p>"},{"location":"software/ides/","title":"IDE:s","text":""},{"location":"software/ides/#ides","title":"IDE:s","text":"<p>RStudio is an IDEs. Here, it is run on Bianca.</p>"},{"location":"software/ides/#introduction","title":"Introduction","text":"<p>IDE (pronounce <code>aj-dee-ee</code>) is short for 'Integrated Development Environment', or 'a program in which you do programming'. The goal of an IDE is to help develop code, with features such as code completion, code hints and interactive debugging.</p> <p>There are many different IDEs), of which some are tailored to one programming language (e.g. RStudio) and some allow multiple programming languages.</p> <p>How to use an IDE depends on the UPPMAX cluster you want to use:</p> <ul> <li>IDEs on Bianca</li> <li>IDEs on Pelle</li> <li>IDEs on Rackham</li> </ul> <p>In general, using an IDE is easiest on the general-purpose HPC clusters (i.e. Pelle and Rackham) and hardest on the sensitive data HPC clusters (i.e. Bianca).</p>"},{"location":"software/ides_on_bianca/","title":"IDEs on Bianca","text":""},{"location":"software/ides_on_bianca/#ides-on-bianca","title":"IDEs on Bianca","text":"<p>RStudio is one of the IDEs that can be used on Bianca.</p> <p>Here we show how to use some IDEs on Bianca.</p> Forgot what an IDE is? <p>See the general page about IDEs.</p> Do you really want to use an IDE on Bianca? <p>Using an IDE on Bianca is cumbersome and there are superior ways to develop code on Bianca.</p> <p>However, using an IDE may make it easier for a new user to feel comfortable using Bianca.</p> <p>The UPPMAX 'Programming Formalisms' course will teach you a superior workflow, where development takes place on your own regular computer and testing is done using simulated/fake data. When development is done, the tested project is uploaded to Bianca and setup to use the real data instead.</p> <p>This avoids using a clumsy remote desktop environment, as well as many added bonuses.</p> <p>Here are step-by-step guides to start these IDEs on Rackham:</p> IDE Languages Screenshot Jupyter Python RStudio R VSCode General-purpose Impossible VSCodium General-purpose"},{"location":"software/ides_on_pelle/","title":"IDEs on Pelle","text":""},{"location":"software/ides_on_pelle/#ides-on-pelle","title":"IDEs on Pelle","text":"<p>Here we show how to use some IDEs on Pelle.</p> Forgot what an IDE is? <p>See the general page about IDEs.</p> Do you really want to use an IDE on Pelle? <p>Using an IDE on Pelle is cumbersome and there are superior ways to develop code on Pelle.</p> <p>However, using an IDE may make it easier for a new user to feel comfortable using Pelle.</p> <p>The UPPMAX 'Programming Formalisms' course will teach you a superior workflow, where development takes place on your own regular computer and testing is done using simulated/fake data. When development is done, the tested project is uploaded to Pelle and setup to use the real data instead.</p> <p>This avoids using a clumsy remote desktop environment, as well as many added bonuses.</p> <p>Here are step-by-step guides to start these IDEs on Pelle:</p> IDE Languages Jupyter Python  RStudio R  VSCode General-purpose  VSCodium General-purpose <p>IDEs on Pelle. IDEs marked with  cannot be run on Pelle.</p>"},{"location":"software/ides_on_rackham/","title":"IDEs on Rackham","text":""},{"location":"software/ides_on_rackham/#ides-on-rackham","title":"IDEs on Rackham","text":"<p>RStudio is one of the IDEs that can be used on Rackham.</p> <p>Here we show how to use some IDEs on Rackham.</p> Forgot what an IDE is? <p>See the general page about IDEs.</p> Do you really want to use an IDE on Rackham? <p>Using an IDE on Rackham is cumbersome and there are superior ways to develop code on Rackham.</p> <p>However, using an IDE may make it easier for a new user to feel comfortable using Rackham.</p> <p>The UPPMAX 'Programming Formalisms' course will teach you a superior workflow, where development takes place on your own regular computer and testing is done using simulated/fake data. When development is done, the tested project is uploaded to Rackham and setup to use the real data instead.</p> <p>This avoids using a clumsy remote desktop environment, as well as many added bonuses.</p> <p>Here are step-by-step guides to start these IDEs on Rackham:</p> IDE Languages Jupyter Python RStudio R VSCode General-purpose VSCodium General-purpose <p>IDEs on Rackham. IDEs marked with  cannot be run on Rackham.</p>"},{"location":"software/ifort/","title":"ifort","text":""},{"location":"software/ifort/#ifort","title":"<code>ifort</code>","text":"<p><code>ifort</code> is an Intel Fortran compiler.</p> <p><code>ifort</code> is part of the <code>intel</code> module and can be used to:</p> <ul> <li>Compile a Fortran program</li> </ul>"},{"location":"software/ifort_compile_fortran/","title":"Compile a Fortran program using ifort","text":""},{"location":"software/ifort_compile_fortran/#compile-a-fortran-program-using-ifort","title":"Compile a Fortran program using <code>ifort</code>","text":"<p>ifort is an Intel Fortran compiler. This page describes how to compile Fortran code using <code>ifort</code>.</p>"},{"location":"software/ifort_compile_fortran/#procedure","title":"Procedure","text":""},{"location":"software/ifort_compile_fortran/#1-load-the-modules","title":"1. Load the modules","text":"<p>Load a recent <code>intel</code> module:</p> <pre><code>module load intel/20.4\n</code></pre>"},{"location":"software/ifort_compile_fortran/#2-write-the-fortran-program","title":"2. Write the Fortran program","text":"<p>Create and write a Fortran source file called <code>hello_world.f</code>:</p> <pre><code>nano hello_world.f\n</code></pre> <p>In nano, write the Fortran program as such:</p> <pre><code>C     HELLO.F :  PRINT MESSAGE ON SCREEN\n      PROGRAM HELLO\n      WRITE(*,*) \"hello, world\";\n      END\n</code></pre>"},{"location":"software/ifort_compile_fortran/#3-compile-the-c-program","title":"3. Compile the C++ program","text":"<p>After saving and closing nano, compile as such:</p> <pre><code>ifort hello_world.f\n</code></pre>"},{"location":"software/ifort_compile_fortran/#4-run-the-executable","title":"4. Run the executable","text":"<p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/ifx/","title":"ifx","text":""},{"location":"software/ifx/#ifx","title":"<code>ifx</code>","text":"<p><code>ifx</code> is an Intel Fortran compiler.</p> <p><code>ifx</code> is part of the <code>intel-oneapi</code> and then <code>compiler</code> module.</p> <p><code>ifx</code> can be used to:</p> <ul> <li>Compile a Fortran program</li> </ul>"},{"location":"software/ifx_compile_fortran/","title":"Compile a Fortran program using ifx","text":""},{"location":"software/ifx_compile_fortran/#compile-a-fortran-program-using-ifx","title":"Compile a Fortran program using <code>ifx</code>","text":"<p>ifx is an Intel Fortran compiler. This page describes how to compile Fortran code using <code>ifx</code>.</p>"},{"location":"software/ifx_compile_fortran/#procedure","title":"Procedure","text":""},{"location":"software/ifx_compile_fortran/#1-load-the-modules","title":"1. Load the modules","text":"<p>Load a recent <code>intel</code> module:</p> <pre><code>module load intel-oneapi\nmodule load compiler/2023.1.0\n</code></pre>"},{"location":"software/ifx_compile_fortran/#2-write-the-fortran-program","title":"2. Write the Fortran program","text":"<p>Create and write a Fortran source file called <code>hello_world.f</code>:</p> <pre><code>nano hello_world.f\n</code></pre> <p>In nano, write the Fortran program as such:</p> <pre><code>C     HELLO.F :  PRINT MESSAGE ON SCREEN\n      PROGRAM HELLO\n      WRITE(*,*) \"hello, world\";\n      END\n</code></pre>"},{"location":"software/ifx_compile_fortran/#3-compile-the-c-program","title":"3. Compile the C++ program","text":"<p>After saving and closing nano, compile as such:</p> <pre><code>ifx hello_world.f\n</code></pre>"},{"location":"software/ifx_compile_fortran/#4-run-the-executable","title":"4. Run the executable","text":"<p>Run the program:</p> <pre><code>./a.out\n</code></pre> <p>Output:</p> <pre><code>hello, world\n</code></pre>"},{"location":"software/igv/","title":"IGV","text":""},{"location":"software/igv/#starting-integrative-genomics-viewer-igv-on-rackhamsnowy","title":"Starting Integrative Genomics Viewer (IGV) on Rackham/Snowy","text":"<p>This guide will go through step by step how to start Integrative Genomics Viewer.</p>"},{"location":"software/igv/#step-1-connect-to-uppmax-with-x-forwarding-enabled-important-step","title":"Step 1: Connect to UPPMAX with X-forwarding enabled. (Important step)","text":"<p>In a terminal, use SSH with X forwarding enabled:</p> <pre><code>ssh -X [user name]@rackham.uppmax.uu.se\n</code></pre> <p>For example:</p> <pre><code>ssh -X sven@rackham.uppmax.uu.se\n</code></pre> <ul> <li>Windows users: we recommend the SSH client MobaXterm</li> <li>MacOS users: the built-in SSH client <code>ssh</code> does need XQuartz installed too</li> </ul>"},{"location":"software/igv/#step-2-reserve-a-node-using-interactive","title":"Step 2: Reserve a node using \"interactive\"","text":"<p>Since genomic sequences require lots of memory, it is not suitable to run IGV on one of the login nodes. That would slow down the response times for all other users on the same login node..</p> <p>Instead, reserve a node that you will have all by yourself. This command will reserve a whole node for 12 hours, the maximum amount of interactive time you can get and still receive a high priority for your job (feel free to change that if you want to).</p> <pre><code>interactive -A [UPPMAX project id] -p node -t 12:00:00\n</code></pre> <p>For example:</p> <pre><code>interactive -A snic2017-7-274 -p node -t 12:00:00\n</code></pre> <p>For interactive session on Snowy add the flag \"-M snowy\":</p> <pre><code>interactive -A snic2017-7-274 -M snowy -p node -t\\ 12:00:00\n</code></pre>"},{"location":"software/igv/#step-3-load-the-igv-module","title":"Step 3: Load the IGV module","text":"<p>When your job has been started, type the following command to load the IGV module:</p> <pre><code>module load bioinfo-tools IGV\n</code></pre>"},{"location":"software/igv/#step-4-start-igv","title":"Step 4: Start IGV","text":"<p>To start IGV, type the following:</p> <pre><code>igv-node\n</code></pre> <p>That's it, now IGV should be loaded and ready to go. For more information about how to use IGV, please visit IGV's user guide.</p>"},{"location":"software/install/","title":"Installing","text":""},{"location":"software/install/#software-and-package-installation","title":"Software and package installation","text":""},{"location":"software/install/#install-software-yourself","title":"Install software yourself","text":""},{"location":"software/install/#build-from-source","title":"Build from source","text":"<ul> <li>To build from source use a compiler module</li> <li>We have several compiler versions from GNU and INTEL</li> <li>check with: <code>$ ml avail gcc</code> and <code>$ ml avail intel</code></li> <li>Guide for compiling serial programs</li> <li>Guide for compiling parallel programs<ul> <li>Available combinations of compilers and parallel libraries</li> </ul> </li> </ul>"},{"location":"software/install/#example","title":"Example","text":"<p>This guide might not work on all programs. Read the installation instructions for your program!</p> <ul> <li>Download the program, with <code>wget</code> or by other means like <code>git clone &lt;https-URL to GITHUB repo&gt;</code>.</li> <li>If the not cloning, unpack it with <code>tar</code>, <code>gunzip</code> or similar.</li> </ul> <pre><code>tar xvfz program.tgz\n</code></pre> <p>The below is more general again:</p> <ul> <li>Read the installation instructions!</li> <li>If Fortran or C or C++, load a compiler. Often you'll have less problems with gcc but intel gives faster code.</li> </ul> <pre><code>module load gcc\n</code></pre> <ul> <li>If applicable, do:</li> </ul> <pre><code>mkdir $HOME/glob/program_name\n./configure --prefix=$HOME/glob/program_name\nmake\nmake test\nmake install\n</code></pre> <ul> <li>Try to find a test on the home page of the program or in the installation instructions and try to run it.</li> </ul>"},{"location":"software/install/#packages-and-libraries-to-scripting-programs","title":"Packages and libraries to scripting programs","text":"<ul> <li>Python, R and Julia all have some centrally installed packages that are available from the modules.</li> <li>R has a special module called <code>R_packages</code>, and some Machine Learning python packages are included in the <code>python_ML_packages</code> module.</li> <li>If not found there you can try to install those by yourself.</li> </ul> <p>Tip Python packages</p> <ul> <li>Try Conda first directly on Bianca and PyPI on Rackham.</li> <li>We have mirrored all major Conda repositories directly on UPPMAX, on both Rackham and Bianca. These are updated every third day.</li> <li>If you want to keep number of files down, use PyPI (pip).</li> <li>Also it is easier to get conflicting environments if using both Python module and Conda in parallel.</li> </ul>"},{"location":"software/install/#conda","title":"Conda","text":"<ul> <li>We have mirrored all major Conda repositories directly on UPPMAX, on both Rackham and Bianca. These are updated every third day.</li> </ul> <p>Available Conda channels</p> <ul> <li>bioconda</li> <li>biocore</li> <li>conda-forge</li> <li>dranew</li> <li>free</li> <li>main</li> <li>pro</li> <li>qiime2</li> <li>r</li> <li>r2018.11</li> <li>scilifelab-lts</li> </ul> <ul> <li>Conda user guide</li> </ul>"},{"location":"software/install/#python-packages-with-pip","title":"Python packages with pip","text":"<ul> <li>Installing with pip</li> </ul>"},{"location":"software/install/#r-packages","title":"R packages","text":"<ul> <li>On UPPMAX the module <code>R_packages</code> is an omnibus package library containing almost all packages in the CRAN and BioConductor repositories.</li> <li> <p>As of 2023-05-31, there were a total of 23100 R packages installed in <code>R_packages/4.2.1</code>.</p> <ul> <li>A total of 23109 packages were available in CRAN and BioConductor, and 23000 of these were installed in <code>R_packages/4.2.1</code></li> <li>The additional 100 R packages available in this module were installed from the CRAN/BioConductor archives, or were hosted on github, gitlab or elsewhere.</li> </ul> </li> <li> <p>Installing R packages</p> </li> </ul>"},{"location":"software/install/#julia-packages","title":"Julia packages","text":"<ul> <li>At UPPMAX there is a central library with installed packages.</li> <li>This is good, especially when working on Bianca, since you then do not need to install via the <code>wharf</code>.</li> <li> <p>A selection of the Julia packages and libraries installed on UPPMAX are:</p> <pre><code>CSV\nCUDA\nMPI\nDistributed\nIJulia\nPlots\nPyPlot\nDataFrames\n</code></pre> </li> <li> <p>Installing julia packages</p> </li> </ul>"},{"location":"software/install/#containers","title":"Containers","text":"<p>Info</p> <ul> <li>Containers let you install programs without needing to think about the computer environment, like<ul> <li>operative system</li> <li>dependencies (libraries and other programs) with correct versions</li> </ul> </li> <li>Everything is included</li> <li>Draw-backs<ul> <li>you install also things that may be already installed</li> <li>therefore, probably more disk space is needed</li> </ul> </li> </ul>"},{"location":"software/install/#singularity","title":"Singularity","text":"<p>See the UPPMAX Singularity user guide:</p> <ul> <li>Create a Singularity container for an R package</li> </ul>"},{"location":"software/install/#docker","title":"Docker","text":"<p>Docker will unfortunately not work on the clusters, since it requires root permission.</p> <p>However, it is possible to convert a Docker script to a Singularity container.</p>"},{"location":"software/install/#spack","title":"Spack","text":"<ul> <li>The UPPMAX staff has already other ways to install most software applications.</li> <li>Please use Spack only if other ways to install your tool is not possible or very difficult, e.g. requiring very many dependencies and it is not available through, e.g. EasyBuild.</li> <li>UPPMAX Spack user guide</li> </ul>"},{"location":"software/install/#own-development","title":"Own development","text":"<ul> <li>You may have your own code that you want to run on UPPMAX.</li> <li>Guide for compiling serial programs</li> <li>Guide for compiling parallel programs<ul> <li>Available combinations of compilers and parallel libraries</li> </ul> </li> <li>User guide for debuggers</li> <li>User guide for profilers</li> </ul>"},{"location":"software/install/#run-own-scripts-or-programs","title":"Run own scripts or programs","text":"<p>Unless your script or program is in the active path, you run it by the full path or <code>./&lt;file&gt;</code> if you are in the present directory.</p>"},{"location":"software/install/#summary","title":"Summary","text":"<p>Keypoints</p> <ul> <li>You have got an overview of the procedures to install packages/libraries and tools on Bianca through the <code>wharf</code></li> <li>If you feel uncomfortable or think that many users would benefit from the software, ask the support to install it.</li> </ul>"},{"location":"software/intel_advisor/","title":"Intel Advisor","text":""},{"location":"software/intel_advisor/#intel-advisor","title":"Intel Advisor","text":"<p>There are multiple profilers available on UPPMAX. This page describes Intel Advisor.</p> <p>Intel Advisor is a broad set of tools with a focus on performance analysis of Intel-compiled code.</p> <p>Intel's performance analysis suite can probably answer any question you have about the performance of your code, including MPI and OpenMP code.</p> <p>In order to use Advisor, do the following:</p> <pre><code>module load intel-oneapi advisor\n</code></pre> <p>Making sure you have a graphical connection through SSH X-forwarding or ThinLinc, then run Advisor graphically like this:</p> <pre><code>advixe-gui\n</code></pre>"},{"location":"software/intel_vtune/","title":"Intel VTune","text":""},{"location":"software/intel_vtune/#intel-vtune","title":"Intel VTune","text":"<p>There are multiple profilers available on UPPMAX. This page describes Intel VTune.</p> <p>Intel VTune is a broad set of tools with a focus on performance improvement of Intel-compiled code.</p> <p>Intel's performance analysis suite can probably answer any question you have about the performance of your code, including MPI and OpenMP code.</p> <p>VTune is focused choosing optimizing techniques that will yield good results, whereas Amplifier is more broadly aimed at performance analysis.</p> <p>In order to use VTune do the following:</p> <pre><code>module load intel-oneapi vtune\n</code></pre> <p>Making sure you have a graphical connection through SSH X-forwarding or ThinLinc, then run VTune graphically like this:</p> <pre><code>vtune-gui\n</code></pre>"},{"location":"software/interactive/","title":"interactive","text":"","tags":["interactive"]},{"location":"software/interactive/#interactive","title":"<code>interactive</code>","text":"<p>The job scheduler consists of many programs to manage jobs. <code>interactive</code> is the program to request to work interactively on a compute node.</p> <p>See the UPPMAX guide on how to start an interactive session.</p>","tags":["interactive"]},{"location":"software/ipython/","title":"IPython","text":""},{"location":"software/ipython/#ipython","title":"IPython","text":"<p><code>IPython</code> is a console program that extends the regular Python interpreter: among others, one can directly run scripts and re-use output.</p> Want to see a video? <p>Here are some videos on IPython:</p> <ul> <li>YouTube video on <code>python</code> versus <code>IPython</code></li> <li>YouTube video on <code>IPython</code></li> </ul> <p>After loading a Python module, you also have the IPython Python command shell available.</p> Forgot how to load a Python module? <p>See the UPPMAX page about Python.</p> What is a Python command shell? <p>In computing, a shell is a a program around something, for example, Bash is a shell around a UNIX-like operating system.</p> <p>In computing, a command shell means that the shell is a command-line shell, i.e. text only.</p> <p>A Python command shell, hence, is a text-only program around Python.</p> <p>Start the IPython command shell by typing:</p> <pre><code>ipython\n</code></pre> <p>or (for explicit Python 3):</p> <pre><code>ipython3\n</code></pre> <p>The <code>ipython3</code> prompt looks like this:</p> <pre><code>[sven@rackham1 ~]$ ipython\nPython 3.11.4 (main, Aug  7 2023, 16:05:58) [GCC 12.2.0]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.14.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]:\n</code></pre> <p>IPython allows one to write code interactively.</p> <p>For example, in IPython, type:</p> <pre><code>print('Hello, world!')\n</code></pre> <p>and IPython will show the result of that line of code.</p> <p>IPython can interact with your file system.</p> How does IPython interact with the file system? <p>For example, within IPython, running ...</p> <p>```python ls ````</p> <p>... displays a list of files in your current working folder in the same way as Bash's <code>ls</code>.</p> <p>The Python interpreter will give an error if you do the same.</p> <p>IPython has an auto-complete triggered by Tab.</p> How do I get auto-complete? <p>As an example, writing this line of code in IPython ...</p> <pre><code>s = 'Hello, world!'\n</code></pre> <p>... and press enter. Now a variable called <code>s</code> will hold some text.</p> <p>Now type ...</p> <pre><code>s.\n</code></pre> <p>and press Tab. You will see a menu of things you can do with that string. Hold tab to scroll through the many options.</p> <p>IPython can show graphics.</p> How do I get IPython to show graphics? <p>In IPython, run this code line-by-line:</p> <pre><code>import matplotlib.pyplot as plt\nplt.plot([1, 2, 3, 4])\nplt.show()\n</code></pre> <p>(or as a one-liner: <code>import matplotlib.pyplot as plt; plt.plot([1, 2, 3, 4]); plt.show()</code>)</p> <p>You will see a window appear:</p> <p></p> <p>You will only see a window appear, if you've logged in to Rackham with SSH with X forwarding enabled.</p> <p>Spoiler to login: <code>ssh -X sven@rackham.uppmax.uu.se</code>.</p> <p>Spoiler to confirm: run <code>xeyes</code>.</p> <p>IPython can directly run scripts.</p> How do I get IPython to directly run scripts? <p>In IPython, run:</p> <pre><code>run [filename]\n</code></pre> <p>where <code>[filename]</code> is the name of a file, for example:</p> <pre><code>run my_script.py\n</code></pre> <p>IPython will run the script and remember variables, functions and classes created in that script.</p>"},{"location":"software/javac/","title":"javac","text":""},{"location":"software/javac/#javac","title":"<code>javac</code>","text":"<p><code>javac</code> is a Java compiler.</p> <p><code>javac</code> is part of the <code>java</code> module and can be used to:</p> <ul> <li>Compile a Java program</li> </ul>"},{"location":"software/javac_compile_java/","title":"Compile Java programs using javac","text":""},{"location":"software/javac_compile_java/#compile-java-programs-using-javac","title":"Compile Java programs using <code>javac</code>","text":"<p>javac is a Java compilers.</p> <p>This page describes how to compile Java code using <code>javac</code>.</p>"},{"location":"software/javac_compile_java/#procedure","title":"Procedure","text":""},{"location":"software/javac_compile_java/#1-load-a-gcc-module","title":"1. Load a GCC module","text":"<p>Before compiling a java program, the <code>java</code> module has to be loaded. To load the java module, enter the command:</p> <pre><code>module load java\n</code></pre>"},{"location":"software/javac_compile_java/#2-create-a-java-source-file","title":"2. Create a Java source file","text":"<p>Create and write a Java source file called <code>hello_world.java</code>:</p> <pre><code>nano hello_world.java\n</code></pre> <p>In nano, write the Java program as such:</p> <pre><code>class hello_world\n{\n  public static void main(String[] args)\n  {\n    System.out.println(\"hello, world\");\n  }\n}\n</code></pre>"},{"location":"software/javac_compile_java/#3-compile-the-source-file","title":"3. Compile the source file","text":"<p>To compile, enter the command:</p> <pre><code>javac hello_world.java\n</code></pre>"},{"location":"software/javac_compile_java/#4-run","title":"4. Run","text":"<p>to run, enter:</p> <pre><code>java hello_world\n</code></pre>"},{"location":"software/jobinfo/","title":"jobinfo","text":""},{"location":"software/jobinfo/#jobinfo","title":"jobinfo","text":"<p><code>jobinfo</code> is a tool.</p>"},{"location":"software/jobinfo/#what-do-the-fields-priority-and-reason-mean-in-jobinfo-output","title":"What do the fields PRIORITY and REASON mean in \"jobinfo\" output?","text":"For staff <p>IG: Running jobs FAQ/Your priority...</p>"},{"location":"software/jobinfo/#initial-priority-at-submit-time","title":"Initial priority, at submit time","text":"<p>One of the columns in \"jobinfo\" output is named PRIORITY. The queue is sorted on priority, i.e. normally the job with the highest priority starts first, so this is an important parameter.</p> <p>When you submit a job at UPPMAX, it gets an initial priority. Normally this is 100000, but some jobs start at a priority of 200000 or more:</p> <ul> <li>On a limited amount of nodes, a group of people get a higher priority, due to e.g. that they have funded those nodes.</li> <li>Jobs that have asked for the interactive priority, with the flag \"--qos=interact\". This is for one-node jobs with a timelimit of at most 12 hours.</li> <li>Jobs that have asked for the short-job priority, with the flag \"--qos=short\". This is for jobs of from one to four nodes, with a timelimit of at most 15 minutes.</li> </ul> <p>When your project has overdrafted its 30 days running core hour allocation, the jobs within your project get a low initial priority of 70000 or less. These jobs are named bonus jobs. Instead of disallowing them from running, they are allowed to start, if there are free resources, when all higher-priority jobs have started. For each 10000 more core hours, that the project overdrafts its allocation, the priority gets lowered by 10000 more. The bottom value is 10000, i.e. a bonus job can start queuing with any of the following priorities, depending on how big the overdraft is: 70000, 60000, 50000, 40000, 30000, 20000, or 10000.</p> <p>For every minute waiting in queue, a job gets a priority increase of approximately one, up to a waiting time of 14 days.</p> <p>Now the waiting for each kind of jobs will be described: For high-priority jobs, bonus jobs and normal jobs.</p>"},{"location":"software/jobinfo/#high-priority-job","title":"High-priority job","text":"<p>Getting a high priority, i.e. a priority higher than 210000, already at submit time, this job will probably start quickly.</p> <p>The priority value will slowly increase, for each minute passing, until the job starts.</p>"},{"location":"software/jobinfo/#bonus-job","title":"Bonus job","text":"<p>Getting a low priority already at submit time, this job may have to wait a long time before starting. It is very difficult to estimate the waiting time, because all new high-priority and normal jobs will have a higher priority.</p> <p>At night or during next weekend, this job may be lucky and start. Waiting long enough, the monthly allocation of the project will not be overdrafted any longer, and the job automatically converted to a normal job.</p> <p>The priority value will slowly increase, for each minute passing, until the job starts.</p> <p>Once the job has started, it will be treated like any other job.</p>"},{"location":"software/jobinfo/#normal-job","title":"Normal job","text":"<p>A normal job, starting at priority 100000, increases slowly in priority and may eventually start at a priority a little above 100000.</p> <p>But more likely, something else will happen to it before that: It will be elevated to a higher starting priority: 190000. At the same time it loses the extra priority it accumulated while waiting at the priority 100000 level.</p> <p>Only normal jobs, will be elevated like this, and only one job or a few jobs for each user may be elevated at the same time.</p> <p>The reason for the elevated level, is to give each user a fair chance to start at least one job within a reasonable time, even if other users have thousands of jobs already waiting in queue. The job start time will not depend mainly on the number of jobs that are waiting, but instead on the number of other users that are waiting.</p> <p>At least one job for each user are permitted to wait at the elevated level. Up to 64 jobs for each user are permitted there, if they are very small. Every five minutes the system will try to elevate more jobs and every five minutes each old, elevated job gets five additional priority points.</p> <p>Once the job has been elevated, its START_TIME approximations will be much more accurate. The main risk for a later start, is that someone submits new, high-priority jobs. On the other hand, running jobs usually terminate earlier than what their timelimit suggests.</p> <p>Here is a detailed description on how jobs are picked for elevation:</p> <ul> <li>Jobs are picked strictly in order of priority.</li> <li>A job is not elevated, if its timelimit does not allow it to finish before next planned maintenance stop.</li> <li>At least one job per user is elevated, regardless of size and regardless of the two limitations mentioned below in this list.</li> <li>The elevated jobs of a user must not together ask for more than 64 cores.</li> <li>The elevated jobs of a user must not together ask for more than 2688 core hours, i.e. 112 core days.</li> </ul>"},{"location":"software/jobinfo/#how-does-slurm-decide-what-job-to-start-next","title":"How does Slurm decide what job to start next?","text":"<p>When there are free nodes, an approximate model of Slurm's behaviour is this:</p> <ul> <li>Step 1: Can the job in position one start now?</li> <li>Step 2: If it can, remove it from the queue, start it, and continue with step 1.</li> <li>Step 3: If it can not, look at next job.</li> <li>Step 4: Can it start now, without risking that the jobs before it in the queue get a higher START_TIME approximation?</li> <li>Step 5: If it can, remove it from the queue, start it, recalculate what nodes are free, look at next job and continue with step 4.</li> <li>Step 6: If it can not, look at next job, and continue with step 4.</li> </ul> <p>As soon as a new job is submitted and as soon as a job finishes, Slurm restarts with step 1, so most of the time only jobs at the top of the queue are tested for the possibility to start it. As a side effect of this restart behaviour, START_TIME approximations are normally NOT CALCULATED FOR ALL JOBS.</p>"},{"location":"software/jobinfo/#more-about-other-jobinfo-columns-for-waiting-jobs","title":"More about other jobinfo columns for waiting jobs","text":"<p>Until now, we have looked into the PRIORITY and USER columns. Let us talk about some of the others, for waiting jobs:</p> <ul> <li>JOBID: This is the best way to identify a job in a unique way. If you succeed to submit a job, it gets a jobid. The jobid of your finished jobs can be found with the finishedjobinfo command.</li> <li>POS: This is a numbering of the lines, by jobinfo, after sorting with PRIORITY as first key and JOBID as the second. This is an approximation of the queue position.</li> <li>PARTITION: A Slurm partition is a set of compute nodes, together with some rules about how jobs must be handled, if they ask for this partition. An UPPMAX cluster normally sports the \"devel\", \"core\" and \"node\" partitions.</li> <li>NAME: This is the job name, specified at submission time with the \"-J\" or \"--job-name\" flag. This name can help you to keep track of what the job was meant to do.</li> <li>ACCOUNT: The specified project name, to keep track of how many core hours each project has needed. The projinfo command sums up those core hours.</li> <li>ST: Means status. Status \"PD\" means pending (waiting), status \"R\" means running, status \"CG\" means completing (the job has finished, but the clean-up after the job is not finished yet).</li> <li>START_TIME: An estimation about when the job will start, if all jobs run until the end of their timelimit. You can make guesses about when nodes gets free also by looking at the TIME_LEFT column of running jobs. Slurm computes START_TIME only when it needs the information, i.e. you can not find that information for all jobs.</li> <li>TIME_LEFT: The specified timelimit for the job. When getting near to a maintenance stop, long jobs can not start, because they may not finish before the maintenance stop starts.</li> <li>REASON: There are a number of possible reasons for a job not to have started yet. Some are explained here:<ul> <li>AssociationJobLimit: probably means that the job never will start, because it breaks some system limit, set by UPPMAX.</li> <li>BeginTime: says that the user has specified that the job must not start until some specified time in the future.</li> <li>Dependency: means that the job will not start until some special other job(s) has (have) finished.</li> <li>JobHeldAdmin: means that some systems administrator has told that the job must not start.</li> <li>JobHeldUser: means that the job owner has told that job must not start.</li> <li>None: might mean that Slurm has not yet had time to put a reason there.</li> <li>Priority, ReqNodeNotAvail, and Resources: are the normal reasons for waiting jobs, meaning that your job can not start     yet, because free nodes for your job are not found.</li> <li>QOSResourceLimit: means that the job has asked for a QOS and that some limit for that QOS has been reached. The job can not start as long as the limit still is reached.</li> <li>QOSJobLimit: probably means that the job can never start, because it breaks some system limit, set by UPPMAX.</li> </ul> </li> <li>FEATURES: There are quite a few of these and some are explained here:<ul> <li>null: means that no special features have been asked for.</li> <li>fat: means that a fat node (a node with a more-than-standard -- for this cluster -- amount of memory) is needed.</li> <li>null: means that no special features have been asked for.</li> <li>thin: means that a standard (i.e. non-fat) node must be used, and this feature is automatically set for most jobs with no memory requirements and a high timelimit, so the job will not unnecessarily hog a fat node for a long time.</li> </ul> </li> </ul>"},{"location":"software/jobstats/","title":"jobstats","text":""},{"location":"software/jobstats/#jobstats","title":"<code>jobstats</code>","text":"<p>An example plot produced by <code>jobstats</code></p> <p><code>jobststats</code> is an UPPMAX tool to enable discovery of resource usage for jobs submitted to the Slurm job queue.</p> <p>At this page, it is described:</p> <ul> <li><code>jobstats --plot</code>: How to use is <code>jobstats --plot</code> to see resource use in a graphical plot</li> <li>Efficient use: How to use your resources efficiently</li> <li>Examples: Examples of ineffective resource use plots</li> <li>Other <code>jobstats</code> functionality<ul> <li>Using <code>jobstats --help</code></li> </ul> </li> </ul>"},{"location":"software/jobstats/#jobstats-plot","title":"<code>jobstats --plot</code>","text":"<p>With the <code>--plot</code> (or <code>-p</code>) option, a plot is produced showing the resource use per node for a job that completed successfully and took longer than 5 minutes.</p> <p>There are many ways to use <code>--plot</code>, a minimal use could be:</p> <pre><code>jobstats --plot [job_id]\n</code></pre> <p>for example:</p> <pre><code>jobstats --plot 12345678\n</code></pre> <p>The produced plot will be produced in the local folder with name <code>[cluster_name]-[project_name]-[user_name]-[jobid].png</code>, for example <code>rackham-uppmax1234-sven-876543.png</code>. Use any image viewer, e.g. eog to see it.</p> <p>Each plot shows:</p> <ul> <li>detailed information in the title.</li> <li>CPU usage in blue</li> <li>current memory usage in solid black</li> <li>overall memory usage in dotted black (if available)</li> </ul>"},{"location":"software/jobstats/#interpreting-a-plot","title":"Interpreting a plot","text":"<p>For example, in this plot:</p> <p></p> <ul> <li>the title shows the detailed info. <code>milou</code> is the name of a former UPPMAX cluster.</li> <li>CPU usage in blue, which is around 1000%, which is the equivalent of 10 cores   being used 100%</li> <li>current memory usage in solid black, which is around 20 GB (left-side vertical   axis) or a little bit less than 1 core (right-side vertical axis)</li> <li>overall memory usage in dotted black, which is around 340 GB (left-side vertical   axis) or a little bit less than 11 cores     (right-side vertical axis)</li> </ul> <p>For jobs running on multiple nodes, plots have multiple columns:</p> <p></p> <p>Some plots shows suggestions in red:</p> Text in plot Description <code>nodes_overbooked : nodes booked : nodes used:</code> More nodes were booked than used <code>overbooked : % used</code> The maximum percentage of booked cores and/or memory that was used (if &lt; 80%) <code>!!half_overbooked</code> No more than one-half of both cores and memory of a node was used; consider booking half a node instead. <code>!!severely_overbooked</code> No more than one-quarter of both cores and memory of a node was used, examine your job requirements closely. <code>!!swap_used</code> Swap storage was used at any point within the job run <code>node_type_overbooked : type booked : type used:</code> A fat node was requested that was larger than was needed. This flag may be produced spuriously if Slurm ran the job on a fat node when a fat node was not requested by the user. <code>cores_overbooked : cores booked : cores used:</code> More cores were booked than used (if &lt; 80%) <code>mem_overbooked : GB booked : GB used:</code> More memory was available than was used (if &lt; 25% and more than one core). <code>core_mem_overbooked : GB in used cores : GB used:</code> Less memory was used than was available in the cores that were used (if &lt; 50%). <p>In this example plot, however, the setup is considered good enough.</p>"},{"location":"software/jobstats/#determine-efficient-use","title":"Determine efficient use","text":"<p>To determine if you efficiently use UPPMAX resources, follow this chart:</p> <pre><code>flowchart TD\n  blue_line_close_to_top[CPU usage maximum above 80%?]\n  black_line_close_to_top[Memory usage maximum above 80%?]\n  can_decrease_number_of_cores[Can the number of cores be decreased?]\n  decrease_number_of_cores(Decrease the number of cores)\n  done(Done)\n  blue_line_close_to_top --&gt; |yes| done\n  blue_line_close_to_top --&gt; |no| black_line_close_to_top\n  black_line_close_to_top --&gt; |yes| done\n  black_line_close_to_top --&gt; |no| can_decrease_number_of_cores\n  can_decrease_number_of_cores --&gt; |yes| decrease_number_of_cores\n  can_decrease_number_of_cores --&gt; |no| done</code></pre> <p>If not, follow the strategy at efficient use</p>"},{"location":"software/jobstats/#efficient-use","title":"Efficient use","text":"<p>Here is a strategy to effectively use your UPPMAX resources:</p> <pre><code>flowchart TD\n  lower_limit_based_on_memory(Pick the number of cores to have enough memory)\n  limited_by_cpu(For that amount of cores, would runtime by limited by CPU?)\n  lower_limit_based_on_cpu(Increase the number of cores, so that on average, the right amount of CPUs is booked)\n\n  done(Use that amount of cores)\n\n  add_one(Increase the number of cores by one for safety)\n\n  lower_limit_based_on_memory --&gt; limited_by_cpu\n  limited_by_cpu --&gt; |no| add_one\n  limited_by_cpu --&gt; |yes| lower_limit_based_on_cpu\n  lower_limit_based_on_cpu --&gt; done\n  add_one --&gt; done</code></pre> Why not look at CPU usage? <p>Because CPU is more flexible.</p> <p>For example, imagine a job with a short CPU spike, that can be processed by 16 CPUs. If 1 core of memory is enough, use 1 core or memory: the spike will be turned into a 100% CPU use (of that one core) for a longer duration.</p> Need a worked-out example? <p></p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 1070%. This means that 11 cores (i.e. 1100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'no'. Having 11 cores would mean that most of the time only 10 are used. Only in the CPU spike at the end, the runtime is limited by CPU. This short time only has a minor impact on the runtime speed.</p> <p>Increase the number of cores by one for safety</p> <p>This means booking 12 cores is recommended.</p> Need another worked-out example? <p></p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 90%. This means that 1 core (i.e. 100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'yes'. Having 1 core would mean that around half the time there is too little CPU power. This has an effect.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>This is around 8 cores (800%), as with that amount of cores:</p> <ul> <li>half of the time, there is 1 out of 7 cores booked,   that is 6 too much</li> <li>half of the time, there is 7 out of 13 cores booked,   that is 6 too little</li> </ul> <p>This is not an exact algorithm and all numbers from 2 to 9 cores can be considered okay.</p> <p>Sometimes, however, it is inevitable to use resources inefficiently, see the examples below</p> <p>No queue is possible</p> <p>If everyone followed these guidelines, there would probably not even be a queue most of the time!</p>"},{"location":"software/jobstats/#examples","title":"Examples","text":"<p>Here are some examples of how inefficient jobs can look and what you can do to make them more efficient.</p>"},{"location":"software/jobstats/#inefficient-job-example-1-booking-too-much-cores","title":"Inefficient job example 1: booking too much cores","text":"<p>Here booking 5 cores is considered okay.</p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 390%. This means that 4 cores (i.e. 400%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'no'. Having 4 cores would mean that most of the time only 1 are used. Only for some CPU spikes, the runtime is limited by CPU. This short time only has a minor impact on the runtime speed.</p> <p>Increase the number of cores by one for safety</p> <p>This means booking 5 cores is recommended.</p>"},{"location":"software/jobstats/#inefficient-job-example-2-booking-too-much-cores","title":"Inefficient job example 2: booking too much cores","text":"<p>This is one of the grayer areas: booking 2-9 cores is all considered reasonable.</p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 90%. This means that 1 core (i.e. 100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'yes'. Having 1 core would mean that around half the time there is too little CPU power. This has an effect.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>This is around 8 cores (800%), as with that amount of cores:</p> <ul> <li>half of the time, there is 1 out of 7 cores booked,   that is 6 too much</li> <li>half of the time, there is 7 out of 13 cores booked,   that is 6 too little</li> </ul> <p>This is not an exact algorithm and all numbers from 2 to 9 cores can be considered okay.</p>"},{"location":"software/jobstats/#inefficient-job-example-3","title":"Inefficient job example 3","text":"<p>Here booking 6 cores is considered okay.</p> <p>Pick the number of cores to have enough memory</p> <p>The dotted black line hits the right-hand vertical axis at 40%. This means that 1 core (i.e. 100%) would be enough for this job.</p> <p>For that amount of cores, would runtime by limited by CPU?</p> <p>The answer is 'yes'. Having 1 core would mean that most of the time our run is limited by CPU power. This has an impact on the runtime speed.</p> <p>Increase the number of cores, so that on average the right amount of CPUs are booked</p> <p>This is around 6 cores (600%), as with that amount of cores:</p> <ul> <li>most of the time, there is 6 out of 6 cores booked,   that is 0 too much</li> <li>only rarely, there is a little spike up or a bigger spike down</li> </ul> <p>There are no signs of anything slowing them down, as the line is very even.</p> <p>This jobs should either have been booked with 6 cores, or the program running should be told to use all 8 cores.</p>"},{"location":"software/jobstats/#inefficient-job-example-4-slowdown","title":"Inefficient job example 4: slowdown","text":"<p>This job is using almost all of the cores it has booked, but there seems to be something holding them back. The uneven blue curve tells us that something is slowing down the analysis, and it's not by a constant amount.</p> <p>Usually this is how it looks when the filesystem is the cause of a slowdown. Since the load of the filesystem is constantly changing, so will the speed by which a job can read data from it also change.</p> <p>This job should try to copy all the files it will be working with to the nodes local harddrive before running the analysis, and by doing so not be affected by the speed of the filesystem.</p> <p>Please see the guide How to use the nodes own hard drive for analysis for more information.</p> <p>You basically just add 2 more commands to your script file and the problem should be solved.</p>"},{"location":"software/jobstats/#inefficient-job-example-5","title":"Inefficient job example 5","text":"<p>This job has the same problem as the example above, but in a more extreme way.</p> <p>It's not uncommon that people book whole nodes out of habit and only run single threaded programs that use almost no memory. This job is a bit special in the way that it's being run on a high memory node, as you can see on the left Y-axis, that it goes up to 256 GB RAM. A normal node on Milou only have 128GB. These high memory nodes are only bookable of you book the whole node, so you can't book just a few cores on them. That means that if you need 130GB RAM and the program is only single threaded, your only option is to book a whole high memory node. The job will look really inefficient, but it's the only way to do it on our system. The example in the plot does not fall into this category though, as it uses only ~15GB of RAM, which you could get by booking 2-3 normal cores.</p>"},{"location":"software/jobstats/#jobstats-help","title":"<code>jobstats --help</code>","text":"<p>Use <code>jobstats --help</code> to see the help of <code>jobstats</code>:</p> <pre><code>jobstats --help\n</code></pre> How does the output look like? <pre><code>USAGE\n-----\n\n    jobstats  -p [-r] [-M cluster] [ jobid [ jobid ... ] | -A project | - ] [other options]\n\nDiscover jobstats for the specified job(s) on the specified cluster.  Cluster\ndefaults to the value of $SNIC_RESOURCE ('rackham' on the current system) if\nnot specified.\n\nWith the -p/--plot option, a plot is produced from the jobstats for each\njobid.  Plots contain one panel per booked node showing CPU (blue) and memory\nusage (black) traces and include text lines indicating the job number, cluster,\nend time and duration, user, project, job name, and usage flags (more on those\nbelow).  For memory usage, one or two traces are shown: a solid black line\nshows instantaneous memory usage, and a dotted black line shows overall maximum\nmemory usage if this information is available.\n\nPlots are saved to the current directory with the name\n\n    cluster-project-user-jobid.png\n\nNote that not all jobs will produce jobstats files, particularly if the job was\ncancelled or ran for less than 5 minutes.  Also, if a job booked nodes\ninefficiently by not using nodes it asked for, jobstats files will not be\navailable for the booked but unused nodes.\n\nJOBSTATS DISCOVERY\n------------------\n\nThere are five modes for discovery, depending on what the user provides on the\ncommand line: (1) discovery by job number for a completed job; (2) discovery by\njob number for a currently running job; (3) discovery by node and job number,\nfor a completed or running job; (4) discovery by project; or (5) discovery via\ninformation provided on 'stdin'.  In each of the example command lines below, the\n-p/--plot option requests that plots of job resource usage are created.\n\nMode 1:  jobstats -p jobid1 jobid2 jobid3\n-------\nJob numbers valid on the cluster.  [finishedjobinfo](finishedjobinfo.md) is used to determine further\ninformation for each job.  If multiple queries are expected, it might be quicker\nto run [finishedjobinfo](finishedjobinfo.md) yourself separately, see Mode 5 below.  See Mode 2 for a\ncurrently running job.\n\nMode 2:  jobstats -p -r jobid1 jobid2 jobid3\n-------\nJob numbers of jobs currently running on the cluster.  The Slurm squeue tool is\nused to determine further information for each running job.\n\nMode 3:  jobstats -p -n m15,m16 jobid\n-------\n[finishedjobinfo](finishedjobinfo.md) is *not* called and Uppmax's stored job statistics files are\ndiscovered directly.  If you know which node(s) your job ran on or which nodes\nyou are interested in, this will be much faster than Mode 1.\n\nMode 4:  jobstats -p -A project\n-------\nWhen providing a project name that is valid for the cluster, [finishedjobinfo](finishedjobinfo.md) is\nused to determine further information on jobs run within the project.  As for\nMode 1, this can be rather slow.  Furthermore only [finishedjobinfo](finishedjobinfo.md) defaults for\ntime span etc. are used for job discovery.  If multiple queries are expected or\nadditional [finishedjobinfo](finishedjobinfo.md) options are desired, see Mode 5 below.\n\nMode 5:  [finishedjobinfo](finishedjobinfo.md) project | jobstats - -p\n-------\nAccept input on stdin formatted like [finishedjobinfo](finishedjobinfo.md) output.  The long form of\nthis option is '--stdin'.  This mode can be especially useful if multiple\nqueries of the same job information are expected.  In this case, save the\noutput of a single comprehensive [finishedjobinfo](finishedjobinfo.md) query, and extract the parts\nof interest and present them to this script on stdin.  For example, to produce\nanalyses of all completed jobs in a project during the current calendar year,\nand produce separate tarballs analysing all jobs and providing jobstats plots\nfor each user during this same period:\n\n     [finishedjobinfo](finishedjobinfo.md) -y project &gt; proj-year.txt\n     grep 'jobstat=COMPLETED' proj-year.txt | jobstats - &gt; all-completed-jobs.txt\n     grep 'username=user1' proj-year.txt | jobstats - -p &gt; user1-jobs.txt\n     tar czf user1-jobs.tar.gz user1-jobs.txt *-project-user1-*.png\n     grep 'username=user2' proj-year.txt | jobstats - -p &gt; user2-jobs.txt\n     tar czf user2-jobs.tar.gz user2-jobs.txt *-project-user2-*.png\n     ...\n\nCOMMAND-LINE OPTIONS\n--------------------\n\n    -p | --plot        Produce CPU and memory usage plot for each jobid\n\n    -r | --running     Jobids are for jobs currently running on the cluster. The\n                       Slurm squeue tool is used to discover further information\n                       for the running jobs, and the rightmost extent of the plot\n                       produced will reflect the scheduled end time of the job.\n\n    -A project         Project valid on the cluster.  [finishedjobinfo](finishedjobinfo.md) is used to\n                       discover jobs for the project.  See further comments\n                       under 'Mode 4' above.\n\n    -M cluster         Cluster on which jobs were run [default current cluster]\n\n    -n node[,node...]  Cluster node(s) on which the job was run.  If specified,\n                       then the [finishedjobinfo](finishedjobinfo.md) script is not run and discovery\n                       is restricted to only the specified nodes.  Nodes can be\n                       specified as a comma-separated list of complete node\n                       names, or using the [finishedjobinfo](finishedjobinfo.md) syntax:\n                             m78,m90,m91,m92,m100  or  m[78,90-92,100]\n                       Nonsensical results will occur if the syntaxes are mixed.\n\n    - | --stdin        Accept input on stdin formatted like [finishedjobinfo](finishedjobinfo.md)\n                       output.  The short form of this option is a single dash\n                       '-'.\n\n    -m | --memory      Always include memory usage flags in output.  Default\n                       behaviour is to include memory usage flags only if CPU\n                       usage flags are also present.\n\n    -v | --verbose     Be wordy when describing flag values.\n\n    -b | --big-plot    Produce 'big plot' with double the usual dimensions.\n                       This implies '-p/--plot'.\n\n    -q | --quiet       Do not produce table output\n\n    -Q | --Quick       Run [finishedjobinfo](finishedjobinfo.md) with the -q option, which is slightly\n                       faster but does not include Slurm's record of maximum\n                       memory used. With this option, memory usage analyses can\n                       only rely upon what is reported at 5-minute intervals,\n                       and the trace of maximum memory used (dotted black line)\n                       is not produced.\n\n    --no-extended      Do *not* use extended jobstats information [default is to use it]\n\n    --paging           Include PAGE_IN/PAGE_OUT statistics from extended jobstats [experimental]\n\n    -d                 Produce a header for table output\n\n    --version          Produce version of this script and plot_jobstats, then exit\n\n    -h | -?            Produce brief help\n\n    --help             Produce detailed help information\n\nThe following command-line options are generally useful only for Uppmax staff.\n\n    --cpu-free FLOAT   Maximum CPU busy percentage for the CPU to count as\n                       free at that sampling time.  Default is 3 %.\n    -x directory       Directory prefix to use for jobstats files.  Default is\n                       '/sw/share/slurm', and directory structure is (depending on whether\n                       --no-extended is used):\n\n                       &lt;prefix&gt;/&lt;cluster&gt;/extended_uppmax_jobstats/&lt;node&gt;/&lt;jobid&gt;\n                       &lt;prefix&gt;/&lt;cluster&gt;/uppmax_jobstats/&lt;node&gt;/&lt;jobid&gt;\n\n    -X directory       Hard directory prefix to use for jobstats files.\n                       Jobstats files are assumed available directly:\n                           '&lt;hard-prefix&gt;/&lt;jobid&gt;'\n    --no-multijobs     Run [finishedjobinfo](finishedjobinfo.md) separately for each jobid, rather\n                       than all jobids bundled into one -j option (for debugging)\n    -f file            [finishedjobinfo](finishedjobinfo.md) script [default is '/sw/uppmax/bin/finishedjobinfo']\n    -P file            plot_jobstats script [default is '/sw/uppmax/bin/plot_jobstats']\n\n\nFURTHER DETAILS\n---------------\n\nThis script produces two types of output.  If the -p/--plot command line option\nis provided, a plot is created of core and memory usage across the life of the\njob.  The name of the file produced has the format:\n\n    cluster-project-user-jobid.png\n\nUnless the -q/--quiet option is provided, a table is also produces containing\nlines with the following tab-separated fields:\n\n  jobid cluster jobstate user project endtime runtime flags booked cores node[,node...] jobstats[,jobstats...]\n\nField contents:\n\n  jobid    : Job ID\n  cluster  : Cluster on which the job was run\n  jobstate : End status of the job: COMPLETED, RUNNING, FAILED, TIMEOUT, CANCELLED\n  user     : Username that submitted the job\n  project  : Project account under which the job was run\n  endtime  : End time of the job (with -n/--node, this is '.')\n  runtime  : Runtime of the job (with -n/--node, this is '.')\n  flags    : Flags indicating various types of resource underutilizations\n  booked   : Number of booked cores (with -n/--node, this is '.')\n  maxmem   : Maximum memory used as reported by Slurm (if unavailable, this is '.')\n  cores    : Number of cores represented in the discovered jobstats files.\n  node     : Node(s) booked for the job, expanded into individual node names,\n             separated by commas; if no nodes were found, this is '.'.\n             The nodes for which jobstats files are available are listed first.\n  jobstats : jobstats files for the nodes, in the same order the nodes are\n             listed, separated by commas; if no jobstats files were discovered,\n             this is '.'\n\nIf -r/--running was used, an additional field is present:\n\n  timelimit_minutes : The time limit of the job in minutes\n\n\nFLAGS\n-----\n\nAn important part of jobstats output are usage flags.  These provide indications\nthat booked resources -- processor cores or memory -- might have been\nunderused.\n\nIn both plot and table output, flags are a comma-separated list of cautions\nregarding core and/or memory underutilisation.  The appearance of a flag does\nnot necessarily mean that resources were used incorrectly.  It depends upon the\ntools being used and the contents of the Slurm header, and also depends upon\nthe job profile.  Because usage information is gathered every 5 minutes, higher\ntransient usage of cores or memory may not be captured in the log files.\n\nFlags most likely to represent real overbooking of resources are\nnodes_overbooked, overbooked, !!half_overbooked, !!severely_overbooked, and\n!!swap_used.\n\nFor multinode jobs, flags other than nodes_overbooked are determined based only\non the usage of the first node.  Multinode jobs require careful analysis so as\nto not waste resources unnecessarily, and it is a common mistake among\nbeginning Uppmax users to book multiple nodes and run tools that cannot use\nmore than the first.  In this case, nodes_overbooked will appear.\n\nSome flags have a threshold below which they appear.  The default format is\ngenerally 'flag:value-booked:value-used'.\n\n  nodes_overbooked : nodes booked : nodes used\n      More nodes were booked than used\n  overbooked : % used (if &lt; 80%)\n      The maximum percentage of booked cores and/or memory that was used\n  !!half_overbooked\n      No more than 1/2 of both cores and memory of a node was used; consider booking\n      half a node instead.\n  !!severely_overbooked\n      No more than 1/4 of both cores and memory of a node was used, examine your job\n      requirements closely.\n  !!swap_used\n      Swap storage was used at any point within the job run\n  node_type_overbooked : type booked : type used\n      A fat node was requested that was larger than was needed.  This flag may be\n      produced spuriously if Slurm ran the job on a fat node when a fat node was not\n      requested by the user.\n  cores_overbooked : cores booked : cores used\n      More cores were booked than used (if &lt; 80%)\n  mem_overbooked : GB booked : GB used\n      More memory was available than was used (if &lt; 25% and more than one core).\n  core_mem_overbooked : GB in used cores : GB used\n      Less memory was used than was available in the cores that were used (if &lt; 50%).\n\nBy default no flags are indicated for jobs with memory-only cautions except for\nswap usage, because it is common for jobs to heavily use processor cores\nwithout using a sizable fraction of memory.  Use the -m/--memory option to\ninclude flags for memory underutilisation when those would be the only flags\nproduced.\n\nMore verbose flags are output with the -v/--verbose option.\n\n\nScript:   /sw/uppmax/bin/jobstats\nVersion:  2023-11-16\n</code></pre>"},{"location":"software/jobstats/#modes-of-jobstats-discovery","title":"Modes of <code>jobstats</code> discovery","text":"<p>There are five modes for discovery, depending on what the user provides on the command line:</p> <ul> <li>(1) discovery by job number for a completed job;</li> <li>(2) discovery by job number for a currently running job;</li> <li>(3) discovery by node and job number, for a completed or running job;</li> <li>(4) discovery by project</li> <li>(5) discovery via information provided on stdin.</li> </ul> <p>In the example command lines below, the -p/--plot option requests that plots of job resource usage are created.</p>"},{"location":"software/jobstats/#jobstats-discovery-mode-1-discovery-by-job-number-for-a-completed-job","title":"<code>jobstats</code> discovery mode 1: discovery by job number for a completed job","text":"<p>Discovery by job number for a completed job:</p> <pre><code>jobstats --plot jobid1 jobid2 jobid3\n</code></pre> <p>The job numbers valid on the cluster. finishedjobinfo is used to determine further information for each job. This can be rather slow, and a message asking for your patience is printed for each job.</p> <p>If multiple queries are expected it would be quicker to run finishedjobinfo yourself separately, see Mode 4 below. See Mode 2 for a currently running job.</p>"},{"location":"software/jobstats/#jobstats-discovery-mode-2-discovery-by-job-number-for-a-currently-running-job","title":"<code>jobstats</code> discovery mode 2: discovery by job number for a currently running job","text":"<p>Discovery by job number for a currently running job.</p> <pre><code>jobstats --plot -r jobid1 jobid2 jobid3\n</code></pre> <p>Job numbers of jobs currently running on the cluster. The Slurm schedule is used to determine further information for each running job.</p>"},{"location":"software/jobstats/#jobstats-discovery-mode-3-discovery-by-node-and-job-number-for-a-completed-or-running-job","title":"<code>jobstats</code> discovery mode 3: discovery by node and job number, for a completed or running job","text":"<p>Discovery by node and job number, for a completed or running job.</p> <pre><code>jobstats --plot -n m15,m16 jobid\n</code></pre> <p>finishedjobinfo is not called and UPPMAX's stored job statistics files for the cluster of interest are discovered directly. If you know which node(s) your job ran on or which nodes you are interested in, this will be much faster than Mode 1.</p>"},{"location":"software/jobstats/#jobstats-discovery-mode-4-discovery-by-project","title":"<code>jobstats</code> discovery mode 4: discovery by project","text":"<p>Discovery by project.</p> <pre><code>jobstats --plot -A project\n</code></pre> <p>When providing a project name that is valid for the cluster, finishedjobinfo is used to determine further information on jobs run within the project. As for Mode 1, this can be rather slow, and a message asking for your patience is printed.</p> <p>Furthermore only finishedjobinfo defaults for time span etc. are used for job discovery. If multiple queries are expected or additional finishedjobinfo options are desired, see Mode 5 below.</p>"},{"location":"software/jobstats/#jobstats-discovery-mode-5-discovery-via-information-provided-on-stdin","title":"<code>jobstats</code> discovery mode 5: discovery via information provided on stdin","text":"<p>Discovery via information provided on <code>stdin</code>:</p> What is <code>stdin</code>? <p><code>stdin</code> is an abbreviation for 'Standard input', see the Wikipedia page on 'stdin'</p> <pre><code>finishedjobinfo -q project | jobstats - --plot\n</code></pre> <p>Accept input on <code>stdin</code> formatted like finishedjobinfo output. Note the single dash (<code>-</code>) option given to jobstats; the long form of this option is <code>--stdin</code>. This mode can be especially useful if multiple queries of the same job information are expected. In this case, save the output of a single comprehensive finishedjobinfo query, and extract the parts of interest and present them to this script on <code>stdin</code>.</p> <p>For example, to produce analyses of all completed jobs in a project during the current calendar year, and produce separate tarballs analysing all jobs and providing jobstats plots for each user during this same period:</p> <pre><code>project=myproj\nfinishedjobinfo -q -y ${project} &gt; ${project}-year.txt\ngrep 'jobstat=COMPLETED' ${project}-year.txt | jobstats - &gt; ${project}-completed-jobs.txt\nfor u in user1 user2 user3 ; do\n    grep \"username=${u}\" ${project}-year.txt | jobstats - --plot &gt; ${u}-jobs.txt\n    tar czf ${u}-jobs.tar.gz ${u}-jobs.txt *-${project}-${u}-*.png\ndone\n</code></pre>"},{"location":"software/julia/","title":"Julia","text":""},{"location":"software/julia/#julia-user-guide","title":"Julia user guide","text":""},{"location":"software/julia/#introduction","title":"Introduction","text":"<p>Julia is according to https://julialang.org/:</p> <ul> <li>Fast</li> <li>Dynamic</li> <li>Reproducible</li> <li>Composable</li> <li>General</li> <li>Open source</li> </ul> <p>Documentation for version 1.8.</p> <p>Julia discussions</p> <p>NAISS Julia workshop</p>"},{"location":"software/julia/#julia-installations","title":"Julia installations","text":"<p>There is no system-installed Julia on the clusters. Therefore you need to load Julia with the module system. Different versions of Julia are available via the module system on Rackham, Snowy, and Bianca. Some installed packages are available via the module.</p> <p>As the time of writing we have the following modules:</p> <pre><code>[user@rackham1 ~]$ module avail julia\n------------------------------------------------------\njulia:\n------------------------------------------------------\nVersions:\n        julia/1.0.5_LTS\n        julia/1.1.1\n        julia/1.4.2\n        julia/1.6.1\n        julia/1.6.3\n        julia/1.6.7_LTS\n        julia/1.7.2\n        julia/1.8.5\n        julia/1.9.1\n        julia/1.9.3 (Default)\n</code></pre> <ul> <li>\"LTS\" stands for Long term support.</li> </ul> <p>To load a specific version of Julia into your environment,  type e.g.</p> <pre><code>module load julia/1.6.7_LTS\n</code></pre> <p>\u200bDoing:</p> <pre><code>module load julia\n</code></pre> <p>will give you the default version (1.9.3), often the latest version.</p> <p>A good and important suggestion is that you always specify a certain version. This is to be able to reproduce your work, a very important key in research!</p> <p>You can run a julia script in the shell by:</p> <pre><code>julia example_script.jl\n</code></pre> <p>After loading the appropriate modules for Julia, you will have access to the read-eval-print-loop (REPL) command line by typing julia.</p> <pre><code>julia\n</code></pre> <p>You will get a prompt like this:</p> <pre><code>julia&gt;\n</code></pre> <p>Julia has different modes, the one mentioned above is the so-called Julian mode where one can execute commands. The description for accessing these modes will be given in the following paragraphs. Once you are done with your work in any of the modes, you can return to the Julian mode by pressing the backspace key.</p> <p>While being on the Julian mode you can enter the shell mode by typing ;:</p> <pre><code>shell&gt; pwd\njulia&gt; ;\n/current-folder-path\n</code></pre> <p>This will allow you to use Linux commands. Notice that the availability of these commands depend on the OS, for instance, on Windows it will depend on the terminal that you have installed and if it is visible to the Julia installation.</p> <p>Info</p> <p>Backspace will get you back to julian mode</p> <p>Another mode available in Julia is the package manager mode, it can be accessed by typing ] in the Julian mode:</p> <pre><code>julia&gt; ]\n(v1.8) pkg&gt;\n</code></pre> <p>This will make your interaction with the package manager Pkg easier, for instance, instead of typing the complete name of Pkg commands such as Pkg.status() in the Julian mode, you can just type status in the package mode.</p> <p>The last mode is the help mode, you can enter this mode from the Julian one by typing ?, then you may type some string from which you need more information:</p> <pre><code>help?&gt; ans\njulia&gt; ?\nsearch: ans transpose transcode contains expanduser instances MathConstants readlines LinearIndices leading_ones leading_zeros\n\n  ans\n  A variable referring to the last computed value, automatically set at the interactive promp\n\njulia&gt;\n</code></pre> <ul> <li>Note that you get back directly to Julian mode!</li> </ul> <p>Info</p> <p>\u200b     Exit with <code>&lt;Ctrl-D&gt;</code> or <code>exit()</code>.</p> <p>See also</p> <p>More detailed information about the modes in Julia can be found here: https://docs.julialang.org/en/v1/stdlib/REPL/</p>"},{"location":"software/julia/#packages","title":"Packages","text":"<p>Some packages are pre-installed. That means that they are available also on Bianca.</p> Which are these? <ul> <li>\"BenchmarkTools\"</li> <li>\"CSV\"</li> <li>\"CUDA\"</li> <li>DataFrames\"</li> <li>\"Distributed\"</li> <li>\"DistributedArrays\"</li> <li>\"Gadfly\"</li> <li>\"IJulia\"</li> <li>\"MPI\"</li> <li>\"Plots\"</li> <li>\"PlotlyJS\"</li> <li>\"PyPlot\"</li> <li>all \"standard\" libraries.</li> </ul> <p>This list will be extended while you, as users, may wish more packages.</p> <p>You may control the present \"central library\" by typing in julia shell :</p> <pre><code>julia&gt; using Pkg\njulia&gt; Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.8\");     #change version accordingly\njulia&gt; Pkg.status()\njulia&gt; Pkg.activate(DEPOT_PATH[1]*\"/environments/v1.8\");     #to return to user library\n</code></pre> <p>Packages are imported or loaded by the commands <code>import</code> and <code>using</code>, respectively. The difference is shown here. Or briefly:</p> <p>To use module functions, use import Module to import the module, and Module.fn(x) to use the functions. Alternatively, using Module will import all exported Module functions into the current namespace.</p>"},{"location":"software/julia/#use-centrally-installed-packages-the-first-time","title":"Use centrally installed packages the first time","text":"<p>You may have to build the package the first time you run it. Julia will in such case ask you to do so. Then:</p> <pre><code>julia&gt; using Pkg\njulia&gt; Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.9\");      #change version accordingly\njulia&gt; Pkg.build(&lt;package_name&gt;)\n</code></pre>"},{"location":"software/julia/#how-to-install-personal-packages","title":"How to install personal packages","text":"<p>You may ignore the pre-installed packages. They are there mainly for Bianca users, but may help you to relieving some disk space! If you ignore you can jump over the this section.</p>"},{"location":"software/julia/#check-if-packages-are-installed-centrally","title":"Check if packages are installed centrally","text":"<p>To make sure that the package is not already installed, type in Julia:</p> <pre><code>julia&gt; using Pkg\njulia&gt; Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.8\");  #change version accordingly\njulia&gt; Pkg.status()\n</code></pre> <p>To go back to your own personal packages:</p> <pre><code>julia&gt; Pkg.activate(DEPOT_PATH[1]*\"/environments/v1.8\");\njulia&gt; Pkg.status()\n</code></pre> <p>You can load (using/import) ANY package from both local and central installation irrespective to which environment you activate. However, the setup is that your package is prioritized if there are similar names.</p>"},{"location":"software/julia/#start-an-installation-locally","title":"Start an installation locally","text":"<p>To install personal packages, start to be sure that you are in your local environment. You type within Julia:</p> <pre><code>     Pkg.activate(DEPOT_PATH[1]*\"/environmentts/v1.8\");\n     Pkg.add(\"&lt;package_name&gt;\")\n</code></pre> <p>This will install under the path ~/.julia/packages/. Then you can load it by just doing \"using/import \".</p> <pre><code>      using &lt;package_name&gt;\n</code></pre> <p>You can also activate a \"package prompt\" in julia with   ']':</p> <pre><code>(@v1.8) pkg&gt; add &lt;package name&gt;\n</code></pre> <p>For installing specific versions specify with <code>&lt;package name&gt;@&lt;X.Y.Z&gt;</code>.</p> <p>After adding you may be asked to precompile or build. Do so according to instruction given on the screen. Otherwise, first time importing or using the package, Julia may start a precompilation that will take a few seconds up to several minutes.</p> <p>Exit with <code>&lt;backspace&gt;</code>:</p> <pre><code>julia&gt;\n</code></pre>"},{"location":"software/julia/#own-packages-on-bianca","title":"Own packages on Bianca","text":"<p>You can use make an installation on Rackham and then use the wharf to copy it over to your ~/.julia/ directory.</p> <p>Otherwise, send an email to <code>support@uppmax.uu.se</code> and we'll help you.</p>"},{"location":"software/julia/#running-ijulia-from-jupyter-notebook","title":"Running IJulia from Jupyter notebook","text":"<p>Like for python it is possible to run a Julia in a notebook, i.e. in a web interface with possibility of inline figures and debugging. An easy way to do this is to load the python module as well. In shell:</p> <pre><code>module load julia/1.8.5\nmodule load python/3.10.8\njulia\n</code></pre> <p>In Julia:</p> <p>using IJulia</p> <pre><code>notebook(dir=\"&lt;/path/to/work/dir/&gt;\")\n</code></pre> <p>A Firefox session will start with the Jupyter notebook interface.</p> <p>If not, you may have to build IJulia the first time with Pkg.build(\u201cIJulia\u201d). Since \u201cIJulia\u201d is pre-installed centrally on UPPMAX you must activate the central environment by following these steps belo. This should only be needed the first time like this</p> <pre><code>&gt; using Pkg\n&gt; Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.8\");\n&gt; Pkg.build(\"IJulia\")\n&gt; notebook(dir=\"&lt;/path/to/work/dir/&gt;\")\n</code></pre> <p>This builds the package also locally before starting the notebook. If not done, Jupyter will not find the julia kernel of that version. With notebook(dir=\"\", detached=true) the notebook will not be killed when you exit your REPL julia session in the terminal.</p>"},{"location":"software/julia/#how-to-run-parallel-jobs","title":"How to run parallel jobs","text":"<p>There are several packages available for Julia that let you run parallel jobs. Some of them are only able to run on one node, while others try to leverage several machines. You'll find an introduction here.</p>"},{"location":"software/julia/#run-interactively-on-compute-node","title":"Run interactively on compute node","text":"<p>Always run parallel only on the compute nodes. This is an example with 4 cores on Rackham</p> <pre><code>$ interactive -A &lt;proj&gt; -n 4 -t 3:00:00\nRunning interactively at UPPMAX\n</code></pre> <p>Slurm user guide</p>"},{"location":"software/julia/#threading","title":"Threading","text":"<p>Threading divides up your work among a number of cores within a node. The threads share their memory. Below is an example from within Julia. First, in the shell type:</p> <pre><code>export JULIA_NUM_THREADS=4\njulia\n</code></pre> <p>in Julia:</p> <pre><code>using Base.Threads\nnthreads()\n      a = zeros(10)\n@threads for i = 1:10\n        a[i] = Threads.threadid()\nend\n</code></pre>"},{"location":"software/julia/#distributed-computing","title":"Distributed computing","text":"<p>Distributed processing uses individual processes with individual memory, that communicate with each other. In this case, data movement and communication is explicit. Julia supports various forms of distributed computing.</p> <ul> <li>A native master-worker system based on remote procedure calls: Distributed.jl</li> <li>MPI through MPI.jl : a Julia wrapper for the MPI protocol, see further down.</li> <li>DistributedArrays.jl: distribute an array among workers</li> </ul> <p>If choosing between distributed and MPI, distributed is easier to program, whereas MPI may be more suitable for multi-node applications.</p> <p>For more detailed info please confer the manual for distributed computing and julia MPI.</p>"},{"location":"software/julia/#master-worker-model","title":"Master-Worker model","text":"<p>We need to launch Julia with</p> <pre><code>julia -p 4\n</code></pre> <p>then inside Julia you can check</p> <pre><code>nprocs()\nworkers()\n</code></pre> <p>which should print 5 and [2,3,4,5]. Why 5, you ask? Because \"worker 1\" is the \"boss\". And bosses don't work.</p> <p>As you can see, you can run distributed computing directly from the julia shell.</p>"},{"location":"software/julia/#batch-example","title":"Batch example","text":"<p>Julia script hello_world_distributed.jl:</p> <pre><code>using Distributed\n# launch worker processes\nnum_cores = parse(Int, ENV[\"SLURM_CPUS_PER_TASK\"])\naddprocs(19)\nprintln(\"Number of cores: \", nprocs())\nprintln(\"Number of workers: \", nworkers())\n# each worker gets its id, process id and hostname\nfor i in workers()\n    id, pid, host = fetch(@spawnat i (myid(), getpid(), gethostname()))\n    println(id, \" \" , pid, \" \", host)\nend\n# remove the workers\nfor i in workers()\n    rmprocs(i)\nend\n</code></pre> <ul> <li>Batch script job_distributed.slurm:</li> </ul> <pre><code>#!/bin/bash\n#SBATCH -A j&lt;proj&gt;\n#SBATCH -p devel\n#SBATCH --job-name=distrib_jl     # create a short name for your job\n#SBATCH --nodes=1                # node count\n#SBATCH --ntasks=20              # total number of tasks across all nodes\n#SBATCH --cpus-per-task=1        # cpu-cores per task (&gt;1 if multi-threaded tasks)\n#SBATCH --time=00:01:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n#SBATCH --mail-user=&lt;email&gt;\nmodule load julia/1.8.5\njulia hello_world_distributed.jl\n</code></pre> <p>\u200bPut job in queue:</p> <pre><code>sbatch job_distributed.slurm\n</code></pre>"},{"location":"software/julia/#interactive-example","title":"Interactive example","text":"<pre><code>salloc -A &lt;proj&gt; -p node -N 1 -n 10 -t 1:0:0\njulia hello_world_distributed.jl\n</code></pre>"},{"location":"software/julia/#mpi","title":"MPI","text":"<p>The Threaded and Distributed packages are included in the Base installation. However, in order to use MPI with Julia you will need to follow the next steps (only the first time):</p> <ul> <li>Load the tool chain which contains a MPI library</li> </ul> <p>For julia/1.6.3 and earlier:</p> <pre><code>module load gcc/9.3.0 openmpi/3.1.5\n</code></pre> <p>For julia/1.6.7_LTS &amp; 1.7.2:</p> <pre><code>module load gcc/10.3.0 openmpi/3.1.6\n</code></pre> <p>For julia/1.8.5:</p> <pre><code>module load gcc/11.3.0 openmpi/4.1.3\n</code></pre> <ul> <li>Load Julia</li> </ul> <pre><code>ml julia/1.8.5   # or other\n</code></pre> <ul> <li>Start Julia on the command line</li> </ul> <pre><code>julia\n</code></pre> <ul> <li>Change to <code>package mode</code> and add the <code>MPI</code> package</li> </ul> <pre><code>(v1.8) pkg&gt; add MPI\n</code></pre> <ul> <li>In the <code>julian</code> mode run these commands:</li> </ul> <pre><code>julia&gt; using MPI\njulia&gt; MPI.install_mpiexecjl()\n[ Info: Installing `mpiexecjl` to `~/.julia/bin`...\n[ Info: Done!\n</code></pre> <ul> <li>Add the installed <code>mpiexecjl</code> wrapper to your path on the Linux command line</li> </ul> <pre><code>export PATH=~/.julia/bin:$PATH\n</code></pre> <ul> <li>Now the wrapper should be available on the command line</li> </ul> <p>Because of how MPI works, we need to explicitly write our code into a file, juliaMPI.jl:</p> <pre><code>import MPI\nMPI.Init()\ncomm = MPI.COMM_WORLD\nMPI.Barrier(comm)\nroot = 0\nr = MPI.Comm_rank(comm)\nsr = MPI.Reduce(r, MPI.SUM, root, comm)\nif(MPI.Comm_rank(comm) == root)\n@printf(\"sum of ranks: %s\\n\", sr)\nend\nMPI.Finalize()\n</code></pre> <p>You can execute your code as in an interactive session with several cores (at least 3 in this case):</p> <pre><code>module load gcc/11.3.0 openmpi/4.1.3\nmpiexecjl -np 3 julia juliaMPI.jl\n</code></pre> <p>A batch script, job_MPI.slurm, should include a \"module load gcc/XXX openmpi/XXX\"</p> <pre><code>#!/bin/bash\n#SBATCH -A j&lt;proj&gt;\n#SBATCH -p devel\n#SBATCH --job-name=MPI_jl        # create a short name for your job\n#SBATCH --nodes=1                # node count\n#SBATCH --ntasks=20              # total number of tasks across all nodes\n#SBATCH --cpus-per-task=1        # cpu-cores per task (&gt;1 if multi-threaded tasks)\n#SBATCH --time=00:05:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n#SBATCH --mail-user=&lt;email&gt;\nmodule load julia/1.8.5\nmodule load gcc/11.3.0 openmpi/4.1.3\nexport PATH=~/.julia/bin:$PATH\nmpiexecjl -n 20 julia juliaMPI.jl\n</code></pre> <ul> <li>Run with</li> </ul> <pre><code>sbatch job_MPI.slurm\n</code></pre> <p>See the MPI.jl examples for more input!</p>"},{"location":"software/julia/#gpu","title":"GPU","text":"<p>Example Julia script, juliaCUDA.jl:</p> <pre><code>using CUDA, Test\nN = 2^20\nx_d = CUDA.fill(1.0f0, N)\ny_d = CUDA.fill(2.0f0, N)\ny_d .+= x_d\n@test all(Array(y_d) .== 3.0f0)\nprintln(\"Success\")\n</code></pre> <p>Batch script juliaGPU.slurm, note settings for Bianca vs. Snowy:</p> <pre><code>#!/bin/bash\n#SBATCH -A &lt;proj-id&gt;\n#SBATCH -M &lt;snowy OR bianca&gt;\n#SBATCH -p node\n#SBATCH -C gpu   #NB: Only for Bianca\n#SBATCH -N 1\n#SBATCH --job-name=juliaGPU         # create a short name for your job\n#SBATCH --gpus-per-node=&lt;1 OR 2&gt;             # number of gpus per node (Bianca 2, Snowy 1)\n#SBATCH --time=00:15:00          # total run time limit (HH:MM:SS)\n#SBATCH --qos=short              # if test run t&lt;15 min\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n#SBATCH --mail-user=&lt;email&gt;\nmodule purge\nmodule load julia/1.8.5          # system CUDA works as of today\njulia juliaCUDA.jl\n</code></pre> <ul> <li>Put job in queue:</li> </ul> <pre><code>sbatch juliaGPU.slurm\n</code></pre>"},{"location":"software/julia/#interactive-session-with-gpu","title":"Interactive session with GPU","text":"<p>On Snowy, getting 1 cpu and 1 gpu:</p> <pre><code>interactive -A &lt;proj&gt; -n 1 -M snowy --gres=gpu:1  -t 3:00:00\n</code></pre> <p>On Bianca, getting 2 cpu:s and 1 gpu:</p> <pre><code>interactive -A &lt;proj&gt; -n 2 -C gpu --gres=gpu:1 -t 01:10:00\n</code></pre> <ul> <li>wait until session is started</li> </ul> <pre><code>modul load julia/1.8.5\n</code></pre>"},{"location":"software/julia_pelle/","title":"Julia on Pelle","text":""},{"location":"software/jupyter/","title":"Jupyter","text":"","tags":["Jupyter","Python","IDE"]},{"location":"software/jupyter/#jupyter","title":"Jupyter","text":"<p>There are multiple IDEs on the UPPMAX clusters, among other Jupyter. Here we describe how to run Jupyter.</p> <p>Jupyter is an IDE specialized for the Python programming language.</p> <p>Info</p> <ul> <li>You can run Python in a Jupyter-notebook,   i.e. in a web interface with possibility of inline figures and debugging.</li> <li>Jupyter-lab is installed in the python&gt;=3.10.8 module</li> </ul> <p>Warning</p> <p>Always start Jupyter in a ThinLinc session and preferably in an interactive session.</p>","tags":["Jupyter","Python","IDE"]},{"location":"software/jupyter/#introduction","title":"Introduction","text":"<p>Jupyter is web application that allows literature programming for Python. That is, Jupyter allows to create documents where Python code is shown and run and its results shown, surrounded by written text (e.g. English).</p> <p>Additionally, Jupyter allows to share files and hence includes a file manager.</p> <p>Jupyter is:</p> <ul> <li>started and running on a server, for example, an interactive session</li> <li>displayed in a web browser, such as <code>firefox</code>.</li> </ul> <p>Jupyter can be slow when using remote desktop webpage (e.g. <code>https://rackham-gui.uppmax.uu.se</code>).</p> <ul> <li> <p>For UPPMAX, one can use a locally installed ThinLinc client to speed up Jupyter. See <code>the UPPMAX documentation on ThinLinc &lt;https://www.uppmax.uu.se/support/user-guides/thinlinc-graphical-connection-guide&gt;</code>_ on how to install the ThinLinc client locally.</p> </li> <li> <p>It is also possible to run Jupyter with a local browser to speed up the graphics but still use the benefits of many CPU:s and much RAM.</p> <ul> <li>Run Jupyter in your local browser</li> </ul> </li> </ul>","tags":["Jupyter","Python","IDE"]},{"location":"software/jupyter/#how-to-start-jupyter","title":"How to start Jupyter","text":"<ul> <li>Run Jupyter on Bianca</li> <li>Run Jupyter on Rackham</li> <li>Run Jupyter in the UPPMAX lab</li> <li>Run Jupyter in your local browser</li> <li>Run Jupyter in a virtual environment (see below)</li> </ul>","tags":["Jupyter","Python","IDE"]},{"location":"software/jupyter/#run-jupyter-in-a-virtual-environment-venv","title":"Run Jupyter in a virtual environment (venv)","text":"<p>You could also use jupyter- (lab or notebook) in a <code>venv</code> virtual environment.</p> <p>If you decide to use the <code>--system-site-packages</code> configuration you will get <code>jupyter</code> from the python modules you created you virtual environment with. However, you won't find your locally installed packages from that jupyter session. To solve this, reinstall jupyter within the virtual environment by force (option <code>-I</code>):</p> <pre><code>pip install -I jupyter\n</code></pre> <p>and run it as above.</p> <p>Be sure to start the kernel with the virtual environment name, like \"project A\", and not \"Python 3 (ipykernel)\".</p>","tags":["Jupyter","Python","IDE"]},{"location":"software/jupyter/#links","title":"Links","text":"<ul> <li>The Jupyter project contains a lot of information and inspiration</li> <li>The Jupyter Notebook documentation</li> </ul>","tags":["Jupyter","Python","IDE"]},{"location":"software/jupyter_local/","title":"Jupyter in local browser","text":"","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_local/#jupyter-in-local-browser","title":"Jupyter in local browser","text":"<p>To increase the speed of graphics it is possible to run Jupyter on a compute node, but using the graphics on your local computer. That will speed up the interaction with plotting figures and GUI management.</p> <p>This possible for the [Rackham[(../cluster_guides/rackham.md) and Snowy clusters.</p> <p>Warning</p> <p>This feature is not possible for Bianca</p>","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_local/#step-1-login-to-an-uppmax-cluster","title":"Step 1: Login to an UPPMAX cluster","text":"<ul> <li>Using ThinLinc or a terminal does not matter.</li> </ul>","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_local/#step-2-start-an-interactive-session","title":"Step 2: start an interactive session","text":"<p>Start a terminal. Within that terminal, start an interactive session from the login node (change to the correct NAISS project ID).</p>","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_local/#for-rackham","title":"For Rackham","text":"<pre><code>interactive -A &lt;naiss-project-id&gt;  -t 4:00:00\n</code></pre>","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_local/#for-snowy","title":"For Snowy","text":"<pre><code>interactive -M snowy -A &lt;naiss-project-id&gt;  -t 4:00:00\n</code></pre>","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_local/#step-3-start-jupyter-in-the-interactive-session","title":"Step 3: start Jupyter in the interactive session","text":"<p>Within your terminal with the interactive session, load a modern Python module:</p> <pre><code>module load python/3.11.8\n</code></pre> <p>Then, start <code>jupyter-notebook</code> (or <code>jupyter-lab</code>):</p> <pre><code>jupyter-notebook --ip 0.0.0.0 --no-browser\n</code></pre> <p>Leave this terminal open.</p> <p>The terminal will display multiple URLs.</p> <p>Copy one of these, like:</p> <pre><code>http://r486:8888/?token=5c3aeee9fbfc75f7a11c4a64b2b5b7ec49622231388241c2\n</code></pre>","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_local/#step-4-on-own-computer","title":"Step 4: On own computer","text":"<ul> <li>If you use ssh to connect to Rackham, you need to forward the port of the interactive session to your local computer.<ul> <li>On Linux or Mac this is done by running in another terminal. Make sure you have the ports changed if they are not at the default <code>8888</code>.</li> </ul> </li> </ul> <pre><code>ssh -L 8888:r486:8888 username@rackham.uppmax.uu.se\n</code></pre> <ul> <li>Replace <code>r486</code> if you got another node</li> <li>If you use Windows it may be better to do this in the PowerShell instead of a WSL2 terminal.</li> <li>If you use PuTTY - you need to change the settings in \"Tunnels\" accordingly (could be done for the current connection as well).</li> </ul> <p>SSH port forwarding</p> <p>On your computer open the address you got but replace <code>r486</code> with <code>localhost</code> or <code>127.0.0.0</code> i.e.</p> <pre><code>http://localhost:8888/?token=5c3aeee9fbfc75f7a11c4a64b2b5b7ec49622231388241c2\n</code></pre> <p>or</p> <pre><code>http://127.0.0.1:8888/?token=5c3aeee9fbfc75f7a11c4a64b2b5b7ec49622231388241c2\n</code></pre> <p>This should bring the jupyter interface on your computer and all calculations and files will be on Rackham compute node.</p> <p>Back to jupyter page</p>","tags":["Jupyter","local","Rackham","Snowy"]},{"location":"software/jupyter_on_bianca/","title":"Jupyter on Bianca","text":"","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#jupyter-on-bianca","title":"Jupyter on Bianca","text":"<p>There are multiple IDEs on the UPPMAX clusters, among other Jupyter. Here we describe how to run Jupyter on Bianca.</p> <p>Jupyter is an IDE specialized for the Python programming language.</p>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#procedure","title":"Procedure","text":"Prefer a video? <p>See the YouTube video Jupyter on Bianca</p>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#1-get-within-sunet","title":"1. Get within SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#2-start-the-bianca-remote-desktop-environment","title":"2. Start the Bianca remote desktop environment","text":"Forgot how to start Bianca's remote desktop environment? <p>See the 'Logging in to Bianca' page.</p>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#3-start-an-interactive-session","title":"3. Start an interactive session","text":"<p>Within the Bianca remote desktop environment, start a terminal. Within that terminal, start an interactive session:</p> <pre><code>interactive -A [project_number] -t 8:00:00\n</code></pre> <p>Where <code>[project_number]</code> is your UPPMAX project, for example:</p> <pre><code>interactive -A sens2016001 -t 8:00:00\n</code></pre> What is my UPPMAX project number? <p>Easy answers that is probably true:</p> <p>The one you used to login, which is part of your prompt. For example, in the prompt below, the project is <code>sens2016001</code>.</p> <pre><code>[sven@sens2016001-bianca sven]$\n</code></pre>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#4-load-a-python-module","title":"4. Load a Python module","text":"<p>Within the terminal of the interactive session, load a Python module</p> <pre><code>module load python/3.11.4\n</code></pre> Forgot what the module system is? <p>See the UPPMAX pages on the module system.</p> Can I use other Python modules? <p>Yes, you can use any module later than (and including) the <code>python/3.10.8</code> module.</p>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#5-start-the-jupyter-notebook","title":"5. Start the Jupyter notebook","text":"<p>Still within the terminal of the interactive session, start a notebook like this:</p> <pre><code>jupyter-notebook --ip 0.0.0.0 --no-browser\n</code></pre> <p>or jupyter lab:</p> <pre><code>jupyter-lab --ip 0.0.0.0 --no-browser\n</code></pre> <p>Jupyter will show some IP address in the terminal, which you will need in the next step.</p>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_bianca/#6-browser-to-the-jupyter-notebook","title":"6. Browser to the Jupyter notebook","text":"<p>In the remote desktop environment on Bianca, start Firefox. Set Firefox to the URL addresses from the Jupyter output.</p> Can I start Firefox from the terminal too? <p>Yes, in another terminal, one can use:</p> <pre><code>firefox [URL]\n</code></pre> <p>where <code>[URL]</code> is a URL produced by Jupyter, for example:</p> <pre><code>firefox http://127.0.0.1:8889/tree?token=7c305e62f7dacf65d74a4b966e2851987479ad0a258de34f\n</code></pre>","tags":["Jupyter","Bianca"]},{"location":"software/jupyter_on_rackham/","title":"Jupyter on Rackham","text":"","tags":["Jupyter","Rackham"]},{"location":"software/jupyter_on_rackham/#jupyter-on-rackham","title":"Jupyter on Rackham","text":"<p>There are multiple IDEs on the UPPMAX clusters, among other Jupyter. Here we describe how to run Jupyter on Rackham.</p> <p>Jupyter is an IDE specialized for the Python programming language.</p>","tags":["Jupyter","Rackham"]},{"location":"software/jupyter_on_rackham/#procedure","title":"Procedure","text":"Prefer a video? <p>This procedure is also demonstrated in this YouTube video</p>","tags":["Jupyter","Rackham"]},{"location":"software/jupyter_on_rackham/#1-start-a-rackham-remote-desktop-environment","title":"1. Start a Rackham remote desktop environment","text":"<p>This can be either:</p> <ul> <li>Login to the Rackham remote desktop environment using the website</li> <li>Login to the Rackham remote desktop environment using a local ThinLinc client</li> </ul>","tags":["Jupyter","Rackham"]},{"location":"software/jupyter_on_rackham/#2-start-an-interactive-session","title":"2. Start an interactive session","text":"<p>Within the Rackham remote desktop environment, start a terminal. Within that terminal, start an interactive session:</p> <pre><code>interactive -A [project_number] -t 8:00:00\n</code></pre> <p>Where <code>[project_number]</code> is your UPPMAX project, for example:</p> <pre><code>interactive -A sens2016001 -t 8:00:00\n</code></pre> What is my UPPMAX project number? <p>See the UPPMAX documentation on how to see your UPPMAX projects</p>","tags":["Jupyter","Rackham"]},{"location":"software/jupyter_on_rackham/#3-load-a-python-module","title":"3. Load a Python module","text":"<p>Within the terminal of the interactive session, load a Python module</p> <pre><code>module load python/3.11.4\n</code></pre> Forgot what the module system is? <p>See the UPPMAX pages on the module system.</p> Can I use other Python modules? <p>Yes, you can use any module later than (and including) the <code>python/3.10.8</code> module.</p>","tags":["Jupyter","Rackham"]},{"location":"software/jupyter_on_rackham/#4-start-the-jupyter-notebook","title":"4. Start the Jupyter notebook","text":"<p>Still within the terminal of the interactive session, start a notebook like this:</p> <pre><code>jupyter-notebook --ip 0.0.0.0 --no-browser\n</code></pre> <p>or jupyter lab:</p> <pre><code>jupyter-lab --ip 0.0.0.0 --no-browser\n</code></pre> <p>Jupyter will show some IP address in the terminal, which you will need in the next step.</p>","tags":["Jupyter","Rackham"]},{"location":"software/jupyter_on_rackham/#5-browser-to-the-jupyter-notebook","title":"5. Browser to the Jupyter notebook","text":"<p>In the remote desktop environment on Rackham, start Firefox. Set Firefox to the URL addresses from the Jupyter output.</p> Can I start Firefox from the terminal too? <p>Yes, in another terminal, one can use:</p> <pre><code>firefox [URL]\n</code></pre> <p>where <code>[URL]</code> is a URL produced by Jupyter, for example:</p> <pre><code>firefox http://127.0.0.1:8889/tree?token=7c305e62f7dacf65d74a4b966e2851987479ad0a258de34f\n</code></pre>","tags":["Jupyter","Rackham"]},{"location":"software/jvarkit/","title":"jvarkit","text":""},{"location":"software/jvarkit/#jvarkit","title":"jvarkit","text":"<p>According to the <code>jvarkit</code> GitHub repository <code>jvarkit</code> is 'Java utilities for Bioinformatics',</p> <p><code>jvarkit</code> is unavailable in the UPPMAX module system.</p>"},{"location":"software/jvarkit/#create-a-jvarkit-singularity-container","title":"Create a <code>jvarkit</code> Singularity container","text":"<p>To create a Singularity container one can follow the procedure documented at 'Create a Singularity container from Docker_Hub'.</p> <p>Spoiler:</p> <pre><code>sudo singularity build my_container.sif docker:lindenb/jvarkit:1b2aedf24\n</code></pre> <p>Note that <code>1b2aedf24</code> is the tag of the latest version of this Docker script. In the future, they may be newer tags.</p> <p>Usage:</p> <pre><code>./jvarkit.sif java -jar /opt/jvarkit/dist/jvarkit.jar --help\n</code></pre>"},{"location":"software/jvarkit/#links","title":"Links","text":"<ul> <li>the <code>jvarkit</code> GitHub repository</li> </ul>"},{"location":"software/matlab/","title":"MATLAB","text":""},{"location":"software/matlab/#matlab-user-guide","title":"MATLAB user guide","text":""},{"location":"software/matlab/#the-matlab-module","title":"The MATLAB module","text":"<p>MATLAB can be started only if you load the matlab module first. Most of available official toolboxes are also available. At the time of this writing, our most recent installation is: <code>matlab/R2023b</code></p> <p>Doing:</p> <pre><code>module load matlab\n</code></pre> <p>will give you the latest version.</p> <p>If you need a different version, check the availability by:</p> <pre><code>module avail matlab\n</code></pre> <p>To get started with MATLAB do (for instance):</p> <pre><code>module load matlab/R2023a\nmatlab &amp;\n</code></pre> <p>That will start a matlab session with the common GUI. Use <code>&amp;</code> to have MATLAB in background making terminal still active for other work.</p> <p>A good and important suggestion is that you always specify a certain version. This is to be able to reproduce your work, a very important key in research!</p>"},{"location":"software/matlab/#introduction","title":"Introduction","text":"<p>Using MATLAB on the cluster enables you to utilize high performance facilities like:</p> <ul> <li>Parallel computing<ul> <li>Parallel for-loops</li> <li>Evaluate functions in the background</li> </ul> </li> <li>Big data processing<ul> <li>Analyze big data sets in parallel</li> </ul> </li> <li>Batch Processing<ul> <li>Offload execution of functions to run in the background</li> </ul> </li> <li>GPU computing (Available on Bianca and Snowy)<ul> <li>Accelerate your code by running it on a GPU</li> </ul> </li> <li>Machine &amp; Deep learning<ul> <li>Statistics and Machine Learning</li> <li>Deep Learning</li> </ul> </li> </ul> <p>See MathWorks's complete user guide</p> <p>Some online tutorials and courses:</p> <ul> <li>Parallel computing</li> <li>Machine Learning<ul> <li>Machine learning article</li> <li>Machine learning tutorial</li> </ul> </li> <li>Deep Learning<ul> <li>Deep learning article</li> <li>Deep learning tutorial</li> </ul> </li> </ul>"},{"location":"software/matlab/#running-matlab","title":"Running MATLAB","text":"<p>Warning</p> <ul> <li>It is possible to start Matlab on the Login node.</li> <li> <p>This can be a way to work if you</p> <ul> <li>work with just light analysis</li> <li>just use Matlab to start batch jobs from the graphical user interface.</li> </ul> </li> <li> <p>Then you should start matlab with just ONE thread</p> </li> </ul> <pre><code>matlab -singleCompThread &amp;\n</code></pre>"},{"location":"software/matlab/#graphical-user-interface","title":"Graphical user interface","text":"<p>To start MATLAB with its usual graphical interface (GUI), start it with:</p> <pre><code>matlab\n</code></pre> <p>If you will use significant resources, like processor or RAM, you should start an interactive session on a calculation node. Use at least 2 cores (-n 2), when running interactive. Otherwise MATLAB may not start. You can use several cores if you will do some parallel calculations (see parallel section below). Example:</p> <pre><code>interactive -A &lt;proj&gt; -p core -n 2 -t 1:0:0\n</code></pre> <p>This example starts a session with 2 cores for a wall time of 1 hour.</p>"},{"location":"software/matlab/#matlab-in-terminal","title":"MATLAB in terminal","text":"<p>For simple calculations it is possible to start just a command shell in your terminal:</p> <pre><code>matlab -nodisplay\n</code></pre> <p>Exit with 'exit'.</p> <p>Run script from terminal or bash script</p> <p>In order to run a script directly from terminal:</p> <pre><code>matlab -batch \"run('&lt;path/to/script.m&gt;')\" | tail -n +2\n</code></pre> <p>List all ways to run/start MATLAB:</p> <pre><code>matlab -h\n</code></pre>"},{"location":"software/matlab/#thinlinc","title":"ThinLinc","text":"<p>You may get the best of the MATLAB graphics by running it the ThinLinc environment.</p> <ul> <li> <p>For rackham (in ThinLinc app): <code>rackham-gui.uppmax.uu.se</code></p> </li> <li> <p>For Bianca (from web-browser): https://bianca.uppmax.uu.se</p> </li> </ul> <p>You may want to confer our UPPMAX ThinLinc user guide.</p>"},{"location":"software/matlab/#how-to-run-parallel-jobs","title":"How to run parallel jobs","text":""},{"location":"software/matlab/#how-to-run-parallel-jobs-for-the-first-time-since-may-13-2024","title":"How to run parallel jobs for the first time, since May 13 2024","text":"<ul> <li> <p>If you use MATLAB after May 13 2024, of any version, you have to do the following step to be able to use the full features of running parallel jobs.</p> </li> <li> <p>Instruction here</p> </li> </ul>"},{"location":"software/matlab/#two-matlab-commands","title":"Two MATLAB commands","text":"<p>Two commands in MATLAB are important to make your code parallel:</p> <ul> <li><code>parfor</code> will distribute your \"for loop\" among several workers (cores)</li> <li><code>parfeval</code> runs a section or a function on workers in the background</li> </ul>"},{"location":"software/matlab/#use-interactive-matlab","title":"Use interactive matlab","text":"<p>First, start an interactive session on a calculation node with, for instance 8 cores by:</p> <pre><code>interactive -A &lt;project&gt; -p core -n 8 -t 3:00:00\n</code></pre> <p>In MATLAB open a parallel pool of 8 local workers:</p> <pre><code>&gt;&gt; p = parpool(8)\n</code></pre> <p>What happens if you try to run the above command twice?  You can't run multiple parallel pools at the same time. Query the number of workers in the parallel pool:</p> <pre><code>&gt;&gt; p.NumWorkers\n</code></pre> <p><code>gcp</code> will \"get current pool\" and return a handle to it.  If a pool has not already been started, it will create a new one first and then return the handle to it:</p> <pre><code>&gt;&gt; p = gcp\n</code></pre> <p>Shutdown the parallel pool:</p> <pre><code>&gt;&gt; delete(p)\n</code></pre> <p>Will check to see if a pool is open and if so, deletes it.</p> <pre><code>&gt;&gt; delete(gcp('nocreate'))\n</code></pre> <p>This will delete a pool if it exists, but won't create one first if it doesn't already exist.</p> <p>With parpool('local') or parcluster('local') you will use settings for 'local' . With parpool('local',20) you will get 20 cores, but else the 'local' settings, like automatic shutdown after 30 minutes. You can change your settings here: HOME &gt; ENVIRONMENT &gt; Parallel &gt; Parallel preferences.</p>"},{"location":"software/matlab/#matlab-batch","title":"MATLAB Batch","text":"<p>With MATLAB you can e.g. submit jobs directly to our job queue scheduler, without having to use Slurm's commands directly. Let us first make two small function. The first one, little simpler, saved in the file <code>parallel_example.m</code>:</p> <pre><code>    function t = parallel_example(nLoopIters, sleepTime)\n      t0 = tic;\n      parfor idx = 1:nLoopIters\n        A(idx) = idx;\n        pause(sleepTime);\n      end\n      t = toc(t0);\n</code></pre> <p>and the second, little longer, saved in <code>parallel_example_hvy.m</code>:</p> <pre><code>    function t = parallel_example_hvy(nLoopIters, sleepTime)\n      t0 = tic;\n      ml = 'module list';\n      [status, cmdout] = system(ml);\n      parfor idx = 1:nLoopIters\n        A(idx) = idx;\n        for foo = 1:nLoopIters*sleepTime\n          A(idx) = A(idx) + A(idx);\n          A(idx) = A(idx)/3;\n        end\n      end\n</code></pre> <p>Begin by running the command</p> <pre><code>&gt;&gt; configCluster %(on Bianca it will look a little different)\n</code></pre> <p>in Matlab Command Window to choose a cluster configuration. Matlab will set up a configuration and will then print out some instructions, seen below. You can also set environments that is read if you don't specify it. Go to HOME &gt; ENVIRONMENT &gt; Parallel &gt; Parallel preferences.</p> <pre><code>       [1] rackham\n       [2] snowy\n    Select a cluster [1-2]: 1\n    &gt;&gt;\n    &gt;&gt; c = parcluster('rackham'); %on Bianca 'bianca Rxxxxx'\n    &gt;&gt; c.AdditionalProperties.AccountName = 'snic2021-X-YYY';\n    &gt;&gt; c.AdditionalProperties.QueueName = 'node';\n    &gt;&gt; c.AdditionalProperties.WallTime = '00:10:00';\n    &gt;&gt; c.saveProfile\n    &gt;&gt; job = c.batch(@parallel_example, 1, {90, 5}, 'pool', 19) %19 is for 20 cores. On Snowy and Bianca use 15.\n    &gt;&gt; job.wait\n    &gt;&gt; job.fetchOutputs{:}\n</code></pre> <p>Follow them. These inform you what is needed in your script or in command line to run in parallel on the cluster. The line <code>c.batch(@parallel_example, 1, {90, 5}, 'pool', 19)</code> can be understood as put the function <code>parallel_example</code> to the batch queue. The arguments to batch are:</p> <pre><code>    c.batch(function name, number of output arguments, {the inputs to the function}, 'pool', no of additional workers to the master)\n\n    c.batch(@parallel_example, 1 (t=toc(t0)), {nLoopIters=90, sleepTime=5}, 'pool', 19)\n</code></pre> <p>To see the output to screen from jobs, use job.Tasks.Diary. Output from the submitted function is fetched with 'fetchOutputs()'.</p> <p>For jobs using several nodes (in this case 2) you may modify the call to:</p> <pre><code>    &gt;&gt; configCluster\n       [1] rackham\n       [2] snowy\n    Select a cluster [1-2]: 1\n    &gt;&gt;\n    &gt;&gt; c = parcluster('rackham'); %on Bianca 'bianca R&lt;version&gt;'\n    &gt;&gt; c.AdditionalProperties.AccountName = 'snic2021-X-YYY';\n    &gt;&gt; c.AdditionalProperties.QueueName = 'node';\n    &gt;&gt; c.AdditionalProperties.WallTime = '00:10:00';\n    &gt;&gt; c.saveProfile\n    &gt;&gt; job = c.batch(@parallel_example_hvy, 1, {1000, 1000000}, 'pool', 39)% 31 on Bianca or Snowy\n    &gt;&gt; job.wait\n    &gt;&gt; job.fetchOutputs{:}\n</code></pre> <p>where parallel_example-hvy.m was the script presented above.</p> <p>For the moment jobs are hard coded to be node jobs. This means that if you request 21 tasks instead (20 + 1) you will get a 2 node job, but only 1 core will be used on the second node. In this case you'd obviously request 40 tasks (39 + 1) instead.</p> <p>For more information about Matlab's Distributed Computing features please see Matlab's HPC Portal.</p>"},{"location":"software/matlab/#gpu","title":"GPU","text":"<p>Running MATLAB with GPU is, as of now, only possible on the Snowy and Bianca clusters. Uppsala University affiliated staff and students with allocation on Snowy can use this resource.</p> <p>Start an interactive session with at least 2 cores (otherwise MATLAB may not start). On Snowy, getting (for instance) 2 cpu:s (-n 2) and 1 gpu:</p> <pre><code>interactive -A &lt;proj&gt; -n 2 -M snowy --gres=gpu:1  -t 3:00:00\n</code></pre> <p>On Bianca, getting 3 cpu:s and 1 gpu:</p> <pre><code>interactive -A &lt;proj&gt; -n 3 -C gpu --gres=gpu:1 -t 01:10:00\n</code></pre> <p>Note that wall time <code>-t</code> should be set to more than one hour to not automatically put job in <code>devel</code> or <code>devcore</code> queue, which is not allowed for gpu jobs. Also check the GPU quide for Snowy at Using the GPU nodes on Snowy.</p> <p>Load MATLAB module and start matlab as usual (with &amp;) in the new session. Then test if the gpu device is found by typing:</p> <pre><code>&gt;&gt; gpuDevice\n&gt;&gt; gpuDeviceCount\n</code></pre> <p>On Bianca you may get an error. Follow the instructons and you can run anyway. Example code:</p> <pre><code>&gt;&gt; A = gpuArray([1 0 1; -1 -2 0; 0 1 -1]);\n&gt;&gt; e = eig(A);\n</code></pre> <p>For more information about GPU computing confer the MathWorks web about GPU computing.</p>"},{"location":"software/matlab/#deep-learning-with-gpus","title":"Deep Learning with GPUs","text":"<p>For many functions in Deep Learning Toolbox, GPU support is automatic if you have a suitable GPU and Parallel Computing Toolbox\u2122. You do not need to convert your data to gpuArray. The following is a non-exhaustive list of functions that, by default, run on the GPU if available.</p> <ul> <li> <p>trainNetwork (Deep Learning Toolbox)</p> </li> <li> <p>predict (Deep Learning Toolbox)</p> </li> <li> <p>predictAndUpdateState (Deep Learning Toolbox)</p> </li> <li> <p>classify (Deep Learning Toolbox)</p> </li> <li> <p>classifyAndUpdateState (Deep Learning Toolbox)</p> </li> <li> <p>activations (Deep Learning Toolbox)</p> </li> </ul>"},{"location":"software/matlab/#shell-batch-jobs","title":"Shell batch jobs","text":"<p>Sometimes when matlab scripts are part of workflows/pipelines it may be easier to work directly with the batch scripts.</p> <p>Batch script example with 2 nodes (Rackham), matlab_submit.sh.</p> <pre><code>#!/bin/bash -l\n#SBATCH -A &lt;proj&gt;\n#SBATCH -p devel\n#SBATCH -N 2\n#SBATCH -n 40\nmodule load matlab/R2020b &amp;&gt; /dev/null\nsrun -N 2 -n 40  matlab -batch \"run('&lt;path/to/m-script&gt;')\"\n</code></pre> <p>Run with</p> <pre><code>sbatch matlab_submit.sh\n</code></pre>"},{"location":"software/matlab/#to-learn-more-about-the-matlab-parallel-computing-toolbox","title":"To learn more about the MATLAB Parallel Computing Toolbox","text":"<p>Check out these resources:</p> <ul> <li>Parallel Computing Coding Examples</li> <li>Parallel Computing Documentation</li> <li>Parallel Computing Overview</li> <li>Parallel Computing Tutorials</li> <li>Parallel Computing Videos</li> <li>Parallel Computing Webinars</li> </ul>"},{"location":"software/matlab/#common-problems","title":"Common problems","text":"<p>Sometimes things do not work out.</p> <p>As a first step, try with removing local files:</p> <pre><code>rm -rf ~/.matlab\n</code></pre> <p>If the graphics is slow, try:</p> <pre><code>vglrun matlab -nosoftwareopengl\n</code></pre> <p>Unfortunately this only works from login nodes.</p> <p>You may want to run MATLAB on a single thread. This makes it work:</p> <pre><code>matlab -singleCompThread\n</code></pre>"},{"location":"software/matlab/#matlab-add-ons","title":"Matlab Add-Ons","text":"<p>Matlab Add-ons</p>"},{"location":"software/matlab/#matlab-client-on-the-desktop","title":"MATLAB client on the desktop","text":"<p>Guideline here</p>"},{"location":"software/matlab_addons/","title":"Matlab Add-Ons","text":""},{"location":"software/matlab_addons/#matlab-add-ons","title":"Matlab Add-Ons","text":"<p>MATLAB Add-Ons</p> <ul> <li>Add-ons extend the capabilities of MATLAB\u00ae by providing additional functionality for specific tasks and applications, such as:<ul> <li>connecting to hardware devices</li> <li>additional algorithms</li> <li>interactive apps</li> </ul> </li> <li>Available from:<ul> <li>MathWorks\u00ae</li> <li>the global MATLAB user community</li> </ul> </li> <li>Encompass a wide variety of resources<ul> <li>products</li> <li>apps</li> <li>toolboxes</li> <li>support packages</li> </ul> </li> <li>More information from Mathworks</li> </ul> <p>Learners should be able to</p> <ul> <li>navigate to toolboxes and Add-Ons</li> <li>view Add-Ons and toolboxes</li> <li>install and use Add-Ons</li> </ul> <ul> <li>Before going into installing Add-Ons let's have a background to the MATLAB environments and ecosystem!</li> </ul>"},{"location":"software/matlab_addons/#matlab-add-ons-manager","title":"MATLAB Add-Ons manager","text":"<ul> <li> <p>In the GUI, the Add-Ons manager can be selected from the menu at the top. The drop-down menu options allow users to:</p> <ul> <li>Browse a library of Add-Ons to download. Note that some Add-Ons require a separate license.</li> </ul> <p></p> <ul> <li>Manage Add-Ons already downloaded.</li> </ul> <p></p> <ul> <li> <p>Package user-generated code as a Toolbox or App</p> </li> <li> <p>Get hardware-related support packages</p> </li> </ul> </li> <li> <p>Here we will only focus on the first two options.</p> </li> </ul> <p>Note</p> <p>Note that very many packages are already included in the  Academic installation and license</p> <p></p> <p>Some toolboxes</p> <ul> <li>Matlab products<ul> <li>Parallel Computing Toolbox</li> <li>MATLAB Parallel Server</li> <li>Deep Learning Toolbox</li> <li>Statistics and Machine Learning Toolbox</li> </ul> </li> <li>Simulink<ul> <li>Stateflow</li> <li>SimEvents</li> <li>Simscape</li> </ul> </li> </ul> <p>Some toolboxes provides GUI for their tools Apps</p> <ul> <li>Matlab products<ul> <li>Deep Network Designer - Design and visualize deep learning networks Deep Network Designer</li> <li>Curve Fitter - Fit curves and surfaces to data</li> <li>Deep Learning Toolbox</li> <li>Statistics and Machine Learning Toolbox</li> </ul> </li> <li>Simulink<ul> <li>Stateflow</li> <li>SimEvents</li> <li>Simscape</li> </ul> </li> </ul> <p></p> <ul> <li>We won't cover the usage of the toolboxes here!</li> </ul>"},{"location":"software/matlab_addons/#install-add-ons","title":"Install Add-Ons","text":"<ul> <li>Search in add-ons explorer and install.</li> </ul> <ul> <li> <p>Ends up in local folder and is in the part so it should be reached wherever you are in the file tree.</p> </li> <li> <p><code>~/MATLAB Add-Ons</code></p> </li> <li> <p>It's in the path so it should be possible to run directly if you don't need to run a installation file.</p> </li> <li> <p>For more information about a specific support package install location, see the documentation for the package.</p> </li> </ul> <p>Warning</p> <p>To be able to install you need to use the email for a personal mathworks account.</p> <p>Seealso</p> <p>You can install some Add-Ons manually using an installation file. This is useful in several situations:</p> <ul> <li>The add-on is not available for installation through the Add-On Explorer, for example, if you create a custom add-on yourself or receive one from someone else.</li> <li>You downloaded the add-on from the Add-On Explorer without installing it.</li> <li>You downloaded the add-on from the File Exchange at MATLAB Central\u2122.</li> <li>MathWorkds page on getting Add-Ons</li> </ul> <p>Demo</p> <ul> <li>Search for <code>kalmanf</code></li> <li>Click \"Learning the Kalman Filter\"</li> <li>Look at the documentation</li> <li>Test if the command works today:</li> </ul> <pre><code>  &gt;&gt; kalmanf\n  Unrecognized function or variable 'kalmanf'.\n</code></pre> <ul> <li>OK, it is not there</li> <li>Click \"Add\", and \"Download and Add to path\"</li> <li>Type email address connected to your MathWorks account</li> <li>Installation starts</li> <li>It will end up in:</li> </ul> <pre><code>  $ tree MATLAB\\ Add-Ons/\n  MATLAB\\ Add-Ons/\n  \u2514\u2500\u2500 Collections\n  |   \u2514\u2500\u2500 Efficient\\ GRIB1\\ data\\ reader\n  |       \u251c\u2500\u2500 core.28328\n  |       \u251c\u2500\u2500 license.txt\n  |       \u251c\u2500\u2500 readGRIB1.c\n  |       \u251c\u2500\u2500 readGRIB1.mexa64\n  |       \u2514\u2500\u2500 resources\n  |           \u251c\u2500\u2500 addons_core.xml\n  |           \u251c\u2500\u2500 matlab_path_entries.xml\n  |           \u251c\u2500\u2500 metadata.xml\n  |           \u251c\u2500\u2500 previewImage.png\n  |           \u251c\u2500\u2500 readGRIB1.zip\n  |           \u2514\u2500\u2500 screenshot.png\n  \u2514\u2500\u2500 Functions\n      \u2514\u2500\u2500 Learning\\ the\\ Kalman\\ Filter\n          \u251c\u2500\u2500 kalmanf.m\n          \u2514\u2500\u2500 resources\n              \u251c\u2500\u2500 addons_core.xml\n              \u251c\u2500\u2500 kalmanf.zip\n              \u251c\u2500\u2500 matlab_path_entries.xml\n              \u251c\u2500\u2500 metadata.xml\n              \u251c\u2500\u2500 previewImage.png\n              \u2514\u2500\u2500 screenshot.png\n</code></pre> <ul> <li>Evidently it is a <code>function</code>. Note that I already have something classified as <code>collections</code></li> <li>Now test:</li> </ul> <pre><code>  &gt;&gt; kalmanf()\n  'kalmanf' requires Learning the Kalman Filter version 1.0.0.0 to be enabled.\n</code></pre> <ul> <li>OK. It is installed but may need some other things. Just an example!!</li> </ul> <p>Keypoints</p> <ul> <li>Many Add-Ons, like toolboxes and packages are available at the Clusters</li> <li> <p>You can view Add-Ons and toolboxes</p> <ul> <li>It is all more or less graphical</li> </ul> </li> <li> <p>To install Add-Ons</p> <ul> <li>Search in Add-Ons explorer and install.</li> <li>Ends up in local folder and is in the path so it should be reached wherever you are in the file tree.</li> </ul> </li> </ul>"},{"location":"software/matlab_conf/","title":"Matlab configure for the cluster","text":""},{"location":"software/matlab_conf/#matlab-configure-for-the-cluster","title":"Matlab configure for the cluster","text":""},{"location":"software/matlab_conf/#first-time-since-may-13-2024","title":"First time, since May 13 2024","text":"<ul> <li> <p>If you use MATLAB after May 13 2024, of any version, you have to do the following step to be able to use the full features of running parallel jobs.</p> <ul> <li>Only needs to be run once.</li> <li>Note, however, that on Bianca this has to be done separately.</li> </ul> </li> <li> <p>After logging into the cluster, configure MATLAB to run parallel jobs on the cluster by calling the shell script configCluster.sh.</p> </li> </ul> <pre><code>module load matlab/&lt;version&gt;\nconfigCluster.sh &lt;project-ID&gt;    # Note: no '-A'\n</code></pre> <ul> <li>This will run a short configuration job in an interactive session.</li> <li>Jobs will now default to the cluster rather than submit to the local machine.</li> <li>It should look like this (example for Bianca)</li> </ul> <p></p> <ul> <li>The session should exit automatically but if not you can end the session by<ul> <li><code>exit</code></li> <li>or <code>&lt;CTRL-C&gt;</code></li> </ul> </li> <li>When done, start Matlab as you usually do with  <code>matlab &amp;</code>.</li> </ul> <p>Warning</p> <ul> <li>On Bianca you need to do this for each sens project that will use MATLAB, as well.</li> </ul>"},{"location":"software/matlab_local/","title":"MATLAB client on the desktop","text":""},{"location":"software/matlab_local/#matlab-client-on-the-desktop","title":"MATLAB client on the desktop","text":"<p>Use own computer's matlab</p> <ul> <li>Would you like to try run batch jobs on the Rackham or Snowy cluster but use the faster graphics that you can achieve on your own computer?</li> <li>Do you have all your work locally but sometimes need the cluster to do parallel runs?</li> <li>UPPMAX offers this now.</li> </ul> <p>Warning</p> <ul> <li> <p>This solution is possible only if:</p> <ul> <li>you have an UPPMAX compute project</li> <li> <p>a working matlab on your computer with one of the version available on the cluster:</p> </li> <li> <p>check with <code>module avail matlab</code></p> </li> <li> <p>Examples of the newest ones:</p> <ul> <li>R2020b</li> <li>R2022a</li> <li>R2022b</li> <li>R2023a</li> <li>R2023b</li> </ul> </li> </ul> </li> </ul>"},{"location":"software/matlab_local/#lets-get-started","title":"Let's get started","text":"<p>The Rackham MATLAB support package can be found at uppsala.Desktop.zip.</p> <ul> <li>Download the ZIP file and start MATLAB locally.</li> <li>The ZIP file should be unzipped in the location returned by calling.</li> </ul> <pre><code>&gt;&gt; userpath\n</code></pre> <ul> <li>You can unzip from MATLAB's Command window.</li> <li>Configure MATLAB to run parallel jobs on the cluster by calling <code>configCluster</code>. <code>configCluster</code> only needs to be called once per version of MATLAB.</li> </ul> <pre><code>&gt;&gt; configCluster\nUsername on RACKHAM (e.g. jdoe):\n</code></pre> <ul> <li>Type your rackham user name.</li> <li>As a result:</li> </ul> <pre><code>Complete.  Default cluster profile set to \"Rackham R2022b\".\n</code></pre> <p>Note</p> <ul> <li>To submit jobs to the local machine instead of the cluster, run the following:</li> </ul> <pre><code>&gt;&gt; % Get a handle to the local resources\n&gt;&gt; c = parcluster('local');\n</code></pre>"},{"location":"software/matlab_local/#configuring-slurm-details","title":"Configuring Slurm details","text":"<p>Prior to submitting the job, various parameters can be assigned, such as queue, e-mail, walltime, etc.  The following is a partial list of parameters.  See AdditionalProperties for the complete list.  Only AccountName, Partition, MemUsage and WallTime.</p> <pre><code>&gt;&gt; % Get a handle to the cluster\n&gt;&gt; c = parcluster;\n\nc =\n\n  Generic Cluster\n\n    Properties:\n\n                      Profile: Rackham R2022b\n                     Modified: false\n                         Host: UUC-4GM8L33.user.uu.se\n                   NumWorkers: 100000\n                   NumThreads: 1\n\n        JobStorageLocation: &lt;path to job outputs locally&gt;\n         ClusterMatlabRoot: /sw/apps/matlab/x86_64/R2022b\n           OperatingSystem: unix\n</code></pre> <ul> <li>Set some additional parameters related to Slurm on Rackham</li> </ul> <pre><code>&gt;&gt; % Specify the account\n&gt;&gt; c.AdditionalProperties.AccountName = 'naiss2024-22-1202';\n\n&gt;&gt; % Specify the wall time (e.g., 1 day, 5 hours, 30 minutes\n&gt;&gt; c.AdditionalProperties.WallTime = '00:30:00';\n\n&gt;&gt; % Specify cores per node\n&gt;&gt; c.AdditionalProperties.ProcsPerNode = 20;\n\n[OPTIONAL]\n\n&gt;&gt; % Specify the partition\n&gt;&gt; c.AdditionalProperties.Partition = 'devcore';\n\n&gt;&gt; % Specify another cluster: 'snowy'\n&gt;&gt; c.AdditionalProperties.ClusterName='snowy'\n&gt;&gt; c.AdditionalProperties.ProcsPerNode = 16;\n\n&gt;&gt; % Specify number of GPUs\n&gt;&gt; c.AdditionalProperties.GPUsPerNode = 1;\n&gt;&gt; c.AdditionalProperties.GPUCard = 'gpu-card';\n</code></pre> <ul> <li>Save the profile</li> </ul> <pre><code>&gt;&gt; c.saveProfile\n</code></pre> <p>To see the values of the current configuration options, display AdditionalProperties.</p> <pre><code>&gt;&gt; % To view current properties\n&gt;&gt; c.AdditionalProperties\n</code></pre> <p>Unset a value when no longer needed.</p> <pre><code>&gt;&gt; % Example Turn off email notifications\n&gt;&gt; c.AdditionalProperties.EmailAddress = '';\n&gt;&gt; c.saveProfile\n</code></pre>"},{"location":"software/matlab_local/#start-job","title":"Start job","text":"<ul> <li> <p>Copy this script and paste in a new file <code>parallel_example_local.m</code> that you save in the working directory where you are (check with <code>pwd</code> in the Matlab Command Window).</p> <ul> <li>The script is supposed to loop over <code>sleepTime</code> seconds of work <code>nLoopIters</code> times.</li> <li>We will define the number of processes in the batch submit line.</li> </ul> </li> </ul> <pre><code>   function t = parallel_example_local(nLoopIters, sleepTime)\n   t0 = tic;\n   parfor idx = 1:nLoopIters\n      A(idx) = idx;\n      pause(sleepTime);\n   end\n   t = toc(t0);\n</code></pre> <pre><code>&gt;&gt; job = c.batch(@parallel_example_local, 1, {16,1}, 'Pool',8,'CurrentFolder','.');\n\n- Submission to the cluster requires SSH credentials.\n- You will be prompted for username and password or identity file (private key).\n    - It will not ask again until you define a new cluster handle ``c`` or in next session.\n</code></pre> <p></p> <p></p> <ul> <li>Jobs will now default to the cluster rather than submit to the local machine.</li> </ul> <pre><code>&gt;&gt; job.State\n\nans =\n\n    'running'\n</code></pre> <ul> <li>You can run this several times until it gives:</li> </ul> <pre><code>&gt;&gt; job.State\n\nans =\n\n    'finished'\n</code></pre> <ul> <li>You can also watch queue</li> </ul> <p></p> <ul> <li>Or on Rackham (it really runs there!):</li> </ul> <pre><code>[bjornc2@rackham2 ~]$ squeue -u bjornc2\n        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n        50827312   devcore MATLAB_R  bjornc2  R       2:20      1 r483\n</code></pre> <pre><code>&gt;&gt; job.fetchOutputs{:}\n\nans =\n\n    2.4853\n</code></pre> <ul> <li>The script looped over 1 s work 16 times, but with 8 processes.</li> <li>In an ideal world it would have taken <code>16 / 8 = 2 s</code>. Now it took 2.5 s with some \"overhead\"</li> </ul> <p>Run on Snowy</p> <pre><code>&gt;&gt; c.AdditionalProperties.ClusterName='snowy'\n&gt;&gt; c.AdditionalProperties.ProcsPerNode = 16;\n</code></pre>"},{"location":"software/matlab_local/#helper-functions","title":"Helper functions","text":"Function Description Applies Only to Desktop clusterFeatures List of cluster features/constraint - clusterGpuCards List of cluster GPU cards - clusterPartitionNames List of cluster partition - disableArchiving Modify file archiving to resolve file mirroring issue true fixConnection Reestablish cluster connection (e.g., after reconnection of VPN) true willRun Explain why job is queued -"},{"location":"software/matlab_local/#debugging","title":"Debugging","text":"<p>If a serial job produces an error, call the getDebugLog method to view the error log file.  When submitting an independent job, specify the task.</p> <pre><code>&gt;&gt; c.getDebugLog(job.Tasks)\n</code></pre> <p>For Pool jobs, only specify the job object.</p> <pre><code>&gt;&gt; c.getDebugLog(job)\n</code></pre> <p>When troubleshooting a job, the cluster admin may request the scheduler ID of the job.  This can be derived by calling getTaskSchedulerIDs (call schedID(job) before R2019b).</p> <pre><code>&gt;&gt; job.getTaskSchedulerIDs()\n   ans =\n    25539\n</code></pre> <p>Keypoints</p> <ul> <li>Steps to configure<ul> <li>First time download and decompress UPPMAX configure file.</li> <li>run configCluster on local MATLAB and set user name</li> </ul> </li> <li>Steps to run<ul> <li>set <code>parcluster</code> settings, like you do otherwise.</li> </ul> </li> <li>Note: only <code>parcluster</code> will work, not <code>parpool</code>.</li> </ul>"},{"location":"software/metontiime/","title":"MetONTIIME","text":""},{"location":"software/metontiime/#metontiime","title":"MetONTIIME","text":"<p>MetONTIIME is a Nextflow pipeline that is not part of <code>nf-core</code>.</p> <p>It is not installed as a module.</p> User tickets (for UPPMAX staff) <p>ticket_287014</p>"},{"location":"software/metontiime/#links","title":"Links","text":"<ul> <li>MetONTIIME GitHub repository</li> </ul>"},{"location":"software/mobaxterm/","title":"MobaXterm","text":""},{"location":"software/mobaxterm/#mobaxterm","title":"MobaXterm","text":"<p>There are multiple SSH clients. This page describes the MobaXterm SSH clients.</p> <p>MobaXterm is an SSH client that is easy to use and install for Windows. When MobaXterm is started, start a terminal to run <code>ssh</code>. The usage of <code>ssh</code> is described at the UPPMAX page on <code>ssh</code>.</p> <p>In MobaXterm you can use the internal <code>MobAgent</code> or/and the <code>Peagent</code> from the <code>PuTTy</code> tools.</p> <p></p> <ul> <li>MobaXterm homepage</li> </ul>"},{"location":"software/multiqc/","title":"MultiQC","text":""},{"location":"software/multiqc/#multiqc","title":"MultiQC","text":"<p>MultiQC is a tool with homepage https://github.com/ewels/MultiQC.</p> <p>MultiQC can be found among the UPPMAX modules.</p> <pre><code>module spider MultiQC\n</code></pre> How does that look like? <p>You output will look similar to this:</p> <pre><code>[sven@rackham2 ~]$ module spider MultiQC\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  MultiQC:\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        MultiQC/0.6\n        MultiQC/0.7\n        MultiQC/0.8\n        MultiQC/0.9\n        MultiQC/1.0\n        MultiQC/1.2\n        MultiQC/1.3\n        MultiQC/1.5\n        MultiQC/1.6\n        MultiQC/1.7\n        MultiQC/1.8\n        MultiQC/1.9\n        MultiQC/1.10\n        MultiQC/1.10.1\n        MultiQC/1.11\n        MultiQC/1.12\n        MultiQC/1.22.2\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"MultiQC\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider MultiQC/1.22.2\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>To find out how to load a specific version:</p> <pre><code>module spider MultiQC/1.22.2\n</code></pre> How does that look like? <p>Output will look similar to:</p> <pre><code>[sven@rackham2 ~]$ module spider MultiQC/1.22.2\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  MultiQC: MultiQC/1.22.2\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"MultiQC/1.22.2\" module is available to load.\n\n      bioinfo-tools\n\n    Help:\n       MultiQC - use MultiQC 1.22.2\n\n       Version 1.22.2\n\n\n      Version 1.22.2 is installed using python/3.8.7\n</code></pre> <p>After reading that documentation, we know how to load it:</p> <pre><code>module load bioinfo-tools\nmodule load MultiQC/1.22.2\n</code></pre> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham2 ~]$ module load bioinfo-tools\n[sven@rackham2 ~]$ module load MultiQC/1.22.2\n[sven@rackham2 ~]$\n</code></pre>"},{"location":"software/multiqc/#singularity-script","title":"Singularity script","text":"<p>If you want to put MultiQC in a Singularity container, here is an example script:</p> <pre><code>BootStrap: library\nFrom: ubuntu:18.04\n\n%runscript\n  multiqc \"$@\"\n\n%post\n  echo \"Hello from inside the container\"\n  apt-get update\n  apt-get -y dist-upgrade\n  apt-get clean\n  apt-get -y install python-pip\n  pip install multiqc\n</code></pre> <p>See the documentation on Singularity how to do so.</p>"},{"location":"software/nano/","title":"nano","text":""},{"location":"software/nano/#nano","title":"<code>nano</code>","text":"<p>Using <code>nano</code> to edit the file <code>my_file.txt</code>.</p> <p>UPPMAX has multiple text editors available. This page describes the GNU nano text editor.</p> Want to see a video? <p>See the YouTube video using <code>nano</code> on Rackham</p> <p>GNU nano is a simple terminal text editor that is easy to learn.</p>"},{"location":"software/nano/#starting-nano","title":"Starting <code>nano</code>","text":"<p>Start <code>nano</code> on a terminal with:</p> <pre><code>nano\n</code></pre> <p>To start <code>nano</code> to edit a file (for example, <code>my_file.txt</code>, use:</p> <pre><code>nano my_file.txt\n</code></pre>"},{"location":"software/nano/#using-nano","title":"Using <code>nano</code>","text":"<p>The keyboard shortcuts are shown on-screen, where <code>^</code> denotes <code>Ctrl</code> and <code>M</code> the meta key.</p> <p>OS specifics:</p> <ul> <li>On Windows, <code>Alt</code> is the meta key</li> <li>On Mac: in the <code>Terminal.app</code>, go to 'Preferences -&gt; Settings -&gt; Keyboard'   and turn on \"Use option as meta key\", after which <code>Alt</code> is the meta key</li> </ul> <p>Common tasks:</p> <ul> <li>Save a file: <code>CTRL + O</code> , then edit the filename and press enter</li> <li>Exit: <code>CTRL + X</code>, press \"y\" or \"n\" on some questions   and/or press Enter to confirm.</li> <li>Help: <code>CTRL + G</code></li> </ul> <p>More tips can be found at the <code>nano</code> cheat sheet.</p>"},{"location":"software/nextflow/","title":"Nextflow & nf-core","text":""},{"location":"software/nextflow/#nextflow-nf-core-on-uppmax","title":"<code>nextflow</code> &amp; <code>nf-core</code> on UPPMAX","text":"<p>https://www.nextflow.io</p> <ul> <li>Official documentation: https://www.nextflow.io/docs/latest/index.html</li> </ul>"},{"location":"software/nextflow/#nextflow-from-the-module-system","title":"<code>nextflow</code> from the module system","text":"<ul> <li>latest <code>nextflow</code></li> </ul> <pre><code>module load bioinfo-tools\nmodule load Nextflow/latest  # this also loads java as reqirement\n\nnextflow -v\nnextflow version 24.04.4.5917\n</code></pre> <ul> <li>alternative versions</li> </ul> <pre><code>export NXF_VER=23.10.1\n\nnextflow -v\nnextflow version 23.10.1.5891\n</code></pre> <pre><code># To check the available versions on Rackham and Bianca\nls /sw/bioinfo/Nextflow/latest/rackham/nxf_home/framework/\n20.04.1  20.10.0  21.10.6  22.10.1  22.10.3  22.10.8  23.04.2  23.04.4  23.10.1  24.04.2  24.04.4\n20.07.1  21.04.3  22.10.0  22.10.2  22.10.4  23.04.1  23.04.3  23.10.0  24.04.1  24.04.3\n</code></pre>"},{"location":"software/nextflow/#nf-core-from-the-module-system","title":"<code>nf-core</code> from the module system","text":"<p>https://nf-co.re</p> <p><code>nf-core</code> and and all other required modules are available on the <code>transit</code> server as well.</p> <pre><code>module load bioinfo-tools\nmodule load nf-core   # this also load the nextflow and java as requirements\n</code></pre>"},{"location":"software/nextflow/#nf-core-pipelines-on-bianca","title":"<code>nf-core</code> pipelines on Bianca","text":"<ol> <li>Login to <code>transit.uppmax.uu.se</code> - documentation</li> <li> <p>Mount the <code>wharf</code> of your project.</p> <pre><code>user@transit:~$ mount_wharf sens2023531\nMounting wharf (accessible for you only) to /home/&lt;user&gt;/sens2023531\n&lt;user&gt;-sens2023531@bianca-sftp.uppmax.uu.se's password:\n</code></pre> </li> <li> <p>Navigate to your <code>wharf</code> folder</p> </li> <li> <p>Disable Singularity cache</p> <pre><code>export SINGULARITY_DISABLE_CACHE=true\nexport APPTAINER_DISABLE_CACHE=true\nunset NXF_SINGULARITY_CACHEDIR\n</code></pre> </li> <li> <p>Load nf-core software module</p> <pre><code>module load uppmax bioinfo-tools nf-core\n</code></pre> </li> <li> <p>Run <code>nf-core</code> to download the pipeline.</p> <pre><code>nf-core download pixelator\n                                      ,--./,-.\n      ___     __   __   __   ___     /,-._.--~\\\n|\\ | |__  __ /  ` /  \\ |__) |__         }  {\n| \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                      `._,._,'\n\nnf-core/tools version 2.11.1 - https://nf-co.re\n\nWARNING  Could not find GitHub authentication token. Some API requests may fail.\n? Select release / branch: 1.0.2  [release]\n? Include the nf-core's default institutional configuration files into the download? Yes\n\nIn addition to the pipeline code, this tool can download software containers.\n? Download software container images: singularity\n\nNextflow and nf-core can use an environment variable called $NXF_SINGULARITY_CACHEDIR that is a path to a directory where remote\nSingularity images are stored. This allows downloaded images to be cached in a central location.\n? Define $NXF_SINGULARITY_CACHEDIR for a shared Singularity image download folder? [y/n]: n\n\nIf transferring the downloaded files to another system, it can be convenient to have everything compressed in a single file.\nThis is not recommended when downloading Singularity images, as it can take a long time and saves very little space.\n? Choose compression type: none\nINFO     Saving 'nf-core/pixelator'\n          Pipeline revision: '1.0.2'\n          Use containers: 'singularity'\n          Container library: 'quay.io'\n          Output directory: 'nf-core-pixelator_1.0.2'\n          Include default institutional configuration: 'True'\nINFO     Downloading centralised configs from GitHub\nINFO     Downloading workflow files from GitHub\nINFO     Processing workflow revision 1.0.2, found 4 container images in     total.\nDownloading singularity images ???????????????????????????????????????????????????????????????????????????????? 100% ? 4/4 completed\n</code></pre> </li> <li> <p>Running on Bianca</p> <pre><code>module load bioinfo-tools Nextflow\nnextflow run ... -profile uppmax --project sens-XXXX-XX ....\n</code></pre> </li> </ol> <p>Note: you might need <code>-c configs/conf/uppmax.config</code>, make sure you have the file (it is an option to download it during the pipeline download process). https://github.com/nf-core/configs/blob/master/conf/uppmax.config https://nf-co.re/configs/uppmax</p>"},{"location":"software/nextflow/#common-problems","title":"Common problems","text":"<ul> <li>Task is running out of resources (memory or time)</li> </ul> <p>Add lines to your configuration that overrides the settings for the problematic task, for example:</p> <pre><code>process {\n    withName: 'NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:STAR_ALIGN' {\n        cpus   = 12\n        memory = '72.GB'\n        time   = '24.h'\n    }\n}\n</code></pre> <p>More: https://www.nextflow.io/docs/latest/config.html#process-selectors</p>"},{"location":"software/nextflow/#troubleshooting-nf-core","title":"Troubleshooting - nf-core","text":""},{"location":"software/nvidia-deep-learning-frameworks/","title":"NVIDIA DLF","text":""},{"location":"software/nvidia-deep-learning-frameworks/#nvidia-deep-learning-frameworks","title":"NVIDIA Deep Learning Frameworks","text":"<p>Here is how easy one can use an NVIDIA environment for deep learning with all the following tools preset. A screenshot of that page is shown below.</p> <p></p> <p>First - pull the container (6.5GB).</p> <pre><code>singularity pull docker://nvcr.io/nvidia/pytorch:22.03-py3\n</code></pre> <p>Get an interactive shell.</p> <pre><code>singularity shell --nv ~/external_1TB/tmp/pytorch_22.03-py3.sif\n\nSingularity&gt; python3\nPython 3.8.12 | packaged by conda-forge | (default, Jan 30 2022, 23:42:07)\n[GCC 9.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n&gt;&gt;&gt; import torch\n# Check torch version\n&gt;&gt;&gt; print(torch.__version__)\n1.12.0a0+2c916ef\n\n# Check if CUDA is available\n&gt;&gt;&gt; print(torch.cuda.is_available())\nTrue\n\n# Check which GPU architectures are supported\n&gt;&gt;&gt; print(torch.cuda.get_arch_list())\n['sm_52', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_86']\n\n# test torch\n&gt;&gt;&gt; torch.zeros(1).to('cuda')\ntensor([0.], device='cuda:0')\n</code></pre> <p>From the container shell, check what else is available...</p> <pre><code>Singularity&gt; nvcc -V\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Thu_Feb_10_18:23:41_PST_2022\nCuda compilation tools, release 11.6, V11.6.112\nBuild cuda_11.6.r11.6/compiler.30978841_0\n\n# Check what conda packages are already there\nSingularity&gt; conda list -v\n\n# Start a jupyter-lab (keep in mind the hostname)\nSingularity&gt; jupyter-lab\n...\n[I 13:35:46.270 LabApp] [jupyter_nbextensions_configurator] enabled 0.4.1\n[I 13:35:46.611 LabApp] jupyter_tensorboard extension loaded.\n[I 13:35:46.615 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.8/site-packages/jupyterlab\n[I 13:35:46.615 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab\n[I 13:35:46.616 LabApp] [Jupytext Server Extension] NotebookApp.contents_manager_class is (a subclass of) jupytext.TextFileContentsManager already - OK\n[I 13:35:46.616 LabApp] Serving notebooks from local directory: /home/pmitev\n[I 13:35:46.616 LabApp] Jupyter Notebook 6.4.8 is running at:\n[I 13:35:46.616 LabApp] http://hostname:8888/?token=d6e865a937e527ff5bbccfb3f150480b76566f47eb3808b1\n[I 13:35:46.616 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n...\n</code></pre> <p>You can use this container to add more packages.</p> <pre><code>Bootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:22.03-py3\n...\n</code></pre> <p>Just keep in mind that \"upgrading\" the build-in torch package might install a package that is compatible with less GPU architectures and it might not work anymore on your hardware.</p> <pre><code>Singularity&gt; python3 -c \"import torch; print(torch.__version__); print(torch.cuda.is_available()); print(torch.cuda.get_arch_list()); torch.zeros(1).to('cuda')\"\n\n1.10.0+cu102\nTrue\n['sm_37', 'sm_50', 'sm_60', 'sm_70']\nNVIDIA A100-PCIE-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\nThe current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n</code></pre>"},{"location":"software/openmolcas/","title":"MOLCAS user guide","text":""},{"location":"software/openmolcas/#molcas-user-guide","title":"MOLCAS user guide","text":"<p>How to run the program MOLCAS on UPPMAX</p>"},{"location":"software/openmolcas/#information","title":"Information","text":"<p>MOLCAS is an ab initio computational chemistry program. Focus in the program is placed on methods for calculating general electronic structures in molecular systems in both ground and excited states. MOLCAS is, in particular, designed to study the potential surfaces of excited states</p> <p>This guide will help you get started running MOLCAS on UPPMAX. More detailed information on how to use Molcas can be found on the official website.</p>"},{"location":"software/openmolcas/#licensing","title":"Licensing","text":"<p>A valid license key is required to run Molcas on UPPMAX. The licence key should be kept in a directory named .Molcas under the home directory.</p> <p>Molcas is currently free of charge for academic researchers active in the Nordic countries. You can get hold of a license by following these instructions.</p>"},{"location":"software/openmolcas/#versions-installed-at-uppmax","title":"Versions installed at UPPMAX","text":"<p>At UPPMAX the following versions are installed:</p> <ul> <li>8.0 (serial)</li> <li>7.8 (serial)</li> </ul>"},{"location":"software/openmolcas/#modules-needed-to-run-molcas","title":"Modules needed to run MOLCAS","text":"<p>In order to run MOLCAS you must first load the <code>molcas</code> module. You can see all available versions of MOLCAS installed at UPPMAX with:</p> <pre><code>module avail molcas\n</code></pre> <p>Load a MOLCAS module with, eg:</p> <pre><code>module load molcas/7.8.082\n</code></pre>"},{"location":"software/openmolcas/#how-to-run-molcas-interactively","title":"How to run MOLCAS interactively","text":"<p>If you would like to do tests or short runs, we recommend using the interactive command:</p> <pre><code>interactive -A your_project_name\n</code></pre> <p>This will reserve a node for you to do your test on. Note that you must provide the name of an active project in order to run on UPPMAX resources. After a short wait you will get access to the node. Then you can run MOLCAS by:</p> <pre><code>module load molcas/7.8.082\nmolcas -f test000.input\n</code></pre> <p>The <code>test000.input</code> looks like:</p> <pre><code>*$Revision: 7.7 $\n************************************************************************\n* Molecule: H2\n* Basis: DZ\n* Symmetry: x y z\n* SCF: conventional\n*\n*  This is a test to be run during first run to verify\n*  that seward and scf works at all\n*\n\n&gt;export MOLCAS_PRINT=VERBOSE\n &amp;GATEWAY\ncoord\n2\nangstrom\nH  0.350000000  0.000000000  0.000000000\nH -0.350000000  0.000000000  0.000000000\nbasis\nH.DZ....\n\n &amp;SEWARD\n\n &amp;SCF\nTitle\n H2, DZ Basis set\n\n &amp;RASSCF\nTitle\n H2, DZ Basis set\nnActEl\n 2  0 0\nRas2\n 1 1 0 0 0 0 0 0\n\n &amp;ALASKA\n\n &amp;SLAPAF\n\n &amp;CASPT2\n</code></pre> <p>See the Slurm user guide for more information on the interactive command. Don't forget to exit your interactive job when you have finished your calculation. Exiting will free the resource for others to use.</p>"},{"location":"software/openmolcas/#batch-scripts-for-slurm","title":"Batch scripts for Slurm","text":"<p>It's possible to run MOLCAS in the batch queue. Here is an example running MOLCAS on one core:</p> <pre><code>#!/bin/bash -l\n#\n#SBATCH -A &lt;em&gt;your_project_name&lt;/em&gt;\n#SBATCH -J molcastest\n#SBATCH -t 00:10:00\n#SBATCH -p core -n 1\n\nmodule load molcas/7.8.082\n\n#In order to let MOLCAS use more memory\nexport MOLCASMEM=2000\n\nmolcas -f test000.input\n</code></pre> <p>Again you'll have to provide your project name.</p> <p>If the script is called <code>test000.job</code> you can submit it to the batch queue with:</p> <pre><code>sbatch test000.job\n</code></pre>"},{"location":"software/orthofinder/","title":"OrthoFinder","text":""},{"location":"software/orthofinder/#orthofinder","title":"OrthoFinder","text":"<p>'OrthoFinder is a software program for phylogenetic orthology inference' (from OrthoFinder tutorials).</p> <p>It is not installed via the module system.</p> For UPPMAX staff <p>Tickets:</p> <ul> <li><code>https://support.naiss.se/Ticket/Display.html?id=293272</code></li> </ul>"},{"location":"software/orthofinder/#links","title":"Links","text":"<ul> <li>OrthoFinder GitHub repository</li> <li>OrthoFinder tutorials</li> </ul>"},{"location":"software/overview/","title":"Overview","text":""},{"location":"software/overview/#software","title":"Software","text":"<p>At the UPPMAX clusters, a lot of software is pre-installed and accessible via the module system.</p> What are the UPPMAX clusters? <p>See the UPPMAX documentation on its clusters</p> What is the module system? <p>See the UPPMAX documentation on modules</p>"},{"location":"software/overview/#software-table","title":"Software table","text":"<p>Automatically updated software table</p>"},{"location":"software/overview/#conflicting-modules","title":"Conflicting modules","text":"<ul> <li>Conflicting modules</li> </ul>"},{"location":"software/overview/#reach-the-bioinformatics-tools","title":"Reach the Bioinformatics tools","text":"<ul> <li>Before you can list available bioinformatics tools you need to issue the command:</li> </ul> <pre><code>module load bioinfo-tools\n</code></pre> <ul> <li> <p>When you list available modules with <code>module avail</code> after this, you will see that the bioinformatics tools are now also available in the listing.</p> </li> <li> <p>Note that the <code>module spider</code> command will show bioinformatics modules regardless of whether you have loaded the bioinfo-tools module.</p> </li> <li>This command can also tell you whether a particular module requires the bioinfo-tools module, e.g. \"module spider GEMINI/0.18.3\".</li> </ul>"},{"location":"software/overview/#how-can-i-request-new-software-to-be-installed","title":"How can I request new software to be installed?","text":"<p>You can always install software in your home on any UPPMAX system. If there are many users who would like to request the same software, it can be installed by UPPMAX application or system experts.</p> <p>Please send such requests to <code>support@uppmax.uu.se</code>.</p>"},{"location":"software/overview/#installing-yourself","title":"Installing yourself","text":"<p>Go to our installation page</p>"},{"location":"software/parallel_comb/","title":"Combinations of parallel libraries and compilers","text":""},{"location":"software/parallel_comb/#combinations-of-parallel-libraries-and-compilers","title":"Combinations of parallel libraries and compilers","text":"<p>Before compiling a program for MPI we must choose, in addition to the compiler, which version of MPI we want to use. At UPPMAX there are two, openmpi and intelmpi. These, with their versions, are compatible only to a subset of the gcc and intel compiler versions. The lists below summarise the best choices.</p>"},{"location":"software/parallel_comb/#suggestions-for-compatibility-rackham-snowy-bianca","title":"Suggestions for compatibility Rackham, Snowy, Bianca","text":""},{"location":"software/parallel_comb/#gcc","title":"GCC","text":"<ul> <li>v5: gcc/5.3.0 openmpi/1.10.3</li> <li>v6: gcc/6.3.0 openmpi/2.1.0</li> <li>v7: gcc/7.4.0 openmpi/3.1.3</li> <li>v8: gcc/8.3.0 openmpi/3.1.3</li> <li>v9: gcc/9.3.0 openmpi/3.1.5</li> <li>v10: gcc/10.3.0 openmpi/3.1.6 or openmpi/4.1.0</li> <li>v11: gcc/11.2.0 openmpi/4.1.1 will work also on Miarka</li> <li>v12: gcc/12.2.0 openmpi/4.1.4</li> <li>v13: gcc/13.2.0 openmpi/4.1.5</li> </ul>"},{"location":"software/parallel_comb/#intel","title":"Intel","text":"<ul> <li>v18: intel/18.3 openmpi/3.1.3</li> <li>v20: intel/20.4 openmpi/3.1.6 or openmpi/4.0.4</li> </ul>"},{"location":"software/parallel_comb/#intel-intelmpi","title":"Intel &amp; intelmpi","text":"<ul> <li>Load the corresponding version of intelmpi as of the intel compiler (versions up to 20.4)</li> </ul>"},{"location":"software/parallel_comb/#intel-after-version-204","title":"Intel after version 20.4","text":"<ul> <li>For all versions of intel from 2021 there is not necessarily a mpi library with same version as the compiler.</li> </ul> <pre><code>module load intel-oneapi\n</code></pre> <ul> <li>Check availability and load desired version</li> </ul> <pre><code>module avail mpi  # showing both compilers and mpi ;-)\n</code></pre> <ul> <li>Example:</li> </ul> <pre><code>module load compiler/2023.1.0 mpi/2021.9.0\n</code></pre>"},{"location":"software/parallel_comb/#suggestions-for-compatibility-rackham-and-snowy","title":"Suggestions for compatibility Rackham and Snowy","text":"<ul> <li> <p>GCC</p> <ul> <li>v4: gcc/4.8.2 openmpi/1.7.4</li> <li>v5: gcc/5.3.0 openmpi/1.10.3</li> <li>v6: gcc/6.3.0 openmpi/2.1.0</li> <li>v7: gcc/7.4.0 openmpi/3.1.3</li> <li>v8: gcc/8.3.0 openmpi/3.1.3</li> <li>v9: gcc/9.3.0 openmpi/3.1.3 or openmpi/4.0.3</li> <li>v10: gcc/10.3.0 openmpi/3.1.6*- #or openmpi/4.1.1**</li> <li>v11: gcc/11.3.0 openmpi/4.1.2</li> <li>v12: gcc/12.2.0 openmpi/4.1.4</li> <li>v13: gcc/13.1.0 openmpi/4.1.5</li> </ul> </li> <li> <p>Intel</p> <ul> <li>v18: intel/18.3 openmpi/3.1.3</li> <li>v20: intel/20.4 openmpi/3.1.6*- # or openmpi/4.1.1**</li> </ul> </li> </ul>"},{"location":"software/parallel_comb/#rackham","title":"Rackham","text":"<ul> <li>Also on Snowy in italic</li> <li>Also on Snowy AND Bianca in bold</li> </ul> GCC openmpi 4.8.2 1.7.4 5.2.0 1.8.8 5.3.0 1.10.1 5.5.0 1.10.3 6.3.0 2.0.1, 2.0.2, 2.1.0 6.4.0 2.1.1 7.1.0 2.1.0, 2.1.1 7.2.0 2.1.1, 2.1.2, 3.0.0 7.3.0 2.1.3, 3.0.0, 3.1.0 7.4.0 3.1.3 8.1.0 3.0.1, 3.1.0 8.2.0 3.0.2, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 4.0.0 8.3.0 3.1.3 8.4.0 3.1.5, 4.0.2 9.1.0 3.1.3 9.2.0 3.1.3, 3.1.4, 3.1.5, 4.0.2 9.3.0 3.1.5, 4.0.2, 4.0.3 10.1.0 3.1.6, 4.0.3 10.2.0 3.1.6, 4.0.4, 4.1.0 10.3.0 3.1.6, 4.0.5, 4.1.0, 4.1.1 11.2.0 4.1.1, 4.1.2 11.3.0 4.1.2, 4.1.3 12.1.0 4.1.3 12.2.0 4.1.3, 4.1.4 12.3.0 4.1.5 13.1.0 4.1.5 Intel openmpi 15.3 1.10.0, 1.10.1, 2.1.0 16.1 1.10.1, 1.10.2 17.1 2.0.1, 2.0.2, 17.2 2.0.2, 2.1.0 17.4 2.1.1, 3.0.0 18.0 3.0.0 18.1 2.1.2, 2.1.3, 3.0.0 18.2 2.1.3, 3.0.0, 3.1.0 18.3 3.0.2, 3.1.0, 3.1.1, 3.1.2, 3.1.3 19.4 3.1.4 19.5 3.1.4 20.0 3.1.5, 3.1.6, 4.0.3, 4.0.4 20.2 3.1.6, 4.0.4 20.4 3.1.6, 4.0.4, 4.1.0, 4.1.1 openmpi gcc intel pgi 1.7.4 4.8.2 - - 1.8.8 5.2.0 - - 1.10.0 15.3 - 1.10.1 5.3.0 15.3, 16.1 - 1.10.2 16.1 16.9, 17.4, 17.7, 17.10 1.10.3 5.5.0 - - 2.0.1 6.3.0 17.1 - 2.0.2 6.3.0 17.1, 17.2 - 2.1.0 6.3.0, 7.1.0 15.3, 17.2 - 2.1.1 6.4.0, 7.1.0, 7.2.0 17.4 17.4, 17.7 2.1.2 7.2.0 18.1 17.10, 18.1, 18.3 2.1.3 7.3.0 18.1, 18.2              18.1 3.0.0 7.2.0, 7.3.0 17.4, 18.0, 18.1, 18.2 17.7, 17.10, 18.0 18.1 3.0.1 8.1.0 - - 3.0.2 8.2.0 18.3 - 3.1.0 7.3.0, 8.1.0, 8.2.0 18.2, 18.3 18.3 3.1.1 8.2.0 18.3 - 3.1.2 8.2.0 18.3 18.3 3.1.3 7.4.0, 8.2.0, 8.3.0, 9.1.0, 9.2.0 18.3 18.3 3.1.4 9.2.0 19.4, 19.5 - 3.1.5 8.4.0, 9.2.0, 9.3.0 20.0 - 3.1.6 10.1.0, 10.2.0, 10.3.0 20.0, 20.2, 20.4 - 4.0.0 8.2.0 - 4.0.2 8.4.0, 9.2.0, 9.3.0 - 4.0.3 9.3.0, 10.1.0 20.0 - 4.0.4 10.2 20.0, 20.2, 20.4 - 4.0.5 10.3.0 - - 4.1.0 10.2.0, 10.3.0 20.4 - 4.1.1 10.3.0, 11.2.0 20.4 - 4.1.2 11.2.0 - - 4.1.3 12.1.0, 12.2.0 - - 4.1.4 12.2.0 - - 4.1.5 12.3.0, 13.1.0 - -"},{"location":"software/parallel_comb/#bianca","title":"Bianca","text":"GCC openmpi 5.3.0 1.10.1 5.4.0 2.0.0, 2.0.1 6.1.0 2.0.0, 2.0.1 6.2.0 2.0.1 6.3.0 2.0.1, 2.0.2, 2.1.0 6.4.0 2.1.1 7.1.0 2.1.0, 2.1.1 7.2.0 2.1.1, 3.0.0 7.3.0 3.0.0 8.1.0 3.1.0 8.2.0 3.1.2, 3.1.3 8.3.0 3.1.3 9.3.0 3.1.5 10.1.0 3.1.6 10.2.0 4.1.0 10.3.0 3.1.6, 4.0.5, 4.1.0 11.2.0 4.1.1 Intel openmpi 15.3 1.10.0, 1.10.1 16.1 1.10.1, 1.10.2 16.3 2.0.0, 2.0.1 17.0 2.0.1 17.1 2.0.1, 2.0.2 17.2 2.0.2, 2.1.0 17.4 2.1.1, 3.0.0 18.3 3.1.2, 3.1.3 20.2 3.1.6, 4.0.4 20.4 3.1.6, 4.0.4"},{"location":"software/pelle_file_transfer_using_filezilla/","title":"File transfer to/from Pelle using FileZilla","text":"","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#file-transfer-tofrom-pelle-using-filezilla","title":"File transfer to/from Pelle using FileZilla","text":"<p>There are multiple ways to transfer data to/from Pelle.</p> <p>Here, we show how to transfer files using a graphical tool called FileZilla, which is a secure file transfer tool that works under Linux, Mac and Windows.</p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"Would you like a video? <p>Watch the YouTube video File transfer to/from Pelle using FileZilla</p> <p>To transfer files to/from Pelle using FileZilla, do the following steps:</p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#1-start-filezilla","title":"1. Start FileZilla","text":"<p>Start FileZilla.</p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#2-start-filezillas-site-manager","title":"2. Start FileZilla's site manager","text":"<p>From the menu, select 'File | Site manager'</p> Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#3-add-a-new-site-in-filezillas-site-manager","title":"3. Add a new site in FileZilla's site manager","text":"<p>In FileZilla's site manager, click 'New site'</p> Where is that? <p>It is here:</p> <p></p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#4-setup-the-site","title":"4. Setup the site","text":"<p>In FileZilla's site manager:</p> <ul> <li>create a name for the site, e.g. <code>Pelle</code>.</li> <li>for that site, use all standards, except:<ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>pelle.uppmax.uu.se</code></li> <li>Set user to your UPPMAX username, e.g. <code>sven</code></li> <li>Set password to your UPPMAX password, e.g. <code>VerySecret</code></li> </ul> </li> </ul> How does that look like? <p>It looks similar to this:</p> <p>`</p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#5-connect-to-the-site","title":"5. Connect to the site","text":"<p>Click 'Connect'.</p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_filezilla/#6-ready-to-transfer-files","title":"6. Ready to transfer files","text":"<p>Now you can transfer files between your local computer and Pelle.</p> How does that look like? <p>It looks like this:</p> <p></p>","tags":["FileZilla","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/","title":"Data transfer to/from Pelle using rsync","text":"","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#data-transfer-tofrom-pelle-using-rsync","title":"Data transfer to/from Pelle using rsync","text":"<p>There are multiple ways to transfer files to or from Pelle. Here it is described how to do file transfer to/from Pelle using rsync.</p>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#procedure","title":"Procedure","text":"Prefer a video? <p>Watch the YouTube video Data transfer to/from Pelle using rsync.</p>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#2-transfer-files-to-pelle","title":"2. Transfer files to Pelle","text":"<p>You can transfer files to Pelle by:</p> <ul> <li>2a. Transfer individual files to Pelle</li> <li>2b. Transfer all files in a folder to Pelle</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#2a-transfer-individual-files-to-pelle","title":"2a. Transfer individual files to Pelle","text":"<p>On local computer, do:</p> <pre><code>rsync [my_local_file] [username]@pelle.uppmax.uu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your file to</li> <li><code>[username]</code> is your UPPMAX username</li> </ul> <p>for example:</p> <pre><code>rsync my_local_file.txt sven@pelle.uppmax.uu.se:/home/sven/\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#2b-transfer-all-files-in-a-folder-to-pelle","title":"2b. Transfer all files in a folder to Pelle","text":"<p>On local computer, do:</p> <pre><code>rsync --recursive my_folder [username]@pelle.uppmax.uu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[target_folder]</code> is the target folder</li> <li><code>[username]</code> is your UPPMAX username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive my_folder sven@pelle.uppmax.uu.se:/home/sven/\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p> <p>Note that in <code>rsync</code>, a slash (<code>/</code>) matters:</p> Command Effect <code>rsync --recursive my_folder sven@pelle.uppmax.uu.se:/home/sven</code> Will put the files in <code>my_folder</code> in the Pelle home folder <code>rsync --recursive my_folder sven@pelle.uppmax.uu.se:/home/sven/</code> Will put the folder <code>my_folder</code> in the Pelle home folder","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#3-transfer-files-from-pelle-to-you-local-computer","title":"3. Transfer files from Pelle to you local computer","text":"<p>You can transfer files from Pelle to your local computer by:</p> <ul> <li>3a. Transfer individual files from Pelle to your local computer</li> <li>3b. Transfer all folders from Pelle to you local computer</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#3a-transfer-individual-files-from-pelle-to-your-local-computer","title":"3a. Transfer individual files from Pelle to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync [username]@pelle.uppmax.uu.se:[path_to_file] .\n</code></pre> <p>where</p> <ul> <li><code>[path_to_file]</code> is the path to the file you want to download</li> <li><code>[username]</code> is your UPPMAX username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync sven@pelle.uppmax.uu.se:/home/sven/my_file.txt .\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_rsync/#3b-transfer-all-folders-from-pelle-to-you-local-computer","title":"3b. Transfer all folders from Pelle to you local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync --recursive [username]@pelle.uppmax.uu.se:/[source_folder] .\n</code></pre> <p>where</p> <ul> <li><code>[source_folder]</code> is the path to the folder you want to download</li> <li><code>[username]</code> is your UPPMAX username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive sven@pelle.uppmax.uu.se:/home/sven/my_folder .\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p>","tags":["transfer","data transfer","file transfer","rsync","Pelle"]},{"location":"software/pelle_file_transfer_using_scp/","title":"Data transfer to/from Pelle using SCP","text":"","tags":["transfer","data transfer","file transfer","scp","SCP","Pelle"]},{"location":"software/pelle_file_transfer_using_scp/#data-transfer-tofrom-pelle-using-scp","title":"Data transfer to/from Pelle using SCP","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer files to or from Pelle.</p> <p>This page shows you how to do so using <code>scp</code>.</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Pelle"]},{"location":"software/pelle_file_transfer_using_scp/#procedure","title":"Procedure","text":"Prefer a video? <p>See this procedure as a video at YouTube</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Pelle"]},{"location":"software/pelle_file_transfer_using_scp/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Pelle"]},{"location":"software/pelle_file_transfer_using_scp/#2-copy-files-using-scp","title":"2. Copy files using <code>scp</code>","text":"<p>In the terminal, copy files using <code>scp</code> to connect to Pelle:</p> <pre><code>scp [from] [to]\n</code></pre> <p>Where <code>[from]</code> is the file(s) you want to copy, and <code>[to]</code> is the destination. This is quite a shorthand notation!</p> <p>This is how you copy a file from your local computer to Pelle:</p> <pre><code>scp [local_filename] [username]@pelle.uppmax.uu.se:/home/[username]\n</code></pre> <p>where <code>[local_filename]</code> is the path to a local filename, and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_file.txt sven@pelle.uppmax.uu.se:/home/sven\n</code></pre> <p>To copy a file from Pelle to your local computer, do the command above in reverse order:</p> <pre><code>scp [username]@pelle.uppmax.uu.se:/home/[username]/[remote_filename] [local_folder]\n</code></pre> <p>where <code>[remote_filename]</code> is the path to a remote filename, <code>[username]</code> is your UPPMAX username, and <code>[local_folder]</code> is your local folder, for example:</p> <pre><code>scp sven@pelle.uppmax.uu.se:/home/sven/my_remote_file.txt /home/sven\n</code></pre>","tags":["transfer","data transfer","file transfer","scp","SCP","Pelle"]},{"location":"software/pelle_file_transfer_using_scp/#3-if-asked-give-your-uppmax-password","title":"3. If asked, give your UPPMAX password","text":"<p>If asked, give your UPPMAX password. You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Pelle"]},{"location":"software/pelle_file_transfer_using_sftp/","title":"Data transfer to/from Pelle using SFTP","text":""},{"location":"software/pelle_file_transfer_using_sftp/#data-transfer-tofrom-pelle-using-sftp","title":"Data transfer to/from Pelle using SFTP","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer data to/from Pelle.</p> <p>Data transfer to/from Pelle using SFTP is one of the ways ways to transfer files to/from Pelle</p> What are the other ways? <p>See the other ways to transfer data to/from Pelle</p> <p>One can transfer files to/from Pelle using SFTP. SFTP is an abbreviation of 'SSH File Transfer Protocol', where 'SSH' is an abbreviation of 'Secure Shell protocol' The program <code>sftp</code> allows you to transfer files to/from Pelle using SFTP.</p> <p>The process is described here:</p>"},{"location":"software/pelle_file_transfer_using_sftp/#step-1-start-a-terminal-on-your-local-computer","title":"Step 1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer.</p>"},{"location":"software/pelle_file_transfer_using_sftp/#step-2-run-sftp-to-connect-to-pelle","title":"Step 2. Run <code>sftp</code> to connect to Pelle","text":"<p>In the terminal, run <code>sftp</code> to connect to Pelle by doing:</p> <pre><code>sftp [username]@pelle.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>sftp sven@pelle.uppmax.uu.se\n</code></pre>"},{"location":"software/pelle_file_transfer_using_sftp/#step-3-if-asked-give-your-uppmax-password","title":"Step 3. If asked, give your UPPMAX password","text":"<p>If asked, give your UPPMAX password. You can get rid of this prompt if you have setup SSH keys</p>"},{"location":"software/pelle_file_transfer_using_sftp/#step-4-uploaddownload-files-tofrom-pelle","title":"Step 4. Upload/download files to/from Pelle","text":"<p>In <code>sftp</code>, upload/download files to/from Pelle.</p> <pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n\n    user(User)\n      user_local_files(Files on user computer):::file_node\n\n    subgraph sub_inside[SUNET]\n      subgraph sub_pelle_shared_env[Pelle]\n          login_node(login/calculation/interactive session):::calculation_node\n          files_in_pelle_home(Files in Pelle home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#fcc,color:#000,stroke:#fcc\n    style sub_pelle_shared_env fill:#ffc,color:#000,stroke:#ffc\n\n    user --&gt; |logs in |login_node\n    user --&gt; |uses| user_local_files\n\n    login_node --&gt; |can use|files_in_pelle_home\n    %% user_local_files &lt;--&gt; |graphical tool|files_in_pelle_home\n    %% user_local_files &lt;--&gt; |SCP|files_in_pelle_home\n    user_local_files &lt;==&gt; |SFTP|files_in_pelle_home\n\n    %% Aligns nodes prettier\n    user_local_files ~~~ login_node</code></pre> <p>Overview of file transfer on Pelle The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>"},{"location":"software/pelle_file_transfer_using_transit_scp/","title":"Data transfer to/from Pelle using Transit using SCP","text":"","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp/#data-transfer-tofrom-pelle-using-transit-using-scp","title":"Data transfer to/from Pelle using Transit using SCP","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer data to/from Pelle.</p> <p>One can use SCP to copy files between Pelle and Transit, from either Pelle or Transit.</p> <p>Both ways are shown step-by-step below.</p> <ul> <li>Using SCP from Pelle</li> <li>Using SCP from transit</li> </ul>","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_pelle/","title":"Data transfer to/from Pelle using Transit using SCP from Pelle","text":"","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Pelle"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_pelle/#data-transfer-tofrom-pelle-using-transit-using-scp-from-pelle","title":"Data transfer to/from Pelle using Transit using SCP from Pelle","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer data to/from Pelle.</p> <p>One can transfer files to/from Pelle using the UPPMAX Transit server, using SCP. The program <code>scp</code> allows you to copy file between Pelle and Transit.</p> <p>The process is:</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Pelle"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_pelle/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Pelle"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_pelle/#2-use-the-terminal-to-login-to-pelle","title":"2. Use the terminal to login to Pelle","text":"<p>Use a terminal to login to Pelle.</p> Forgot how to login to Pelle? <p>See this step-by-step guide how to login to Pelle.</p> <p>Spoiler: <code>ssh [username]@pelle.uppmax.uu.se</code></p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Pelle"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_pelle/#3a-run-scp-to-copy-files-from-pelle-to-transit","title":"3a. Run <code>scp</code> to copy files from Pelle to Transit","text":"<p>This is how you would copy a file from Pelle to Transit: in the terminal, run <code>scp</code> to copy files from Pelle to Transit by doing:</p> <pre><code>scp [file_on_pelle] [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[file_on_pelle]</code> is the name of a file on Pelle and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_pelle_file.txt [username]@transit.uppmax.uu.se\n</code></pre> <p>However, Transit is a service, not a file server. The <code>scp</code> command will complete successfully, yet the file will not be found on Transit.</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Pelle"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_pelle/#3b-run-scp-to-copy-files-from-transit-to-pelle","title":"3b. Run <code>scp</code> to copy files from Transit to Pelle","text":"<p>In the terminal, run <code>scp</code> to copy files from Transit to Pelle by doing:</p> <pre><code>scp [file_on_pelle] [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[file_on_pelle]</code> is the name of a file on Pelle and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_pelle_file.txt [username]@transit.uppmax.uu.se\n</code></pre>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Pelle"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_pelle/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Pelle"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_transit/","title":"Data transfer to/from Pelle using Transit using SCP from Transit","text":"","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_transit/#data-transfer-tofrom-pelle-using-transit-using-scp-from-transit","title":"Data transfer to/from Pelle using Transit using SCP from Transit","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer data to/from Pelle.</p> <p>One can use SCP to copy files between Pelle and Transit, from either Pelle or Transit.</p> <p>One can transfer files to/from Pelle using the UPPMAX Transit server, using <code>scp</code>.</p> <p>The process is:</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_transit/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_transit/#2-use-the-terminal-to-login-to-transit","title":"2. Use the terminal to login to Transit","text":"<p>Use a terminal to login to Transit.</p> Forgot how to login to Transit? <p>A step-by-step guide how to login to Transit See our step-by-step guide how to login to Transit.</p> <p>Spoiler: <code>ssh [username]@transit.uppmax.uu.se</code></p>","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_transit/#3a-run-scp-to-copy-files-from-transit-to-pelle","title":"3a. Run <code>scp</code> to copy files from Transit to Pelle","text":"<p>In the terminal, run <code>scp</code> to copy files from Transit to Pelle by doing:</p> <pre><code>scp [username]@pelle.uppmax.uu.se:/home/[username]/[file_on_pelle] [path_on_transit]\n</code></pre> <p>where <code>[file_on_pelle]</code> is the name of a file on Pelle, <code>[username]</code> is your UPPMAX username, and <code>[path_on_transit]</code> is the target path on Transit, for example:</p> <pre><code>scp sven@pelle.uppmax.uu.se:/home/sven/my_pelle_file.txt .\n</code></pre> <p>Where <code>.</code> means 'the directory where I am now on Transit'.</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_transit/#3b-run-scp-to-copy-files-from-pelle-to-transit","title":"3b.  Run <code>scp</code> to copy files from Pelle to Transit","text":"<p>This is how you would copy a file from Pelle to Transit: in the terminal, run <code>scp</code> to copy files from Pelle to Transit by doing:</p> <pre><code>scp [file_on_pelle] [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[file_on_transit]</code> is the name of a file on Transit and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_local_pelle_file.txt [username]@transit.uppmax.uu.se\n</code></pre> <p>However, Transit is a service, not a file server. The <code>scp</code> command will complete successfully, yet the file will not be found on Transit.</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_scp_from_transit/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Pelle","SCP","scp"]},{"location":"software/pelle_file_transfer_using_transit_sftp/","title":"Data transfer to/from Pelle using Transit and SFTP","text":""},{"location":"software/pelle_file_transfer_using_transit_sftp/#data-transfer-tofrom-pelle-using-transit-and-sftp","title":"Data transfer to/from Pelle using Transit and SFTP","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer data to/from Pelle.</p> <p>Data transfer to/from Pelle using Transit is one of the ways ways to transfer files to/from Pelle</p> <p>One can use SFTP to copy files between Pelle and Transit, from either Pelle or Transit.</p> <p>Both ways are shown step-by-step below.</p> <ul> <li>Using SFTP from Pelle</li> <li>Using SFTP from transit</li> </ul>"},{"location":"software/pelle_file_transfer_using_transit_sftp/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[SUNET]\n      direction LR\n      user(User)\n      subgraph sub_transit_env[Transit]\n        transit_login(Transit login):::calculation_node\n        files_on_transit(Files posted to Transit):::transit_node\n      end\n      subgraph sub_pelle_shared_env[Pelle]\n          files_in_pelle_home(Files in Pelle home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#ccc,color:#000,stroke:#000\n    style sub_transit_env fill:#cfc,color:#000,stroke:#000\n    style sub_pelle_shared_env fill:#fcc,color:#000,stroke:#000\n\n    user --&gt; |logs in |transit_login\n\n    transit_login --&gt; |can use|files_on_transit\n    %% user_local_files &lt;--&gt; |graphical tool|files_in_pelle_home\n    %% user_local_files &lt;--&gt; |SCP|files_in_pelle_home\n    files_on_transit &lt;==&gt; |transfer|files_in_pelle_home</code></pre> <p>Overview of file transfer on Pelle The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_pelle/","title":"Data transfer to/from Pelle using Transit and SFTP from Pelle","text":""},{"location":"software/pelle_file_transfer_using_transit_sftp_from_pelle/#data-transfer-tofrom-pelle-using-transit-and-sftp-from-pelle","title":"Data transfer to/from Pelle using Transit and SFTP from Pelle","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer data to/from Pelle.</p> <p>Data transfer to/from Pelle using Transit is one of the ways ways to transfer files to/from Pelle</p> <p>One can transfer files to/from Pelle using the UPPMAX Transit server. Transit is an abbreviation of 'SSH File Transfer Protocol', where 'SSH' is an abbreviation of 'Secure Shell protocol' The program <code>sftp</code> allows you to transfer files to/from Pelle using Transit.</p> <p>The process is:</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_pelle/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_pelle/#2-use-the-terminal-to-login-to-pelle","title":"2. Use the terminal to login to Pelle","text":"<p>Use a terminal to login to Pelle.</p> Forgot how to login to Pelle? <p>See step-by-step guide how to login to Pelle.</p> <p>Spoiler: <code>ssh [username]@pelle.uppmax.uu.se</code></p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_pelle/#3-run-sftp-to-connect-to-transit","title":"3. Run <code>sftp</code> to connect to Transit","text":"<p>In the terminal, run <code>sftp</code> to connect to Transit by doing:</p> <pre><code>sftp [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>sftp sven@transit.uppmax.uu.se\n</code></pre>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_pelle/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_pelle/#5-in-sftp-uploaddownload-files-tofrom-transit","title":"5. In <code>sftp</code>, upload/download files to/from Transit","text":"<p>Transit is a service, not a file server. This means that if you upload files to Transit using SFTP, they will remain there as long a the connection is active. These files need to be forwarded to more permanent storage.</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_transit/","title":"Data transfer to/from Pelle using Transit and SFTP from Transit","text":""},{"location":"software/pelle_file_transfer_using_transit_sftp_from_transit/#data-transfer-tofrom-pelle-using-transit-and-sftp-from-transit","title":"Data transfer to/from Pelle using Transit and SFTP from Transit","text":"<p>Does not work yet</p> <p>Pelle is a new UPPMAX HPC cluster that is in the process of being deployed.</p> <p>The procedure as described on this page does not work yet or is untested.</p> <p>This page will be updated when this works.</p> <p>There are multiple ways to transfer data to/from Pelle.</p> <p>Data transfer to/from Pelle using Transit is one of the ways ways to transfer files to/from Pelle</p> <p>One can transfer files to/from Pelle using the UPPMAX Transit server. Transit is an abbreviation of 'SSH File Transfer Protocol', where 'SSH' is an abbreviation of 'Secure Shell protocol' The program <code>sftp</code> allows you to transfer files to/from Pelle using Transit.</p> <p>The process is:</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_transit/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_transit/#2-use-the-terminal-to-login-to-transit","title":"2. Use the terminal to login to Transit","text":"<p>Use a terminal to login to Transit</p> Forgot how to login to Transit? <p>See our step-by-step guide how to login to Transit.</p> <p>Spoiler: <code>ssh [username]@transit.uppmax.uu.se</code></p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_transit/#3-run-sftp-to-connect-to-pelle","title":"3. Run <code>sftp</code> to connect to Pelle","text":"<p>In the terminal, run <code>sftp</code> to connect to Pelle by doing:</p> <pre><code>sftp [username]@pelle.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>sftp sven@pelle.uppmax.uu.se\n</code></pre>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_transit/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>"},{"location":"software/pelle_file_transfer_using_transit_sftp_from_transit/#5-in-sftp-uploaddownload-files-tofrom-pelle","title":"5. In <code>sftp</code>, upload/download files to/from Pelle","text":"<p>Transit is a service, not a file server. This means that if you upload files to Transit using SFTP, they will remain there as long a the connection is active. These files need to be forwarded to more permanent storage.</p>"},{"location":"software/pelle_file_transfer_using_winscp/","title":"File transfer to/from Pelle using WinSCP","text":"","tags":["transfer","data transfer","file transfer","Rackham","WinSCP"]},{"location":"software/pelle_file_transfer_using_winscp/#file-transfer-tofrom-pelle-using-winscp","title":"File transfer to/from Pelle using WinSCP","text":"<p>There are multiple ways to transfer data to/from Pelle.</p> <p>Here, we show how to transfer files using a graphical tool called WinSCP.</p> <p>To transfer files to/from Rackham using WinSCP, do:</p> <ul> <li>Start WinSCP</li> </ul> <p></p> <ul> <li>Create a new site</li> <li>For that site, use all standards, except:<ul> <li>Set file protocol to 'SFTP'</li> <li>Set host name to <code>pelle.uppmax.uu.se</code> (<code>pelle1</code> or <code>pelle2</code> will work as well)</li> <li>Set user name to <code>[username]</code>, e.g. <code>sven</code></li> <li>Save</li> </ul> </li> <li>Double-click on the saved session to the left OR Press the \"Login\" button</li> <li>Enter password</li> <li>Enter 2nd-factor (TOTP)</li> </ul>","tags":["transfer","data transfer","file transfer","Rackham","WinSCP"]},{"location":"software/perl/","title":"Perl","text":""},{"location":"software/perl/#perl_modules-guide","title":"Perl_modules guide","text":"<p>A number of modules/packages are available by default with all Perl versions.</p> <p>This is a list of modules for <code>perl/5.26.2</code> available by loading the module <code>perl_modules/5.26.2</code>.</p> <p>For previous Perl versions 5.18.4 and 5.24.1 (available through the software module system as <code>perl/5.18.4</code> and <code>perl/5.24.1</code>), many more Perl modules are available by loading the software module <code>perl_modules/5.18.4</code> or <code>perl_modules/5.24.1</code>.</p> <p>A complete list of the Perl modules available in <code>perl_modules/5.26.2</code> module is as follows:</p>"},{"location":"software/perl/#perl-module-search-on-perl_modules5262rackham","title":"Perl Module Search on perl_modules/5.26.2/rackham","text":"Module name Version $pkg 2.019 Acme::Damn 0.08 Algorithm::C3 0.10 Algorithm::Combinatorics 0.27 Algorithm::Diff 1.1903 Algorithm::FastPermute 0.999 Algorithm::Loops 1.032 Algorithm::Munkres 0.08 Algorithm::Permute 0.16 aliased 0.34 Apache::Htpasswd 1.9 Apache::LogFormat::Compiler 0.35 Apache::SOAP 1.27 App::Ack 2.24 App::Ack::ConfigDefault unknown App::Ack::ConfigFinder unknown App::Ack::ConfigLoader unknown App::Ack::Filter unknown App::Ack::Filter::Collection unknown App::Ack::Filter::Default unknown App::Ack::Filter::Extension unknown App::Ack::Filter::ExtensionGroup unknown App::Ack::Filter::FirstLineMatch unknown App::Ack::Filter::Inverse unknown App::Ack::Filter::Is unknown App::Ack::Filter::IsGroup unknown App::Ack::Filter::IsPath unknown App::Ack::Filter::IsPathGroup unknown App::Ack::Filter::Match unknown App::Ack::Filter::MatchGroup unknown App::Ack::Resource unknown App::Ack::Resources unknown App::Cmd 0.331 App::Cmd::ArgProcessor 0.331 App::Cmd::Command 0.331 App::Cmd::Command::commands 0.331 App::Cmd::Command::help 0.331 App::Cmd::Command::version 0.331 App::Cmd::Plugin 0.331 App::Cmd::Setup 0.331 App::Cmd::Simple 0.331 App::Cmd::Subdispatch 0.331 App::Cmd::Subdispatch::DashedStyle 0.331 App::Cmd::Tester 0.331 App::Cmd::Tester::CaptureExternal 0.331 App::cpanminus 1.7044 App::cpanminus::fatscript 1.7044 App::FatPacker 0.010007 App::FatPacker::Trace unknown App::Nopaste 1.012 App::Nopaste::Command 1.012 App::Nopaste::Service 1.012 App::Nopaste::Service::Codepeek 1.012 App::Nopaste::Service::Debian 1.012 App::Nopaste::Service::Gist 1.012 App::Nopaste::Service::GitLab 1.012 App::Nopaste::Service::Mojopaste 1.012 App::Nopaste::Service::PastebinCom 1.012 App::Nopaste::Service::Pastie 1.012 App::Nopaste::Service::Shadowcat 1.012 App::Nopaste::Service::Snitch 1.012 App::Nopaste::Service::ssh 1.012 App::Nopaste::Service::Ubuntu 1.012 App::perlbrew 0.84 App::Pinto 0.14 App::Pinto::Command 0.14 App::Pinto::Command::add 0.14 App::Pinto::Command::clean 0.14 App::Pinto::Command::copy 0.14 App::Pinto::Command::default 0.14 App::Pinto::Command::delete 0.14 App::Pinto::Command::diff 0.14 App::Pinto::Command::help 0.14 App::Pinto::Command::init 0.14 App::Pinto::Command::install 0.14 App::Pinto::Command::kill 0.14 App::Pinto::Command::list 0.14 App::Pinto::Command::lock 0.14 App::Pinto::Command::log 0.14 App::Pinto::Command::look 0.14 App::Pinto::Command::manual 0.14 App::Pinto::Command::merge 0.14 App::Pinto::Command::migrate 0.14 App::Pinto::Command::new 0.14 App::Pinto::Command::nop 0.14 App::Pinto::Command::pin 0.14 App::Pinto::Command::props 0.14 App::Pinto::Command::pull 0.14 App::Pinto::Command::register 0.14 App::Pinto::Command::rename 0.14 App::Pinto::Command::reset 0.14 App::Pinto::Command::revert 0.14 App::Pinto::Command::roots 0.14 App::Pinto::Command::stacks 0.14 App::Pinto::Command::statistics 0.14 App::Pinto::Command::thanks 0.14 App::Pinto::Command::unlock 0.14 App::Pinto::Command::unpin 0.14 App::Pinto::Command::unregister 0.14 App::Pinto::Command::update 0.14 App::Pinto::Command::verify 0.14 App::Prove 3.42 App::Prove::State 3.42 App::Prove::State::Result 3.42 App::Prove::State::Result::Test 3.42 AppConfig 1.71 AppConfig::Args 1.71 AppConfig::CGI 1.71 AppConfig::File 1.71 AppConfig::Getopt 1.71 AppConfig::State 1.71 AppConfig::Sys 1.71 Archive::Any::Create 0.03 Archive::Any::Create::Tar unknown Archive::Any::Create::Zip unknown Archive::Extract 0.80 Archive::Zip 1.60 Archive::Zip::Archive 1.60 Archive::Zip::BufferedFileHandle 1.60 Archive::Zip::DirectoryMember 1.60 Archive::Zip::FileMember 1.60 Archive::Zip::Member 1.60 Archive::Zip::MemberRead 1.60 Archive::Zip::MockFileHandle 1.60 Archive::Zip::NewFileMember 1.60 Archive::Zip::StringMember 1.60 Archive::Zip::Tree 1.60 Archive::Zip::ZipFileMember 1.60 Array::Compare 3.0.1 Array::Unique 0.08 Array::Utils 0.5 asa 1.03 Astro::FITS::Header 3.04 Astro::FITS::Header::AST 3.01 Astro::FITS::Header::CFITSIO 3.02 Astro::FITS::Header::GSD 3.01 Astro::FITS::Header::Item 3.02 Astro::FITS::Header::NDF 3.02 Authen::SASL 2.16 Authen::SASL::CRAM_MD5 2.14 Authen::SASL::EXTERNAL 2.14 Authen::SASL::Perl 2.14 Authen::SASL::Perl::ANONYMOUS 2.14 Authen::SASL::Perl::CRAM_MD5 2.14 Authen::SASL::Perl::DIGEST_MD5 2.14 Authen::SASL::Perl::EXTERNAL 2.14 Authen::SASL::Perl::GSSAPI 0.05 Authen::SASL::Perl::LOGIN 2.14 Authen::SASL::Perl::PLAIN 2.14 Authen::Simple 0.5 Authen::Simple::Adapter unknown Authen::Simple::Apache unknown Authen::Simple::Log unknown Authen::Simple::Passwd 0.6 Authen::Simple::Password unknown autobox unknown autobox::universal unknown B::Hooks::EndOfScope 0.24 B::Hooks::EndOfScope::PP 0.24 B::Hooks::EndOfScope::XS 0.24 B::Hooks::OP::Check 0.22 B::Hooks::OP::Check::Install::Files unknown B::Hooks::OP::PPAddr 0.06 B::Hooks::OP::PPAddr::Install::Files unknown B::Keywords 1.18 B::Utils 0.27 B::Utils::Install::Files unknown B::Utils::OP 0.27 bareword::filehandles 0.006 Bit::Vector 7.4 Bit::Vector::Overload 7.4 Bit::Vector::String 7.4 boolean 0.46 Browser::Open 0.04 Bundle::DBD::mysql 4.046 Bundle::DBI 12.008696 Bundle::Object::InsideOut 4.04 C::StructType unknown C::Type unknown C::Var unknown Cache::BaseCache unknown Cache::BaseCacheTester unknown Cache::Cache 1.08 Cache::CacheMetaData unknown Cache::CacheSizer unknown Cache::CacheTester unknown Cache::CacheUtils unknown Cache::FileBackend unknown Cache::FileCache unknown Cache::LRU 0.04 Cache::MemoryBackend unknown Cache::MemoryCache unknown Cache::NullCache unknown Cache::Object unknown Cache::SharedMemoryBackend unknown Cache::SharedMemoryCache unknown Cache::SizeAwareCache unknown Cache::SizeAwareCacheTester unknown Cache::SizeAwareFileCache unknown Cache::SizeAwareMemoryCache unknown Cache::SizeAwareSharedMemoryCache unknown Capture::Tiny 0.48 Carp::Always 0.13 Carp::Assert 0.21 Carp::Assert::More 1.16 Carp::Clan 6.06 Carp::REPL 0.18 Carton unknown Carton::Builder unknown Carton::CLI unknown Carton::CPANfile unknown Carton::Dependency unknown Carton::Dist unknown Carton::Dist::Core unknown Carton::Environment unknown Carton::Error unknown Carton::Index unknown Carton::Mirror unknown Carton::Package unknown Carton::Packer unknown Carton::Snapshot unknown Carton::Snapshot::Emitter unknown Carton::Snapshot::Parser unknown Carton::Tree unknown Carton::Util unknown Catalyst 5.90118 Catalyst::Action unknown Catalyst::Action::Deserialize 1.21 Catalyst::Action::Deserialize::Callback 1.21 Catalyst::Action::Deserialize::JSON 1.21 Catalyst::Action::Deserialize::JSON::XS 1.21 Catalyst::Action::Deserialize::View 1.21 Catalyst::Action::Deserialize::XML::Simple 1.21 Catalyst::Action::Deserialize::YAML 1.21 Catalyst::Action::DeserializeMultiPart 1.21 Catalyst::Action::RenderView 0.16 Catalyst::Action::REST 1.21 Catalyst::Action::REST::ForBrowsers 1.21 Catalyst::Action::Role::ACL 0.07 Catalyst::Action::Serialize 1.21 Catalyst::Action::Serialize::Callback 1.21 Catalyst::Action::Serialize::JSON 1.21 Catalyst::Action::Serialize::JSON::XS 1.21 Catalyst::Action::Serialize::JSONP 1.21 Catalyst::Action::Serialize::View 1.21 Catalyst::Action::Serialize::XML::Simple 1.21 Catalyst::Action::Serialize::YAML 1.21 Catalyst::Action::Serialize::YAML::HTML 1.21 Catalyst::Action::SerializeBase 1.21 Catalyst::ActionChain unknown Catalyst::ActionContainer unknown Catalyst::ActionRole::ACL 0.07 Catalyst::ActionRole::ConsumesContent unknown Catalyst::ActionRole::HTTPMethods unknown Catalyst::ActionRole::NeedsLogin unknown Catalyst::ActionRole::QueryMatching unknown Catalyst::ActionRole::Scheme unknown Catalyst::Authentication::Credential::HTTP 1.018 Catalyst::Authentication::Credential::NoPassword unknown Catalyst::Authentication::Credential::Password unknown Catalyst::Authentication::Credential::Remote unknown Catalyst::Authentication::Realm unknown Catalyst::Authentication::Realm::Compatibility unknown Catalyst::Authentication::Realm::Progressive unknown Catalyst::Authentication::Realm::SimpleDB unknown Catalyst::Authentication::Store::DBIx::Class 0.1506 Catalyst::Authentication::Store::DBIx::Class::User unknown Catalyst::Authentication::Store::Minimal unknown Catalyst::Authentication::Store::Null unknown Catalyst::Authentication::User unknown Catalyst::Authentication::User::Hash unknown Catalyst::Base unknown Catalyst::ClassData unknown Catalyst::Component unknown Catalyst::Component::ApplicationAttribute unknown Catalyst::Component::ContextClosure unknown Catalyst::Component::InstancePerContext 0.001001 Catalyst::Controller unknown Catalyst::Controller::ActionRole 0.17 Catalyst::Controller::REST 1.21 Catalyst::Devel 1.39 Catalyst::Dispatcher unknown Catalyst::DispatchType unknown Catalyst::DispatchType::Chained unknown Catalyst::DispatchType::Default unknown Catalyst::DispatchType::Index unknown Catalyst::DispatchType::Path unknown Catalyst::Engine unknown Catalyst::EngineLoader unknown Catalyst::Exception unknown Catalyst::Exception::Basic unknown Catalyst::Exception::Detach unknown Catalyst::Exception::Go unknown Catalyst::Exception::Interface unknown Catalyst::Helper 1.39 Catalyst::Helper::Model::Adaptor unknown Catalyst::Helper::Model::DBIC::Schema 0.65 Catalyst::Helper::Model::Factory unknown Catalyst::Helper::Model::Factory::PerRequest unknown Catalyst::Helper::View::Email 0.36 Catalyst::Helper::View::Email::Template 0.36 Catalyst::Helper::View::TT 0.44 Catalyst::Helper::View::TTSite 0.44 Catalyst::Log unknown Catalyst::Manual 5.9009 Catalyst::Middleware::Stash unknown Catalyst::Model unknown Catalyst::Model::Adaptor 0.10 Catalyst::Model::Adaptor::Base unknown Catalyst::Model::DBIC::Schema 0.65 Catalyst::Model::Factory 0.10 Catalyst::Model::Factory::PerRequest 0.10 Catalyst::Plugin::Authentication 0.10023 Catalyst::Plugin::Authentication::Credential::Password unknown Catalyst::Plugin::Authentication::Store::Minimal unknown Catalyst::Plugin::Authentication::User unknown Catalyst::Plugin::Authentication::User::Hash unknown Catalyst::Plugin::ConfigLoader 0.34 Catalyst::Plugin::I18N 0.10 Catalyst::Plugin::Session 0.40 Catalyst::Plugin::Session::State unknown Catalyst::Plugin::Session::State::Cookie 0.17 Catalyst::Plugin::Session::Store unknown Catalyst::Plugin::Session::Store::DBIC 0.14 Catalyst::Plugin::Session::Store::DBIC::Delegate unknown Catalyst::Plugin::Session::Store::Delegate 0.06 Catalyst::Plugin::Session::Store::Dummy unknown Catalyst::Plugin::Session::Store::File 0.18 Catalyst::Plugin::Session::Test::Store 123 Catalyst::Plugin::StackTrace 0.12 Catalyst::Plugin::Static::Simple 0.36 Catalyst::Plugin::Unicode::Encoding 5.90118 Catalyst::Request unknown Catalyst::Request::PartData unknown Catalyst::Request::REST 1.21 Catalyst::Request::REST::ForBrowsers 1.21 Catalyst::Request::Upload unknown Catalyst::Response unknown Catalyst::Response::Writer unknown Catalyst::Restarter unknown Catalyst::Restarter::Forking unknown Catalyst::Restarter::Win32 unknown Catalyst::Runtime 5.90118 Catalyst::Script::CGI unknown Catalyst::Script::Create unknown Catalyst::Script::FastCGI unknown Catalyst::Script::Server unknown Catalyst::Script::Test unknown Catalyst::ScriptRole unknown Catalyst::ScriptRunner unknown Catalyst::Stats unknown Catalyst::Test unknown Catalyst::TraitFor::Model::DBIC::Schema::Caching unknown Catalyst::TraitFor::Model::DBIC::Schema::PerRequestSchema unknown Catalyst::TraitFor::Model::DBIC::Schema::Replicated unknown Catalyst::TraitFor::Model::DBIC::Schema::SchemaProxy unknown Catalyst::TraitFor::Request::REST 1.21 Catalyst::TraitFor::Request::REST::ForBrowsers 1.21 Catalyst::Utils unknown Catalyst::View unknown Catalyst::View::Email 0.36 Catalyst::View::Email::Template 0.36 Catalyst::View::TT 0.44 CatalystX::Component::Traits 0.19 CatalystX::InjectComponent 0.025 CatalystX::LeakChecker 0.06 CatalystX::Profile 0.02 CatalystX::Profile::Controller::ControlProfiling 0.02 CatalystX::REPL 0.04 CatalystX::SimpleLogin 0.20 CatalystX::SimpleLogin::Controller::Login unknown CatalystX::SimpleLogin::Form::Login unknown CatalystX::SimpleLogin::Form::LoginOpenID unknown CatalystX::SimpleLogin::TraitFor::Controller::Login::Logout unknown CatalystX::SimpleLogin::TraitFor::Controller::Login::OpenID unknown CatalystX::SimpleLogin::TraitFor::Controller::Login::RenderAsTTTemplate unknown CatalystX::SimpleLogin::TraitFor::Controller::Login::WithRedirect unknown CGI 4.38 CGI::Carp 4.38 CGI::Cookie 4.38 CGI::File::Temp 4.38 CGI::FormBuilder 3.10 CGI::FormBuilder::Field 3.10 CGI::FormBuilder::Field::button 3.10 CGI::FormBuilder::Field::checkbox 3.10 CGI::FormBuilder::Field::date 3.10 CGI::FormBuilder::Field::datetime 3.10 CGI::FormBuilder::Field::datetime_local 3.10 CGI::FormBuilder::Field::email 3.10 CGI::FormBuilder::Field::file 3.10 CGI::FormBuilder::Field::hidden 3.10 CGI::FormBuilder::Field::image 3.10 CGI::FormBuilder::Field::number 3.10 CGI::FormBuilder::Field::password 3.10 CGI::FormBuilder::Field::radio 3.10 CGI::FormBuilder::Field::select 3.10 CGI::FormBuilder::Field::static 3.10 CGI::FormBuilder::Field::submit 3.10 CGI::FormBuilder::Field::text 3.10 CGI::FormBuilder::Field::textarea 3.10 CGI::FormBuilder::Field::time 3.10 CGI::FormBuilder::Field::url 3.10 CGI::FormBuilder::Messages 3.10 CGI::FormBuilder::Messages::base 3.10 CGI::FormBuilder::Messages::default 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Messages::locale 3.10 CGI::FormBuilder::Multi 3.10 CGI::FormBuilder::Source 3.10 CGI::FormBuilder::Source::File 3.10 CGI::FormBuilder::Source::Perl 0.01 CGI::FormBuilder::Template 3.10 CGI::FormBuilder::Template::Builtin 3.10 CGI::FormBuilder::Template::CGI_SSI 3.10 CGI::FormBuilder::Template::Div 3.10 CGI::FormBuilder::Template::Fast 3.10 CGI::FormBuilder::Template::HTML 3.10 CGI::FormBuilder::Template::Text 3.10 CGI::FormBuilder::Template::TT2 3.10 CGI::FormBuilder::Test 3.10 CGI::FormBuilder::Util 3.10 CGI::HTML::Functions unknown CGI::Pretty 4.38 CGI::Push 4.38 CGI::Simple 1.15 CGI::Simple::Cookie 1.15 CGI::Simple::Standard 1.15 CGI::Simple::Util 1.15 CGI::Struct 1.21 CGI::Util 4.38 CHI 0.60 CHI::CacheObject 0.60 CHI::Constants 0.60 CHI::Driver 0.60 CHI::Driver::Base::CacheContainer 0.60 CHI::Driver::CacheCache 0.60 CHI::Driver::FastMmap 0.60 CHI::Driver::File 0.60 CHI::Driver::Memory 0.60 CHI::Driver::Metacache 0.60 CHI::Driver::Null 0.60 CHI::Driver::RawMemory 0.60 CHI::Driver::Role::HasSubcaches 0.60 CHI::Driver::Role::IsSizeAware 0.60 CHI::Driver::Role::IsSubcache 0.60 CHI::Driver::Role::Universal 0.60 CHI::Serializer::JSON 0.60 CHI::Serializer::Storable 0.60 CHI::Stats 0.60 CHI::t::Bugs 0.60 CHI::t::Config 0.60 CHI::t::Constants 0.60 CHI::t::Driver 0.60 CHI::t::Driver::CacheCache 0.60 CHI::t::Driver::FastMmap 0.60 CHI::t::Driver::File 0.60 CHI::t::Driver::File::DepthZero 0.60 CHI::t::Driver::Memory 0.60 CHI::t::Driver::NonMoose 0.60 CHI::t::Driver::RawMemory 0.60 CHI::t::Driver::Subcache 0.60 CHI::t::Driver::Subcache::l1_cache 0.60 CHI::t::Driver::Subcache::mirror_cache 0.60 CHI::t::GetError 0.60 CHI::t::Initialize 0.60 CHI::t::Null 0.60 CHI::t::RequiredModules 0.60 CHI::t::Sanity 0.60 CHI::t::SetError 0.60 CHI::t::Subcache 0.60 CHI::t::Subclass 0.60 CHI::t::Util 0.60 CHI::Test 0.60 CHI::Test::Class 0.60 CHI::Test::Driver::NonMoose 0.60 CHI::Test::Driver::Readonly 0.60 CHI::Test::Driver::Role::CheckKeyValidity 0.60 CHI::Test::Driver::Writeonly 0.60 CHI::Test::Util 0.60 CHI::Types 0.60 CHI::Util 0.60 Class::Accessor 0.51 Class::Accessor::Chained 0.01 Class::Accessor::Chained::Fast unknown Class::Accessor::Fast 0.51 Class::Accessor::Faster 0.51 Class::Accessor::Grouped 0.10014 Class::Accessor::Lite 0.08 Class::AutoClass 1.56 Class::AutoClass::Root 1 Class::C3 0.34 Class::C3::Adopt::NEXT 0.14 Class::C3::Componentised 1.001002 Class::C3::Componentised::ApplyHooks unknown Class::Data::Inheritable 0.08 Class::Factory::Util 1.7 Class::Inspector 1.32 Class::Inspector::Functions 1.32 Class::Load 0.25 Class::Load::PP 0.25 Class::Load::XS 0.10 Class::Method::Modifiers 2.12 Class::MethodMaker 2.24 Class::MethodMaker::array unknown Class::MethodMaker::Constants unknown Class::MethodMaker::Engine 2.24 Class::MethodMaker::hash unknown Class::MethodMaker::OptExt unknown Class::MethodMaker::scalar unknown Class::MethodMaker::V1Compat unknown Class::MOP 2.2011 Class::MOP::Attribute 2.2011 Class::MOP::Class 2.2011 Class::MOP::Class::Immutable::Trait 2.2011 Class::MOP::Deprecated 2.2011 Class::MOP::Instance 2.2011 Class::MOP::Method 2.2011 Class::MOP::Method::Accessor 2.2011 Class::MOP::Method::Constructor 2.2011 Class::MOP::Method::Generated 2.2011 Class::MOP::Method::Inlined 2.2011 Class::MOP::Method::Meta 2.2011 Class::MOP::Method::Wrapped 2.2011 Class::MOP::MiniTrait 2.2011 Class::MOP::Mixin 2.2011 Class::MOP::Mixin::AttributeCore 2.2011 Class::MOP::Mixin::HasAttributes 2.2011 Class::MOP::Mixin::HasMethods 2.2011 Class::MOP::Mixin::HasOverloads 2.2011 Class::MOP::Module 2.2011 Class::MOP::Object 2.2011 Class::MOP::Overload 2.2011 Class::MOP::Package 2.2011 Class::Singleton 1.5 Class::Tiny 1.006 Class::Trigger 0.14 Class::Unload 0.11 Class::XSAccessor 1.19 Class::XSAccessor::Array 1.19 Clipboard 0.13 Clipboard::MacPasteboard unknown Clipboard::Win32 unknown Clipboard::Xclip unknown Clone 0.39 Clone::Choose 0.010 Clone::PP 1.07 Commandable 0.01 Commandable::Invocation 0.01 common::sense 3.74 Compress::Bzip2 2.26 Compress::Raw::Bzip2 2.081 Compress::Raw::Zlib 2.081 Config::Any 0.32 Config::Any::Base unknown Config::Any::General unknown Config::Any::INI unknown Config::Any::JSON unknown Config::Any::Perl unknown Config::Any::XML unknown Config::Any::YAML unknown Config::General 2.63 Config::General::Extended 2.07 Config::General::Interpolated 2.15 Config::INI 0.025 Config::INI::Reader 0.025 Config::INI::Writer 0.025 Config::MVP 2.200011 Config::MVP::Assembler 2.200011 Config::MVP::Assembler::WithBundles 2.200011 Config::MVP::Error 2.200011 Config::MVP::Reader 2.200011 Config::MVP::Reader::Findable 2.200011 Config::MVP::Reader::Findable::ByExtension 2.200011 Config::MVP::Reader::Finder 2.200011 Config::MVP::Reader::Hash 2.200011 Config::MVP::Reader::INI 2.101463 Config::MVP::Section 2.200011 Config::MVP::Sequence 2.200011 Config::Tiny 2.23 constant::boolean 0.02 Context::Preserve 0.03 Contextual::Return 0.004014 Contextual::Return::Failure unknown Convert::Binary::C 0.78 Convert::Binary::C::Cached 0.78 Convert::BinHex 1.125 Convert::Color 0.11 Convert::Color::CMY 0.11 Convert::Color::CMYK 0.11 Convert::Color::HSL 0.11 Convert::Color::HSV 0.11 Convert::Color::RGB 0.11 Convert::Color::RGB16 0.11 Convert::Color::RGB8 0.11 Convert::Color::VGA 0.11 Convert::Color::X11 0.11 Convert::Color::XTerm 0.05 Convert::UU 0.5201 Cookie::Baker 0.09 CPAN::Changes 0.400002 CPAN::Changes::Group unknown CPAN::Changes::Release unknown CPAN::Checksums 2.12 CPAN::Common::Index 0.010 CPAN::Common::Index::LocalPackage 0.010 CPAN::Common::Index::MetaDB 0.010 CPAN::Common::Index::Mirror 0.010 CPAN::Common::Index::Mux::Ordered 0.010 CPAN::DistnameInfo 0.12 CPAN::Meta::Check 0.014 CPAN::Mini 1.111016 CPAN::Mini::App 1.111016 CPAN::Perl::Releases 3.68 CPAN::Uploader 0.103013 Cpanel::JSON::XS 4.04 Cpanel::JSON::XS::Type unknown Crypt::Blowfish 2.14 Crypt::CBC 2.33 Crypt::PasswdMD5 1.40 Crypt::Random::Seed 0.03 Crypt::Random::TESHA2 0.01 Crypt::Random::TESHA2::Config 0.01 Crypt::RC4 2.02 CSS::Tiny 1.20 curry 1.001000 curry::weak unknown Curses::Window 1.36 Cwd 3.74 Cwd::Guard 0.05 Dancer 1.3400 Dancer2 0.206000 Dancer2::CLI 0.206000 Dancer2::CLI::Command::gen 0.206000 Dancer2::CLI::Command::version 0.206000 Dancer2::Core 0.206000 Dancer2::Core::App 0.206000 Dancer2::Core::Cookie 0.206000 Dancer2::Core::Dispatcher 0.206000 Dancer2::Core::DSL 0.206000 Dancer2::Core::Error 0.206000 Dancer2::Core::Factory 0.206000 Dancer2::Core::Hook 0.206000 Dancer2::Core::HTTP 0.206000 Dancer2::Core::MIME 0.206000 Dancer2::Core::Request 0.206000 Dancer2::Core::Request::Upload 0.206000 Dancer2::Core::Response 0.206000 Dancer2::Core::Response::Delayed 0.206000 Dancer2::Core::Role::ConfigReader 0.206000 Dancer2::Core::Role::DSL 0.206000 Dancer2::Core::Role::Engine 0.206000 Dancer2::Core::Role::Handler 0.206000 Dancer2::Core::Role::HasLocation 0.206000 Dancer2::Core::Role::Hookable 0.206000 Dancer2::Core::Role::Logger 0.206000 Dancer2::Core::Role::Serializer 0.206000 Dancer2::Core::Role::SessionFactory 0.206000 Dancer2::Core::Role::SessionFactory::File 0.206000 Dancer2::Core::Role::StandardResponses 0.206000 Dancer2::Core::Role::Template 0.206000 Dancer2::Core::Route 0.206000 Dancer2::Core::Runner 0.206000 Dancer2::Core::Session 0.206000 Dancer2::Core::Time 0.206000 Dancer2::Core::Types 0.206000 Dancer2::FileUtils 0.206000 Dancer2::Handler::AutoPage 0.206000 Dancer2::Handler::File 0.206000 Dancer2::Logger::Capture 0.206000 Dancer2::Logger::Capture::Trap 0.206000 Dancer2::Logger::Console 0.206000 Dancer2::Logger::Diag 0.206000 Dancer2::Logger::File 0.206000 Dancer2::Logger::LogReport 1.27 Dancer2::Logger::Note 0.206000 Dancer2::Logger::Null 0.206000 Dancer2::Plugin 0.206000 Dancer2::Plugin::LogReport 1.27 Dancer2::Plugin::LogReport::Message 1.27 Dancer2::Serializer::Dumper 0.206000 Dancer2::Serializer::JSON 0.206000 Dancer2::Serializer::Mutable 0.206000 Dancer2::Serializer::YAML 0.206000 Dancer2::Session::Simple 0.206000 Dancer2::Session::YAML 0.206000 Dancer2::Template::Implementation::ForkedTiny 0.206000 Dancer2::Template::Simple 0.206000 Dancer2::Template::TemplateToolkit 0.206000 Dancer2::Template::Tiny 0.206000 Dancer2::Test 0.206000 Dancer::App 1.3400 Dancer::Config 1.3400 Dancer::Config::Object 1.3400 Dancer::Continuation 1.3400 Dancer::Continuation::Halted 1.3400 Dancer::Continuation::Route 1.3400 Dancer::Continuation::Route::ErrorSent 1.3400 Dancer::Continuation::Route::FileSent 1.3400 Dancer::Continuation::Route::Forwarded 1.3400 Dancer::Continuation::Route::Passed 1.3400 Dancer::Continuation::Route::Templated 1.3400 Dancer::Cookie 1.3400 Dancer::Cookies 1.3400 Dancer::Deprecation 1.3400 Dancer::Engine 1.3400 Dancer::Error 1.3400 Dancer::Exception 1.3400 Dancer::Exception::Base 1.3400 Dancer::Factory::Hook 1.3400 Dancer::FileUtils 1.3400 Dancer::GetOpt 1.3400 Dancer::Handler 1.3400 Dancer::Handler::Debug 1.3400 Dancer::Handler::PSGI 1.3400 Dancer::Handler::Standalone 1.3400 Dancer::Hook 1.3400 Dancer::Hook::Properties 1.3400 Dancer::HTTP 1.3400 Dancer::Logger 1.3400 Dancer::Logger::Abstract 1.3400 Dancer::Logger::Capture 1.3400 Dancer::Logger::Capture::Trap 1.3400 Dancer::Logger::Console 1.3400 Dancer::Logger::Diag 1.3400 Dancer::Logger::File 1.3400 Dancer::Logger::LogReport 1.27 Dancer::Logger::Note 1.3400 Dancer::Logger::Null 1.3400 Dancer::MIME 1.3400 Dancer::ModuleLoader 1.3400 Dancer::Object 1.3400 Dancer::Object::Singleton 1.3400 Dancer::Plugin 1.3400 Dancer::Plugin::Ajax 1.3400 Dancer::Renderer 1.3400 Dancer::Request 1.3400 Dancer::Request::Upload 1.3400 Dancer::Response 1.3400 Dancer::Route 1.3400 Dancer::Route::Cache 1.3400 Dancer::Route::Registry 1.3400 Dancer::Serializer 1.3400 Dancer::Serializer::Abstract 1.3400 Dancer::Serializer::Dumper 1.3400 Dancer::Serializer::JSON 1.3400 Dancer::Serializer::JSONP 1.3400 Dancer::Serializer::Mutable 1.3400 Dancer::Serializer::XML 1.3400 Dancer::Serializer::YAML 1.3400 Dancer::Session 1.3400 Dancer::Session::Abstract 1.3400 Dancer::Session::Simple 1.3400 Dancer::Session::YAML 1.3400 Dancer::SharedData 1.3400 Dancer::Template 1.3400 Dancer::Template::Abstract 1.3400 Dancer::Template::Simple 1.3400 Dancer::Template::TemplateToolkit 1.3400 Dancer::Test 1.3400 Dancer::Timer 1.3400 Data::Clone 0.004 Data::Compare 1.25 Data::Compare::Plugins::Scalar::Properties 1 Data::Dump 1.23 Data::Dump::FilterContext unknown Data::Dump::Filtered unknown Data::Dump::Streamer 2.40 Data::Dump::Streamer::_::StringPrinter 0.1 Data::Dump::Trace 0.02 Data::Dumper::Again 0.01 Data::Dumper::Concise 2.023 Data::Dumper::Concise::Sugar 2.023 Data::Dumper::Perltidy 0.03 Data::Grove 0.08 Data::Grove::Parent 0.08 Data::Grove::Visitor 0.08 Data::Munge 0.097 Data::OptList 0.110 Data::Page 2.02 Data::Paginator 0.08 Data::Paginator::Types 0.08 Data::Perl 0.002009 Data::Perl::Bool 0.002009 Data::Perl::Bool::MooseLike 0.001008 Data::Perl::Code 0.002009 Data::Perl::Collection::Array 0.002009 Data::Perl::Collection::Array::MooseLike 0.001008 Data::Perl::Collection::Hash 0.002009 Data::Perl::Collection::Hash::MooseLike 0.001008 Data::Perl::Counter 0.002009 Data::Perl::Number 0.002009 Data::Perl::Number::MooseLike 0.001008 Data::Perl::Role::Bool 0.002009 Data::Perl::Role::Code 0.002009 Data::Perl::Role::Collection::Array 0.002009 Data::Perl::Role::Collection::Hash 0.002009 Data::Perl::Role::Counter 0.002009 Data::Perl::Role::Number 0.002009 Data::Perl::Role::String 0.002009 Data::Perl::String 0.002009 Data::Perl::String::MooseLike 0.001008 Data::PowerSet 0.05 Data::Printer 0.40 Data::Printer::Filter unknown Data::Printer::Filter::DateTime unknown Data::Printer::Filter::DB unknown Data::Printer::Filter::Digest unknown Data::Section 0.200007 Data::Stag 0.14 Data::Stag::Arr2HTML 0.14 Data::Stag::Base 0.14 Data::Stag::BaseGenerator unknown Data::Stag::ChainHandler 0.14 Data::Stag::DTDWriter 0.14 Data::Stag::GraphHandler 0.14 Data::Stag::HashDB 0.14 Data::Stag::IndentParser 0.14 Data::Stag::IndentWriter 0.14 Data::Stag::ITextParser 0.14 Data::Stag::ITextWriter 0.14 Data::Stag::JSONWriter 0.14 Data::Stag::null 0.14 Data::Stag::PerlWriter 0.14 Data::Stag::PodParser 0.14 Data::Stag::SAX2Stag 0.14 Data::Stag::Simple 0.14 Data::Stag::StagDB 0.14 Data::Stag::StagI unknown Data::Stag::StagImpl 0.14 Data::Stag::SxprParser 0.14 Data::Stag::SxprWriter 0.14 Data::Stag::Util 0.14 Data::Stag::Writer 0.14 Data::Stag::XMLParser 0.14 Data::Stag::XMLWriter 0.14 Data::Stag::XSLHandler unknown Data::Stag::XSLTHandler unknown Data::UUID 1.221 Data::Validate::Domain 0.14 Data::Validate::IP 0.27 Data::Validate::URI 0.07 Data::Visitor 0.30 Data::Visitor::Callback 0.30 Date::Format 2.24 Date::Language 1.10 Date::Language::Afar 0.99 Date::Language::Amharic 1.00 Date::Language::Austrian 1.01 Date::Language::Brazilian 1.01 Date::Language::Bulgarian 1.01 Date::Language::Chinese 1.00 Date::Language::Chinese_GB 1.01 Date::Language::Czech 1.01 Date::Language::Danish 1.01 Date::Language::Dutch 1.02 Date::Language::English 1.01 Date::Language::Finnish 1.01 Date::Language::French 1.04 Date::Language::Gedeo 0.99 Date::Language::German 1.02 Date::Language::Greek 1.00 Date::Language::Hungarian 1.01 Date::Language::Icelandic 1.01 Date::Language::Italian 1.01 Date::Language::Norwegian 1.01 Date::Language::Oromo 0.99 Date::Language::Romanian 1.01 Date::Language::Russian 1.01 Date::Language::Russian_cp1251 1.01 Date::Language::Russian_koi8r 1.01 Date::Language::Sidama 0.99 Date::Language::Somali 0.99 Date::Language::Spanish 1.00 Date::Language::Swedish 1.01 Date::Language::Tigrinya 1.00 Date::Language::TigrinyaEritrean 1.00 Date::Language::TigrinyaEthiopian 1.00 Date::Language::Turkish 1.0 Date::Parse 2.30 Date::Tiny 1.07 DateTime 1.49 DateTime::Astro 1.03 DateTime::Astro unknown DateTime::Astro unknown DateTime::Calendar::Chinese 1.00 DateTime::Calendar::Japanese::Era 0.08003 DateTime::Duration 1.49 DateTime::Event::Chinese 1.00 DateTime::Event::ICal 0.13 DateTime::Event::SolarTerm unknown DateTime::Format::Builder 0.81 DateTime::Format::Builder::Parser 0.81 DateTime::Format::Builder::Parser::Dispatch 0.81 DateTime::Format::Builder::Parser::generic 0.81 DateTime::Format::Builder::Parser::Quick 0.81 DateTime::Format::Builder::Parser::Regex 0.81 DateTime::Format::Builder::Parser::Strptime 0.81 DateTime::Format::DateParse 0.05 DateTime::Format::Duration 1.04 DateTime::Format::Epoch 0.16 DateTime::Format::Epoch::ActiveDirectory 0.13 DateTime::Format::Epoch::DotNet 0.13 DateTime::Format::Epoch::JD 0.13 DateTime::Format::Epoch::Lilian 0.13 DateTime::Format::Epoch::MacOS 0.13 DateTime::Format::Epoch::MJD 0.13 DateTime::Format::Epoch::NTP 0.14 DateTime::Format::Epoch::RataDie 0.13 DateTime::Format::Epoch::RJD 0.13 DateTime::Format::Epoch::TAI64 0.13 DateTime::Format::Epoch::TJD 0.13 DateTime::Format::Epoch::Unix 0.13 DateTime::Format::Flexible 0.30 DateTime::Format::Flexible::lang unknown DateTime::Format::Flexible::lang::de unknown DateTime::Format::Flexible::lang::en unknown DateTime::Format::Flexible::lang::es unknown DateTime::Format::ICal 0.09 DateTime::Format::Mail 0.403 DateTime::Format::Natural 1.05 DateTime::Format::Natural::Calc 1.41 DateTime::Format::Natural::Compat 0.07 DateTime::Format::Natural::Duration 0.06 DateTime::Format::Natural::Duration::Checks 0.04 DateTime::Format::Natural::Expand 0.03 DateTime::Format::Natural::Extract 0.11 DateTime::Format::Natural::Formatted 0.07 DateTime::Format::Natural::Helpers 0.06 DateTime::Format::Natural::Lang::Base 1.08 DateTime::Format::Natural::Lang::EN 1.62 DateTime::Format::Natural::Rewrite 0.06 DateTime::Format::Natural::Test 0.10 DateTime::Format::Natural::Utils 0.05 DateTime::Format::Natural::Wrappers 0.03 DateTime::Format::Strptime 1.75 DateTime::Format::Strptime::Types 1.75 DateTime::Format::W3CDTF 0.07 DateTime::Helpers 1.49 DateTime::Infinite 1.49 DateTime::LeapSecond 1.49 DateTime::Locale 1.22 DateTime::Locale::Base 1.22 DateTime::Locale::Catalog 1.22 DateTime::Locale::Data 1.22 DateTime::Locale::FromData 1.22 DateTime::Locale::Util 1.22 DateTime::PP 1.49 DateTime::PPExtra 1.49 DateTime::Set 0.3900 DateTime::Set::ICal 0.19 DateTime::Span unknown DateTime::SpanSet unknown DateTime::TimeZone 2.19 DateTime::TimeZone::Africa::Abidjan 2.19 DateTime::TimeZone::Africa::Accra 2.19 DateTime::TimeZone::Africa::Algiers 2.19 DateTime::TimeZone::Africa::Bissau 2.19 DateTime::TimeZone::Africa::Cairo 2.19 DateTime::TimeZone::Africa::Casablanca 2.19 DateTime::TimeZone::Africa::Ceuta 2.19 DateTime::TimeZone::Africa::El_Aaiun 2.19 DateTime::TimeZone::Africa::Johannesburg 2.19 DateTime::TimeZone::Africa::Juba 2.19 DateTime::TimeZone::Africa::Khartoum 2.19 DateTime::TimeZone::Africa::Lagos 2.19 DateTime::TimeZone::Africa::Maputo 2.19 DateTime::TimeZone::Africa::Monrovia 2.19 DateTime::TimeZone::Africa::Nairobi 2.19 DateTime::TimeZone::Africa::Ndjamena 2.19 DateTime::TimeZone::Africa::Sao_Tome 2.19 DateTime::TimeZone::Africa::Tripoli 2.19 DateTime::TimeZone::Africa::Tunis 2.19 DateTime::TimeZone::Africa::Windhoek 2.19 DateTime::TimeZone::America::Adak 2.19 DateTime::TimeZone::America::Anchorage 2.19 DateTime::TimeZone::America::Araguaina 2.19 DateTime::TimeZone::America::Argentina::Buenos_Aires 2.19 DateTime::TimeZone::America::Argentina::Catamarca 2.19 DateTime::TimeZone::America::Argentina::Cordoba 2.19 DateTime::TimeZone::America::Argentina::Jujuy 2.19 DateTime::TimeZone::America::Argentina::La_Rioja 2.19 DateTime::TimeZone::America::Argentina::Mendoza 2.19 DateTime::TimeZone::America::Argentina::Rio_Gallegos 2.19 DateTime::TimeZone::America::Argentina::Salta 2.19 DateTime::TimeZone::America::Argentina::San_Juan 2.19 DateTime::TimeZone::America::Argentina::San_Luis 2.19 DateTime::TimeZone::America::Argentina::Tucuman 2.19 DateTime::TimeZone::America::Argentina::Ushuaia 2.19 DateTime::TimeZone::America::Asuncion 2.19 DateTime::TimeZone::America::Atikokan 2.19 DateTime::TimeZone::America::Bahia 2.19 DateTime::TimeZone::America::Bahia_Banderas 2.19 DateTime::TimeZone::America::Barbados 2.19 DateTime::TimeZone::America::Belem 2.19 DateTime::TimeZone::America::Belize 2.19 DateTime::TimeZone::America::Blanc_Sablon 2.19 DateTime::TimeZone::America::Boa_Vista 2.19 DateTime::TimeZone::America::Bogota 2.19 DateTime::TimeZone::America::Boise 2.19 DateTime::TimeZone::America::Cambridge_Bay 2.19 DateTime::TimeZone::America::Campo_Grande 2.19 DateTime::TimeZone::America::Cancun 2.19 DateTime::TimeZone::America::Caracas 2.19 DateTime::TimeZone::America::Cayenne 2.19 DateTime::TimeZone::America::Chicago 2.19 DateTime::TimeZone::America::Chihuahua 2.19 DateTime::TimeZone::America::Costa_Rica 2.19 DateTime::TimeZone::America::Creston 2.19 DateTime::TimeZone::America::Cuiaba 2.19 DateTime::TimeZone::America::Curacao 2.19 DateTime::TimeZone::America::Danmarkshavn 2.19 DateTime::TimeZone::America::Dawson 2.19 DateTime::TimeZone::America::Dawson_Creek 2.19 DateTime::TimeZone::America::Denver 2.19 DateTime::TimeZone::America::Detroit 2.19 DateTime::TimeZone::America::Edmonton 2.19 DateTime::TimeZone::America::Eirunepe 2.19 DateTime::TimeZone::America::El_Salvador 2.19 DateTime::TimeZone::America::Fort_Nelson 2.19 DateTime::TimeZone::America::Fortaleza 2.19 DateTime::TimeZone::America::Glace_Bay 2.19 DateTime::TimeZone::America::Godthab 2.19 DateTime::TimeZone::America::Goose_Bay 2.19 DateTime::TimeZone::America::Grand_Turk 2.19 DateTime::TimeZone::America::Guatemala 2.19 DateTime::TimeZone::America::Guayaquil 2.19 DateTime::TimeZone::America::Guyana 2.19 DateTime::TimeZone::America::Halifax 2.19 DateTime::TimeZone::America::Havana 2.19 DateTime::TimeZone::America::Hermosillo 2.19 DateTime::TimeZone::America::Indiana::Indianapolis 2.19 DateTime::TimeZone::America::Indiana::Knox 2.19 DateTime::TimeZone::America::Indiana::Marengo 2.19 DateTime::TimeZone::America::Indiana::Petersburg 2.19 DateTime::TimeZone::America::Indiana::Tell_City 2.19 DateTime::TimeZone::America::Indiana::Vevay 2.19 DateTime::TimeZone::America::Indiana::Vincennes 2.19 DateTime::TimeZone::America::Indiana::Winamac 2.19 DateTime::TimeZone::America::Inuvik 2.19 DateTime::TimeZone::America::Iqaluit 2.19 DateTime::TimeZone::America::Jamaica 2.19 DateTime::TimeZone::America::Juneau 2.19 DateTime::TimeZone::America::Kentucky::Louisville 2.19 DateTime::TimeZone::America::Kentucky::Monticello 2.19 DateTime::TimeZone::America::La_Paz 2.19 DateTime::TimeZone::America::Lima 2.19 DateTime::TimeZone::America::Los_Angeles 2.19 DateTime::TimeZone::America::Maceio 2.19 DateTime::TimeZone::America::Managua 2.19 DateTime::TimeZone::America::Manaus 2.19 DateTime::TimeZone::America::Martinique 2.19 DateTime::TimeZone::America::Matamoros 2.19 DateTime::TimeZone::America::Mazatlan 2.19 DateTime::TimeZone::America::Menominee 2.19 DateTime::TimeZone::America::Merida 2.19 DateTime::TimeZone::America::Metlakatla 2.19 DateTime::TimeZone::America::Mexico_City 2.19 DateTime::TimeZone::America::Miquelon 2.19 DateTime::TimeZone::America::Moncton 2.19 DateTime::TimeZone::America::Monterrey 2.19 DateTime::TimeZone::America::Montevideo 2.19 DateTime::TimeZone::America::Nassau 2.19 DateTime::TimeZone::America::New_York 2.19 DateTime::TimeZone::America::Nipigon 2.19 DateTime::TimeZone::America::Nome 2.19 DateTime::TimeZone::America::Noronha 2.19 DateTime::TimeZone::America::North_Dakota::Beulah 2.19 DateTime::TimeZone::America::North_Dakota::Center 2.19 DateTime::TimeZone::America::North_Dakota::New_Salem 2.19 DateTime::TimeZone::America::Ojinaga 2.19 DateTime::TimeZone::America::Panama 2.19 DateTime::TimeZone::America::Pangnirtung 2.19 DateTime::TimeZone::America::Paramaribo 2.19 DateTime::TimeZone::America::Phoenix 2.19 DateTime::TimeZone::America::Port_au_Prince 2.19 DateTime::TimeZone::America::Port_of_Spain 2.19 DateTime::TimeZone::America::Porto_Velho 2.19 DateTime::TimeZone::America::Puerto_Rico 2.19 DateTime::TimeZone::America::Punta_Arenas 2.19 DateTime::TimeZone::America::Rainy_River 2.19 DateTime::TimeZone::America::Rankin_Inlet 2.19 DateTime::TimeZone::America::Recife 2.19 DateTime::TimeZone::America::Regina 2.19 DateTime::TimeZone::America::Resolute 2.19 DateTime::TimeZone::America::Rio_Branco 2.19 DateTime::TimeZone::America::Santarem 2.19 DateTime::TimeZone::America::Santiago 2.19 DateTime::TimeZone::America::Santo_Domingo 2.19 DateTime::TimeZone::America::Sao_Paulo 2.19 DateTime::TimeZone::America::Scoresbysund 2.19 DateTime::TimeZone::America::Sitka 2.19 DateTime::TimeZone::America::St_Johns 2.19 DateTime::TimeZone::America::Swift_Current 2.19 DateTime::TimeZone::America::Tegucigalpa 2.19 DateTime::TimeZone::America::Thule 2.19 DateTime::TimeZone::America::Thunder_Bay 2.19 DateTime::TimeZone::America::Tijuana 2.19 DateTime::TimeZone::America::Toronto 2.19 DateTime::TimeZone::America::Vancouver 2.19 DateTime::TimeZone::America::Whitehorse 2.19 DateTime::TimeZone::America::Winnipeg 2.19 DateTime::TimeZone::America::Yakutat 2.19 DateTime::TimeZone::America::Yellowknife 2.19 DateTime::TimeZone::Antarctica::Casey 2.19 DateTime::TimeZone::Antarctica::Davis 2.19 DateTime::TimeZone::Antarctica::DumontDUrville 2.19 DateTime::TimeZone::Antarctica::Macquarie 2.19 DateTime::TimeZone::Antarctica::Mawson 2.19 DateTime::TimeZone::Antarctica::Palmer 2.19 DateTime::TimeZone::Antarctica::Rothera 2.19 DateTime::TimeZone::Antarctica::Syowa 2.19 DateTime::TimeZone::Antarctica::Troll 2.19 DateTime::TimeZone::Antarctica::Vostok 2.19 DateTime::TimeZone::Asia::Almaty 2.19 DateTime::TimeZone::Asia::Amman 2.19 DateTime::TimeZone::Asia::Anadyr 2.19 DateTime::TimeZone::Asia::Aqtau 2.19 DateTime::TimeZone::Asia::Aqtobe 2.19 DateTime::TimeZone::Asia::Ashgabat 2.19 DateTime::TimeZone::Asia::Atyrau 2.19 DateTime::TimeZone::Asia::Baghdad 2.19 DateTime::TimeZone::Asia::Baku 2.19 DateTime::TimeZone::Asia::Bangkok 2.19 DateTime::TimeZone::Asia::Barnaul 2.19 DateTime::TimeZone::Asia::Beirut 2.19 DateTime::TimeZone::Asia::Bishkek 2.19 DateTime::TimeZone::Asia::Brunei 2.19 DateTime::TimeZone::Asia::Chita 2.19 DateTime::TimeZone::Asia::Choibalsan 2.19 DateTime::TimeZone::Asia::Colombo 2.19 DateTime::TimeZone::Asia::Damascus 2.19 DateTime::TimeZone::Asia::Dhaka 2.19 DateTime::TimeZone::Asia::Dili 2.19 DateTime::TimeZone::Asia::Dubai 2.19 DateTime::TimeZone::Asia::Dushanbe 2.19 DateTime::TimeZone::Asia::Famagusta 2.19 DateTime::TimeZone::Asia::Gaza 2.19 DateTime::TimeZone::Asia::Hebron 2.19 DateTime::TimeZone::Asia::Ho_Chi_Minh 2.19 DateTime::TimeZone::Asia::Hong_Kong 2.19 DateTime::TimeZone::Asia::Hovd 2.19 DateTime::TimeZone::Asia::Irkutsk 2.19 DateTime::TimeZone::Asia::Jakarta 2.19 DateTime::TimeZone::Asia::Jayapura 2.19 DateTime::TimeZone::Asia::Jerusalem 2.19 DateTime::TimeZone::Asia::Kabul 2.19 DateTime::TimeZone::Asia::Kamchatka 2.19 DateTime::TimeZone::Asia::Karachi 2.19 DateTime::TimeZone::Asia::Kathmandu 2.19 DateTime::TimeZone::Asia::Khandyga 2.19 DateTime::TimeZone::Asia::Kolkata 2.19 DateTime::TimeZone::Asia::Krasnoyarsk 2.19 DateTime::TimeZone::Asia::Kuala_Lumpur 2.19 DateTime::TimeZone::Asia::Kuching 2.19 DateTime::TimeZone::Asia::Macau 2.19 DateTime::TimeZone::Asia::Magadan 2.19 DateTime::TimeZone::Asia::Makassar 2.19 DateTime::TimeZone::Asia::Manila 2.19 DateTime::TimeZone::Asia::Nicosia 2.19 DateTime::TimeZone::Asia::Novokuznetsk 2.19 DateTime::TimeZone::Asia::Novosibirsk 2.19 DateTime::TimeZone::Asia::Omsk 2.19 DateTime::TimeZone::Asia::Oral 2.19 DateTime::TimeZone::Asia::Pontianak 2.19 DateTime::TimeZone::Asia::Pyongyang 2.19 DateTime::TimeZone::Asia::Qatar 2.19 DateTime::TimeZone::Asia::Qyzylorda 2.19 DateTime::TimeZone::Asia::Riyadh 2.19 DateTime::TimeZone::Asia::Sakhalin 2.19 DateTime::TimeZone::Asia::Samarkand 2.19 DateTime::TimeZone::Asia::Seoul 2.19 DateTime::TimeZone::Asia::Shanghai 2.19 DateTime::TimeZone::Asia::Singapore 2.19 DateTime::TimeZone::Asia::Srednekolymsk 2.19 DateTime::TimeZone::Asia::Taipei 2.19 DateTime::TimeZone::Asia::Tashkent 2.19 DateTime::TimeZone::Asia::Tbilisi 2.19 DateTime::TimeZone::Asia::Tehran 2.19 DateTime::TimeZone::Asia::Thimphu 2.19 DateTime::TimeZone::Asia::Tokyo 2.19 DateTime::TimeZone::Asia::Tomsk 2.19 DateTime::TimeZone::Asia::Ulaanbaatar 2.19 DateTime::TimeZone::Asia::Urumqi 2.19 DateTime::TimeZone::Asia::Ust_Nera 2.19 DateTime::TimeZone::Asia::Vladivostok 2.19 DateTime::TimeZone::Asia::Yakutsk 2.19 DateTime::TimeZone::Asia::Yangon 2.19 DateTime::TimeZone::Asia::Yekaterinburg 2.19 DateTime::TimeZone::Asia::Yerevan 2.19 DateTime::TimeZone::Atlantic::Azores 2.19 DateTime::TimeZone::Atlantic::Bermuda 2.19 DateTime::TimeZone::Atlantic::Canary 2.19 DateTime::TimeZone::Atlantic::Cape_Verde 2.19 DateTime::TimeZone::Atlantic::Faroe 2.19 DateTime::TimeZone::Atlantic::Madeira 2.19 DateTime::TimeZone::Atlantic::Reykjavik 2.19 DateTime::TimeZone::Atlantic::South_Georgia 2.19 DateTime::TimeZone::Atlantic::Stanley 2.19 DateTime::TimeZone::Australia::Adelaide 2.19 DateTime::TimeZone::Australia::Brisbane 2.19 DateTime::TimeZone::Australia::Broken_Hill 2.19 DateTime::TimeZone::Australia::Currie 2.19 DateTime::TimeZone::Australia::Darwin 2.19 DateTime::TimeZone::Australia::Eucla 2.19 DateTime::TimeZone::Australia::Hobart 2.19 DateTime::TimeZone::Australia::Lindeman 2.19 DateTime::TimeZone::Australia::Lord_Howe 2.19 DateTime::TimeZone::Australia::Melbourne 2.19 DateTime::TimeZone::Australia::Perth 2.19 DateTime::TimeZone::Australia::Sydney 2.19 DateTime::TimeZone::Catalog 2.19 DateTime::TimeZone::CET 2.19 DateTime::TimeZone::CST6CDT 2.19 DateTime::TimeZone::EET 2.19 DateTime::TimeZone::EST 2.19 DateTime::TimeZone::EST5EDT 2.19 DateTime::TimeZone::Europe::Amsterdam 2.19 DateTime::TimeZone::Europe::Andorra 2.19 DateTime::TimeZone::Europe::Astrakhan 2.19 DateTime::TimeZone::Europe::Athens 2.19 DateTime::TimeZone::Europe::Belgrade 2.19 DateTime::TimeZone::Europe::Berlin 2.19 DateTime::TimeZone::Europe::Brussels 2.19 DateTime::TimeZone::Europe::Bucharest 2.19 DateTime::TimeZone::Europe::Budapest 2.19 DateTime::TimeZone::Europe::Chisinau 2.19 DateTime::TimeZone::Europe::Copenhagen 2.19 DateTime::TimeZone::Europe::Dublin 2.19 DateTime::TimeZone::Europe::Gibraltar 2.19 DateTime::TimeZone::Europe::Helsinki 2.19 DateTime::TimeZone::Europe::Istanbul 2.19 DateTime::TimeZone::Europe::Kaliningrad 2.19 DateTime::TimeZone::Europe::Kiev 2.19 DateTime::TimeZone::Europe::Kirov 2.19 DateTime::TimeZone::Europe::Lisbon 2.19 DateTime::TimeZone::Europe::London 2.19 DateTime::TimeZone::Europe::Luxembourg 2.19 DateTime::TimeZone::Europe::Madrid 2.19 DateTime::TimeZone::Europe::Malta 2.19 DateTime::TimeZone::Europe::Minsk 2.19 DateTime::TimeZone::Europe::Monaco 2.19 DateTime::TimeZone::Europe::Moscow 2.19 DateTime::TimeZone::Europe::Oslo 2.19 DateTime::TimeZone::Europe::Paris 2.19 DateTime::TimeZone::Europe::Prague 2.19 DateTime::TimeZone::Europe::Riga 2.19 DateTime::TimeZone::Europe::Rome 2.19 DateTime::TimeZone::Europe::Samara 2.19 DateTime::TimeZone::Europe::Saratov 2.19 DateTime::TimeZone::Europe::Simferopol 2.19 DateTime::TimeZone::Europe::Sofia 2.19 DateTime::TimeZone::Europe::Stockholm 2.19 DateTime::TimeZone::Europe::Tallinn 2.19 DateTime::TimeZone::Europe::Tirane 2.19 DateTime::TimeZone::Europe::Ulyanovsk 2.19 DateTime::TimeZone::Europe::Uzhgorod 2.19 DateTime::TimeZone::Europe::Vienna 2.19 DateTime::TimeZone::Europe::Vilnius 2.19 DateTime::TimeZone::Europe::Volgograd 2.19 DateTime::TimeZone::Europe::Warsaw 2.19 DateTime::TimeZone::Europe::Zaporozhye 2.19 DateTime::TimeZone::Europe::Zurich 2.19 DateTime::TimeZone::Floating 2.19 DateTime::TimeZone::HST 2.19 DateTime::TimeZone::Indian::Chagos 2.19 DateTime::TimeZone::Indian::Christmas 2.19 DateTime::TimeZone::Indian::Cocos 2.19 DateTime::TimeZone::Indian::Kerguelen 2.19 DateTime::TimeZone::Indian::Mahe 2.19 DateTime::TimeZone::Indian::Maldives 2.19 DateTime::TimeZone::Indian::Mauritius 2.19 DateTime::TimeZone::Indian::Reunion 2.19 DateTime::TimeZone::Local 2.19 DateTime::TimeZone::Local::Android 2.19 DateTime::TimeZone::Local::Unix 2.19 DateTime::TimeZone::Local::VMS 2.19 DateTime::TimeZone::MET 2.19 DateTime::TimeZone::MST 2.19 DateTime::TimeZone::MST7MDT 2.19 DateTime::TimeZone::OffsetOnly 2.19 DateTime::TimeZone::OlsonDB 2.19 DateTime::TimeZone::OlsonDB::Change 2.19 DateTime::TimeZone::OlsonDB::Observance 2.19 DateTime::TimeZone::OlsonDB::Rule 2.19 DateTime::TimeZone::OlsonDB::Zone 2.19 DateTime::TimeZone::Pacific::Apia 2.19 DateTime::TimeZone::Pacific::Auckland 2.19 DateTime::TimeZone::Pacific::Bougainville 2.19 DateTime::TimeZone::Pacific::Chatham 2.19 DateTime::TimeZone::Pacific::Chuuk 2.19 DateTime::TimeZone::Pacific::Easter 2.19 DateTime::TimeZone::Pacific::Efate 2.19 DateTime::TimeZone::Pacific::Enderbury 2.19 DateTime::TimeZone::Pacific::Fakaofo 2.19 DateTime::TimeZone::Pacific::Fiji 2.19 DateTime::TimeZone::Pacific::Funafuti 2.19 DateTime::TimeZone::Pacific::Galapagos 2.19 DateTime::TimeZone::Pacific::Gambier 2.19 DateTime::TimeZone::Pacific::Guadalcanal 2.19 DateTime::TimeZone::Pacific::Guam 2.19 DateTime::TimeZone::Pacific::Honolulu 2.19 DateTime::TimeZone::Pacific::Kiritimati 2.19 DateTime::TimeZone::Pacific::Kosrae 2.19 DateTime::TimeZone::Pacific::Kwajalein 2.19 DateTime::TimeZone::Pacific::Majuro 2.19 DateTime::TimeZone::Pacific::Marquesas 2.19 DateTime::TimeZone::Pacific::Nauru 2.19 DateTime::TimeZone::Pacific::Niue 2.19 DateTime::TimeZone::Pacific::Norfolk 2.19 DateTime::TimeZone::Pacific::Noumea 2.19 DateTime::TimeZone::Pacific::Pago_Pago 2.19 DateTime::TimeZone::Pacific::Palau 2.19 DateTime::TimeZone::Pacific::Pitcairn 2.19 DateTime::TimeZone::Pacific::Pohnpei 2.19 DateTime::TimeZone::Pacific::Port_Moresby 2.19 DateTime::TimeZone::Pacific::Rarotonga 2.19 DateTime::TimeZone::Pacific::Tahiti 2.19 DateTime::TimeZone::Pacific::Tarawa 2.19 DateTime::TimeZone::Pacific::Tongatapu 2.19 DateTime::TimeZone::Pacific::Wake 2.19 DateTime::TimeZone::Pacific::Wallis 2.19 DateTime::TimeZone::PST8PDT 2.19 DateTime::TimeZone::UTC 2.19 DateTime::TimeZone::WET 2.19 DateTime::Tiny 1.07 DateTime::Types 1.49 DateTimeX::Easy 0.089 DB unknown DBD::DBM 0.08 DBD::ExampleP 12.014311 DBD::File 0.44 DBD::Gofer 0.015327 DBD::Gofer::Policy::Base 0.010088 DBD::Gofer::Policy::classic 0.010088 DBD::Gofer::Policy::pedantic 0.010088 DBD::Gofer::Policy::rush 0.010088 DBD::Gofer::Transport::Base 0.014121 DBD::Gofer::Transport::corostream unknown DBD::Gofer::Transport::null 0.010088 DBD::Gofer::Transport::pipeone 0.010088 DBD::Gofer::Transport::stream 0.014599 DBD::Mem 0.001 DBD::mysql 4.046 DBD::mysql::GetInfo unknown DBD::NullP 12.014715 DBD::Proxy 0.2004 DBD::Sponge 12.010003 DBD::SQLite 1.58 DBD::SQLite::Constants unknown DBD::SQLite::VirtualTable 1.58 DBD::SQLite::VirtualTable::FileContent unknown DBD::SQLite::VirtualTable::PerlData unknown DBI unknown DBI::Const::GetInfo::ANSI 2.008697 DBI::Const::GetInfo::ODBC 2.011374 DBI::Const::GetInfoReturn 2.008697 DBI::Const::GetInfoType 2.008697 DBI::DBD 12.015129 DBI::DBD::Metadata 2.014214 DBI::DBD::SqlEngine 0.06 DBI::FAQ 1.014935 DBI::Gofer::Execute 0.014283 DBI::Gofer::Request 0.012537 DBI::Gofer::Response 0.011566 DBI::Gofer::Serializer::Base 0.009950 DBI::Gofer::Serializer::DataDumper 0.009950 DBI::Gofer::Serializer::Storable 0.015586 DBI::Gofer::Transport::Base 0.012537 DBI::Gofer::Transport::pipeone 0.012537 DBI::Gofer::Transport::stream 0.012537 DBI::Profile 2.015065 DBI::ProfileData 2.010008 DBI::ProfileDumper 2.015325 DBI::ProfileDumper::Apache 2.014121 DBI::ProfileSubs 0.009396 DBI::ProxyServer 0.3005 DBI::SQL::Nano 1.015544 DBI::Util::_accessor 0.009479 DBI::Util::CacheMemory 0.010315 DBIx::Class 0.082841 DBIx::Class::AccessorGroup unknown DBIx::Class::Admin unknown DBIx::Class::CDBICompat unknown DBIx::Class::CDBICompat::Iterator unknown DBIx::Class::CDBICompat::SQLTransformer unknown DBIx::Class::CDBICompat::Tied::ColumnValue unknown DBIx::Class::Core unknown DBIx::Class::Cursor unknown DBIx::Class::Cursor::Cached 1.001004 DBIx::Class::DB unknown DBIx::Class::Exception unknown DBIx::Class::FilterColumn unknown DBIx::Class::InflateColumn unknown DBIx::Class::InflateColumn::DateTime unknown DBIx::Class::InflateColumn::File unknown DBIx::Class::Optional::Dependencies unknown DBIx::Class::Ordered unknown DBIx::Class::PK unknown DBIx::Class::PK::Auto unknown DBIx::Class::Relationship unknown DBIx::Class::Relationship::Base unknown DBIx::Class::ResultClass::HashRefInflator unknown DBIx::Class::ResultSet unknown DBIx::Class::ResultSetColumn unknown DBIx::Class::ResultSetManager unknown DBIx::Class::ResultSource unknown DBIx::Class::ResultSource::Table unknown DBIx::Class::ResultSource::View unknown DBIx::Class::ResultSourceHandle unknown DBIx::Class::ResultSourceProxy::Table unknown DBIx::Class::Row unknown DBIx::Class::Schema unknown DBIx::Class::Schema::Loader 0.07049 DBIx::Class::Schema::Loader::Base 0.07049 DBIx::Class::Schema::Loader::Column unknown DBIx::Class::Schema::Loader::DBI 0.07049 DBIx::Class::Schema::Loader::DBI::ADO 0.07049 DBIx::Class::Schema::Loader::DBI::ADO::Microsoft_SQL_Server 0.07049 DBIx::Class::Schema::Loader::DBI::ADO::MS_Jet 0.07049 DBIx::Class::Schema::Loader::DBI::Component::QuotedDefault 0.07049 DBIx::Class::Schema::Loader::DBI::DB2 0.07049 DBIx::Class::Schema::Loader::DBI::Firebird 0.07049 DBIx::Class::Schema::Loader::DBI::Informix 0.07049 DBIx::Class::Schema::Loader::DBI::InterBase 0.07049 DBIx::Class::Schema::Loader::DBI::MSSQL 0.07049 DBIx::Class::Schema::Loader::DBI::mysql 0.07049 DBIx::Class::Schema::Loader::DBI::ODBC 0.07049 DBIx::Class::Schema::Loader::DBI::ODBC::ACCESS 0.07049 DBIx::Class::Schema::Loader::DBI::ODBC::Firebird 0.07049 DBIx::Class::Schema::Loader::DBI::ODBC::Microsoft_SQL_Server 0.07049 DBIx::Class::Schema::Loader::DBI::ODBC::SQL_Anywhere 0.07049 DBIx::Class::Schema::Loader::DBI::Oracle 0.07049 DBIx::Class::Schema::Loader::DBI::Pg 0.07049 DBIx::Class::Schema::Loader::DBI::SQLAnywhere 0.07049 DBIx::Class::Schema::Loader::DBI::SQLite 0.07049 DBIx::Class::Schema::Loader::DBI::Sybase 0.07049 DBIx::Class::Schema::Loader::DBI::Sybase::Common 0.07049 DBIx::Class::Schema::Loader::DBI::Sybase::Microsoft_SQL_Server 0.07049 DBIx::Class::Schema::Loader::DBI::Writing 0.07049 DBIx::Class::Schema::Loader::DBObject unknown DBIx::Class::Schema::Loader::DBObject::Informix unknown DBIx::Class::Schema::Loader::DBObject::Sybase unknown DBIx::Class::Schema::Loader::Optional::Dependencies unknown DBIx::Class::Schema::Loader::RelBuilder 0.07049 DBIx::Class::Schema::Loader::RelBuilder::Compat::v0_040 0.07049 DBIx::Class::Schema::Loader::RelBuilder::Compat::v0_05 0.07049 DBIx::Class::Schema::Loader::RelBuilder::Compat::v0_06 0.07049 DBIx::Class::Schema::Loader::RelBuilder::Compat::v0_07 0.07049 DBIx::Class::Schema::Loader::Table unknown DBIx::Class::Schema::Loader::Table::Informix unknown DBIx::Class::Schema::Loader::Table::Sybase unknown DBIx::Class::Serialize::Storable unknown DBIx::Class::SQLMaker unknown DBIx::Class::SQLMaker::LimitDialects unknown DBIx::Class::SQLMaker::OracleJoins unknown DBIx::Class::StartupCheck unknown DBIx::Class::Storage unknown DBIx::Class::Storage::DBI unknown DBIx::Class::Storage::DBI::ACCESS unknown DBIx::Class::Storage::DBI::ADO unknown DBIx::Class::Storage::DBI::ADO::Microsoft_SQL_Server unknown DBIx::Class::Storage::DBI::ADO::Microsoft_SQL_Server::Cursor unknown DBIx::Class::Storage::DBI::ADO::MS_Jet unknown DBIx::Class::Storage::DBI::ADO::MS_Jet::Cursor unknown DBIx::Class::Storage::DBI::AutoCast unknown DBIx::Class::Storage::DBI::Cursor unknown DBIx::Class::Storage::DBI::DB2 unknown DBIx::Class::Storage::DBI::Firebird unknown DBIx::Class::Storage::DBI::Firebird::Common unknown DBIx::Class::Storage::DBI::IdentityInsert unknown DBIx::Class::Storage::DBI::Informix unknown DBIx::Class::Storage::DBI::InterBase unknown DBIx::Class::Storage::DBI::MSSQL unknown DBIx::Class::Storage::DBI::mysql unknown DBIx::Class::Storage::DBI::NoBindVars unknown DBIx::Class::Storage::DBI::ODBC unknown DBIx::Class::Storage::DBI::ODBC::ACCESS unknown DBIx::Class::Storage::DBI::ODBC::DB2_400_SQL unknown DBIx::Class::Storage::DBI::ODBC::Firebird unknown DBIx::Class::Storage::DBI::ODBC::Microsoft_SQL_Server unknown DBIx::Class::Storage::DBI::ODBC::SQL_Anywhere unknown DBIx::Class::Storage::DBI::Oracle unknown DBIx::Class::Storage::DBI::Oracle::Generic unknown DBIx::Class::Storage::DBI::Oracle::WhereJoins unknown DBIx::Class::Storage::DBI::Pg unknown DBIx::Class::Storage::DBI::Replicated unknown DBIx::Class::Storage::DBI::Replicated::Balancer unknown DBIx::Class::Storage::DBI::Replicated::Balancer::First unknown DBIx::Class::Storage::DBI::Replicated::Balancer::Random unknown DBIx::Class::Storage::DBI::Replicated::Pool unknown DBIx::Class::Storage::DBI::Replicated::Replicant unknown DBIx::Class::Storage::DBI::Replicated::WithDSN unknown DBIx::Class::Storage::DBI::SQLAnywhere unknown DBIx::Class::Storage::DBI::SQLAnywhere::Cursor unknown DBIx::Class::Storage::DBI::SQLite unknown DBIx::Class::Storage::DBI::Sybase unknown DBIx::Class::Storage::DBI::Sybase::ASE unknown DBIx::Class::Storage::DBI::Sybase::ASE::NoBindVars unknown DBIx::Class::Storage::DBI::Sybase::FreeTDS unknown DBIx::Class::Storage::DBI::Sybase::Microsoft_SQL_Server unknown DBIx::Class::Storage::DBI::Sybase::Microsoft_SQL_Server::NoBindVars unknown DBIx::Class::Storage::DBI::Sybase::MSSQL unknown DBIx::Class::Storage::DBI::UniqueIdentifier unknown DBIx::Class::Storage::Debug::PrettyPrint unknown DBIx::Class::Storage::Statistics unknown DBIx::Class::Storage::TxnScopeGuard unknown DBIx::Class::UTF8Columns unknown DBIx::Connector 0.56 DBIx::Connector::Driver 0.56 DBIx::Connector::Driver::Firebird 0.56 DBIx::Connector::Driver::MSSQL 0.56 DBIx::Connector::Driver::mysql 0.56 DBIx::Connector::Driver::Oracle 0.56 DBIx::Connector::Driver::Pg 0.56 DBIx::Connector::Driver::SQLite 0.56 DDP unknown Declare::Constraints::Simple 0.03 Declare::Constraints::Simple::Library unknown Declare::Constraints::Simple::Library::Array unknown Declare::Constraints::Simple::Library::Base unknown Declare::Constraints::Simple::Library::Exportable unknown Declare::Constraints::Simple::Library::General unknown Declare::Constraints::Simple::Library::Hash unknown Declare::Constraints::Simple::Library::Numerical unknown Declare::Constraints::Simple::Library::OO unknown Declare::Constraints::Simple::Library::Operators unknown Declare::Constraints::Simple::Library::Referencial unknown Declare::Constraints::Simple::Library::Scalar unknown Declare::Constraints::Simple::Result unknown Devel::AssertC99 unknown Devel::AssertOS 1.21 Devel::AssertOS::AIX 1.2 Devel::AssertOS::Amiga 1.2 Devel::AssertOS::Android 1.2 Devel::AssertOS::Apple 1.3 Devel::AssertOS::BeOS 1.4 Devel::AssertOS::Bitrig 1.0 Devel::AssertOS::BSDOS 1.2 Devel::AssertOS::Cygwin 1.3 Devel::AssertOS::DEC 1.4 Devel::AssertOS::DGUX 1.2 Devel::AssertOS::DragonflyBSD 1.2 Devel::AssertOS::Dynix 1.2 Devel::AssertOS::EBCDIC 1.0 Devel::AssertOS::FreeBSD 1.2 Devel::AssertOS::GNUkFreeBSD 1.1 Devel::AssertOS::Haiku 1.1 Devel::AssertOS::HPUX 1.2 Devel::AssertOS::Hurd 1.0 Devel::AssertOS::Interix 1.2 Devel::AssertOS::iOS 1.0 Devel::AssertOS::Irix 1.2 Devel::AssertOS::Linux 1.3 Devel::AssertOS::Linux::Debian 1.0 Devel::AssertOS::Linux::v2_6 1.3 Devel::AssertOS::MachTen 1.2 Devel::AssertOS::MacOSclassic 1.2 Devel::AssertOS::MacOSX 1.2 Devel::AssertOS::MacOSX::v10_0 1.0 Devel::AssertOS::MacOSX::v10_1 1.0 Devel::AssertOS::MacOSX::v10_10 1.0 Devel::AssertOS::MacOSX::v10_11 1.0 Devel::AssertOS::MacOSX::v10_12 1.0 Devel::AssertOS::MacOSX::v10_2 1.0 Devel::AssertOS::MacOSX::v10_3 1.0 Devel::AssertOS::MacOSX::v10_4 1.4 Devel::AssertOS::MacOSX::v10_5 1.0 Devel::AssertOS::MacOSX::v10_6 1.0 Devel::AssertOS::MacOSX::v10_7 1.0 Devel::AssertOS::MacOSX::v10_8 1.0 Devel::AssertOS::MacOSX::v10_9 1.0 Devel::AssertOS::MicrosoftWindows 1.3 Devel::AssertOS::MidnightBSD 1.1 Devel::AssertOS::Minix 1.0 Devel::AssertOS::MirOSBSD 1.2 Devel::AssertOS::MPEiX 1.2 Devel::AssertOS::MSDOS 1.2 Devel::AssertOS::MSWin32 1.3 Devel::AssertOS::NetBSD 1.2 Devel::AssertOS::Netware 1.2 Devel::AssertOS::NeXT 1.2 Devel::AssertOS::OpenBSD 1.2 Devel::AssertOS::OS2 1.1 Devel::AssertOS::OS390 1.2 Devel::AssertOS::OS400 1.2 Devel::AssertOS::OSF 1.2 Devel::AssertOS::OSFeatures::POSIXShellRedirection 1.4 Devel::AssertOS::POSIXBC 1.2 Devel::AssertOS::QNX 1.2 Devel::AssertOS::QNX::Neutrino 1.1 Devel::AssertOS::QNX::v4 1.1 Devel::AssertOS::Realtime 1.2 Devel::AssertOS::RISCOS 1.2 Devel::AssertOS::SCO 1.2 Devel::AssertOS::Solaris 1.2 Devel::AssertOS::Sun 1.3 Devel::AssertOS::SunOS 1.2 Devel::AssertOS::SysVr4 1.2 Devel::AssertOS::SysVr5 1.2 Devel::AssertOS::Unicos 1.2 Devel::AssertOS::Unix 1.6 Devel::AssertOS::VMESA 1.2 Devel::AssertOS::VMS 1.2 Devel::AssertOS::VOS 1.2 Devel::Caller 2.06 Devel::CheckBin 0.04 Devel::CheckCompiler 0.07 Devel::CheckLib 1.13 Devel::CheckOS 1.81 Devel::Confess 0.009004 Devel::Confess::_Util unknown Devel::Confess::Builtin 0.009004 Devel::Confess::Source unknown Devel::Cover 1.30 Devel::Cover::Annotation::Git 1.30 Devel::Cover::Annotation::Random 1.30 Devel::Cover::Annotation::Svk 1.30 Devel::Cover::Branch 1.30 Devel::Cover::Collection 1.30 Devel::Cover::Condition 1.30 Devel::Cover::Condition_and_2 1.30 Devel::Cover::Condition_and_3 1.30 Devel::Cover::Condition_or_2 1.30 Devel::Cover::Condition_or_3 1.30 Devel::Cover::Condition_xor_4 1.30 Devel::Cover::Criterion 1.30 Devel::Cover::DB 1.30 Devel::Cover::DB::Criterion 1.30 Devel::Cover::DB::Digests 1.30 Devel::Cover::DB::File 1.30 Devel::Cover::DB::IO 1.30 Devel::Cover::DB::IO::Base 1.30 Devel::Cover::DB::IO::JSON 1.30 Devel::Cover::DB::IO::Sereal 1.30 Devel::Cover::DB::IO::Storable 1.30 Devel::Cover::DB::Structure 1.30 Devel::Cover::Html_Common 1.30 Devel::Cover::Inc 1.30 Devel::Cover::Op 1.30 Devel::Cover::Pod 1.30 Devel::Cover::Report::Compilation 1.30 Devel::Cover::Report::Html 1.30 Devel::Cover::Report::Html_basic 1.30 Devel::Cover::Report::Html_minimal 1.30 Devel::Cover::Report::Html_subtle 1.30 Devel::Cover::Report::Json 1.30 Devel::Cover::Report::Sort 1.30 Devel::Cover::Report::Text 1.30 Devel::Cover::Report::Text2 1.30 Devel::Cover::Report::Vim 1.30 Devel::Cover::Statement 1.30 Devel::Cover::Subroutine 1.30 Devel::Cover::Test 1.30 Devel::Cover::Time 1.30 Devel::Cover::Util 1.30 Devel::Cover::Web 1.30 Devel::Cycle 1.12 Devel::Declare 0.006019 Devel::Declare::Context::Simple 0.006019 Devel::Declare::MethodInstaller::Simple 0.006019 Devel::Dwarn unknown Devel::FindPerl 0.014 Devel::GlobalDestruction 0.14 Devel::GlobalPhase 0.003003 Devel::GraphVizProf 2.24 Devel::Hide 0.0010 Devel::InnerPackage 0.4 Devel::Leak 0.03 Devel::LexAlias 0.05 Devel::MAT 0.36 Devel::MAT::Context 0.36 Devel::MAT::Dumper 0.36 Devel::MAT::Dumpfile 0.36 Devel::MAT::Graph 0.36 Devel::MAT::InternalTools 0.36 Devel::MAT::SV 0.36 Devel::MAT::Tool 0.36 Devel::MAT::Tool::Callstack 0.36 Devel::MAT::Tool::Count 0.36 Devel::MAT::Tool::Find 0.36 Devel::MAT::Tool::Identify 0.36 Devel::MAT::Tool::Inrefs 0.36 Devel::MAT::Tool::IO 0.36 Devel::MAT::Tool::Outrefs 0.36 Devel::MAT::Tool::Reachability 0.36 Devel::MAT::Tool::Roots 0.36 Devel::MAT::Tool::Show 0.36 Devel::MAT::Tool::Sizes 0.36 Devel::MAT::Tool::Symbols 0.36 Devel::NYTProf 6.06 Devel::NYTProf::Apache 4.00 Devel::NYTProf::Constants unknown Devel::NYTProf::Core 6.06 Devel::NYTProf::Data 4.02 Devel::NYTProf::FileHandle unknown Devel::NYTProf::FileInfo unknown Devel::NYTProf::Reader 4.06 Devel::NYTProf::ReadStream 4.00 Devel::NYTProf::Run unknown Devel::NYTProf::SubCallInfo unknown Devel::NYTProf::SubInfo unknown Devel::NYTProf::Util 4.00 Devel::OverloadInfo 0.005 Devel::PartialDump 0.20 Devel::PatchPerl 1.52 Devel::PatchPerl::Hints 1.52 Devel::PatchPerl::Plugin 1.52 Devel::REPL 1.003028 Devel::REPL::Error 1.003028 Devel::REPL::Meta::Plugin 1.003028 Devel::REPL::Plugin 1.003028 Devel::REPL::Plugin::B::Concise 1.003028 Devel::REPL::Plugin::Carp::REPL 0.18 Devel::REPL::Plugin::Colors 1.003028 Devel::REPL::Plugin::Commands 1.003028 Devel::REPL::Plugin::Completion 1.003028 Devel::REPL::Plugin::CompletionDriver::Globals 1.003028 Devel::REPL::Plugin::CompletionDriver::INC 1.003028 Devel::REPL::Plugin::CompletionDriver::Keywords 1.003028 Devel::REPL::Plugin::CompletionDriver::LexEnv 1.003028 Devel::REPL::Plugin::CompletionDriver::Methods 1.003028 Devel::REPL::Plugin::CompletionDriver::Turtles 1.003028 Devel::REPL::Plugin::DDC 1.003028 Devel::REPL::Plugin::DDS 1.003028 Devel::REPL::Plugin::DumpHistory 1.003028 Devel::REPL::Plugin::FancyPrompt 1.003028 Devel::REPL::Plugin::FindVariable 1.003028 Devel::REPL::Plugin::History 1.003028 Devel::REPL::Plugin::Interrupt 1.003028 Devel::REPL::Plugin::LexEnv 1.003028 Devel::REPL::Plugin::MultiLine::PPI 1.003028 Devel::REPL::Plugin::Nopaste 1.003028 Devel::REPL::Plugin::OutputCache 1.003028 Devel::REPL::Plugin::Packages 1.003028 Devel::REPL::Plugin::Peek 1.003028 Devel::REPL::Plugin::PPI 1.003028 Devel::REPL::Plugin::ReadLineHistory 1.003028 Devel::REPL::Plugin::Refresh 1.003028 Devel::REPL::Plugin::Selenium 1.36 Devel::REPL::Plugin::ShowClass 1.003028 Devel::REPL::Plugin::Timing 1.003028 Devel::REPL::Plugin::Turtles 1.003028 Devel::REPL::Profile 1.003028 Devel::REPL::Profile::Default 1.003028 Devel::REPL::Profile::Minimal 1.003028 Devel::REPL::Profile::Standard 1.003028 Devel::REPL::Script 1.003028 Devel::StackTrace 2.03 Devel::StackTrace::AsHTML 0.15 Devel::StackTrace::Frame 2.03 Devel::StackTrace::WithLexicals 2.01 Devel::StackTrace::WithLexicals::Frame unknown Devel::Symdump 2.18 Devel::Symdump::Export unknown Devel::TypeTiny::Perl56Compat 1.002002 Devel::TypeTiny::Perl58Compat 1.002002 Digest::HMAC 1.03 Digest::HMAC_MD5 1.01 Digest::HMAC_SHA1 1.03 Digest::JHash 0.10 Digest::Perl::MD5 1.9 Digest::SHA1 2.13 Dist::CheckConflicts 0.11 Dist::Metadata 0.927 Dist::Metadata::Archive 0.927 Dist::Metadata::Dir 0.927 Dist::Metadata::Dist 0.927 Dist::Metadata::Struct 0.927 Dist::Metadata::Tar 0.927 Dist::Metadata::Zip 0.927 Dist::Zilla::PluginBundle::Example unknown Email::Abstract 3.008 Email::Abstract::EmailMIME 3.008 Email::Abstract::EmailSimple 3.008 Email::Abstract::MailInternet 3.008 Email::Abstract::MailMessage 3.008 Email::Abstract::MIMEEntity 3.008 Email::Abstract::Plugin 3.008 Email::Address 1.909 Email::Address::XS 1.04 Email::Date::Format 1.005 Email::MessageID 1.406 Email::MIME 1.946 Email::MIME::ContentType 1.022 Email::MIME::Creator 1.946 Email::MIME::Encode 1.946 Email::MIME::Encodings 1.315 Email::MIME::Header 1.946 Email::MIME::Header::AddressList 1.946 Email::MIME::Kit 3.000006 Email::MIME::Kit::Assembler::Standard 3.000006 Email::MIME::Kit::KitReader::Dir 3.000006 Email::MIME::Kit::ManifestReader::JSON 3.000006 Email::MIME::Kit::ManifestReader::YAML 3.000006 Email::MIME::Kit::Renderer::TestRenderer 3.000006 Email::MIME::Kit::Role::Assembler 3.000006 Email::MIME::Kit::Role::Component 3.000006 Email::MIME::Kit::Role::KitReader 3.000006 Email::MIME::Kit::Role::ManifestDesugarer 3.000006 Email::MIME::Kit::Role::ManifestReader 3.000006 Email::MIME::Kit::Role::Renderer 3.000006 Email::MIME::Kit::Role::Validator 3.000006 Email::MIME::Modifier 1.946 Email::Sender 1.300031 Email::Sender::Failure 1.300031 Email::Sender::Failure::Multi 1.300031 Email::Sender::Failure::Permanent 1.300031 Email::Sender::Failure::Temporary 1.300031 Email::Sender::Manual 1.300031 Email::Sender::Manual::QuickStart 1.300031 Email::Sender::Role::CommonSending 1.300031 Email::Sender::Role::HasMessage 1.300031 Email::Sender::Simple 1.300031 Email::Sender::Success 1.300031 Email::Sender::Success::Partial 1.300031 Email::Sender::Transport 1.300031 Email::Sender::Transport::DevNull 1.300031 Email::Sender::Transport::Failable 1.300031 Email::Sender::Transport::Maildir 1.300031 Email::Sender::Transport::Mbox 1.300031 Email::Sender::Transport::Print 1.300031 Email::Sender::Transport::Sendmail 1.300031 Email::Sender::Transport::SMTP 1.300031 Email::Sender::Transport::SMTP::Persistent 1.300031 Email::Sender::Transport::Test 1.300031 Email::Sender::Transport::Wrapper 1.300031 Email::Sender::Util 1.300031 Email::Simple 2.216 Email::Simple::Creator 2.216 Email::Simple::Header 2.216 Email::Valid 1.202 Encode 2.98 Encode::Alias 2.24 Encode::Byte 2.04 Encode::CJKConstants 2.02 Encode::CN 2.03 Encode::CN::HZ 2.10 Encode::Config 2.05 Encode::ConfigLocal 1532079134 Encode::EBCDIC 2.02 Encode::Encoder 2.03 Encode::Encoding 2.08 Encode::GSM0338 2.07 Encode::Guess 2.07 Encode::JP 2.04 Encode::JP::H2Z 2.02 Encode::JP::JIS7 2.08 Encode::KR 2.03 Encode::KR::2022_KR 2.04 Encode::Locale 1.05 Encode::MIME::Header 2.28 Encode::MIME::Header::ISO_2022_JP 1.09 Encode::MIME::Name 1.03 Encode::Symbol 2.02 Encode::TW 2.03 Encode::Unicode 2.17 Encode::Unicode::UTF7 2.10 encoding 2.22 Env::Path 0.19 Error 0.17026 Error::Simple 0.17026 Error::TypeTiny 1.002002 Error::TypeTiny::Assertion 1.002002 Error::TypeTiny::Compilation 1.002002 Error::TypeTiny::WrongNumberOfParameters 1.002002 Eval::Closure 0.14 Eval::TypeTiny 1.002002 Eval::WithLexicals 1.003006 Eval::WithLexicals::WithHintPersistence 1.003006 Excel::Writer::XLSX 0.98 Excel::Writer::XLSX::Chart 0.98 Excel::Writer::XLSX::Chart::Area 0.98 Excel::Writer::XLSX::Chart::Bar 0.98 Excel::Writer::XLSX::Chart::Column 0.98 Excel::Writer::XLSX::Chart::Doughnut 0.98 Excel::Writer::XLSX::Chart::Line 0.98 Excel::Writer::XLSX::Chart::Pie 0.98 Excel::Writer::XLSX::Chart::Radar 0.98 Excel::Writer::XLSX::Chart::Scatter 0.98 Excel::Writer::XLSX::Chart::Stock 0.98 Excel::Writer::XLSX::Chartsheet 0.98 Excel::Writer::XLSX::Drawing 0.98 Excel::Writer::XLSX::Examples 0.98 Excel::Writer::XLSX::Format 0.98 Excel::Writer::XLSX::Package::App 0.98 Excel::Writer::XLSX::Package::Comments 0.98 Excel::Writer::XLSX::Package::ContentTypes 0.98 Excel::Writer::XLSX::Package::Core 0.98 Excel::Writer::XLSX::Package::Custom 0.98 Excel::Writer::XLSX::Package::Packager 0.98 Excel::Writer::XLSX::Package::Relationships 0.98 Excel::Writer::XLSX::Package::SharedStrings 0.98 Excel::Writer::XLSX::Package::Styles 0.98 Excel::Writer::XLSX::Package::Table 0.98 Excel::Writer::XLSX::Package::Theme 0.98 Excel::Writer::XLSX::Package::VML 0.98 Excel::Writer::XLSX::Package::XMLwriter 0.98 Excel::Writer::XLSX::Shape 0.98 Excel::Writer::XLSX::Utility 0.98 Excel::Writer::XLSX::Workbook 0.98 Excel::Writer::XLSX::Worksheet 0.98 Exception::Assertion 0.0504 Exception::Base 0.2501 Exception::Class 1.44 Exception::Class::Base 1.44 Expect 1.35 Expect::Simple 0.04 Exporter::Declare 0.114 Exporter::Declare::Export unknown Exporter::Declare::Export::Alias unknown Exporter::Declare::Export::Generator unknown Exporter::Declare::Export::Sub unknown Exporter::Declare::Export::Variable unknown Exporter::Declare::Meta unknown Exporter::Declare::Specs unknown Exporter::Lite 0.08 Exporter::Shiny 1.002001 Exporter::Tidy 0.08 Exporter::Tiny 1.002001 ExtUtils::CChecker 0.10 ExtUtils::Config 0.008 ExtUtils::CppGuess 0.12 ExtUtils::Depends 0.405 ExtUtils::Helpers 0.026 ExtUtils::Helpers::Unix 0.026 ExtUtils::Helpers::VMS 0.026 ExtUtils::Helpers::Windows 0.026 ExtUtils::InstallPaths 0.012 ExtUtils::MakeMaker::CPANfile 0.08 ExtUtils::Manifest 1.71 ExtUtils::PkgConfig 1.16 FAlite unknown FCGI 0.78 FCGI::ProcManager 0.28 FCGI::ProcManager::Constrained unknown Fennec::Lite 0.004 Fh 4.38 File::ChangeNotify 0.28 File::ChangeNotify::Event 0.28 File::ChangeNotify::Watcher 0.28 File::ChangeNotify::Watcher::Default 0.28 File::ChangeNotify::Watcher::Inotify 0.28 File::ChangeNotify::Watcher::KQueue 0.28 File::Copy::Link 0.06 File::Copy::Recursive 0.44 File::Copy::Recursive::Reduced 0.006 File::Find::Rule 0.34 File::Grep 0.02 File::HomeDir 1.004 File::HomeDir::Darwin 1.004 File::HomeDir::Darwin::Carbon 1.004 File::HomeDir::Darwin::Cocoa 1.004 File::HomeDir::Driver 1.004 File::HomeDir::FreeDesktop 1.004 File::HomeDir::MacOS9 1.004 File::HomeDir::Test 1.004 File::HomeDir::Unix 1.004 File::HomeDir::Windows 1.004 File::Listing 6.04 File::Map 0.65 File::Next 1.16 File::NFSLock 1.27 File::Path 2.15 File::pushd 1.016 File::Remove 1.57 File::Share 0.25 File::ShareDir 1.116 File::ShareDir::Install 0.13 File::Slurp 9999.19 File::Slurp::Tiny 0.004 File::Slurper 0.012 File::Spec 3.74 File::Spec::AmigaOS 3.74 File::Spec::Cygwin 3.74 File::Spec::Epoc 3.74 File::Spec::Functions 3.74 File::Spec::Link 0.073 File::Spec::Mac 3.74 File::Spec::Native 1.004 File::Spec::OS2 3.74 File::Spec::Unix 3.74 File::Spec::VMS 3.74 File::Spec::Win32 3.74 File::Temp 0.2308 File::Which 1.22 Filesys::Notify::Simple 0.13 Font::TTF 1.06 Font::TTF::AATKern unknown Font::TTF::AATutils unknown Font::TTF::Anchor unknown Font::TTF::Bsln unknown Font::TTF::Cmap unknown Font::TTF::Coverage unknown Font::TTF::Cvt_ 0.0001 Font::TTF::Delta unknown Font::TTF::DSIG unknown Font::TTF::Dumper unknown Font::TTF::EBDT unknown Font::TTF::EBLC unknown Font::TTF::Fdsc unknown Font::TTF::Feat unknown Font::TTF::Features::Cvar unknown Font::TTF::Features::Size unknown Font::TTF::Features::Sset unknown Font::TTF::Fmtx unknown Font::TTF::Font 0.39 Font::TTF::Fpgm 0.0001 Font::TTF::GDEF unknown Font::TTF::Glat unknown Font::TTF::Gloc unknown Font::TTF::Glyf unknown Font::TTF::Glyph unknown Font::TTF::GPOS unknown Font::TTF::GrFeat unknown Font::TTF::GSUB unknown Font::TTF::Hdmx unknown Font::TTF::Head unknown Font::TTF::Hhea unknown Font::TTF::Hmtx unknown Font::TTF::Kern unknown Font::TTF::Kern::ClassArray unknown Font::TTF::Kern::CompactClassArray unknown Font::TTF::Kern::OrderedList unknown Font::TTF::Kern::StateTable unknown Font::TTF::Kern::Subtable unknown Font::TTF::Loca unknown Font::TTF::LTSH unknown Font::TTF::Maxp unknown Font::TTF::Mort unknown Font::TTF::Mort::Chain unknown Font::TTF::Mort::Contextual unknown Font::TTF::Mort::Insertion unknown Font::TTF::Mort::Ligature unknown Font::TTF::Mort::Noncontextual unknown Font::TTF::Mort::Rearrangement unknown Font::TTF::Mort::Subtable unknown Font::TTF::Name 1.1 Font::TTF::OldCmap unknown Font::TTF::OldMort unknown Font::TTF::OS_2 unknown Font::TTF::OTTags unknown Font::TTF::PCLT unknown Font::TTF::Post 0.01 Font::TTF::Prep 0.0001 Font::TTF::Prop unknown Font::TTF::PSNames unknown Font::TTF::Segarr 0.0001 Font::TTF::Silf unknown Font::TTF::Sill unknown Font::TTF::Table 0.0001 Font::TTF::Ttc 0.0001 Font::TTF::Ttopen unknown Font::TTF::Utils 0.0001 Font::TTF::Vhea unknown Font::TTF::Vmtx unknown Font::TTF::Win32 unknown Font::TTF::Woff unknown Font::TTF::Woff::MetaData unknown Font::TTF::Woff::PrivateData unknown Font::TTF::XMLparse unknown forks 0.36 forks::shared 0.36 Future 0.38 Future::Mutex 0.38 Future::Utils 0.38 GD 2.68 GD::Graph 1.54 GD::Graph::area unknown GD::Graph::axestype unknown GD::Graph::bars unknown GD::Graph::colour unknown GD::Graph::Data unknown GD::Graph::Error unknown GD::Graph::hbars unknown GD::Graph::lines unknown GD::Graph::linespoints unknown GD::Graph::mixed unknown GD::Graph::pie unknown GD::Graph::points unknown GD::Graph::utils unknown GD::Group 1 GD::Image 2.67 GD::Polygon unknown GD::Polyline 0.2 GD::Simple unknown GD::SVG 0.33 GD::Text 0.86 GD::Text::Align unknown GD::Text::Wrap unknown Getopt::Long::Descriptive 0.102 Getopt::Long::Descriptive::Opts 0.102 Getopt::Long::Descriptive::Usage 0.102 Graph 0.9704 Graph::AdjacencyMap unknown Graph::AdjacencyMap::Heavy unknown Graph::AdjacencyMap::Light unknown Graph::AdjacencyMap::Vertex unknown Graph::AdjacencyMatrix unknown Graph::Attribute unknown Graph::BitMatrix unknown Graph::Directed unknown Graph::Matrix unknown Graph::MSTHeapElem unknown Graph::Reader 2.09 Graph::Reader::Dot 2.09 Graph::Reader::HTK 2.09 Graph::Reader::XML 2.09 Graph::ReadWrite 2.09 Graph::SPTHeapElem unknown Graph::TransitiveClosure unknown Graph::TransitiveClosure::Matrix unknown Graph::Traversal unknown Graph::Traversal::BFS unknown Graph::Traversal::DFS unknown Graph::Undirected unknown Graph::UnionFind unknown Graph::Writer 2.09 Graph::Writer::daVinci 2.09 Graph::Writer::Dot 2.09 Graph::Writer::HTK 2.09 Graph::Writer::VCG 2.09 Graph::Writer::XML 2.09 GraphViz 2.24 GraphViz::Data::Grapher 2.24 GraphViz::No 2.24 GraphViz::Parse::RecDescent 2.24 GraphViz::Parse::Yacc 2.24 GraphViz::Parse::Yapp 2.24 GraphViz::Regex 2.24 GraphViz::Small 2.24 GraphViz::XML 2.24 Hash::AutoHash 1.17 Hash::AutoHash::Args 1.18 Hash::AutoHash::Args::V0 1.18 Hash::Merge 0.300 Hash::Merge::Simple 0.051 Hash::MoreUtils 0.06 Hash::MultiValue 0.16 Hash::Util::FieldHash::Compat 0.11 Hash::Util::FieldHash::Compat::Heavy 0.11 Heap071::Elem unknown Heap071::Fibonacci unknown Hook::LexWrap 0.26 HPC::Runner 2.48 HPC::Runner::Scheduler 0.09 HPC::Runner::Slurm 2.58 HTML::AsSubs 5.07 HTML::Element 5.07 HTML::Element::traverse 5.07 HTML::Entities 3.69 HTML::Filter 3.72 HTML::Form 6.03 HTML::FormHandler 0.40068 HTML::FormHandler::Base 0.40068 HTML::FormHandler::Blocks 0.40068 HTML::FormHandler::BuildFields 0.40068 HTML::FormHandler::BuildPages 0.40068 HTML::FormHandler::Field 0.40068 HTML::FormHandler::Field::AddElement 0.40068 HTML::FormHandler::Field::Boolean 0.40068 HTML::FormHandler::Field::BoolSelect 0.40068 HTML::FormHandler::Field::Button 0.40068 HTML::FormHandler::Field::Captcha 0.40068 HTML::FormHandler::Field::Checkbox 0.40068 HTML::FormHandler::Field::Compound 0.40068 HTML::FormHandler::Field::Date 0.40068 HTML::FormHandler::Field::DateMDY 0.40068 HTML::FormHandler::Field::DateTime 0.40068 HTML::FormHandler::Field::Display 0.40068 HTML::FormHandler::Field::Duration 0.40068 HTML::FormHandler::Field::Email 0.40068 HTML::FormHandler::Field::File 0.40068 HTML::FormHandler::Field::Float 0.40068 HTML::FormHandler::Field::Hidden 0.40068 HTML::FormHandler::Field::Hour 0.40068 HTML::FormHandler::Field::Integer 0.40068 HTML::FormHandler::Field::IntRange 0.40068 HTML::FormHandler::Field::Minute 0.40068 HTML::FormHandler::Field::Money 0.40068 HTML::FormHandler::Field::Month 0.40068 HTML::FormHandler::Field::MonthDay 0.40068 HTML::FormHandler::Field::MonthName 0.40068 HTML::FormHandler::Field::Multiple 0.40068 HTML::FormHandler::Field::Nested 0.40068 HTML::FormHandler::Field::NonEditable 0.40068 HTML::FormHandler::Field::NoValue 0.40068 HTML::FormHandler::Field::Password 0.40068 HTML::FormHandler::Field::PasswordConf 0.40068 HTML::FormHandler::Field::PosInteger 0.40068 HTML::FormHandler::Field::PrimaryKey 0.40068 HTML::FormHandler::Field::Repeatable 0.40068 HTML::FormHandler::Field::RequestToken 0.40068 HTML::FormHandler::Field::Reset 0.40068 HTML::FormHandler::Field::Result 0.40068 HTML::FormHandler::Field::RmElement 0.40068 HTML::FormHandler::Field::Role::RequestToken 0.40068 HTML::FormHandler::Field::Second 0.40068 HTML::FormHandler::Field::Select 0.40068 HTML::FormHandler::Field::SelectCSV 0.40068 HTML::FormHandler::Field::Submit 0.40068 HTML::FormHandler::Field::Text 0.40068 HTML::FormHandler::Field::TextArea 0.40068 HTML::FormHandler::Field::TextCSV 0.40068 HTML::FormHandler::Field::Upload 0.40068 HTML::FormHandler::Field::Weekday 0.40068 HTML::FormHandler::Field::Year 0.40068 HTML::FormHandler::Fields 0.40068 HTML::FormHandler::Foo 0.40068 HTML::FormHandler::I18N 0.40068 HTML::FormHandler::I18N::ar_kw 0.40068 HTML::FormHandler::I18N::bg_bg 0.40068 HTML::FormHandler::I18N::ca_es 0.40068 HTML::FormHandler::I18N::cs_cz 0.40068 HTML::FormHandler::I18N::de_de 0.40068 HTML::FormHandler::I18N::en_us 0.40068 HTML::FormHandler::I18N::es_es 0.40068 HTML::FormHandler::I18N::hu_hu 0.40068 HTML::FormHandler::I18N::it_it 0.40068 HTML::FormHandler::I18N::ja_jp 0.40068 HTML::FormHandler::I18N::pt_br 0.40068 HTML::FormHandler::I18N::ru_ru 0.40068 HTML::FormHandler::I18N::sv_se 0.40068 HTML::FormHandler::I18N::tr_tr 0.40068 HTML::FormHandler::I18N::ua_ua 0.40068 HTML::FormHandler::InitResult 0.40068 HTML::FormHandler::Merge 0.40068 HTML::FormHandler::Model 0.40068 HTML::FormHandler::Model::Object 0.40068 HTML::FormHandler::Moose 0.40068 HTML::FormHandler::Moose::Role 0.40068 HTML::FormHandler::Page 0.40068 HTML::FormHandler::Page::Simple 0.40068 HTML::FormHandler::Pages 0.40068 HTML::FormHandler::Render::RepeatableJs 0.40068 HTML::FormHandler::Render::Simple 0.40068 HTML::FormHandler::Render::Table 0.40068 HTML::FormHandler::Render::Util 0.40068 HTML::FormHandler::Render::WithTT 0.40068 HTML::FormHandler::Result 0.40068 HTML::FormHandler::Result::Role 0.40068 HTML::FormHandler::Test 0.40068 HTML::FormHandler::TraitFor::Captcha 0.40068 HTML::FormHandler::TraitFor::I18N 0.40068 HTML::FormHandler::TraitFor::Types 0.40068 HTML::FormHandler::Traits 0.40068 HTML::FormHandler::Types 0.40068 HTML::FormHandler::Validate 0.40068 HTML::FormHandler::Widget::ApplyRole 0.40068 HTML::FormHandler::Widget::Block 0.40068 HTML::FormHandler::Widget::Block::Bootstrap 0.40068 HTML::FormHandler::Widget::Field::Button 0.40068 HTML::FormHandler::Widget::Field::ButtonTag 0.40068 HTML::FormHandler::Widget::Field::Captcha 0.40068 HTML::FormHandler::Widget::Field::Checkbox 0.40068 HTML::FormHandler::Widget::Field::CheckboxGroup 0.40068 HTML::FormHandler::Widget::Field::Compound 0.40068 HTML::FormHandler::Widget::Field::Hidden 0.40068 HTML::FormHandler::Widget::Field::HorizCheckboxGroup 0.40068 HTML::FormHandler::Widget::Field::NoRender 0.40068 HTML::FormHandler::Widget::Field::Password 0.40068 HTML::FormHandler::Widget::Field::RadioGroup 0.40068 HTML::FormHandler::Widget::Field::Repeatable 0.40068 HTML::FormHandler::Widget::Field::Reset 0.40068 HTML::FormHandler::Widget::Field::Role::HTMLAttributes 0.40068 HTML::FormHandler::Widget::Field::Role::SelectedOption 0.40068 HTML::FormHandler::Widget::Field::Select 0.40068 HTML::FormHandler::Widget::Field::Span 0.40068 HTML::FormHandler::Widget::Field::Submit 0.40068 HTML::FormHandler::Widget::Field::Text 0.40068 HTML::FormHandler::Widget::Field::Textarea 0.40068 HTML::FormHandler::Widget::Field::Upload 0.40068 HTML::FormHandler::Widget::Form::Role::HTMLAttributes 0.40068 HTML::FormHandler::Widget::Form::Simple 0.40068 HTML::FormHandler::Widget::Form::Table 0.40068 HTML::FormHandler::Widget::Theme::Bootstrap 0.40068 HTML::FormHandler::Widget::Theme::Bootstrap3 0.40068 HTML::FormHandler::Widget::Theme::BootstrapFormMessages 0.40068 HTML::FormHandler::Widget::Wrapper::Base 0.40068 HTML::FormHandler::Widget::Wrapper::Bootstrap 0.40068 HTML::FormHandler::Widget::Wrapper::Bootstrap3 0.40068 HTML::FormHandler::Widget::Wrapper::Fieldset 0.40068 HTML::FormHandler::Widget::Wrapper::None 0.40068 HTML::FormHandler::Widget::Wrapper::Simple 0.40068 HTML::FormHandler::Widget::Wrapper::SimpleInline 0.40068 HTML::FormHandler::Widget::Wrapper::Table 0.40068 HTML::FormHandler::Widget::Wrapper::TableInline 0.40068 HTML::FormHandler::Wizard 0.40068 HTML::HeadParser 3.71 HTML::LinkExtor 3.69 HTML::Parse 5.07 HTML::Parser 3.72 HTML::Perlinfo 1.69 HTML::Perlinfo::Apache unknown HTML::Perlinfo::Base unknown HTML::Perlinfo::Common unknown HTML::Perlinfo::General unknown HTML::Perlinfo::Loaded 1.02 HTML::Perlinfo::Modules 1.19 HTML::PullParser 3.57 HTML::TableExtract 2.15 HTML::Tagset 3.20 HTML::TokeParser 3.69 HTML::Tree 5.07 HTML::TreeBuilder 5.07 HTTP::Body 1.22 HTTP::Body::MultiPart 1.22 HTTP::Body::OctetStream 1.22 HTTP::Body::UrlEncoded 1.22 HTTP::Body::XForms 1.22 HTTP::Body::XFormsMultipart 1.22 HTTP::Config 6.18 HTTP::CookieJar 0.008 HTTP::CookieJar::LWP 0.008 HTTP::Cookies 6.04 HTTP::Cookies::Microsoft 6.04 HTTP::Cookies::Netscape 6.04 HTTP::Daemon 6.01 HTTP::Date 6.02 HTTP::Entity::Parser 0.21 HTTP::Entity::Parser::JSON unknown HTTP::Entity::Parser::MultiPart unknown HTTP::Entity::Parser::OctetStream unknown HTTP::Entity::Parser::UrlEncoded unknown HTTP::Headers 6.18 HTTP::Headers::Auth 6.18 HTTP::Headers::ETag 6.18 HTTP::Headers::Fast 0.21 HTTP::Headers::Util 6.18 HTTP::Message 6.18 HTTP::Message::PSGI unknown HTTP::MultiPartParser 0.02 HTTP::Negotiate 6.01 HTTP::Parser::XS 0.17 HTTP::Parser::XS::PP unknown HTTP::Request 6.18 HTTP::Request::AsCGI 1.2 HTTP::Request::Common 6.18 HTTP::Response 6.18 HTTP::Server::PSGI unknown HTTP::Server::PSGI::Net::Server::PreFork unknown HTTP::Server::Simple 0.52 HTTP::Server::Simple::CGI unknown HTTP::Server::Simple::CGI::Environment unknown HTTP::Server::Simple::PSGI 0.16 HTTP::Status 6.18 HTTP::Thin 0.006 HTTP::Tinyish 0.14 HTTP::Tinyish::Base unknown HTTP::Tinyish::Curl unknown HTTP::Tinyish::HTTPTiny unknown HTTP::Tinyish::LWP unknown HTTP::Tinyish::Wget unknown HTTP::XSCookies 0.000021 Image::PNG 0.23 Image::PNG::Const 0.45 Image::PNG::Container 0.23 Image::PNG::Libpng 0.45 Image::PNG::Util unknown Import::Into 1.002005 Importer 0.025 inc::Module::Install 1.19 inc::Module::Install::DSL 1.19 indirect 0.38 Inline 0.80 Inline::C 0.78 Inline::C::Parser unknown Inline::C::Parser::Pegex unknown Inline::C::Parser::Pegex::AST unknown Inline::C::Parser::Pegex::Grammar unknown Inline::C::Parser::RecDescent unknown Inline::C::Parser::RegExp unknown Inline::denter unknown Inline::Foo 0.80 Inline::MakeMaker 0.80 Inline::MakePdlppInstallable unknown Inline::Pdlpp 0.4 IO::All 0.87 IO::All::Base unknown IO::All::DBM unknown IO::All::Dir unknown IO::All::File unknown IO::All::Filesys unknown IO::All::Link unknown IO::All::MLDBM unknown IO::All::Pipe unknown IO::All::Socket unknown IO::All::STDIO unknown IO::All::String unknown IO::All::Temp unknown IO::Async 0.72 IO::Async::Channel 0.72 IO::Async::Debug 0.72 IO::Async::File 0.72 IO::Async::FileStream 0.72 IO::Async::Function 0.72 IO::Async::Future 0.72 IO::Async::Handle 0.72 IO::Async::Internals::ChildManager 0.72 IO::Async::Listener 0.72 IO::Async::Loop 0.72 IO::Async::Loop::Poll 0.72 IO::Async::Loop::Select 0.72 IO::Async::LoopTests 0.72 IO::Async::Notifier 0.72 IO::Async::OS 0.72 IO::Async::OS::cygwin 0.72 IO::Async::OS::linux 0.72 IO::Async::OS::MSWin32 0.72 IO::Async::PID 0.72 IO::Async::Process 0.72 IO::Async::Protocol 0.72 IO::Async::Protocol::LineStream 0.72 IO::Async::Protocol::Stream 0.72 IO::Async::Resolver 0.72 IO::Async::Routine 0.72 IO::Async::Signal 0.72 IO::Async::Socket 0.72 IO::Async::Stream 0.72 IO::Async::Test 0.72 IO::Async::Timer 0.72 IO::Async::Timer::Absolute 0.72 IO::Async::Timer::Countdown 0.72 IO::Async::Timer::Periodic 0.72 IO::AtomicFile 2.111 IO::CaptureOutput 1.1104 IO::HTML 1.001 IO::InnerFile 2.111 IO::Interactive 1.022 IO::Lines 2.111 IO::Pipely 0.005 IO::Prompt 0.997004 IO::Pty 1.12 IO::Scalar 2.111 IO::ScalarArray 2.111 IO::SessionData 1.03 IO::SessionSet unknown IO::Socket::SSL 2.058 IO::Socket::SSL::Intercept 2.056 IO::Socket::SSL::PublicSuffix unknown IO::Socket::SSL::Utils 2.014 IO::Socket::Timeout 0.32 IO::String 1.08 IO::Stringy 2.111 IO::TieCombine 1.005 IO::TieCombine::Handle 1.005 IO::TieCombine::Scalar 1.005 IO::Tty 1.12 IO::Tty::Constant unknown IO::Wrap 2.111 IO::WrapTie 2.111 IPC::Run 20180523.0 IPC::Run3 0.048 IPC::Run3::ProfArrayBuffer 0.048 IPC::Run3::ProfLogger 0.048 IPC::Run3::ProfLogReader 0.048 IPC::Run3::ProfPP 0.048 IPC::Run3::ProfReporter 0.048 IPC::Run::Debug 20180523.0 IPC::Run::IO 20180523.0 IPC::Run::Timer 20180523.0 IPC::Run::Win32Helper 20180523.0 IPC::Run::Win32IO 20180523.0 IPC::Run::Win32Pump 20180523.0 IPC::ShareLite 0.17 IPC::System::Simple 1.25 JSON 2.97001 JSON::Any 1.39 JSON::MaybeXS 1.004000 Lexical::Persistence 1.020 lib::core::only unknown Lingua::EN::FindNumber 1.32 Lingua::EN::Inflect 1.903 Lingua::EN::Inflect::Number 1.12 Lingua::EN::Inflect::Phrase 0.20 Lingua::EN::Number::IsOrdinal 0.05 Lingua::EN::Tagger 0.29 Lingua::EN::Words2Nums unknown Lingua::GL::Stemmer 0.02 Lingua::PT::Stemmer 0.02 Lingua::Stem 0.84 Lingua::Stem::AutoLoader 1.02 Lingua::Stem::Da 1.01 Lingua::Stem::De 1.01 Lingua::Stem::En 2.16 Lingua::Stem::EnBroken 2.13 Lingua::Stem::Fr 0.02 Lingua::Stem::Gl 1.02 Lingua::Stem::It 0.02 Lingua::Stem::No 1.01 Lingua::Stem::Pt 1.01 Lingua::Stem::Ru 0.04 Lingua::Stem::Snowball::Da 1.01 Lingua::Stem::Snowball::No 1.2 Lingua::Stem::Snowball::Se 1.2 Lingua::Stem::Sv 1.01 List::AllUtils 0.14 List::MoreUtils 0.428 List::MoreUtils::PP 0.428 List::MoreUtils::XS 0.428 List::SomeUtils 0.56 List::SomeUtils::PP 0.56 List::SomeUtils::XS 0.58 List::Util 1.50 List::Util::XS 1.50 List::UtilsBy 0.11 local::lib 2.000024 Locale::Maketext::Extract 1.00 Locale::Maketext::Extract::Plugin::Base 1.00 Locale::Maketext::Extract::Plugin::FormFu 1.00 Locale::Maketext::Extract::Plugin::Generic 1.00 Locale::Maketext::Extract::Plugin::Haml 1.00 Locale::Maketext::Extract::Plugin::Mason 1.00 Locale::Maketext::Extract::Plugin::Perl 1.00 Locale::Maketext::Extract::Plugin::PPI 1.00 Locale::Maketext::Extract::Plugin::TextTemplate 1.00 Locale::Maketext::Extract::Plugin::TT2 1.00 Locale::Maketext::Extract::Plugin::YAML 1.00 Locale::Maketext::Extract::Run 1.00 Locale::Maketext::Lexicon 1.00 Locale::Maketext::Lexicon::Auto 1.00 Locale::Maketext::Lexicon::Gettext 1.00 Locale::Maketext::Lexicon::Msgcat 1.00 Locale::Maketext::Lexicon::Tie 1.00 Log::Any 1.706 Log::Any::Adapter 1.706 Log::Any::Adapter::Base 1.706 Log::Any::Adapter::File 1.706 Log::Any::Adapter::Null 1.706 Log::Any::Adapter::Stderr 1.706 Log::Any::Adapter::Stdout 1.706 Log::Any::Adapter::Syslog 1.706 Log::Any::Adapter::Test 1.706 Log::Any::Adapter::Util 1.706 Log::Any::Manager 1.706 Log::Any::Proxy 1.706 Log::Any::Proxy::Null 1.706 Log::Any::Proxy::Test 1.706 Log::Any::Test 1.706 Log::Contextual 0.008001 Log::Contextual::Easy::Default 0.008001 Log::Contextual::Easy::Package 0.008001 Log::Contextual::Role::Router 0.008001 Log::Contextual::Role::Router::HasLogger 0.008001 Log::Contextual::Role::Router::SetLogger 0.008001 Log::Contextual::Role::Router::WithLogger 0.008001 Log::Contextual::Router 0.008001 Log::Contextual::SimpleLogger 0.008001 Log::Contextual::TeeLogger 0.008001 Log::Contextual::WarnLogger 0.008001 Log::Dispatch 2.67 Log::Dispatch::ApacheLog 2.67 Log::Dispatch::Array 1.003 Log::Dispatch::Base 2.67 Log::Dispatch::Code 2.67 Log::Dispatch::Config 1.04 Log::Dispatch::Configurator 1.00 Log::Dispatch::Configurator::AppConfig 1.00 Log::Dispatch::Email 2.67 Log::Dispatch::Email::MailSend 2.67 Log::Dispatch::Email::MailSender 2.67 Log::Dispatch::Email::MailSendmail 2.67 Log::Dispatch::Email::MIMELite 2.67 Log::Dispatch::File 2.67 Log::Dispatch::File::Locked 2.67 Log::Dispatch::Handle 2.67 Log::Dispatch::Null 2.67 Log::Dispatch::Output 2.67 Log::Dispatch::Screen 2.67 Log::Dispatch::Syslog 2.67 Log::Dispatch::Types 2.67 Log::Dispatch::Vars 2.67 Log::Dispatchouli 2.016 Log::Dispatchouli::Global 2.016 Log::Dispatchouli::Proxy 2.016 Log::Log4perl 1.49 Log::Log4perl::Appender unknown Log::Log4perl::Appender::Buffer unknown Log::Log4perl::Appender::DBI unknown Log::Log4perl::Appender::File unknown Log::Log4perl::Appender::Limit unknown Log::Log4perl::Appender::RRDs unknown Log::Log4perl::Appender::Screen unknown Log::Log4perl::Appender::ScreenColoredLevels unknown Log::Log4perl::Appender::Socket unknown Log::Log4perl::Appender::String unknown Log::Log4perl::Appender::Synchronized unknown Log::Log4perl::Appender::TestArrayBuffer unknown Log::Log4perl::Appender::TestBuffer unknown Log::Log4perl::Appender::TestFileCreeper unknown Log::Log4perl::Catalyst unknown Log::Log4perl::Config unknown Log::Log4perl::Config::BaseConfigurator unknown Log::Log4perl::Config::DOMConfigurator 0.03 Log::Log4perl::Config::PropertyConfigurator unknown Log::Log4perl::Config::Watch unknown Log::Log4perl::DateFormat unknown Log::Log4perl::Filter unknown Log::Log4perl::Filter::Boolean unknown Log::Log4perl::Filter::LevelMatch unknown Log::Log4perl::Filter::LevelRange unknown Log::Log4perl::Filter::MDC unknown Log::Log4perl::Filter::StringMatch unknown Log::Log4perl::InternalDebug unknown Log::Log4perl::JavaMap unknown Log::Log4perl::JavaMap::ConsoleAppender unknown Log::Log4perl::JavaMap::FileAppender unknown Log::Log4perl::JavaMap::JDBCAppender unknown Log::Log4perl::JavaMap::NTEventLogAppender unknown Log::Log4perl::JavaMap::RollingFileAppender unknown Log::Log4perl::JavaMap::SyslogAppender unknown Log::Log4perl::JavaMap::TestBuffer unknown Log::Log4perl::Layout unknown Log::Log4perl::Layout::NoopLayout unknown Log::Log4perl::Layout::PatternLayout unknown Log::Log4perl::Layout::PatternLayout::Multiline unknown Log::Log4perl::Layout::SimpleLayout unknown Log::Log4perl::Level unknown Log::Log4perl::Logger unknown Log::Log4perl::MDC unknown Log::Log4perl::NDC unknown Log::Log4perl::Resurrector unknown Log::Log4perl::Util unknown Log::Log4perl::Util::Semaphore unknown Log::Log4perl::Util::TimeTracker unknown Log::Message 0.08 Log::Message::Config 0.08 Log::Message::Handlers 0.08 Log::Message::Item 0.08 Log::Message::Simple 0.10 Log::Report 1.27 Log::Report::DBIC::Profiler 1.27 Log::Report::Die 1.27 Log::Report::Dispatcher 1.27 Log::Report::Dispatcher::Callback 1.27 Log::Report::Dispatcher::File 1.27 Log::Report::Dispatcher::Log4perl 1.27 Log::Report::Dispatcher::LogDispatch 1.27 Log::Report::Dispatcher::Perl 1.27 Log::Report::Dispatcher::Syslog 1.27 Log::Report::Dispatcher::Try 1.27 Log::Report::Domain 1.27 Log::Report::Exception 1.27 Log::Report::Message 1.27 Log::Report::Minimal 1.06 Log::Report::Minimal::Domain 1.06 Log::Report::Optional 1.06 Log::Report::Translator 1.27 Log::Report::Util 1.06 Logger::Simple 2.0 LWP 6.35 LWP::Authen::Basic 6.35 LWP::Authen::Digest 6.35 LWP::Authen::Ntlm 6.35 LWP::ConnCache 6.35 LWP::Debug 6.35 LWP::Debug unknown LWP::Debug::TraceHTTP 6.35 LWP::DebugFile 6.35 LWP::MediaTypes 6.02 LWP::MemberMixin 6.35 LWP::Protocol 6.35 LWP::Protocol::cpan 6.35 LWP::Protocol::data 6.35 LWP::Protocol::file 6.35 LWP::Protocol::ftp 6.35 LWP::Protocol::gopher 6.35 LWP::Protocol::http 6.35 LWP::Protocol::https 6.07 LWP::Protocol::loopback 6.35 LWP::Protocol::mailto 6.35 LWP::Protocol::nntp 6.35 LWP::Protocol::nogo 6.35 LWP::RobotUA 6.35 LWP::Simple 6.35 LWP::UserAgent 6.35 Mail::Address 2.20 Mail::Cap 2.20 Mail::Field 2.20 Mail::Field::AddrList 2.20 Mail::Field::Date 2.20 Mail::Field::Generic 2.20 Mail::Filter 2.20 Mail::Header 2.20 Mail::Internet 2.20 Mail::Mailer 2.20 Mail::Mailer::qmail 2.20 Mail::Mailer::rfc822 2.20 Mail::Mailer::sendmail 2.20 Mail::Mailer::smtp 2.20 Mail::Mailer::smtps 2.20 Mail::Mailer::testfile 2.20 Mail::Send 2.20 Mail::Util 2.20 MailTools 2.20 Math::Bezier 0.01 Math::BigFloat 1.999813 Math::BigInt 1.999813 Math::BigInt::Calc 1.999813 Math::BigInt::CalcEmu 1.999813 Math::BigInt::Lib 1.999813 Math::CDF 0.1 Math::Cephes 0.5305 Math::Cephes::Complex 0.5305 Math::Cephes::Fraction 0.5305 Math::Cephes::Matrix 0.5305 Math::Cephes::Polynomial 0.5305 Math::Combinatorics 0.09 Math::Counting 0.1305 Math::Derivative 1.01 Math::GSL::Linalg::SVD 0.0.2 Math::MatrixReal 2.13 Math::Prime::Util 0.70 Math::Prime::Util::ChaCha 0.70 Math::Prime::Util::ECAffinePoint 0.70 Math::Prime::Util::ECProjectivePoint 0.70 Math::Prime::Util::Entropy 0.70 Math::Prime::Util::GMP 0.50 Math::Prime::Util::MemFree 0.70 Math::Prime::Util::PP 0.70 Math::Prime::Util::PPFE unknown Math::Prime::Util::PrimalityProving 0.70 Math::Prime::Util::PrimeArray 0.70 Math::Prime::Util::PrimeIterator 0.70 Math::Prime::Util::RandomPrimes 0.70 Math::Prime::Util::ZetaBigFloat 0.70 Math::Random 0.72 Math::Random::ISAAC 1.004 Math::Random::ISAAC::PP 1.004 Math::Random::MT::Auto 6.22 Math::Random::MT::Auto::Range 6.22 Math::Round 0.07 Math::Spline 0.02 Math::Utils 1.12 Math::VecStat 0.08 Memoize::ExpireLRU 0.56 Menlo 1.9019 Menlo::Builder::Static unknown Menlo::CLI::Compat 1.9022 Menlo::Dependency unknown Menlo::Index::MetaCPAN unknown Menlo::Index::MetaDB 1.9019 Menlo::Index::Mirror unknown Menlo::Legacy 1.9022 Menlo::Util unknown Meta::Builder 0.003 Meta::Builder::Base unknown Meta::Builder::Util unknown metaclass 2.2011 Method::Generate::Accessor unknown Method::Generate::BuildAll unknown Method::Generate::Constructor unknown Method::Generate::DemolishAll unknown Method::Inliner unknown MIME::Charset 1.012.2 MIME::Charset::_Compat 1.003.1 MIME::Charset::UTF 1.010 MIME::Type 2.17 MIME::Types 2.17 Mixin::Linewise 0.108 Mixin::Linewise::Readers 0.108 Mixin::Linewise::Writers 0.108 Mock::Config 0.03 Modern::Perl 1.20180701 Module::AutoInstall 1.19 Module::Build 0.4224 Module::Build::Base 0.4224 Module::Build::Compat 0.4224 Module::Build::Config 0.4224 Module::Build::ConfigData unknown Module::Build::Cookbook 0.4224 Module::Build::Dumper 0.4224 Module::Build::Notes 0.4224 Module::Build::Platform::aix 0.4224 Module::Build::Platform::cygwin 0.4224 Module::Build::Platform::darwin 0.4224 Module::Build::Platform::Default 0.4224 Module::Build::Platform::MacOS 0.4224 Module::Build::Platform::os2 0.4224 Module::Build::Platform::Unix 0.4224 Module::Build::Platform::VMS 0.4224 Module::Build::Platform::VOS 0.4224 Module::Build::Platform::Windows 0.4224 Module::Build::PodParser 0.4224 Module::Build::PPMMaker 0.4224 Module::Build::Tiny 0.039 Module::Build::XSUtil 0.19 Module::Compile 0.37 Module::Compile::Opt unknown Module::CPANfile 1.1004 Module::CPANfile::Environment unknown Module::CPANfile::Prereq unknown Module::CPANfile::Prereqs unknown Module::CPANfile::Requirement unknown Module::Faker 0.020 Module::Faker::Appendix 0.020 Module::Faker::Dist 0.020 Module::Faker::File 0.020 Module::Faker::Heavy 0.020 Module::Faker::Module 0.020 Module::Faker::Package 0.020 Module::Find 0.13 Module::Implementation 0.09 Module::Install 1.19 Module::Install::Admin 1.19 Module::Install::Admin::Bundle 1.19 Module::Install::Admin::Compiler 1.19 Module::Install::Admin::Find 1.19 Module::Install::Admin::Include 1.19 Module::Install::Admin::Makefile 1.19 Module::Install::Admin::Manifest 1.19 Module::Install::Admin::Metadata 1.19 Module::Install::Admin::ScanDeps 1.19 Module::Install::Admin::WriteAll 1.19 Module::Install::AutoInstall 1.19 Module::Install::Base 1.19 Module::Install::Bundle 1.19 Module::Install::Can 1.19 Module::Install::Catalyst unknown Module::Install::Compiler 1.19 Module::Install::Deprecated 1.19 Module::Install::DSL 1.19 Module::Install::External 1.19 Module::Install::Fetch 1.19 Module::Install::Include 1.19 Module::Install::Inline 1.19 Module::Install::Makefile 1.19 Module::Install::MakeMaker 1.19 Module::Install::Metadata 1.19 Module::Install::PAR 1.19 Module::Install::Run 1.19 Module::Install::Scripts 1.19 Module::Install::Share 1.19 Module::Install::Win32 1.19 Module::Install::With 1.19 Module::Install::WriteAll 1.19 Module::Optimize unknown Module::Path 0.19 Module::Pluggable 5.2 Module::Pluggable::Object 5.2 Module::Runtime 0.016 Module::Runtime::Conflicts 0.003 Module::ScanDeps 1.24 Module::ScanDeps::Cache unknown Module::Util 1.09 Mojo unknown Mojo::Asset unknown Mojo::Asset::File unknown Mojo::Asset::Memory unknown Mojo::Base unknown Mojo::ByteStream unknown Mojo::Cache unknown Mojo::Collection unknown Mojo::Content unknown Mojo::Content::MultiPart unknown Mojo::Content::Single unknown Mojo::Cookie unknown Mojo::Cookie::Request unknown Mojo::Cookie::Response unknown Mojo::Date unknown Mojo::DOM unknown Mojo::DOM::CSS unknown Mojo::DOM::HTML unknown Mojo::EventEmitter unknown Mojo::Exception unknown Mojo::File unknown Mojo::Headers unknown Mojo::HelloWorld unknown Mojo::Home unknown Mojo::IOLoop unknown Mojo::IOLoop::Client unknown Mojo::IOLoop::Delay unknown Mojo::IOLoop::Server unknown Mojo::IOLoop::Stream unknown Mojo::IOLoop::Stream::HTTPClient unknown Mojo::IOLoop::Stream::HTTPServer unknown Mojo::IOLoop::Stream::WebSocketClient unknown Mojo::IOLoop::Stream::WebSocketServer unknown Mojo::IOLoop::Subprocess unknown Mojo::IOLoop::TLS unknown Mojo::JSON unknown Mojo::JSON::Pointer unknown Mojo::Loader unknown Mojo::Log unknown Mojo::Message unknown Mojo::Message::Request unknown Mojo::Message::Response unknown Mojo::Parameters unknown Mojo::Path unknown Mojo::Promise unknown Mojo::Reactor unknown Mojo::Reactor::EV unknown Mojo::Reactor::Poll unknown Mojo::Server unknown Mojo::Server::CGI unknown Mojo::Server::Daemon unknown Mojo::Server::Hypnotoad unknown Mojo::Server::Morbo unknown Mojo::Server::Morbo::Backend unknown Mojo::Server::Morbo::Backend::Poll unknown Mojo::Server::Prefork unknown Mojo::Server::PSGI unknown Mojo::Template unknown Mojo::Transaction unknown Mojo::Transaction::HTTP unknown Mojo::Transaction::WebSocket unknown Mojo::Upload unknown Mojo::URL unknown Mojo::UserAgent unknown Mojo::UserAgent::CookieJar unknown Mojo::UserAgent::Proxy unknown Mojo::UserAgent::Server unknown Mojo::UserAgent::Transactor unknown Mojo::Util unknown Mojo::WebSocket unknown Mojolicious 7.88 Mojolicious::Command unknown Mojolicious::Command::cgi unknown Mojolicious::Command::cpanify unknown Mojolicious::Command::daemon unknown Mojolicious::Command::eval unknown Mojolicious::Command::generate unknown Mojolicious::Command::generate::app unknown Mojolicious::Command::generate::lite_app unknown Mojolicious::Command::generate::makefile unknown Mojolicious::Command::generate::plugin 0.01 Mojolicious::Command::get unknown Mojolicious::Command::inflate unknown Mojolicious::Command::prefork unknown Mojolicious::Command::psgi unknown Mojolicious::Command::routes unknown Mojolicious::Command::test unknown Mojolicious::Command::version unknown Mojolicious::Commands unknown Mojolicious::Controller unknown Mojolicious::Lite unknown Mojolicious::Plugin unknown Mojolicious::Plugin::Config unknown Mojolicious::Plugin::DefaultHelpers unknown Mojolicious::Plugin::EPLRenderer unknown Mojolicious::Plugin::EPRenderer unknown Mojolicious::Plugin::HeaderCondition unknown Mojolicious::Plugin::JSONConfig unknown Mojolicious::Plugin::Mount unknown Mojolicious::Plugin::PODRenderer unknown Mojolicious::Plugin::TagHelpers unknown Mojolicious::Plugins unknown Mojolicious::Renderer unknown Mojolicious::Routes unknown Mojolicious::Routes::Match unknown Mojolicious::Routes::Pattern unknown Mojolicious::Routes::Route unknown Mojolicious::Sessions unknown Mojolicious::Static unknown Mojolicious::Types unknown Mojolicious::Validator unknown Mojolicious::Validator::Validation unknown MojoX::Log::Report 1.27 MojoX::MIME::Types 2.17 Moo 2.001001 Moo::_mro unknown Moo::_strictures unknown Moo::_Utils unknown Moo::HandleMoose unknown Moo::HandleMoose::_TypeMap unknown Moo::HandleMoose::FakeMetaClass unknown Moo::Object unknown Moo::Role 2.001001 Moo::sification unknown Moose 2.2011 Moose::Autobox 0.16 Moose::Autobox::Array 0.16 Moose::Autobox::Code 0.16 Moose::Autobox::Defined 0.16 Moose::Autobox::Hash 0.16 Moose::Autobox::Indexed 0.16 Moose::Autobox::Item 0.16 Moose::Autobox::List 0.16 Moose::Autobox::Number 0.16 Moose::Autobox::Ref 0.16 Moose::Autobox::Scalar 0.16 Moose::Autobox::String 0.16 Moose::Autobox::Undef 0.16 Moose::Autobox::Value 0.16 Moose::Deprecated 2.2011 Moose::Exception 2.2011 Moose::Exception::AccessorMustReadWrite 2.2011 Moose::Exception::AddParameterizableTypeTakesParameterizableType 2.2011 Moose::Exception::AddRoleTakesAMooseMetaRoleInstance 2.2011 Moose::Exception::AddRoleToARoleTakesAMooseMetaRole 2.2011 Moose::Exception::ApplyTakesABlessedInstance 2.2011 Moose::Exception::AttachToClassNeedsAClassMOPClassInstanceOrASubclass 2.2011 Moose::Exception::AttributeConflictInRoles 2.2011 Moose::Exception::AttributeConflictInSummation 2.2011 Moose::Exception::AttributeExtensionIsNotSupportedInRoles 2.2011 Moose::Exception::AttributeIsRequired 2.2011 Moose::Exception::AttributeMustBeAnClassMOPMixinAttributeCoreOrSubclass 2.2011 Moose::Exception::AttributeNamesDoNotMatch 2.2011 Moose::Exception::AttributeValueIsNotAnObject 2.2011 Moose::Exception::AttributeValueIsNotDefined 2.2011 Moose::Exception::AutoDeRefNeedsArrayRefOrHashRef 2.2011 Moose::Exception::BadOptionFormat 2.2011 Moose::Exception::BothBuilderAndDefaultAreNotAllowed 2.2011 Moose::Exception::BuilderDoesNotExist 2.2011 Moose::Exception::BuilderMethodNotSupportedForAttribute 2.2011 Moose::Exception::BuilderMethodNotSupportedForInlineAttribute 2.2011 Moose::Exception::BuilderMustBeAMethodName 2.2011 Moose::Exception::CallingMethodOnAnImmutableInstance 2.2011 Moose::Exception::CallingReadOnlyMethodOnAnImmutableInstance 2.2011 Moose::Exception::CanExtendOnlyClasses 2.2011 Moose::Exception::CannotAddAdditionalTypeCoercionsToUnion 2.2011 Moose::Exception::CannotAddAsAnAttributeToARole 2.2011 Moose::Exception::CannotApplyBaseClassRolesToRole 2.2011 Moose::Exception::CannotAssignValueToReadOnlyAccessor 2.2011 Moose::Exception::CannotAugmentIfLocalMethodPresent 2.2011 Moose::Exception::CannotAugmentNoSuperMethod 2.2011 Moose::Exception::CannotAutoDereferenceTypeConstraint 2.2011 Moose::Exception::CannotAutoDerefWithoutIsa 2.2011 Moose::Exception::CannotCalculateNativeType 2.2011 Moose::Exception::CannotCallAnAbstractBaseMethod 2.2011 Moose::Exception::CannotCallAnAbstractMethod 2.2011 Moose::Exception::CannotCoerceAttributeWhichHasNoCoercion 2.2011 Moose::Exception::CannotCoerceAWeakRef 2.2011 Moose::Exception::CannotCreateHigherOrderTypeWithoutATypeParameter 2.2011 Moose::Exception::CannotCreateMethodAliasLocalMethodIsPresent 2.2011 Moose::Exception::CannotCreateMethodAliasLocalMethodIsPresentInClass 2.2011 Moose::Exception::CannotDelegateLocalMethodIsPresent 2.2011 Moose::Exception::CannotDelegateWithoutIsa 2.2011 Moose::Exception::CannotFindDelegateMetaclass 2.2011 Moose::Exception::CannotFindType 2.2011 Moose::Exception::CannotFindTypeGivenToMatchOnType 2.2011 Moose::Exception::CannotFixMetaclassCompatibility 2.2011 Moose::Exception::CannotGenerateInlineConstraint 2.2011 Moose::Exception::CannotInitializeMooseMetaRoleComposite 2.2011 Moose::Exception::CannotInlineTypeConstraintCheck 2.2011 Moose::Exception::CannotLocatePackageInINC 2.2011 Moose::Exception::CannotMakeMetaclassCompatible 2.2011 Moose::Exception::CannotOverrideALocalMethod 2.2011 Moose::Exception::CannotOverrideBodyOfMetaMethods 2.2011 Moose::Exception::CannotOverrideLocalMethodIsPresent 2.2011 Moose::Exception::CannotOverrideNoSuperMethod 2.2011 Moose::Exception::CannotRegisterUnnamedTypeConstraint 2.2011 Moose::Exception::CannotUseLazyBuildAndDefaultSimultaneously 2.2011 Moose::Exception::CanOnlyConsumeRole 2.2011 Moose::Exception::CanOnlyWrapBlessedCode 2.2011 Moose::Exception::CanReblessOnlyIntoASubclass 2.2011 Moose::Exception::CanReblessOnlyIntoASuperclass 2.2011 Moose::Exception::CircularReferenceInAlso 2.2011 Moose::Exception::ClassDoesNotHaveInitMeta 2.2011 Moose::Exception::ClassDoesTheExcludedRole 2.2011 Moose::Exception::ClassNamesDoNotMatch 2.2011 Moose::Exception::CloneObjectExpectsAnInstanceOfMetaclass 2.2011 Moose::Exception::CodeBlockMustBeACodeRef 2.2011 Moose::Exception::CoercingWithoutCoercions 2.2011 Moose::Exception::CoercionAlreadyExists 2.2011 Moose::Exception::CoercionNeedsTypeConstraint 2.2011 Moose::Exception::ConflictDetectedInCheckRoleExclusions 2.2011 Moose::Exception::ConflictDetectedInCheckRoleExclusionsInToClass 2.2011 Moose::Exception::ConstructClassInstanceTakesPackageName 2.2011 Moose::Exception::CouldNotCreateMethod 2.2011 Moose::Exception::CouldNotCreateWriter 2.2011 Moose::Exception::CouldNotEvalConstructor 2.2011 Moose::Exception::CouldNotEvalDestructor 2.2011 Moose::Exception::CouldNotFindTypeConstraintToCoerceFrom 2.2011 Moose::Exception::CouldNotGenerateInlineAttributeMethod 2.2011 Moose::Exception::CouldNotLocateTypeConstraintForUnion 2.2011 Moose::Exception::CouldNotParseType 2.2011 Moose::Exception::CreateMOPClassTakesArrayRefOfAttributes 2.2011 Moose::Exception::CreateMOPClassTakesArrayRefOfSuperclasses 2.2011 Moose::Exception::CreateMOPClassTakesHashRefOfMethods 2.2011 Moose::Exception::CreateTakesArrayRefOfRoles 2.2011 Moose::Exception::CreateTakesHashRefOfAttributes 2.2011 Moose::Exception::CreateTakesHashRefOfMethods 2.2011 Moose::Exception::DefaultToMatchOnTypeMustBeCodeRef 2.2011 Moose::Exception::DelegationToAClassWhichIsNotLoaded 2.2011 Moose::Exception::DelegationToARoleWhichIsNotLoaded 2.2011 Moose::Exception::DelegationToATypeWhichIsNotAClass 2.2011 Moose::Exception::DoesRequiresRoleName 2.2011 Moose::Exception::EnumCalledWithAnArrayRefAndAdditionalArgs 2.2011 Moose::Exception::EnumValuesMustBeString 2.2011 Moose::Exception::ExtendsMissingArgs 2.2011 Moose::Exception::HandlesMustBeAHashRef 2.2011 Moose::Exception::IllegalInheritedOptions 2.2011 Moose::Exception::IllegalMethodTypeToAddMethodModifier 2.2011 Moose::Exception::IncompatibleMetaclassOfSuperclass 2.2011 Moose::Exception::InitializeTakesUnBlessedPackageName 2.2011 Moose::Exception::InitMetaRequiresClass 2.2011 Moose::Exception::InstanceBlessedIntoWrongClass 2.2011 Moose::Exception::InstanceMustBeABlessedReference 2.2011 Moose::Exception::InvalidArgPassedToMooseUtilMetaRole 2.2011 Moose::Exception::InvalidArgumentsToTraitAliases 2.2011 Moose::Exception::InvalidArgumentToMethod 2.2011 Moose::Exception::InvalidBaseTypeGivenToCreateParameterizedTypeConstraint 2.2011 Moose::Exception::InvalidHandleValue 2.2011 Moose::Exception::InvalidHasProvidedInARole 2.2011 Moose::Exception::InvalidNameForType 2.2011 Moose::Exception::InvalidOverloadOperator 2.2011 Moose::Exception::InvalidRoleApplication 2.2011 Moose::Exception::InvalidTypeConstraint 2.2011 Moose::Exception::InvalidTypeGivenToCreateParameterizedTypeConstraint 2.2011 Moose::Exception::InvalidValueForIs 2.2011 Moose::Exception::IsaDoesNotDoTheRole 2.2011 Moose::Exception::IsaLacksDoesMethod 2.2011 Moose::Exception::LazyAttributeNeedsADefault 2.2011 Moose::Exception::Legacy 2.2011 Moose::Exception::MatchActionMustBeACodeRef 2.2011 Moose::Exception::MessageParameterMustBeCodeRef 2.2011 Moose::Exception::MetaclassIsAClassNotASubclassOfGivenMetaclass 2.2011 Moose::Exception::MetaclassIsARoleNotASubclassOfGivenMetaclass 2.2011 Moose::Exception::MetaclassIsNotASubclassOfGivenMetaclass 2.2011 Moose::Exception::MetaclassMustBeASubclassOfMooseMetaClass 2.2011 Moose::Exception::MetaclassMustBeASubclassOfMooseMetaRole 2.2011 Moose::Exception::MetaclassMustBeDerivedFromClassMOPClass 2.2011 Moose::Exception::MetaclassNotLoaded 2.2011 Moose::Exception::MetaclassTypeIncompatible 2.2011 Moose::Exception::MethodExpectedAMetaclassObject 2.2011 Moose::Exception::MethodExpectsFewerArgs 2.2011 Moose::Exception::MethodExpectsMoreArgs 2.2011 Moose::Exception::MethodModifierNeedsMethodName 2.2011 Moose::Exception::MethodNameConflictInRoles 2.2011 Moose::Exception::MethodNameNotFoundInInheritanceHierarchy 2.2011 Moose::Exception::MethodNameNotGiven 2.2011 Moose::Exception::MOPAttributeNewNeedsAttributeName 2.2011 Moose::Exception::MustDefineAMethodName 2.2011 Moose::Exception::MustDefineAnAttributeName 2.2011 Moose::Exception::MustDefineAnOverloadOperator 2.2011 Moose::Exception::MustHaveAtLeastOneValueToEnumerate 2.2011 Moose::Exception::MustPassAHashOfOptions 2.2011 Moose::Exception::MustPassAMooseMetaRoleInstanceOrSubclass 2.2011 Moose::Exception::MustPassAPackageNameOrAnExistingClassMOPPackageInstance 2.2011 Moose::Exception::MustPassEvenNumberOfArguments 2.2011 Moose::Exception::MustPassEvenNumberOfAttributeOptions 2.2011 Moose::Exception::MustProvideANameForTheAttribute 2.2011 Moose::Exception::MustSpecifyAtleastOneMethod 2.2011 Moose::Exception::MustSpecifyAtleastOneRole 2.2011 Moose::Exception::MustSpecifyAtleastOneRoleToApplicant 2.2011 Moose::Exception::MustSupplyAClassMOPAttributeInstance 2.2011 Moose::Exception::MustSupplyADelegateToMethod 2.2011 Moose::Exception::MustSupplyAMetaclass 2.2011 Moose::Exception::MustSupplyAMooseMetaAttributeInstance 2.2011 Moose::Exception::MustSupplyAnAccessorTypeToConstructWith 2.2011 Moose::Exception::MustSupplyAnAttributeToConstructWith 2.2011 Moose::Exception::MustSupplyArrayRefAsCurriedArguments 2.2011 Moose::Exception::MustSupplyPackageNameAndName 2.2011 Moose::Exception::NeedsTypeConstraintUnionForTypeCoercionUnion 2.2011 Moose::Exception::NeitherAttributeNorAttributeNameIsGiven 2.2011 Moose::Exception::NeitherClassNorClassNameIsGiven 2.2011 Moose::Exception::NeitherRoleNorRoleNameIsGiven 2.2011 Moose::Exception::NeitherTypeNorTypeNameIsGiven 2.2011 Moose::Exception::NoAttributeFoundInSuperClass 2.2011 Moose::Exception::NoBodyToInitializeInAnAbstractBaseClass 2.2011 Moose::Exception::NoCasesMatched 2.2011 Moose::Exception::NoConstraintCheckForTypeConstraint 2.2011 Moose::Exception::NoDestructorClassSpecified 2.2011 Moose::Exception::NoImmutableTraitSpecifiedForClass 2.2011 Moose::Exception::NoParentGivenToSubtype 2.2011 Moose::Exception::OnlyInstancesCanBeCloned 2.2011 Moose::Exception::OperatorIsRequired 2.2011 Moose::Exception::OverloadConflictInSummation 2.2011 Moose::Exception::OverloadRequiresAMetaClass 2.2011 Moose::Exception::OverloadRequiresAMetaMethod 2.2011 Moose::Exception::OverloadRequiresAMetaOverload 2.2011 Moose::Exception::OverloadRequiresAMethodNameOrCoderef 2.2011 Moose::Exception::OverloadRequiresAnOperator 2.2011 Moose::Exception::OverloadRequiresNamesForCoderef 2.2011 Moose::Exception::OverrideConflictInComposition 2.2011 Moose::Exception::OverrideConflictInSummation 2.2011 Moose::Exception::PackageDoesNotUseMooseExporter 2.2011 Moose::Exception::PackageNameAndNameParamsNotGivenToWrap 2.2011 Moose::Exception::PackagesAndModulesAreNotCachable 2.2011 Moose::Exception::ParameterIsNotSubtypeOfParent 2.2011 Moose::Exception::ReferencesAreNotAllowedAsDefault 2.2011 Moose::Exception::RequiredAttributeLacksInitialization 2.2011 Moose::Exception::RequiredAttributeNeedsADefault 2.2011 Moose::Exception::RequiredMethodsImportedByClass 2.2011 Moose::Exception::RequiredMethodsNotImplementedByClass 2.2011 Moose::Exception::Role::Attribute 2.2011 Moose::Exception::Role::AttributeName 2.2011 Moose::Exception::Role::Class 2.2011 Moose::Exception::Role::EitherAttributeOrAttributeName 2.2011 Moose::Exception::Role::Instance 2.2011 Moose::Exception::Role::InstanceClass 2.2011 Moose::Exception::Role::InvalidAttributeOptions 2.2011 Moose::Exception::Role::Method 2.2011 Moose::Exception::Role::ParamsHash 2.2011 Moose::Exception::Role::Role 2.2011 Moose::Exception::Role::RoleForCreate 2.2011 Moose::Exception::Role::RoleForCreateMOPClass 2.2011 Moose::Exception::Role::TypeConstraint 2.2011 Moose::Exception::RoleDoesTheExcludedRole 2.2011 Moose::Exception::RoleExclusionConflict 2.2011 Moose::Exception::RoleNameRequired 2.2011 Moose::Exception::RoleNameRequiredForMooseMetaRole 2.2011 Moose::Exception::RolesDoNotSupportAugment 2.2011 Moose::Exception::RolesDoNotSupportExtends 2.2011 Moose::Exception::RolesDoNotSupportInner 2.2011 Moose::Exception::RolesDoNotSupportRegexReferencesForMethodModifiers 2.2011 Moose::Exception::RolesInCreateTakesAnArrayRef 2.2011 Moose::Exception::RolesListMustBeInstancesOfMooseMetaRole 2.2011 Moose::Exception::SingleParamsToNewMustBeHashRef 2.2011 Moose::Exception::TriggerMustBeACodeRef 2.2011 Moose::Exception::TypeConstraintCannotBeUsedForAParameterizableType 2.2011 Moose::Exception::TypeConstraintIsAlreadyCreated 2.2011 Moose::Exception::TypeParameterMustBeMooseMetaType 2.2011 Moose::Exception::UnableToCanonicalizeHandles 2.2011 Moose::Exception::UnableToCanonicalizeNonRolePackage 2.2011 Moose::Exception::UnableToRecognizeDelegateMetaclass 2.2011 Moose::Exception::UndefinedHashKeysPassedToMethod 2.2011 Moose::Exception::UnionCalledWithAnArrayRefAndAdditionalArgs 2.2011 Moose::Exception::UnionTakesAtleastTwoTypeNames 2.2011 Moose::Exception::ValidationFailedForInlineTypeConstraint 2.2011 Moose::Exception::ValidationFailedForTypeConstraint 2.2011 Moose::Exception::WrapTakesACodeRefToBless 2.2011 Moose::Exception::WrongTypeConstraintGiven 2.2011 Moose::Exporter 2.2011 Moose::Meta::Attribute 2.2011 Moose::Meta::Attribute::Native 2.2011 Moose::Meta::Attribute::Native::Trait 2.2011 Moose::Meta::Attribute::Native::Trait::Array 2.2011 Moose::Meta::Attribute::Native::Trait::Bool 2.2011 Moose::Meta::Attribute::Native::Trait::Code 2.2011 Moose::Meta::Attribute::Native::Trait::Counter 2.2011 Moose::Meta::Attribute::Native::Trait::Hash 2.2011 Moose::Meta::Attribute::Native::Trait::Number 2.2011 Moose::Meta::Attribute::Native::Trait::String 2.2011 Moose::Meta::Class 2.2011 Moose::Meta::Class::Immutable::Trait 2.2011 Moose::Meta::Instance 2.2011 Moose::Meta::Method 2.2011 Moose::Meta::Method::Accessor 2.2011 Moose::Meta::Method::Accessor::Native 2.2011 Moose::Meta::Method::Accessor::Native::Array 2.2011 Moose::Meta::Method::Accessor::Native::Array::accessor 2.2011 Moose::Meta::Method::Accessor::Native::Array::clear 2.2011 Moose::Meta::Method::Accessor::Native::Array::count 2.2011 Moose::Meta::Method::Accessor::Native::Array::delete 2.2011 Moose::Meta::Method::Accessor::Native::Array::elements 2.2011 Moose::Meta::Method::Accessor::Native::Array::first 2.2011 Moose::Meta::Method::Accessor::Native::Array::first_index 2.2011 Moose::Meta::Method::Accessor::Native::Array::get 2.2011 Moose::Meta::Method::Accessor::Native::Array::grep 2.2011 Moose::Meta::Method::Accessor::Native::Array::insert 2.2011 Moose::Meta::Method::Accessor::Native::Array::is_empty 2.2011 Moose::Meta::Method::Accessor::Native::Array::join 2.2011 Moose::Meta::Method::Accessor::Native::Array::map 2.2011 Moose::Meta::Method::Accessor::Native::Array::natatime 2.2011 Moose::Meta::Method::Accessor::Native::Array::pop 2.2011 Moose::Meta::Method::Accessor::Native::Array::push 2.2011 Moose::Meta::Method::Accessor::Native::Array::reduce 2.2011 Moose::Meta::Method::Accessor::Native::Array::set 2.2011 Moose::Meta::Method::Accessor::Native::Array::shallow_clone 2.2011 Moose::Meta::Method::Accessor::Native::Array::shift 2.2011 Moose::Meta::Method::Accessor::Native::Array::shuffle 2.2011 Moose::Meta::Method::Accessor::Native::Array::sort 2.2011 Moose::Meta::Method::Accessor::Native::Array::sort_in_place 2.2011 Moose::Meta::Method::Accessor::Native::Array::splice 2.2011 Moose::Meta::Method::Accessor::Native::Array::uniq 2.2011 Moose::Meta::Method::Accessor::Native::Array::unshift 2.2011 Moose::Meta::Method::Accessor::Native::Array::Writer 2.2011 Moose::Meta::Method::Accessor::Native::Bool::not 2.2011 Moose::Meta::Method::Accessor::Native::Bool::set 2.2011 Moose::Meta::Method::Accessor::Native::Bool::toggle 2.2011 Moose::Meta::Method::Accessor::Native::Bool::unset 2.2011 Moose::Meta::Method::Accessor::Native::Code::execute 2.2011 Moose::Meta::Method::Accessor::Native::Code::execute_method 2.2011 Moose::Meta::Method::Accessor::Native::Collection 2.2011 Moose::Meta::Method::Accessor::Native::Counter::dec 2.2011 Moose::Meta::Method::Accessor::Native::Counter::inc 2.2011 Moose::Meta::Method::Accessor::Native::Counter::reset 2.2011 Moose::Meta::Method::Accessor::Native::Counter::set 2.2011 Moose::Meta::Method::Accessor::Native::Counter::Writer 2.2011 Moose::Meta::Method::Accessor::Native::Hash 2.2011 Moose::Meta::Method::Accessor::Native::Hash::accessor 2.2011 Moose::Meta::Method::Accessor::Native::Hash::clear 2.2011 Moose::Meta::Method::Accessor::Native::Hash::count 2.2011 Moose::Meta::Method::Accessor::Native::Hash::defined 2.2011 Moose::Meta::Method::Accessor::Native::Hash::delete 2.2011 Moose::Meta::Method::Accessor::Native::Hash::elements 2.2011 Moose::Meta::Method::Accessor::Native::Hash::exists 2.2011 Moose::Meta::Method::Accessor::Native::Hash::get 2.2011 Moose::Meta::Method::Accessor::Native::Hash::is_empty 2.2011 Moose::Meta::Method::Accessor::Native::Hash::keys 2.2011 Moose::Meta::Method::Accessor::Native::Hash::kv 2.2011 Moose::Meta::Method::Accessor::Native::Hash::set 2.2011 Moose::Meta::Method::Accessor::Native::Hash::shallow_clone 2.2011 Moose::Meta::Method::Accessor::Native::Hash::values 2.2011 Moose::Meta::Method::Accessor::Native::Hash::Writer 2.2011 Moose::Meta::Method::Accessor::Native::Number::abs 2.2011 Moose::Meta::Method::Accessor::Native::Number::add 2.2011 Moose::Meta::Method::Accessor::Native::Number::div 2.2011 Moose::Meta::Method::Accessor::Native::Number::mod 2.2011 Moose::Meta::Method::Accessor::Native::Number::mul 2.2011 Moose::Meta::Method::Accessor::Native::Number::set 2.2011 Moose::Meta::Method::Accessor::Native::Number::sub 2.2011 Moose::Meta::Method::Accessor::Native::Reader 2.2011 Moose::Meta::Method::Accessor::Native::String::append 2.2011 Moose::Meta::Method::Accessor::Native::String::chomp 2.2011 Moose::Meta::Method::Accessor::Native::String::chop 2.2011 Moose::Meta::Method::Accessor::Native::String::clear 2.2011 Moose::Meta::Method::Accessor::Native::String::inc 2.2011 Moose::Meta::Method::Accessor::Native::String::length 2.2011 Moose::Meta::Method::Accessor::Native::String::match 2.2011 Moose::Meta::Method::Accessor::Native::String::prepend 2.2011 Moose::Meta::Method::Accessor::Native::String::replace 2.2011 Moose::Meta::Method::Accessor::Native::String::substr 2.2011 Moose::Meta::Method::Accessor::Native::Writer 2.2011 Moose::Meta::Method::Augmented 2.2011 Moose::Meta::Method::Constructor 2.2011 Moose::Meta::Method::Delegation 2.2011 Moose::Meta::Method::Destructor 2.2011 Moose::Meta::Method::Meta 2.2011 Moose::Meta::Method::Overridden 2.2011 Moose::Meta::Mixin::AttributeCore 2.2011 Moose::Meta::Object::Trait 2.2011 Moose::Meta::Role 2.2011 Moose::Meta::Role::Application 2.2011 Moose::Meta::Role::Application::RoleSummation 2.2011 Moose::Meta::Role::Application::ToClass 2.2011 Moose::Meta::Role::Application::ToInstance 2.2011 Moose::Meta::Role::Application::ToRole 2.2011 Moose::Meta::Role::Attribute 2.2011 Moose::Meta::Role::Composite 2.2011 Moose::Meta::Role::Method 2.2011 Moose::Meta::Role::Method::Conflicting 2.2011 Moose::Meta::Role::Method::Required 2.2011 Moose::Meta::TypeCoercion 2.2011 Moose::Meta::TypeCoercion::Union 2.2011 Moose::Meta::TypeConstraint 2.2011 Moose::Meta::TypeConstraint::Class 2.2011 Moose::Meta::TypeConstraint::DuckType 2.2011 Moose::Meta::TypeConstraint::Enum 2.2011 Moose::Meta::TypeConstraint::Parameterizable 2.2011 Moose::Meta::TypeConstraint::Parameterized 2.2011 Moose::Meta::TypeConstraint::Registry 2.2011 Moose::Meta::TypeConstraint::Role 2.2011 Moose::Meta::TypeConstraint::Union 2.2011 Moose::Object 2.2011 Moose::Role 2.2011 Moose::Util 2.2011 Moose::Util::MetaRole 2.2011 Moose::Util::TypeConstraints 2.2011 Moose::Util::TypeConstraints::Builtins 2.2011 MooseX::Adopt::Class::Accessor::Fast 0.009032 MooseX::Aliases 0.11 MooseX::Aliases::Meta::Trait::Attribute 0.11 MooseX::Aliases::Meta::Trait::Class 0.11 MooseX::Aliases::Meta::Trait::Method 0.11 MooseX::Aliases::Meta::Trait::Role 0.11 MooseX::Aliases::Meta::Trait::Role::ApplicationToClass 0.11 MooseX::Aliases::Meta::Trait::Role::ApplicationToRole 0.11 MooseX::Aliases::Meta::Trait::Role::Composite 0.11 MooseX::App::Cmd 0.32 MooseX::App::Cmd::Command 0.32 MooseX::ArrayRef 0.005 MooseX::ArrayRef::Meta::Class 0.005 MooseX::ArrayRef::Meta::Instance 0.005 MooseX::ClassAttribute 0.29 MooseX::ClassAttribute::Meta::Role::Attribute 0.29 MooseX::ClassAttribute::Trait::Application 0.29 MooseX::ClassAttribute::Trait::Application::ToClass 0.29 MooseX::ClassAttribute::Trait::Application::ToRole 0.29 MooseX::ClassAttribute::Trait::Attribute 0.29 MooseX::ClassAttribute::Trait::Class 0.29 MooseX::ClassAttribute::Trait::Mixin::HasClassAttributes 0.29 MooseX::ClassAttribute::Trait::Role 0.29 MooseX::ClassAttribute::Trait::Role::Composite 0.29 MooseX::Clone 0.06 MooseX::Clone::Meta::Attribute::Trait::Clone 0.06 MooseX::Clone::Meta::Attribute::Trait::Clone::Base 0.06 MooseX::Clone::Meta::Attribute::Trait::Clone::Std 0.06 MooseX::Clone::Meta::Attribute::Trait::Copy 0.06 MooseX::Clone::Meta::Attribute::Trait::NoClone 0.06 MooseX::Clone::Meta::Attribute::Trait::StorableClone 0.06 MooseX::ConfigFromFile 0.14 MooseX::Configuration 0.02 MooseX::Configuration::Trait::Attribute 0.02 MooseX::Configuration::Trait::Attribute::ConfigKey 0.02 MooseX::Configuration::Trait::Object 0.02 MooseX::Daemonize 0.21 MooseX::Daemonize::Core 0.21 MooseX::Daemonize::Pid 0.21 MooseX::Daemonize::Pid::File 0.21 MooseX::Daemonize::WithPidFile 0.21 MooseX::Declare 0.43 MooseX::Declare::Context 0.43 MooseX::Declare::Context::Namespaced 0.43 MooseX::Declare::Context::Parameterized 0.43 MooseX::Declare::Syntax::EmptyBlockIfMissing 0.43 MooseX::Declare::Syntax::Extending 0.43 MooseX::Declare::Syntax::InnerSyntaxHandling 0.43 MooseX::Declare::Syntax::Keyword::Class 0.43 MooseX::Declare::Syntax::Keyword::Clean 0.43 MooseX::Declare::Syntax::Keyword::Method 0.43 MooseX::Declare::Syntax::Keyword::MethodModifier 0.43 MooseX::Declare::Syntax::Keyword::Namespace 0.43 MooseX::Declare::Syntax::Keyword::Role 0.43 MooseX::Declare::Syntax::Keyword::With 0.43 MooseX::Declare::Syntax::KeywordHandling 0.43 MooseX::Declare::Syntax::MethodDeclaration 0.43 MooseX::Declare::Syntax::MooseSetup 0.43 MooseX::Declare::Syntax::NamespaceHandling 0.43 MooseX::Declare::Syntax::OptionHandling 0.43 MooseX::Declare::Syntax::RoleApplication 0.43 MooseX::Declare::Util 0.43 MooseX::Emulate::Class::Accessor::Fast 0.009032 MooseX::Emulate::Class::Accessor::Fast::Meta::Accessor unknown MooseX::Emulate::Class::Accessor::Fast::Meta::Role::Attribute unknown MooseX::Getopt 0.71 MooseX::Getopt::Basic 0.71 MooseX::Getopt::Dashes 0.71 MooseX::Getopt::GLD 0.71 MooseX::Getopt::Meta::Attribute 0.71 MooseX::Getopt::Meta::Attribute::NoGetopt 0.71 MooseX::Getopt::Meta::Attribute::Trait 0.71 MooseX::Getopt::Meta::Attribute::Trait::NoGetopt 0.71 MooseX::Getopt::OptionTypeMap 0.71 MooseX::Getopt::ProcessedArgv 0.71 MooseX::Getopt::Strict 0.71 MooseX::Getopt::Usage 0.24 MooseX::Getopt::Usage::Formatter 0.24 MooseX::Getopt::Usage::Pod::Text 0.24 MooseX::Getopt::Usage::Role::Man 0.24 MooseX::GlobRef 0.0701 MooseX::GlobRef::Object 0.0701 MooseX::GlobRef::Role::Meta::Instance 0.0701 MooseX::GlobRef::Role::Object 0.0701 MooseX::InsideOut 0.106 MooseX::InsideOut::Role::Meta::Instance 0.106 MooseX::Iterator 0.11 MooseX::Iterator::Array 0.11 MooseX::Iterator::Hash 0.11 MooseX::Iterator::Meta::Iterable 0.11 MooseX::Iterator::Role 0.11 MooseX::LazyLogDispatch 0.02 MooseX::LazyLogDispatch::Levels 0.02 MooseX::LazyRequire 0.11 MooseX::LazyRequire::Meta::Attribute::Trait::LazyRequire 0.11 MooseX::Log::Log4perl 0.47 MooseX::Log::Log4perl::Easy 0.47 MooseX::LogDispatch 1.2002 MooseX::LogDispatch::Levels unknown MooseX::MarkAsMethods 0.15 MooseX::Meta::TypeConstraint::ForceCoercion 0.01 MooseX::Method::Signatures 0.49 MooseX::Method::Signatures::Meta::Method 0.49 MooseX::Method::Signatures::Types 0.49 MooseX::MethodAttributes 0.31 MooseX::MethodAttributes::Inheritable 0.31 MooseX::MethodAttributes::Role 0.31 MooseX::MethodAttributes::Role::AttrContainer 0.31 MooseX::MethodAttributes::Role::AttrContainer::Inheritable 0.31 MooseX::MethodAttributes::Role::Meta::Class 0.31 MooseX::MethodAttributes::Role::Meta::Map 0.31 MooseX::MethodAttributes::Role::Meta::Method 0.31 MooseX::MethodAttributes::Role::Meta::Method::MaybeWrapped 0.31 MooseX::MethodAttributes::Role::Meta::Method::Wrapped 0.31 MooseX::MethodAttributes::Role::Meta::Role 0.31 MooseX::MethodAttributes::Role::Meta::Role::Application 0.31 MooseX::MethodAttributes::Role::Meta::Role::Application::Summation 0.31 MooseX::NonMoose 0.26 MooseX::NonMoose::InsideOut 0.26 MooseX::NonMoose::Meta::Role::Class 0.26 MooseX::NonMoose::Meta::Role::Constructor 0.26 MooseX::Object::Pluggable 0.0014 MooseX::OneArgNew 0.005 MooseX::Param 0.02 MooseX::Params::Validate 0.21 MooseX::Params::Validate::Exception::ValidationFailedForTypeConstraint 0.21 MooseX::POE 0.215 MooseX::POE::Aliased 0.215 MooseX::POE::Meta::Method::State 0.215 MooseX::POE::Meta::Role 0.215 MooseX::POE::Meta::Trait 0.215 MooseX::POE::Meta::Trait::Class 0.215 MooseX::POE::Meta::Trait::Instance 0.215 MooseX::POE::Meta::Trait::Object 0.215 MooseX::POE::Meta::Trait::SweetArgs 0.215 MooseX::POE::Role 0.215 MooseX::POE::SweetArgs 0.215 MooseX::RelatedClassRoles 0.004 MooseX::Role::Cmd 0.10 MooseX::Role::Cmd::Meta::Attribute::Trait unknown MooseX::Role::Parameterised 1.10 MooseX::Role::Parameterized 1.10 MooseX::Role::Parameterized::Meta::Role::Parameterized 1.10 MooseX::Role::Parameterized::Meta::Trait::Parameterizable 1.10 MooseX::Role::Parameterized::Meta::Trait::Parameterized 1.10 MooseX::Role::Parameterized::Parameters 1.10 MooseX::Role::TraitConstructor 0.01 MooseX::Role::WithOverloading 0.17 MooseX::Role::WithOverloading::Meta::Role 0.17 MooseX::Role::WithOverloading::Meta::Role::Application 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::Composite 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::Composite::ToClass 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::Composite::ToInstance 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::Composite::ToRole 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::FixOverloadedRefs 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::ToClass 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::ToInstance 0.17 MooseX::Role::WithOverloading::Meta::Role::Application::ToRole 0.17 MooseX::Role::WithOverloading::Meta::Role::Composite 0.17 MooseX::SemiAffordanceAccessor 0.10 MooseX::SemiAffordanceAccessor::Role::Attribute 0.10 MooseX::SetOnce 0.200002 MooseX::SimpleConfig 0.11 MooseX::Singleton 0.30 MooseX::Singleton::Role::Meta::Class 0.30 MooseX::Singleton::Role::Meta::Instance 0.30 MooseX::Singleton::Role::Meta::Method::Constructor 0.30 MooseX::Singleton::Role::Object 0.30 MooseX::Storage 0.52 MooseX::Storage::Base::WithChecksum 0.52 MooseX::Storage::Basic 0.52 MooseX::Storage::Deferred 0.52 MooseX::Storage::Engine 0.52 MooseX::Storage::Engine::IO::AtomicFile 0.52 MooseX::Storage::Engine::IO::File 0.52 MooseX::Storage::Engine::Trait::DisableCycleDetection 0.52 MooseX::Storage::Engine::Trait::OnlyWhenBuilt 0.52 MooseX::Storage::Format::JSON 0.52 MooseX::Storage::Format::Storable 0.52 MooseX::Storage::Format::YAML 0.52 MooseX::Storage::IO::AtomicFile 0.52 MooseX::Storage::IO::File 0.52 MooseX::Storage::IO::StorableFile 0.52 MooseX::Storage::Meta::Attribute::DoNotSerialize 0.52 MooseX::Storage::Meta::Attribute::Trait::DoNotSerialize 0.52 MooseX::Storage::Traits::DisableCycleDetection 0.52 MooseX::Storage::Traits::OnlyWhenBuilt 0.52 MooseX::Storage::Util 0.52 MooseX::StrictConstructor 0.21 MooseX::StrictConstructor::Trait::Class 0.21 MooseX::StrictConstructor::Trait::Method::Constructor 0.21 MooseX::Traits 0.13 MooseX::Traits::Pluggable 0.12 MooseX::Traits::Util 0.13 MooseX::Types 0.50 MooseX::Types::Base 0.50 MooseX::Types::CheckedUtilExports 0.50 MooseX::Types::Combine 0.50 MooseX::Types::Common 0.001014 MooseX::Types::Common::Numeric 0.001014 MooseX::Types::Common::String 0.001014 MooseX::Types::DateTime 0.13 MooseX::Types::LoadableClass 0.015 MooseX::Types::Moose 0.50 MooseX::Types::Path::Class 0.09 MooseX::Types::Path::Tiny 0.012 MooseX::Types::Perl 0.101343 MooseX::Types::Set::Object 0.05 MooseX::Types::Stringlike 0.003 MooseX::Types::Structured 0.36 MooseX::Types::TypeDecorator 0.50 MooseX::Types::UndefinedType 0.50 MooseX::Types::Util 0.50 MooseX::Types::Wrapper 0.50 MooseX::Workers 0.24 MooseX::Workers::Engine 0.24 MooseX::Workers::Job 0.24 MooX::HandlesVia 0.001008 MooX::Types::MooseLike 0.29 MooX::Types::MooseLike::Base 0.29 MooX::Types::MooseLike::Numeric 1.03 Mozilla::CA 20180117 MRO::Compat 0.13 multidimensional 0.014 MyApp::Schema 0.001 MyApplication::Form::User unknown MyPersonHandler 0.14 namespace::autoclean 0.28 namespace::clean 0.27 Net::Domain::TLD 1.75 Net::EmptyPort unknown Net::HTTP 6.18 Net::HTTP::Methods 6.18 Net::HTTP::NB 6.18 Net::HTTPS 6.18 Net::Server 2.009 Net::Server::Daemonize 0.06 Net::Server::Fork unknown Net::Server::HTTP unknown Net::Server::INET unknown Net::Server::Log::Log::Log4perl unknown Net::Server::Log::Sys::Syslog unknown Net::Server::Multiplex unknown Net::Server::MultiType unknown Net::Server::PreFork unknown Net::Server::PreForkSimple unknown Net::Server::Proto unknown Net::Server::Proto::SSL unknown Net::Server::Proto::SSLEAY unknown Net::Server::Proto::TCP unknown Net::Server::Proto::UDP unknown Net::Server::Proto::UNIX unknown Net::Server::Proto::UNIXDGRAM unknown Net::Server::PSGI unknown Net::Server::SIG 0.03 Net::Server::Single unknown Net::SSLeay 1.85 Net::SSLeay::Handle 0.61 NetAddr::IP 4.079 NetAddr::IP::InetBase 0.08 NetAddr::IP::Lite 1.57 NetAddr::IP::Util 1.53 NetAddr::IP::Util_IS 1 NetAddr::IP::UtilPP 1.09 ntheory 0.70 Number::Compare 0.03 Number::Format 1.75 Number::Misc 1.2 Obj 1.39 Object::InsideOut 4.04 Object::InsideOut unknown Object::InsideOut unknown Object::InsideOut unknown Object::InsideOut 4.04 Object::InsideOut unknown Object::InsideOut unknown Object::InsideOut unknown Object::InsideOut unknown Object::InsideOut unknown Object::InsideOut unknown Object::InsideOut::Exception 4.04 Object::InsideOut::Metadata 4.04 Object::InsideOut::Secure 4.04 Object::InsideOut::Util 4.04 Object::Signature 1.07 Object::Signature::File 1.07 ojo unknown ok 1.302138 OLE::Storage_Lite::PPS 0.19 oo unknown oose 2.2011 Package::DeprecationManager 0.17 Package::Stash 0.37 Package::Stash::PP 0.37 Package::Stash::XS 0.28 Package::Variant 1.003002 PadWalker 2.3 Parallel::ForkManager 1.20 Params::Util 1.07 Params::Validate 1.29 Params::Validate::Constants 1.29 Params::Validate::PP 1.29 Params::Validate::XS 1.29 Params::ValidationCompiler 0.27 Params::ValidationCompiler::Compiler 0.27 Params::ValidationCompiler::Exceptions 0.27 Parse::Method::Signatures 1.003019 Parse::Method::Signatures::Param unknown Parse::Method::Signatures::Param::Bindable unknown Parse::Method::Signatures::Param::Named unknown Parse::Method::Signatures::Param::Placeholder unknown Parse::Method::Signatures::Param::Positional unknown Parse::Method::Signatures::Param::Unpacked unknown Parse::Method::Signatures::Param::Unpacked::Array unknown Parse::Method::Signatures::Param::Unpacked::Hash unknown Parse::Method::Signatures::ParamCollection unknown Parse::Method::Signatures::Sig unknown Parse::Method::Signatures::TypeConstraint unknown Parse::Method::Signatures::Types unknown Parse::PMFile 0.41 Parse::RecDescent 1.967015 Parse::Yapp 1.21 Parse::Yapp::Driver 1.21 Parse::Yapp::Grammar unknown Parse::Yapp::Lalr unknown Parse::Yapp::Options unknown Parse::Yapp::Output unknown Parse::Yapp::Parse unknown Path::Class 0.37 Path::Class::Dir 0.37 Path::Class::Entity 0.37 Path::Class::File 0.37 Path::FindDev unknown Path::FindDev::Object unknown Path::IsDev 1.001003 Path::IsDev::Heuristic::Changelog 1.001003 Path::IsDev::Heuristic::DevDirMarker 1.001003 Path::IsDev::Heuristic::Makefile 1.001003 Path::IsDev::Heuristic::META 1.001003 Path::IsDev::Heuristic::MYMETA 1.001003 Path::IsDev::Heuristic::TestDir 1.001003 Path::IsDev::Heuristic::Tool::Dzil 1.001003 Path::IsDev::Heuristic::Tool::MakeMaker 1.001003 Path::IsDev::Heuristic::Tool::ModuleBuild 1.001003 Path::IsDev::Heuristic::VCS::Git 1.001003 Path::IsDev::HeuristicSet::Basic 1.001003 Path::IsDev::NegativeHeuristic::HomeDir 1.001003 Path::IsDev::NegativeHeuristic::IsDev::IgnoreFile 1.001003 Path::IsDev::NegativeHeuristic::PerlINC 1.001003 Path::IsDev::Object 1.001003 Path::IsDev::Result 1.001003 Path::IsDev::Role::Heuristic 1.001003 Path::IsDev::Role::HeuristicSet 1.001003 Path::IsDev::Role::HeuristicSet::Simple 1.001003 Path::IsDev::Role::Matcher::Child::BaseName::MatchRegexp 1.001003 Path::IsDev::Role::Matcher::Child::BaseName::MatchRegexp::File 1.001003 Path::IsDev::Role::Matcher::Child::Exists::Any 1.001003 Path::IsDev::Role::Matcher::Child::Exists::Any::Dir 1.001003 Path::IsDev::Role::Matcher::Child::Exists::Any::File 1.001003 Path::IsDev::Role::Matcher::FullPath::Is::Any 1.001003 Path::IsDev::Role::NegativeHeuristic 1.001003 Path::Tiny 0.106 PDL unknown PDL::Bad unknown PDL::Basic unknown PDL::CallExt unknown PDL::Char unknown PDL::Complex 2.009 PDL::Compression unknown PDL::Constants 0.02 PDL::Core 2.019 PDL::Core::Dev unknown PDL::Dbg unknown PDL::Demos::BAD2_demo unknown PDL::Demos::BAD_demo unknown PDL::Demos::Cartography_demo unknown PDL::Demos::General unknown PDL::Demos::Gnuplot_demo unknown PDL::Demos::PGPLOT_demo unknown PDL::Demos::PGPLOT_OO_demo unknown PDL::Demos::Prima unknown PDL::Demos::Routines unknown PDL::Demos::Transform_demo unknown PDL::Demos::TriD1 unknown PDL::Demos::TriD2 unknown PDL::Demos::TriDGallery unknown PDL::Doc::Config unknown PDL::Doc::Perldl unknown PDL::FFT unknown PDL::Filter::Linear unknown PDL::Filter::LinSmooth unknown PDL::Fit::Gaussian unknown PDL::Fit::Linfit unknown PDL::Fit::LM unknown PDL::Fit::Polynomial unknown PDL::Func unknown PDL::Graphics2D unknown PDL::Graphics::IIS unknown PDL::Graphics::Limits 0.01 PDL::Graphics::LUT unknown PDL::Graphics::PGPLOT unknown PDL::Graphics::PGPLOT::Window unknown PDL::Graphics::PGPLOTOptions unknown PDL::Graphics::State unknown PDL::GSL::DIFF unknown PDL::GSL::INTEG unknown PDL::GSL::INTERP unknown PDL::GSL::MROOT unknown PDL::GSL::RNG unknown PDL::GSLSF::AIRY unknown PDL::GSLSF::BESSEL unknown PDL::GSLSF::CLAUSEN unknown PDL::GSLSF::COULOMB unknown PDL::GSLSF::COUPLING unknown PDL::GSLSF::DAWSON unknown PDL::GSLSF::DEBYE unknown PDL::GSLSF::DILOG unknown PDL::GSLSF::ELEMENTARY unknown PDL::GSLSF::ELLINT unknown PDL::GSLSF::ELLJAC unknown PDL::GSLSF::ERF unknown PDL::GSLSF::EXP unknown PDL::GSLSF::EXPINT unknown PDL::GSLSF::FERMI_DIRAC unknown PDL::GSLSF::GAMMA unknown PDL::GSLSF::GEGENBAUER unknown PDL::GSLSF::HYPERG unknown PDL::GSLSF::LAGUERRE unknown PDL::GSLSF::LEGENDRE unknown PDL::GSLSF::LOG unknown PDL::GSLSF::POLY unknown PDL::GSLSF::POW_INT unknown PDL::GSLSF::PSI unknown PDL::GSLSF::SYNCHROTRON unknown PDL::GSLSF::TRANSPORT unknown PDL::GSLSF::TRIG unknown PDL::GSLSF::ZETA unknown PDL::Image2D unknown PDL::ImageND unknown PDL::ImageRGB unknown PDL::Install::Files 2.009 PDL::IO::Dicom unknown PDL::IO::Dumper 1.3.2 PDL::IO::FastRaw unknown PDL::IO::FITS 0.92 PDL::IO::FlexRaw unknown PDL::IO::GD unknown PDL::IO::Misc unknown PDL::IO::Pic unknown PDL::IO::Pnm unknown PDL::IO::Storable unknown PDL::Lite unknown PDL::LiteF unknown PDL::Lvalue unknown PDL::Math unknown PDL::Matrix 0.5 PDL::MatrixOps unknown PDL::MyMod unknown PDL::NiceSlice 1.001 PDL::Ops unknown PDL::Opt::Simplex unknown PDL::Options 0.92 PDL::Perldl2::Plugin::CleanErrors unknown PDL::Perldl2::Plugin::NiceSlice unknown PDL::Perldl2::Plugin::PDLCommands unknown PDL::Perldl2::Plugin::PrintControl unknown PDL::Perldl2::Profile::Perldl2 0.008 PDL::Perldl2::Script unknown PDL::PodParser unknown PDL::PP::Code unknown PDL::PP::Dump unknown PDL::PP::PdlDimsObj unknown PDL::PP::PdlParObj unknown PDL::PP::Rule 2.3 PDL::PP::Signature unknown PDL::Primitive unknown PDL::Reduce unknown PDL::Slices unknown PDL::Transform unknown PDL::Transform::Cartography 0.6 PDL::Types unknown PDL::Ufunc unknown PDL::Version 2.019 Pegex 0.64 Pegex::Base unknown Pegex::Bootstrap unknown Pegex::Compiler unknown Pegex::Grammar unknown Pegex::Grammar::Atoms unknown Pegex::Input unknown Pegex::Module unknown Pegex::Optimizer unknown Pegex::Parser unknown Pegex::Parser::Indent unknown Pegex::Pegex::AST unknown Pegex::Pegex::Grammar unknown Pegex::Receiver unknown Pegex::Regex unknown Pegex::Tree unknown Pegex::Tree::Wrap unknown Perl6::Export 0.07 Perl6::Form 0.06 Perl::Critic 1.132 Perl::Critic::Annotation 1.132 Perl::Critic::Command 1.132 Perl::Critic::Config 1.132 Perl::Critic::Document 1.132 Perl::Critic::Exception 1.132 Perl::Critic::Exception::AggregateConfiguration 1.132 Perl::Critic::Exception::Configuration 1.132 Perl::Critic::Exception::Configuration::Generic 1.132 Perl::Critic::Exception::Configuration::NonExistentPolicy 1.132 Perl::Critic::Exception::Configuration::Option 1.132 Perl::Critic::Exception::Configuration::Option::Global 1.132 Perl::Critic::Exception::Configuration::Option::Global::ExtraParameter 1.132 Perl::Critic::Exception::Configuration::Option::Global::ParameterValue 1.132 Perl::Critic::Exception::Configuration::Option::Policy 1.132 Perl::Critic::Exception::Configuration::Option::Policy::ExtraParameter 1.132 Perl::Critic::Exception::Configuration::Option::Policy::ParameterValue 1.132 Perl::Critic::Exception::Fatal 1.132 Perl::Critic::Exception::Fatal::Generic 1.132 Perl::Critic::Exception::Fatal::Internal 1.132 Perl::Critic::Exception::Fatal::PolicyDefinition 1.132 Perl::Critic::Exception::IO 1.132 Perl::Critic::Exception::Parse 1.132 Perl::Critic::OptionsProcessor 1.132 Perl::Critic::Policy 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitBooleanGrep 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitComplexMappings 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitLvalueSubstr 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitReverseSortBlock 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitSleepViaSelect 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitStringyEval 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitStringySplit 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitUniversalCan 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitUniversalIsa 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitUselessTopic 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitVoidGrep 1.132 Perl::Critic::Policy::BuiltinFunctions::ProhibitVoidMap 1.132 Perl::Critic::Policy::BuiltinFunctions::RequireBlockGrep 1.132 Perl::Critic::Policy::BuiltinFunctions::RequireBlockMap 1.132 Perl::Critic::Policy::BuiltinFunctions::RequireGlobFunction 1.132 Perl::Critic::Policy::BuiltinFunctions::RequireSimpleSortBlock 1.132 Perl::Critic::Policy::ClassHierarchies::ProhibitAutoloading 1.132 Perl::Critic::Policy::ClassHierarchies::ProhibitExplicitISA 1.132 Perl::Critic::Policy::ClassHierarchies::ProhibitOneArgBless 1.132 Perl::Critic::Policy::CodeLayout::ProhibitHardTabs 1.132 Perl::Critic::Policy::CodeLayout::ProhibitParensWithBuiltins 1.132 Perl::Critic::Policy::CodeLayout::ProhibitQuotedWordLists 1.132 Perl::Critic::Policy::CodeLayout::ProhibitTrailingWhitespace 1.132 Perl::Critic::Policy::CodeLayout::RequireConsistentNewlines 1.132 Perl::Critic::Policy::CodeLayout::RequireTidyCode 1.132 Perl::Critic::Policy::CodeLayout::RequireTrailingCommas 1.132 Perl::Critic::Policy::ControlStructures::ProhibitCascadingIfElse 1.132 Perl::Critic::Policy::ControlStructures::ProhibitCStyleForLoops 1.132 Perl::Critic::Policy::ControlStructures::ProhibitDeepNests 1.132 Perl::Critic::Policy::ControlStructures::ProhibitLabelsWithSpecialBlockNames 1.132 Perl::Critic::Policy::ControlStructures::ProhibitMutatingListFunctions 1.132 Perl::Critic::Policy::ControlStructures::ProhibitNegativeExpressionsInUnlessAndUntilConditions 1.132 Perl::Critic::Policy::ControlStructures::ProhibitPostfixControls 1.132 Perl::Critic::Policy::ControlStructures::ProhibitUnlessBlocks 1.132 Perl::Critic::Policy::ControlStructures::ProhibitUnreachableCode 1.132 Perl::Critic::Policy::ControlStructures::ProhibitUntilBlocks 1.132 Perl::Critic::Policy::ControlStructures::ProhibitYadaOperator 1.132 Perl::Critic::Policy::Documentation::PodSpelling 1.132 Perl::Critic::Policy::Documentation::RequirePackageMatchesPodName 1.132 Perl::Critic::Policy::Documentation::RequirePodAtEnd 1.132 Perl::Critic::Policy::Documentation::RequirePodLinksIncludeText 1.132 Perl::Critic::Policy::Documentation::RequirePodSections 1.132 Perl::Critic::Policy::ErrorHandling::RequireCarping 1.132 Perl::Critic::Policy::ErrorHandling::RequireCheckingReturnValueOfEval 1.132 Perl::Critic::Policy::InputOutput::ProhibitBacktickOperators 1.132 Perl::Critic::Policy::InputOutput::ProhibitBarewordFileHandles 1.132 Perl::Critic::Policy::InputOutput::ProhibitExplicitStdin 1.132 Perl::Critic::Policy::InputOutput::ProhibitInteractiveTest 1.132 Perl::Critic::Policy::InputOutput::ProhibitJoinedReadline 1.132 Perl::Critic::Policy::InputOutput::ProhibitOneArgSelect 1.132 Perl::Critic::Policy::InputOutput::ProhibitReadlineInForLoop 1.132 Perl::Critic::Policy::InputOutput::ProhibitTwoArgOpen 1.132 Perl::Critic::Policy::InputOutput::RequireBracedFileHandleWithPrint 1.132 Perl::Critic::Policy::InputOutput::RequireBriefOpen 1.132 Perl::Critic::Policy::InputOutput::RequireCheckedClose 1.132 Perl::Critic::Policy::InputOutput::RequireCheckedOpen 1.132 Perl::Critic::Policy::InputOutput::RequireCheckedSyscalls 1.132 Perl::Critic::Policy::InputOutput::RequireEncodingWithUTF8Layer 1.132 Perl::Critic::Policy::Miscellanea::ProhibitFormats 1.132 Perl::Critic::Policy::Miscellanea::ProhibitTies 1.132 Perl::Critic::Policy::Miscellanea::ProhibitUnrestrictedNoCritic 1.132 Perl::Critic::Policy::Miscellanea::ProhibitUselessNoCritic 1.132 Perl::Critic::Policy::Modules::ProhibitAutomaticExportation 1.132 Perl::Critic::Policy::Modules::ProhibitConditionalUseStatements 1.132 Perl::Critic::Policy::Modules::ProhibitEvilModules 1.132 Perl::Critic::Policy::Modules::ProhibitExcessMainComplexity 1.132 Perl::Critic::Policy::Modules::ProhibitMultiplePackages 1.132 Perl::Critic::Policy::Modules::RequireBarewordIncludes 1.132 Perl::Critic::Policy::Modules::RequireEndWithOne 1.132 Perl::Critic::Policy::Modules::RequireExplicitPackage 1.132 Perl::Critic::Policy::Modules::RequireFilenameMatchesPackage 1.132 Perl::Critic::Policy::Modules::RequireNoMatchVarsWithUseEnglish 1.132 Perl::Critic::Policy::Modules::RequireVersionVar 1.132 Perl::Critic::Policy::NamingConventions::Capitalization 1.132 Perl::Critic::Policy::NamingConventions::ProhibitAmbiguousNames 1.132 Perl::Critic::Policy::Objects::ProhibitIndirectSyntax 1.132 Perl::Critic::Policy::References::ProhibitDoubleSigils 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitCaptureWithoutTest 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitComplexRegexes 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitEnumeratedClasses 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitEscapedMetacharacters 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitFixedStringMatches 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitSingleCharAlternation 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitUnusedCapture 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitUnusualDelimiters 1.132 Perl::Critic::Policy::RegularExpressions::ProhibitUselessTopic 1.132 Perl::Critic::Policy::RegularExpressions::RequireBracesForMultiline 1.132 Perl::Critic::Policy::RegularExpressions::RequireDotMatchAnything 1.132 Perl::Critic::Policy::RegularExpressions::RequireExtendedFormatting 1.132 Perl::Critic::Policy::RegularExpressions::RequireLineBoundaryMatching 1.132 Perl::Critic::Policy::Subroutines::ProhibitAmpersandSigils 1.132 Perl::Critic::Policy::Subroutines::ProhibitBuiltinHomonyms 1.132 Perl::Critic::Policy::Subroutines::ProhibitExcessComplexity 1.132 Perl::Critic::Policy::Subroutines::ProhibitExplicitReturnUndef 1.132 Perl::Critic::Policy::Subroutines::ProhibitManyArgs 1.132 Perl::Critic::Policy::Subroutines::ProhibitNestedSubs 1.132 Perl::Critic::Policy::Subroutines::ProhibitReturnSort 1.132 Perl::Critic::Policy::Subroutines::ProhibitSubroutinePrototypes 1.132 Perl::Critic::Policy::Subroutines::ProhibitUnusedPrivateSubroutines 1.132 Perl::Critic::Policy::Subroutines::ProtectPrivateSubs 1.132 Perl::Critic::Policy::Subroutines::RequireArgUnpacking 1.132 Perl::Critic::Policy::Subroutines::RequireFinalReturn 1.132 Perl::Critic::Policy::TestingAndDebugging::ProhibitNoStrict 1.132 Perl::Critic::Policy::TestingAndDebugging::ProhibitNoWarnings 1.132 Perl::Critic::Policy::TestingAndDebugging::ProhibitProlongedStrictureOverride 1.132 Perl::Critic::Policy::TestingAndDebugging::RequireTestLabels 1.132 Perl::Critic::Policy::TestingAndDebugging::RequireUseStrict 1.132 Perl::Critic::Policy::TestingAndDebugging::RequireUseWarnings 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitCommaSeparatedStatements 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitComplexVersion 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitConstantPragma 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitEmptyQuotes 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitEscapedCharacters 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitImplicitNewlines 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitInterpolationOfLiterals 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitLeadingZeros 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitLongChainsOfMethodCalls 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitMagicNumbers 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitMismatchedOperators 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitMixedBooleanOperators 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitNoisyQuotes 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitQuotesAsQuotelikeOperatorDelimiters 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitSpecialLiteralHeredocTerminator 1.132 Perl::Critic::Policy::ValuesAndExpressions::ProhibitVersionStrings 1.132 Perl::Critic::Policy::ValuesAndExpressions::RequireConstantVersion 1.132 Perl::Critic::Policy::ValuesAndExpressions::RequireInterpolationOfMetachars 1.132 Perl::Critic::Policy::ValuesAndExpressions::RequireNumberSeparators 1.132 Perl::Critic::Policy::ValuesAndExpressions::RequireQuotedHeredocTerminator 1.132 Perl::Critic::Policy::ValuesAndExpressions::RequireUpperCaseHeredocTerminator 1.132 Perl::Critic::Policy::Variables::ProhibitAugmentedAssignmentInDeclaration 1.132 Perl::Critic::Policy::Variables::ProhibitConditionalDeclarations 1.132 Perl::Critic::Policy::Variables::ProhibitEvilVariables 1.132 Perl::Critic::Policy::Variables::ProhibitLocalVars 1.132 Perl::Critic::Policy::Variables::ProhibitMatchVars 1.132 Perl::Critic::Policy::Variables::ProhibitPackageVars 1.132 Perl::Critic::Policy::Variables::ProhibitPerl4PackageNames 1.132 Perl::Critic::Policy::Variables::ProhibitPunctuationVars 1.132 Perl::Critic::Policy::Variables::ProhibitReusedNames 1.132 Perl::Critic::Policy::Variables::ProhibitUnusedVariables 1.132 Perl::Critic::Policy::Variables::ProtectPrivateVars 1.132 Perl::Critic::Policy::Variables::RequireInitializationForLocalVars 1.132 Perl::Critic::Policy::Variables::RequireLexicalLoopIterators 1.132 Perl::Critic::Policy::Variables::RequireLocalizedPunctuationVars 1.132 Perl::Critic::Policy::Variables::RequireNegativeIndices 1.132 Perl::Critic::PolicyConfig 1.132 Perl::Critic::PolicyFactory 1.132 Perl::Critic::PolicyListing 1.132 Perl::Critic::PolicyParameter 1.132 Perl::Critic::PolicyParameter::Behavior 1.132 Perl::Critic::PolicyParameter::Behavior::Boolean 1.132 Perl::Critic::PolicyParameter::Behavior::Enumeration 1.132 Perl::Critic::PolicyParameter::Behavior::Integer 1.132 Perl::Critic::PolicyParameter::Behavior::String 1.132 Perl::Critic::PolicyParameter::Behavior::StringList 1.132 Perl::Critic::ProfilePrototype 1.132 Perl::Critic::Statistics 1.132 Perl::Critic::TestUtils 1.132 Perl::Critic::Theme 1.132 Perl::Critic::ThemeListing 1.132 Perl::Critic::UserProfile 1.132 Perl::Critic::Utils 1.132 Perl::Critic::Utils::Constants 1.132 Perl::Critic::Utils::DataConversion 1.132 Perl::Critic::Utils::McCabe 1.132 Perl::Critic::Utils::Perl 1.132 Perl::Critic::Utils::POD 1.132 Perl::Critic::Utils::POD::ParseInteriorSequence 1.132 Perl::Critic::Utils::PPI 1.132 Perl::Critic::Violation 1.132 Perl::PrereqScanner 1.023 Perl::PrereqScanner::Scanner 1.023 Perl::PrereqScanner::Scanner::Aliased 1.023 Perl::PrereqScanner::Scanner::Moose 1.023 Perl::PrereqScanner::Scanner::Perl5 1.023 Perl::PrereqScanner::Scanner::POE 1.023 Perl::PrereqScanner::Scanner::Superclass 1.023 Perl::PrereqScanner::Scanner::TestMore 1.023 Perl::Tidy unknown Perl::Unsafe::Signals 0.03 Perl::Version 1.013 PerlIO::gzip 0.20 PerlIO::Layers 0.011 PerlIO::utf8_strict 0.007 PerlIO::via::Timeout 0.32 Pinto 0.14 Pinto::Action 0.14 Pinto::Action::Add 0.14 Pinto::Action::Clean 0.14 Pinto::Action::Copy 0.14 Pinto::Action::Default 0.14 Pinto::Action::Delete 0.14 Pinto::Action::Diff 0.14 Pinto::Action::Install 0.14 Pinto::Action::Kill 0.14 Pinto::Action::List 0.14 Pinto::Action::Lock 0.14 Pinto::Action::Log 0.14 Pinto::Action::Look 0.14 Pinto::Action::Merge 0.14 Pinto::Action::New 0.14 Pinto::Action::Nop 0.14 Pinto::Action::Pin 0.14 Pinto::Action::Props 0.14 Pinto::Action::Pull 0.14 Pinto::Action::Register 0.14 Pinto::Action::Rename 0.14 Pinto::Action::Reset 0.14 Pinto::Action::Revert 0.14 Pinto::Action::Roots 0.14 Pinto::Action::Stacks 0.14 Pinto::Action::Statistics 0.14 Pinto::Action::Unlock 0.14 Pinto::Action::Unpin 0.14 Pinto::Action::Unregister 0.14 Pinto::Action::Update 0.14 Pinto::Action::Verify 0.14 Pinto::ArchiveUnpacker 0.14 Pinto::Chrome 0.14 Pinto::Chrome::Net 0.14 Pinto::Chrome::Term 0.14 Pinto::Config 0.14 Pinto::Constants 0.14 Pinto::Database 0.14 Pinto::Difference 0.14 Pinto::DifferenceEntry 0.14 Pinto::Editor 0.14 Pinto::Editor::Clip 0.14 Pinto::Editor::Edit 0.14 Pinto::Exception 0.14 Pinto::Globals 0.14 Pinto::IndexReader 0.14 Pinto::IndexWriter 0.14 Pinto::Initializer 0.14 Pinto::Locator 0.14 Pinto::Locator::Mirror 0.14 Pinto::Locator::Multiplex 0.14 Pinto::Locator::Stratopan 0.14 Pinto::Locker 0.14 Pinto::Migrator 0.14 Pinto::ModlistWriter 0.14 Pinto::PackageExtractor 0.14 Pinto::PrerequisiteWalker 0.14 Pinto::Remote 0.14 Pinto::Remote::Action 0.14 Pinto::Remote::Action::Add 0.14 Pinto::Remote::Action::Install 0.14 Pinto::Remote::Result 0.14 Pinto::Repository 0.14 Pinto::Result 0.14 Pinto::RevisionWalker 0.14 Pinto::Role::Committable 0.14 Pinto::Role::Installer 0.14 Pinto::Role::PauseConfig 0.14 Pinto::Role::Plated 0.14 Pinto::Role::Puller 0.14 Pinto::Role::Schema::Result 0.14 Pinto::Role::Transactional 0.14 Pinto::Role::UserAgent 0.14 Pinto::Schema 0.14 Pinto::Schema::Result::Ancestry 0.14 Pinto::Schema::Result::Distribution 0.14 Pinto::Schema::Result::Package 0.14 Pinto::Schema::Result::Prerequisite 0.14 Pinto::Schema::Result::Registration 0.14 Pinto::Schema::Result::Revision 0.14 Pinto::Schema::Result::Stack 0.14 Pinto::Schema::ResultSet::Distribution 0.14 Pinto::Schema::ResultSet::Package 0.14 Pinto::Schema::ResultSet::Registration 0.14 Pinto::Server 0.14 Pinto::Server::Responder 0.14 Pinto::Server::Responder::Action 0.14 Pinto::Server::Responder::File 0.14 Pinto::Server::Router 0.14 Pinto::Shell 0.14 Pinto::Statistics 0.14 Pinto::Store 0.14 Pinto::Target 0.14 Pinto::Target::Distribution 0.14 Pinto::Target::Package 0.14 Pinto::Types 0.14 Pinto::Util 0.14 Plack 1.0047 Plack::App::Cascade unknown Plack::App::CGIBin unknown Plack::App::Directory unknown Plack::App::File unknown Plack::App::PSGIBin unknown Plack::App::URLMap unknown Plack::App::WrapCGI unknown Plack::Builder unknown Plack::Component unknown Plack::Handler unknown Plack::Handler::Apache1 unknown Plack::Handler::Apache2 unknown Plack::Handler::Apache2::Registry unknown Plack::Handler::CGI unknown Plack::Handler::FCGI unknown Plack::Handler::HTTP::Server::PSGI unknown Plack::Handler::HTTP::Server::Simple 0.16 Plack::Handler::Standalone unknown Plack::Handler::Starman unknown Plack::HTTPParser unknown Plack::HTTPParser::PP unknown Plack::Loader unknown Plack::Loader::Delayed unknown Plack::Loader::Restarter unknown Plack::Loader::Shotgun unknown Plack::LWPish unknown Plack::Middleware unknown Plack::Middleware::AccessLog unknown Plack::Middleware::AccessLog::Timed unknown Plack::Middleware::Auth::Basic unknown Plack::Middleware::BufferedStreaming unknown Plack::Middleware::Chunked unknown Plack::Middleware::Conditional unknown Plack::Middleware::ConditionalGET unknown Plack::Middleware::ContentLength unknown Plack::Middleware::ContentMD5 unknown Plack::Middleware::ErrorDocument unknown Plack::Middleware::FixMissingBodyInRedirect 0.12 Plack::Middleware::Head unknown Plack::Middleware::HTTPExceptions unknown Plack::Middleware::IIS6ScriptNameFix unknown Plack::Middleware::IIS7KeepAliveFix unknown Plack::Middleware::JSONP unknown Plack::Middleware::LighttpdScriptNameFix unknown Plack::Middleware::Lint unknown Plack::Middleware::Log4perl unknown Plack::Middleware::LogDispatch unknown Plack::Middleware::MethodOverride 0.20 Plack::Middleware::NullLogger unknown Plack::Middleware::RearrangeHeaders unknown Plack::Middleware::Recursive unknown Plack::Middleware::Refresh unknown Plack::Middleware::RemoveRedundantBody 0.06 Plack::Middleware::ReverseProxy 0.15 Plack::Middleware::Runtime unknown Plack::Middleware::SimpleContentFilter unknown Plack::Middleware::SimpleLogger unknown Plack::Middleware::StackTrace unknown Plack::Middleware::Static unknown Plack::Middleware::XFramework unknown Plack::Middleware::XSendfile unknown Plack::MIME unknown Plack::Request 1.0047 Plack::Request::Upload unknown Plack::Response 1.0047 Plack::Runner unknown Plack::TempBuffer unknown Plack::Test unknown Plack::Test::ExternalServer 0.02 Plack::Test::MockHTTP unknown Plack::Test::Server unknown Plack::Test::Suite unknown Plack::Util unknown Plack::Util::Accessor unknown Pod::Coverage 0.23 Pod::Coverage::CountParents unknown Pod::Coverage::ExportOnly unknown Pod::Coverage::Moose 0.07 Pod::Coverage::Overloader unknown Pod::Coverage::TrustPod 0.100005 Pod::Elemental 0.103004 Pod::Elemental::Autoblank 0.103004 Pod::Elemental::Autochomp 0.103004 Pod::Elemental::Command 0.103004 Pod::Elemental::Document 0.103004 Pod::Elemental::Element::Generic::Blank 0.103004 Pod::Elemental::Element::Generic::Command 0.103004 Pod::Elemental::Element::Generic::Nonpod 0.103004 Pod::Elemental::Element::Generic::Text 0.103004 Pod::Elemental::Element::Nested 0.103004 Pod::Elemental::Element::Pod5::Command 0.103004 Pod::Elemental::Element::Pod5::Data 0.103004 Pod::Elemental::Element::Pod5::Nonpod 0.103004 Pod::Elemental::Element::Pod5::Ordinary 0.103004 Pod::Elemental::Element::Pod5::Region 0.103004 Pod::Elemental::Element::Pod5::Verbatim 0.103004 Pod::Elemental::Flat 0.103004 Pod::Elemental::Node 0.103004 Pod::Elemental::Objectifier 0.103004 Pod::Elemental::Paragraph 0.103004 Pod::Elemental::Selectors 0.103004 Pod::Elemental::Transformer 0.103004 Pod::Elemental::Transformer::Gatherer 0.103004 Pod::Elemental::Transformer::Nester 0.103004 Pod::Elemental::Transformer::Pod5 0.103004 Pod::Elemental::Types 0.103004 Pod::Eventual 0.094001 Pod::Eventual::Simple 0.094001 Pod::Markdown 3.005 Pod::Perldoc::ToMarkdown 3.005 Pod::Readme unknown Pod::Readme::Filter unknown Pod::Readme::Plugin unknown Pod::Readme::Plugin::changes unknown Pod::Readme::Plugin::requires unknown Pod::Readme::Plugin::version unknown Pod::Readme::Types unknown Pod::Spell 1.20 Pod::Wordlist 1.20 POE 1.367 POE::Component 1.367 POE::Component::Client::TCP 1.367 POE::Component::Server::TCP 1.367 POE::Driver 1.367 POE::Driver::SysRW 1.367 POE::Filter 1.367 POE::Filter::Block 1.367 POE::Filter::Grep 1.367 POE::Filter::HTTPD 1.367 POE::Filter::Line 1.367 POE::Filter::Map 1.367 POE::Filter::RecordBlock 1.367 POE::Filter::Reference 1.367 POE::Filter::Stackable 1.367 POE::Filter::Stream 1.367 POE::Kernel 1.367 POE::Loop 1.367 POE::Loop::IO_Poll 1.367 POE::Loop::PerlSignals 1.367 POE::Loop::Select 1.367 POE::NFA 1.367 POE::Pipe 1.367 POE::Pipe::OneWay 1.367 POE::Pipe::TwoWay 1.367 POE::Queue 1.367 POE::Queue::Array 1.367 POE::Resource 1.367 POE::Resource::Aliases 1.367 POE::Resource::Clock 1.367 POE::Resource::Events 1.367 POE::Resource::Extrefs 1.367 POE::Resource::FileHandles 1.367 POE::Resource::Sessions 1.367 POE::Resource::SIDs 1.367 POE::Resource::Signals 1.367 POE::Resources 1.367 POE::Session 1.367 POE::Test::DondeEstan 1.360 POE::Test::Loops 1.360 POE::Test::Sequence unknown POE::Wheel 1.367 POE::Wheel::Curses 1.367 POE::Wheel::FollowTail 1.367 POE::Wheel::ListenAccept 1.367 POE::Wheel::ReadLine 1.367 POE::Wheel::ReadWrite 1.367 POE::Wheel::Run 1.367 POE::Wheel::SocketFactory 1.367 POSIX::strftime::Compiler 0.42 PostScript::Document 0.06 PostScript::Elements unknown PostScript::Metrics 0.06 PostScript::TextBlock 0.06 PPI 1.236 PPI::Cache 1.236 PPI::Document 1.236 PPI::Document::File 1.236 PPI::Document::Fragment 1.236 PPI::Document::Normalized 1.236 PPI::Dumper 1.236 PPI::Element 1.236 PPI::Exception 1.236 PPI::Exception::ParserRejection 1.236 PPI::Find 1.236 PPI::HTML 1.08 PPI::HTML::Fragment 1.08 PPI::Lexer 1.236 PPI::Node 1.236 PPI::Normal 1.236 PPI::Normal::Standard 1.236 PPI::Statement 1.236 PPI::Statement::Break 1.236 PPI::Statement::Compound 1.236 PPI::Statement::Data 1.236 PPI::Statement::End 1.236 PPI::Statement::Expression 1.236 PPI::Statement::Given 1.236 PPI::Statement::Include 1.236 PPI::Statement::Include::Perl6 1.236 PPI::Statement::Null 1.236 PPI::Statement::Package 1.236 PPI::Statement::Scheduled 1.236 PPI::Statement::Sub 1.236 PPI::Statement::Unknown 1.236 PPI::Statement::UnmatchedBrace 1.236 PPI::Statement::Variable 1.236 PPI::Statement::When 1.236 PPI::Structure 1.236 PPI::Structure::Block 1.236 PPI::Structure::Condition 1.236 PPI::Structure::Constructor 1.236 PPI::Structure::For 1.236 PPI::Structure::Given 1.236 PPI::Structure::List 1.236 PPI::Structure::Subscript 1.236 PPI::Structure::Unknown 1.236 PPI::Structure::When 1.236 PPI::Token 1.236 PPI::Token::_QuoteEngine 1.236 PPI::Token::_QuoteEngine::Full 1.236 PPI::Token::_QuoteEngine::Simple 1.236 PPI::Token::ArrayIndex 1.236 PPI::Token::Attribute 1.236 PPI::Token::BOM 1.236 PPI::Token::Cast 1.236 PPI::Token::Comment 1.236 PPI::Token::DashedWord 1.236 PPI::Token::Data 1.236 PPI::Token::End 1.236 PPI::Token::HereDoc 1.236 PPI::Token::Label 1.236 PPI::Token::Magic 1.236 PPI::Token::Number 1.236 PPI::Token::Number::Binary 1.236 PPI::Token::Number::Exp 1.236 PPI::Token::Number::Float 1.236 PPI::Token::Number::Hex 1.236 PPI::Token::Number::Octal 1.236 PPI::Token::Number::Version 1.236 PPI::Token::Operator 1.236 PPI::Token::Pod 1.236 PPI::Token::Prototype 1.236 PPI::Token::Quote 1.236 PPI::Token::Quote::Double 1.236 PPI::Token::Quote::Interpolate 1.236 PPI::Token::Quote::Literal 1.236 PPI::Token::Quote::Single 1.236 PPI::Token::QuoteLike 1.236 PPI::Token::QuoteLike::Backtick 1.236 PPI::Token::QuoteLike::Command 1.236 PPI::Token::QuoteLike::Readline 1.236 PPI::Token::QuoteLike::Regexp 1.236 PPI::Token::QuoteLike::Words 1.236 PPI::Token::Regexp 1.236 PPI::Token::Regexp::Match 1.236 PPI::Token::Regexp::Substitute 1.236 PPI::Token::Regexp::Transliterate 1.236 PPI::Token::Separator 1.236 PPI::Token::Structure 1.236 PPI::Token::Symbol 1.236 PPI::Token::Unknown 1.236 PPI::Token::Whitespace 1.236 PPI::Token::Word 1.236 PPI::Tokenizer 1.236 PPI::Transform 1.236 PPI::Transform::UpdateCopyright 1.236 PPI::Util 1.236 PPI::XSAccessor 1.236 PPIx::QuoteLike 0.006 PPIx::QuoteLike::Constant 0.006 PPIx::QuoteLike::Dumper 0.006 PPIx::QuoteLike::Token 0.006 PPIx::QuoteLike::Token::Control 0.006 PPIx::QuoteLike::Token::Delimiter 0.006 PPIx::QuoteLike::Token::Interpolation 0.006 PPIx::QuoteLike::Token::String 0.006 PPIx::QuoteLike::Token::Structure 0.006 PPIx::QuoteLike::Token::Unknown 0.006 PPIx::QuoteLike::Token::Whitespace 0.006 PPIx::QuoteLike::Utils 0.006 PPIx::Regexp 0.061 PPIx::Regexp::Constant 0.061 PPIx::Regexp::Dumper 0.061 PPIx::Regexp::Element 0.061 PPIx::Regexp::Lexer 0.061 PPIx::Regexp::Node 0.061 PPIx::Regexp::Node::Range 0.061 PPIx::Regexp::Node::Unknown 0.061 PPIx::Regexp::StringTokenizer 0.061 PPIx::Regexp::Structure 0.061 PPIx::Regexp::Structure::Assertion 0.061 PPIx::Regexp::Structure::BranchReset 0.061 PPIx::Regexp::Structure::Capture 0.061 PPIx::Regexp::Structure::CharClass 0.061 PPIx::Regexp::Structure::Code 0.061 PPIx::Regexp::Structure::Main 0.061 PPIx::Regexp::Structure::Modifier 0.061 PPIx::Regexp::Structure::NamedCapture 0.061 PPIx::Regexp::Structure::Quantifier 0.061 PPIx::Regexp::Structure::Regexp 0.061 PPIx::Regexp::Structure::RegexSet 0.061 PPIx::Regexp::Structure::Replacement 0.061 PPIx::Regexp::Structure::Subexpression 0.061 PPIx::Regexp::Structure::Switch 0.061 PPIx::Regexp::Structure::Unknown 0.061 PPIx::Regexp::Support 0.061 PPIx::Regexp::Token 0.061 PPIx::Regexp::Token::Assertion 0.061 PPIx::Regexp::Token::Backreference 0.061 PPIx::Regexp::Token::Backtrack 0.061 PPIx::Regexp::Token::CharClass 0.061 PPIx::Regexp::Token::CharClass::POSIX 0.061 PPIx::Regexp::Token::CharClass::POSIX::Unknown 0.061 PPIx::Regexp::Token::CharClass::Simple 0.061 PPIx::Regexp::Token::Code 0.061 PPIx::Regexp::Token::Comment 0.061 PPIx::Regexp::Token::Condition 0.061 PPIx::Regexp::Token::Control 0.061 PPIx::Regexp::Token::Delimiter 0.061 PPIx::Regexp::Token::Greediness 0.061 PPIx::Regexp::Token::GroupType 0.061 PPIx::Regexp::Token::GroupType::Assertion 0.061 PPIx::Regexp::Token::GroupType::BranchReset 0.061 PPIx::Regexp::Token::GroupType::Code 0.061 PPIx::Regexp::Token::GroupType::Modifier 0.061 PPIx::Regexp::Token::GroupType::NamedCapture 0.061 PPIx::Regexp::Token::GroupType::Subexpression 0.061 PPIx::Regexp::Token::GroupType::Switch 0.061 PPIx::Regexp::Token::Interpolation 0.061 PPIx::Regexp::Token::Literal 0.061 PPIx::Regexp::Token::Modifier 0.061 PPIx::Regexp::Token::NoOp 0.061 PPIx::Regexp::Token::Operator 0.061 PPIx::Regexp::Token::Quantifier 0.061 PPIx::Regexp::Token::Recursion 0.061 PPIx::Regexp::Token::Reference 0.061 PPIx::Regexp::Token::Structure 0.061 PPIx::Regexp::Token::Unknown 0.061 PPIx::Regexp::Token::Unmatched 0.061 PPIx::Regexp::Token::Whitespace 0.061 PPIx::Regexp::Tokenizer 0.061 PPIx::Regexp::Util 0.061 PPIx::Utilities 1.001000 PPIx::Utilities::Exception::Bug 1.001000 PPIx::Utilities::Node 1.001000 PPIx::Utilities::Statement 1.001000 Proc::Fork 0.804 Proc::Terminator::Ctx 0.05 RDF::Trine 1.019 RDF::Trine::Error 1.019 RDF::Trine::Exporter::CSV 1.019 RDF::Trine::Exporter::RDFPatch 1.019 RDF::Trine::Graph 1.019 RDF::Trine::Iterator 1.019 RDF::Trine::Iterator::Bindings 1.019 RDF::Trine::Iterator::Bindings::Materialized 1.019 RDF::Trine::Iterator::Boolean 1.019 RDF::Trine::Iterator::Graph 1.019 RDF::Trine::Iterator::Graph::Materialized 1.019 RDF::Trine::Iterator::JSONHandler 1.019 RDF::Trine::Iterator::SAXHandler 1.019 RDF::Trine::Model 1.019 RDF::Trine::Model::Dataset 1.019 RDF::Trine::Model::StatementFilter 1.019 RDF::Trine::Model::Union 1.019 RDF::Trine::Namespace 1.019 RDF::Trine::NamespaceMap 1.019 RDF::Trine::Node 1.019 RDF::Trine::Node::Blank 1.019 RDF::Trine::Node::Literal 1.019 RDF::Trine::Node::Nil 1.019 RDF::Trine::Node::Resource 1.019 RDF::Trine::Node::Variable 1.019 RDF::Trine::Parser 1.019 RDF::Trine::Parser::LineProtocol 1.019 RDF::Trine::Parser::NQuads 1.019 RDF::Trine::Parser::NTriples 1.019 RDF::Trine::Parser::RDFa 1.019 RDF::Trine::Parser::RDFJSON 1.019 RDF::Trine::Parser::RDFPatch 1.019 RDF::Trine::Parser::RDFXML 1.019 RDF::Trine::Parser::Redland 1.019 RDF::Trine::Parser::TriG 1.019 RDF::Trine::Parser::Turtle 1.019 RDF::Trine::Parser::Turtle::Constants 1.019 RDF::Trine::Parser::Turtle::Lexer 1.019 RDF::Trine::Parser::Turtle::Token unknown RDF::Trine::Pattern 1.019 RDF::Trine::Serializer 1.019 RDF::Trine::Serializer::NQuads 1.019 RDF::Trine::Serializer::NTriples 1.019 RDF::Trine::Serializer::NTriples::Canonical 1.019 RDF::Trine::Serializer::RDFJSON 1.019 RDF::Trine::Serializer::RDFPatch 1.019 RDF::Trine::Serializer::RDFXML 1.019 RDF::Trine::Serializer::TriG 1.019 RDF::Trine::Serializer::TSV 1.019 RDF::Trine::Serializer::Turtle 1.019 RDF::Trine::Statement 1.019 RDF::Trine::Statement::Quad 1.019 RDF::Trine::Store 1.019 RDF::Trine::Store::DBI 1.019 RDF::Trine::Store::DBI::mysql 1.019 RDF::Trine::Store::DBI::Pg 1.019 RDF::Trine::Store::DBI::SQLite 1.019 RDF::Trine::Store::Dydra 1.019 RDF::Trine::Store::Hexastore 1.019 RDF::Trine::Store::LanguagePreference 1.019 RDF::Trine::Store::Memory 1.019 RDF::Trine::Store::Redis 1.019 RDF::Trine::Store::Redland 1.019 RDF::Trine::Store::SPARQL 1.019 RDF::Trine::VariableBindings 1.019 re::engine::RE2 0.13 Readonly 2.05 Redis 1.991 Redis::Hash 1.991 Redis::List 1.991 Redis::Sentinel 1.991 Ref::Util 0.204 Ref::Util::PP 0.204 Ref::Util::XS 0.117 Regexp::Common 2017060201 Regexp::Common::_support 2017060201 Regexp::Common::balanced 2017060201 Regexp::Common::CC 2017060201 Regexp::Common::comment 2017060201 Regexp::Common::delimited 2017060201 Regexp::Common::lingua 2017060201 Regexp::Common::list 2017060201 Regexp::Common::net 2017060201 Regexp::Common::number 2017060201 Regexp::Common::profanity 2017060201 Regexp::Common::SEN 2017060201 Regexp::Common::URI 2017060201 Regexp::Common::URI::fax 2017060201 Regexp::Common::URI::file 2017060201 Regexp::Common::URI::ftp 2017060201 Regexp::Common::URI::gopher 2017060201 Regexp::Common::URI::http 2017060201 Regexp::Common::URI::news 2017060201 Regexp::Common::URI::pop 2017060201 Regexp::Common::URI::prospero 2017060201 Regexp::Common::URI::RFC1035 2017060201 Regexp::Common::URI::RFC1738 2017060201 Regexp::Common::URI::RFC1808 2017060201 Regexp::Common::URI::RFC2384 2017060201 Regexp::Common::URI::RFC2396 2017060201 Regexp::Common::URI::RFC2806 2017060201 Regexp::Common::URI::tel 2017060201 Regexp::Common::URI::telnet 2017060201 Regexp::Common::URI::tv 2017060201 Regexp::Common::URI::wais 2017060201 Regexp::Common::whitespace 2017060201 Regexp::Common::zip 2017060201 Reply::Plugin::TypeTiny 1.002002 Return::MultiLevel 0.05 rlib 0.02 Role::HasMessage 0.006 Role::HasMessage::Errf 0.006 Role::Identifiable::HasIdent 0.007 Role::Identifiable::HasTags 0.007 Role::Tiny 2.000006 Role::Tiny::With 2.000006 Router::Simple 0.17 Router::Simple::Declare unknown Router::Simple::Route unknown Router::Simple::SubMapper unknown Safe::Isa 1.000010 Scalar::Util 1.50 Scalar::Util::Numeric 0.40 Scope::Guard 0.21 Scope::Upper 0.30 Set::Infinite 0.65 Set::Infinite::_recurrence unknown Set::Infinite::Arithmetic unknown Set::Infinite::Basic unknown Set::IntervalTree 0.12 Set::IntSpan 1.19 Set::Object::Weak unknown Set::Scalar 1.29 Set::Scalar::Base 1.29 Set::Scalar::Null 1.29 Set::Scalar::Real 1.29 Set::Scalar::Universe 1.29 Set::Scalar::Valued 1.29 Set::Scalar::ValuedUniverse 1.29 Set::Scalar::Virtual 1.29 Slurp 0.4 Smart::Comments 1.06 SOAP::Constants 1.27 SOAP::Lite 1.27 SOAP::Lite::Deserializer::XMLSchema1999 1.27 SOAP::Lite::Deserializer::XMLSchema2001 1.27 SOAP::Lite::Deserializer::XMLSchemaSOAP1_1 1.27 SOAP::Lite::Deserializer::XMLSchemaSOAP1_2 1.27 SOAP::Lite::Packager 1.27 SOAP::Lite::Utils 1.27 SOAP::Packager 1.27 SOAP::Test 1.27 SOAP::Transport::HTTP 1.27 SOAP::Transport::IO 1.27 SOAP::Transport::LOCAL 1.27 SOAP::Transport::LOOPBACK 1.27 SOAP::Transport::MAILTO 1.27 SOAP::Transport::POP3 1.27 SOAP::Transport::TCP 1.27 Socket::GetAddrInfo 0.22 Socket::GetAddrInfo::Core 0.22 Socket::GetAddrInfo::Emul 0.22 Socket::GetAddrInfo::Socket6api 0.22 Socket::GetAddrInfo::Strict 0.22 Socket::GetAddrInfo::XS 0.22 Software::License 0.103013 Software::License::AGPL_3 0.103013 Software::License::Apache_1_1 0.103013 Software::License::Apache_2_0 0.103013 Software::License::Artistic_1_0 0.103013 Software::License::Artistic_2_0 0.103013 Software::License::BSD 0.103013 Software::License::CC0_1_0 0.103013 Software::License::Custom 0.103013 Software::License::EUPL_1_1 0.103013 Software::License::EUPL_1_2 0.103013 Software::License::FreeBSD 0.103013 Software::License::GFDL_1_2 0.103013 Software::License::GFDL_1_3 0.103013 Software::License::GPL_1 0.103013 Software::License::GPL_2 0.103013 Software::License::GPL_3 0.103013 Software::License::LGPL_2_1 0.103013 Software::License::LGPL_3_0 0.103013 Software::License::MIT 0.103013 Software::License::Mozilla_1_0 0.103013 Software::License::Mozilla_1_1 0.103013 Software::License::Mozilla_2_0 0.103013 Software::License::None 0.103013 Software::License::OpenSSL 0.103013 Software::License::Perl_5 0.103013 Software::License::PostgreSQL 0.103013 Software::License::QPL_1_0 0.103013 Software::License::SSLeay 0.103013 Software::License::Sun 0.103013 Software::License::Zlib 0.103013 Software::LicenseUtils 0.103013 Sort::Naturally 1.03 Specio 0.42 Specio::Coercion 0.42 Specio::Constraint::AnyCan 0.42 Specio::Constraint::AnyDoes 0.42 Specio::Constraint::AnyIsa 0.42 Specio::Constraint::Enum 0.42 Specio::Constraint::Intersection 0.42 Specio::Constraint::ObjectCan 0.42 Specio::Constraint::ObjectDoes 0.42 Specio::Constraint::ObjectIsa 0.42 Specio::Constraint::Parameterizable 0.42 Specio::Constraint::Parameterized 0.42 Specio::Constraint::Role::CanType 0.42 Specio::Constraint::Role::DoesType 0.42 Specio::Constraint::Role::Interface 0.42 Specio::Constraint::Role::IsaType 0.42 Specio::Constraint::Simple 0.42 Specio::Constraint::Structurable 0.42 Specio::Constraint::Structured 0.42 Specio::Constraint::Union 0.42 Specio::Declare 0.42 Specio::DeclaredAt 0.42 Specio::Exception 0.42 Specio::Exporter 0.42 Specio::Helpers 0.42 Specio::Library::Builtins 0.42 Specio::Library::Numeric 0.42 Specio::Library::Perl 0.42 Specio::Library::String 0.42 Specio::Library::Structured 0.42 Specio::Library::Structured::Dict 0.42 Specio::Library::Structured::Map 0.42 Specio::Library::Structured::Tuple 0.42 Specio::OO 0.42 Specio::PartialDump 0.42 Specio::Registry 0.42 Specio::Role::Inlinable 0.42 Specio::Subs 0.42 Specio::TypeChecks 0.42 Spiffy 0.46 Spiffy::mixin unknown Spreadsheet::ParseExcel 0.65 Spreadsheet::ParseExcel::Cell 0.65 Spreadsheet::ParseExcel::Dump 0.65 Spreadsheet::ParseExcel::FmtDefault 0.65 Spreadsheet::ParseExcel::FmtJapan 0.65 Spreadsheet::ParseExcel::FmtJapan2 0.65 Spreadsheet::ParseExcel::FmtUnicode 0.65 Spreadsheet::ParseExcel::Font 0.65 Spreadsheet::ParseExcel::Format 0.65 Spreadsheet::ParseExcel::SaveParser 0.65 Spreadsheet::ParseExcel::SaveParser::Workbook 0.65 Spreadsheet::ParseExcel::SaveParser::Worksheet 0.65 Spreadsheet::ParseExcel::Simple 1.04 Spreadsheet::ParseExcel::Utility 0.65 Spreadsheet::ParseExcel::Workbook 0.65 Spreadsheet::ParseExcel::Worksheet 0.65 Spreadsheet::WriteExcel 2.40 Spreadsheet::WriteExcel::BIFFwriter 2.40 Spreadsheet::WriteExcel::Big 2.40 Spreadsheet::WriteExcel::Chart 2.40 Spreadsheet::WriteExcel::Chart::Area 2.40 Spreadsheet::WriteExcel::Chart::Bar 2.40 Spreadsheet::WriteExcel::Chart::Column 2.40 Spreadsheet::WriteExcel::Chart::External 2.40 Spreadsheet::WriteExcel::Chart::Line 2.40 Spreadsheet::WriteExcel::Chart::Pie 2.40 Spreadsheet::WriteExcel::Chart::Scatter 2.40 Spreadsheet::WriteExcel::Chart::Stock 2.40 Spreadsheet::WriteExcel::Examples 2.40 Spreadsheet::WriteExcel::Format 2.40 Spreadsheet::WriteExcel::Formula 2.40 Spreadsheet::WriteExcel::OLEwriter 2.40 Spreadsheet::WriteExcel::Properties 2.40 Spreadsheet::WriteExcel::Simple 1.04 Spreadsheet::WriteExcel::Utility 2.40 Spreadsheet::WriteExcel::Workbook 2.40 Spreadsheet::WriteExcel::Worksheet 2.40 SQL::Abstract 1.86 SQL::Abstract::Test unknown SQL::Abstract::Tree unknown SQL::Translator 0.11024 SQL::Translator::Diff unknown SQL::Translator::Filter::DefaultExtra 1.59 SQL::Translator::Filter::Globals 1.59 SQL::Translator::Filter::Names 1.59 SQL::Translator::Generator::DDL::MySQL unknown SQL::Translator::Generator::DDL::PostgreSQL unknown SQL::Translator::Generator::DDL::SQLite unknown SQL::Translator::Generator::DDL::SQLServer unknown SQL::Translator::Generator::Role::DDL unknown SQL::Translator::Generator::Role::Quote unknown SQL::Translator::Parser 1.60 SQL::Translator::Parser::Access 1.59 SQL::Translator::Parser::DB2 unknown SQL::Translator::Parser::DB2::Grammar unknown SQL::Translator::Parser::DBI 1.59 SQL::Translator::Parser::DBI::DB2 1.59 SQL::Translator::Parser::DBI::MySQL 1.59 SQL::Translator::Parser::DBI::Oracle 1.59 SQL::Translator::Parser::DBI::PostgreSQL 1.59 SQL::Translator::Parser::DBI::SQLite 1.59 SQL::Translator::Parser::DBI::SQLServer 1.59 SQL::Translator::Parser::DBI::Sybase 1.59 SQL::Translator::Parser::DBIx::Class 1.10 SQL::Translator::Parser::Excel 1.59 SQL::Translator::Parser::JSON 1.00 SQL::Translator::Parser::MySQL 1.59 SQL::Translator::Parser::Oracle 1.59 SQL::Translator::Parser::PostgreSQL 1.59 SQL::Translator::Parser::SQLite 1.59 SQL::Translator::Parser::SQLServer 1.59 SQL::Translator::Parser::Storable 1.59 SQL::Translator::Parser::Sybase 1.59 SQL::Translator::Parser::XML 1.59 SQL::Translator::Parser::XML::SQLFairy 1.59 SQL::Translator::Parser::xSV 1.59 SQL::Translator::Parser::YAML 1.59 SQL::Translator::Producer 1.59 SQL::Translator::Producer::ClassDBI 1.59 SQL::Translator::Producer::DB2 1.59 SQL::Translator::Producer::DBIx::Class::File 0.1 SQL::Translator::Producer::Diagram 1.59 SQL::Translator::Producer::DiaUml 1.59 SQL::Translator::Producer::Dumper 1.59 SQL::Translator::Producer::GraphViz 1.59 SQL::Translator::Producer::HTML 1.59 SQL::Translator::Producer::JSON 1.00 SQL::Translator::Producer::Latex 1.59 SQL::Translator::Producer::MySQL 1.59 SQL::Translator::Producer::Oracle 1.59 SQL::Translator::Producer::POD 1.59 SQL::Translator::Producer::PostgreSQL 1.59 SQL::Translator::Producer::SQLite 1.59 SQL::Translator::Producer::SQLServer 1.59 SQL::Translator::Producer::Storable 1.59 SQL::Translator::Producer::Sybase 1.59 SQL::Translator::Producer::TT::Base 1.59 SQL::Translator::Producer::TT::Table 1.59 SQL::Translator::Producer::TTSchema 1.59 SQL::Translator::Producer::XML 1.59 SQL::Translator::Producer::XML::SQLFairy 1.59 SQL::Translator::Producer::YAML 1.59 SQL::Translator::Role::BuildArgs unknown SQL::Translator::Role::Debug unknown SQL::Translator::Role::Error unknown SQL::Translator::Role::ListAttr unknown SQL::Translator::Schema 1.59 SQL::Translator::Schema::Constants 1.59 SQL::Translator::Schema::Constraint 1.59 SQL::Translator::Schema::Field 1.59 SQL::Translator::Schema::Index 1.59 SQL::Translator::Schema::Object 1.59 SQL::Translator::Schema::Procedure 1.59 SQL::Translator::Schema::Role::Compare unknown SQL::Translator::Schema::Role::Extra unknown SQL::Translator::Schema::Table 1.59 SQL::Translator::Schema::Trigger 1.59 SQL::Translator::Schema::View 1.59 SQL::Translator::Types unknown SQL::Translator::Utils 1.59 StackTrace::Auto 0.200013 Starman 0.4014 Starman::Server unknown Statistics::ANOVA 0.14 Statistics::ANOVA::Compare 0.01 Statistics::ANOVA::EffectSize 0.02 Statistics::ANOVA::Friedman 0.02 Statistics::ANOVA::JT unknown Statistics::ANOVA::KW 0.01 Statistics::ANOVA::Page 0.02 Statistics::Basic 1.6611 Statistics::Basic::_OneVectorBase unknown Statistics::Basic::_TwoVectorBase unknown Statistics::Basic::ComputedVector unknown Statistics::Basic::Correlation unknown Statistics::Basic::Covariance unknown Statistics::Basic::LeastSquareFit unknown Statistics::Basic::Mean unknown Statistics::Basic::Median unknown Statistics::Basic::Mode unknown Statistics::Basic::StdDev unknown Statistics::Basic::Variance unknown Statistics::Basic::Vector unknown Statistics::Candidates unknown Statistics::ChiSquare 1.0000 Statistics::Contingency 0.09 Statistics::Cook 0.0.6 Statistics::Data 0.11 Statistics::Data::Dichotomize 0.05 Statistics::Data::Rank 0.02 Statistics::DependantTTest 0.03 Statistics::Descriptive 3.0701 Statistics::Descriptive::Discrete 0.07 Statistics::Descriptive::Full 3.0701 Statistics::Descriptive::LogScale 0.11 Statistics::Descriptive::Smoother 3.0701 Statistics::Descriptive::Smoother::Exponential 3.0701 Statistics::Descriptive::Smoother::Weightedexponential 3.0701 Statistics::Descriptive::Sparse 3.0701 Statistics::Discrete 0.05.00 Statistics::Distributions 1.02 Statistics::Distributions::Bartlett unknown Statistics::Distributions::GTest unknown Statistics::Diversity::Shannon 0.0102 Statistics::FactorAnalysis unknown Statistics::FisherPitman 0.034 Statistics::Frequency 0.04 Statistics::Histogram 0.1 Statistics::KruskalWallis 0.01 Statistics::Lite 3.62 Statistics::MaxEntropy 1.0 Statistics::Normality 0.01 Statistics::PCA unknown Statistics::PCA::Varimax unknown Statistics::PointEstimation 1.1 Statistics::R 0.34 Statistics::R::Legacy unknown Statistics::R::Win32 unknown Statistics::RankCorrelation 0.1205 Statistics::Robust 0.02 Statistics::Robust::Bootstrap unknown Statistics::Robust::Density unknown Statistics::Robust::Location unknown Statistics::Robust::Scale unknown Statistics::Sampler::Multinomial 0.7 Statistics::Sampler::Multinomial::AliasMethod 0.7 Statistics::Sequences 0.15 Statistics::Sequences::Joins 0.20 Statistics::Sequences::Pot 0.12 Statistics::Sequences::Runs 0.22 Statistics::Sequences::Turns 0.13 Statistics::Sequences::Vnomes 0.20 Statistics::Shannon 0.05 Statistics::Simpson 0.03 Statistics::SparseVector 0.2 Statistics::Standard_Normal unknown Statistics::TopK 0.02 Statistics::TTest 1.1 Statistics::Zed 0.10 Storable 3.11 Stream::Buffered 0.03 Stream::Buffered::Auto unknown Stream::Buffered::File unknown Stream::Buffered::PerlIO unknown strictures 2.000005 strictures::extra unknown String::Diff 0.07 String::Errf 0.008 String::Escape 2010.002 String::Flogger 1.101245 String::Format 1.18 String::Formatter 0.102084 String::Formatter::Cookbook 0.102084 String::Numeric 0.9 String::Numeric::PP 0.9 String::Print 0.93 String::RewritePrefix 0.007 String::ShellQuote 1.04 String::Tagged 0.15 String::Tagged::Terminal 0.02 String::ToIdentifier::EN 0.12 String::ToIdentifier::EN::Unicode 0.12 String::Truncate 1.100602 String::Util 1.26 Struct::Dumb 0.09 Sub::Attribute 0.06 Sub::Defer 2.001001 Sub::Exporter 0.987 Sub::Exporter::ForMethods 0.100052 Sub::Exporter::GlobExporter 0.005 Sub::Exporter::Progressive 0.001013 Sub::Exporter::Util 0.987 Sub::Identify 0.14 Sub::Info 0.002 Sub::Install 0.928 Sub::Name 0.21 Sub::Quote 2.001001 Sub::Uplevel 0.2800 Sub::Util 1.50 SUPER 1.20141117 SVG 2.84 SVG::DOM 2.84 SVG::Element 2.84 SVG::Extension 2.84 SVG::Graph 0.02 SVG::Graph::Data unknown SVG::Graph::Data::Datum unknown SVG::Graph::Data::Node unknown SVG::Graph::Data::Tree unknown SVG::Graph::File unknown SVG::Graph::Frame unknown SVG::Graph::Glyph unknown SVG::Graph::Glyph::axis unknown SVG::Graph::Glyph::bar unknown SVG::Graph::Glyph::barflex unknown SVG::Graph::Glyph::bezier unknown SVG::Graph::Glyph::bubble unknown SVG::Graph::Glyph::heatmap unknown SVG::Graph::Glyph::line unknown SVG::Graph::Glyph::pictogram unknown SVG::Graph::Glyph::scatter unknown SVG::Graph::Glyph::tree unknown SVG::Graph::Glyph::wedge unknown SVG::Graph::Group unknown SVG::XML 2.84 Symbol::Util 0.0203 SymTab unknown syntax 0.004 Syntax::Feature::Junction 0.003008 Syntax::Keyword::Junction 0.003008 Syntax::Keyword::Junction::All 0.003008 Syntax::Keyword::Junction::Any 0.003008 Syntax::Keyword::Junction::Base 0.003008 Syntax::Keyword::Junction::None 0.003008 Syntax::Keyword::Junction::One 0.003008 Sys::SigAction 0.23 Sys::SigAction::Alarm unknown TAP::Base 3.42 TAP::Formatter::Base 3.42 TAP::Formatter::Color 3.42 TAP::Formatter::Console 3.42 TAP::Formatter::Console::ParallelSession 3.42 TAP::Formatter::Console::Session 3.42 TAP::Formatter::File 3.42 TAP::Formatter::File::Session 3.42 TAP::Formatter::Session 3.42 TAP::Harness 3.42 TAP::Harness::Env 3.42 TAP::Object 3.42 TAP::Parser 3.42 TAP::Parser::Aggregator 3.42 TAP::Parser::Grammar 3.42 TAP::Parser::Iterator 3.42 TAP::Parser::Iterator::Array 3.42 TAP::Parser::Iterator::Process 3.42 TAP::Parser::Iterator::Stream 3.42 TAP::Parser::IteratorFactory 3.42 TAP::Parser::Multiplexer 3.42 TAP::Parser::Result 3.42 TAP::Parser::Result::Bailout 3.42 TAP::Parser::Result::Comment 3.42 TAP::Parser::Result::Plan 3.42 TAP::Parser::Result::Pragma 3.42 TAP::Parser::Result::Test 3.42 TAP::Parser::Result::Unknown 3.42 TAP::Parser::Result::Version 3.42 TAP::Parser::Result::YAML 3.42 TAP::Parser::ResultFactory 3.42 TAP::Parser::Scheduler 3.42 TAP::Parser::Scheduler::Job 3.42 TAP::Parser::Scheduler::Spinner 3.42 TAP::Parser::Source 3.42 TAP::Parser::SourceHandler 3.42 TAP::Parser::SourceHandler::Executable 3.42 TAP::Parser::SourceHandler::File 3.42 TAP::Parser::SourceHandler::Handle 3.42 TAP::Parser::SourceHandler::Perl 3.42 TAP::Parser::SourceHandler::RawTAP 3.42 TAP::Parser::YAMLish::Reader 3.42 TAP::Parser::YAMLish::Writer 3.42 Task::Catalyst 4.02 Task::Kensho 0.39 Task::Kensho::Async 0.39 Task::Kensho::CLI 0.39 Task::Kensho::Config 0.39 Task::Kensho::Dates 0.39 Task::Kensho::DBDev 0.39 Task::Kensho::Email 0.39 Task::Kensho::ExcelCSV 0.39 Task::Kensho::Exceptions 0.39 Task::Kensho::Hackery 0.39 Task::Kensho::Logging 0.39 Task::Kensho::ModuleDev 0.39 Task::Kensho::OOP 0.39 Task::Kensho::Scalability 0.39 Task::Kensho::Testing 0.39 Task::Kensho::Toolchain 0.39 Task::Kensho::WebCrawling 0.39 Task::Kensho::WebDev 0.39 Task::Kensho::XML 0.39 Task::Moose 0.03 Task::Weaken 1.06 Template 2.27 Template::Base 2.78 Template::Config 2.75 Template::Constants 2.75 Template::Context 2.98 Template::Directive 2.2 Template::Document 2.79 Template::Exception 2.7 Template::Filters 2.87 Template::Grammar 2.26 Template::Iterator 2.68 Template::Namespace::Constants 1.27 Template::Parser 2.89 Template::Plugin 2.7 Template::Plugin::Assert 1 Template::Plugin::CGI 2.7 Template::Plugin::Datafile 2.72 Template::Plugin::Date 2.78 Template::Plugin::Directory 2.7 Template::Plugin::Dumper 2.7 Template::Plugin::File 2.71 Template::Plugin::Filter 1.38 Template::Plugin::Format 2.7 Template::Plugin::HTML 2.62 Template::Plugin::Image 1.21 Template::Plugin::Iterator 2.68 Template::Plugin::Math 1.16 Template::Plugin::Pod 2.69 Template::Plugin::Procedural 1.17 Template::Plugin::Scalar 1 Template::Plugin::String 2.4 Template::Plugin::Table 2.71 Template::Plugin::URL 2.74 Template::Plugin::View 2.68 Template::Plugin::Wrap 2.68 Template::Plugins 2.77 Template::Provider 2.94 Template::Service 2.8 Template::Stash 2.91 Template::Stash::Context 1.63 Template::Stash::XS unknown Template::Test 2.75 Template::Timer 1.00 Template::Tiny 1.12 Template::Toolkit unknown Template::View 2.91 Template::VMethods 2.16 Term::Encoding 0.02 Term::ProgressBar 2.22 Term::ProgressBar::IO 2.22 Term::ProgressBar::Quiet 0.31 Term::ProgressBar::Simple 0.03 Term::ReadKey 2.37 Term::ReadLine::Perl5 1.45 Term::ReadLine::Perl5::Common unknown Term::ReadLine::Perl5::Dumb unknown Term::ReadLine::Perl5::History unknown Term::ReadLine::Perl5::Keymap unknown Term::ReadLine::Perl5::OO 0.43 Term::ReadLine::Perl5::OO::History unknown Term::ReadLine::Perl5::OO::Keymap unknown Term::ReadLine::Perl5::OO::State unknown Term::ReadLine::Perl5::readline 1.45 Term::ReadLine::Perl5::TermCap unknown Term::ReadLine::Perl5::Tie 1.45 Term::Size 0.207 Term::Table 0.012 Term::Table::Cell 0.012 Term::Table::CellStack 0.012 Term::Table::HashBase 0.003 Term::Table::LineBreak 0.012 Term::Table::Spacer 0.012 Term::Table::Util 0.012 Term::UI 0.46 Term::UI::History 0.46 Test2 1.302138 Test2::API 1.302138 Test2::API::Breakage 1.302138 Test2::API::Context 1.302138 Test2::API::Instance 1.302138 Test2::API::Stack 1.302138 Test2::AsyncSubtest 0.000115 Test2::AsyncSubtest::Event::Attach 0.000115 Test2::AsyncSubtest::Event::Detach 0.000115 Test2::AsyncSubtest::Formatter 0.000115 Test2::AsyncSubtest::Hub 0.000115 Test2::Bundle 0.000115 Test2::Bundle::Extended 0.000115 Test2::Bundle::More 0.000115 Test2::Bundle::Simple 0.000115 Test2::Compare 0.000115 Test2::Compare::Array 0.000115 Test2::Compare::Bag 0.000115 Test2::Compare::Base 0.000115 Test2::Compare::Bool 0.000115 Test2::Compare::Custom 0.000115 Test2::Compare::DeepRef 0.000115 Test2::Compare::Delta 0.000115 Test2::Compare::Event 0.000115 Test2::Compare::EventMeta 0.000115 Test2::Compare::Float 0.000115 Test2::Compare::Hash 0.000115 Test2::Compare::Meta 0.000115 Test2::Compare::Negatable 0.000115 Test2::Compare::Number 0.000115 Test2::Compare::Object 0.000115 Test2::Compare::OrderedSubset 0.000115 Test2::Compare::Pattern 0.000115 Test2::Compare::Ref 0.000115 Test2::Compare::Regex 0.000115 Test2::Compare::Scalar 0.000115 Test2::Compare::Set 0.000115 Test2::Compare::String 0.000115 Test2::Compare::Undef 0.000115 Test2::Compare::Wildcard 0.000115 Test2::Event 1.302138 Test2::Event::Bail 1.302138 Test2::Event::Diag 1.302138 Test2::Event::Encoding 1.302138 Test2::Event::Exception 1.302138 Test2::Event::Fail 1.302138 Test2::Event::Generic 1.302138 Test2::Event::Note 1.302138 Test2::Event::Ok 1.302138 Test2::Event::Pass 1.302138 Test2::Event::Plan 1.302138 Test2::Event::Skip 1.302138 Test2::Event::Subtest 1.302138 Test2::Event::TAP::Version 1.302138 Test2::Event::Times 0.000115 Test2::Event::V2 1.302138 Test2::Event::Waiting 1.302138 Test2::Event::Warning 0.06 Test2::EventFacet 1.302138 Test2::EventFacet::About 1.302138 Test2::EventFacet::Amnesty 1.302138 Test2::EventFacet::Assert 1.302138 Test2::EventFacet::Control 1.302138 Test2::EventFacet::Error 1.302138 Test2::EventFacet::Hub 1.302138 Test2::EventFacet::Info 1.302138 Test2::EventFacet::Meta 1.302138 Test2::EventFacet::Parent 1.302138 Test2::EventFacet::Plan 1.302138 Test2::EventFacet::Render 1.302138 Test2::EventFacet::Trace 1.302138 Test2::Formatter 1.302138 Test2::Formatter::TAP 1.302138 Test2::Hub 1.302138 Test2::Hub::Interceptor 1.302138 Test2::Hub::Interceptor::Terminator 1.302138 Test2::Hub::Subtest 1.302138 Test2::IPC 1.302138 Test2::IPC::Driver 1.302138 Test2::IPC::Driver::Files 1.302138 Test2::Manual 0.000115 Test2::Manual::Anatomy 0.000115 Test2::Manual::Anatomy::API 0.000115 Test2::Manual::Anatomy::Context 0.000115 Test2::Manual::Anatomy::EndToEnd 0.000115 Test2::Manual::Anatomy::Event 0.000115 Test2::Manual::Anatomy::Hubs 0.000115 Test2::Manual::Anatomy::IPC 0.000115 Test2::Manual::Anatomy::Utilities 0.000115 Test2::Manual::Contributing 0.000115 Test2::Manual::Testing 0.000115 Test2::Manual::Testing::Introduction 0.000115 Test2::Manual::Testing::Migrating 0.000115 Test2::Manual::Testing::Planning 0.000115 Test2::Manual::Testing::Todo 0.000115 Test2::Manual::Tooling 0.000115 Test2::Manual::Tooling::FirstTool 0.000115 Test2::Manual::Tooling::Formatter 0.000115 Test2::Manual::Tooling::Nesting 0.000115 Test2::Manual::Tooling::Plugin::TestExit 0.000115 Test2::Manual::Tooling::Plugin::TestingDone 0.000115 Test2::Manual::Tooling::Plugin::ToolCompletes 0.000115 Test2::Manual::Tooling::Plugin::ToolStarts 0.000115 Test2::Manual::Tooling::Subtest 0.000115 Test2::Manual::Tooling::TestBuilder 0.000115 Test2::Manual::Tooling::Testing 0.000115 Test2::Mock 0.000115 Test2::Plugin 0.000115 Test2::Plugin::BailOnFail 0.000115 Test2::Plugin::DieOnFail 0.000115 Test2::Plugin::ExitSummary 0.000115 Test2::Plugin::NoWarnings 0.06 Test2::Plugin::SRand 0.000115 Test2::Plugin::Times 0.000115 Test2::Plugin::UTF8 0.000115 Test2::Require 0.000115 Test2::Require::AuthorTesting 0.000115 Test2::Require::EnvVar 0.000115 Test2::Require::Fork 0.000115 Test2::Require::Module 0.000115 Test2::Require::Perl 0.000115 Test2::Require::RealFork 0.000115 Test2::Require::Threads 0.000115 Test2::Suite 0.000115 Test2::Todo 0.000115 Test2::Tools 0.000115 Test2::Tools::AsyncSubtest 0.000115 Test2::Tools::Basic 0.000115 Test2::Tools::Class 0.000115 Test2::Tools::ClassicCompare 0.000115 Test2::Tools::Compare 0.000115 Test2::Tools::Defer 0.000115 Test2::Tools::Encoding 0.000115 Test2::Tools::Event 0.000115 Test2::Tools::Exception 0.000115 Test2::Tools::Exports 0.000115 Test2::Tools::GenTemp 0.000115 Test2::Tools::Grab 0.000115 Test2::Tools::Mock 0.000115 Test2::Tools::Ref 0.000115 Test2::Tools::Spec 0.000115 Test2::Tools::Subtest 0.000115 Test2::Tools::Target 0.000115 Test2::Tools::Tester 0.000115 Test2::Tools::Tiny 1.302138 Test2::Tools::Warnings 0.000115 Test2::Util 1.302138 Test2::Util::ExternalMeta 1.302138 Test2::Util::Facets2Legacy 1.302138 Test2::Util::Grabber 0.000115 Test2::Util::HashBase 1.302138 Test2::Util::Ref 0.000115 Test2::Util::Stash 0.000115 Test2::Util::Sub 0.000115 Test2::Util::Table 0.000115 Test2::Util::Table::Cell 0.000115 Test2::Util::Table::LineBreak 0.000115 Test2::Util::Term 0.000115 Test2::Util::Times 0.000115 Test2::Util::Trace 1.302138 Test2::V0 0.000115 Test2::Workflow 0.000115 Test2::Workflow::BlockBase 0.000115 Test2::Workflow::Build 0.000115 Test2::Workflow::Runner 0.000115 Test2::Workflow::Task 0.000115 Test2::Workflow::Task::Action 0.000115 Test2::Workflow::Task::Group 0.000115 Test::Assert 0.0504 Test::Base 0.89 Test::Base::Filter unknown Test::Builder 1.302138 Test::Builder::Formatter 1.302138 Test::Builder::IO::Scalar 2.114 Test::Builder::Module 1.302138 Test::Builder::Tester 1.302138 Test::Builder::Tester::Color 1.302138 Test::Builder::TodoDiag 1.302138 Test::Class 0.50 Test::Class::Load 0.50 Test::Class::MethodInfo 0.50 Test::Class::Moose 0.92 Test::Class::Moose::AttributeRegistry 0.92 Test::Class::Moose::CLI 0.92 Test::Class::Moose::Config 0.92 Test::Class::Moose::Deprecated 0.92 Test::Class::Moose::Executor::Parallel 0.92 Test::Class::Moose::Executor::Sequential 0.92 Test::Class::Moose::Load 0.92 Test::Class::Moose::Report 0.92 Test::Class::Moose::Report::Class 0.92 Test::Class::Moose::Report::Instance 0.92 Test::Class::Moose::Report::Method 0.92 Test::Class::Moose::Report::Time 0.92 Test::Class::Moose::Role 0.92 Test::Class::Moose::Role::AutoUse 0.92 Test::Class::Moose::Role::CLI 0.92 Test::Class::Moose::Role::Executor 0.92 Test::Class::Moose::Role::HasTimeReport 0.92 Test::Class::Moose::Role::ParameterizedInstances 0.92 Test::Class::Moose::Role::Reporting 0.92 Test::Class::Moose::Runner 0.92 Test::Class::Moose::Tutorial 0.92 Test::Class::Moose::Util 0.92 Test::CleanNamespaces 0.23 Test::CPAN::Changes 0.400002 Test::CPAN::Meta 0.25 Test::CPAN::Meta::Version 0.25 Test::Deep 1.128 Test::Deep::All unknown Test::Deep::Any unknown Test::Deep::Array unknown Test::Deep::ArrayEach unknown Test::Deep::ArrayElementsOnly unknown Test::Deep::ArrayLength unknown Test::Deep::ArrayLengthOnly unknown Test::Deep::Blessed unknown Test::Deep::Boolean unknown Test::Deep::Cache unknown Test::Deep::Cache::Simple unknown Test::Deep::Class unknown Test::Deep::Cmp unknown Test::Deep::Code unknown Test::Deep::Hash unknown Test::Deep::HashEach unknown Test::Deep::HashElements unknown Test::Deep::HashKeys unknown Test::Deep::HashKeysOnly unknown Test::Deep::Ignore unknown Test::Deep::Isa unknown Test::Deep::JSON 0.05 Test::Deep::ListMethods unknown Test::Deep::Methods unknown Test::Deep::MM unknown Test::Deep::None unknown Test::Deep::NoTest unknown Test::Deep::Number unknown Test::Deep::Obj unknown Test::Deep::Ref unknown Test::Deep::RefType unknown Test::Deep::Regexp unknown Test::Deep::RegexpMatches unknown Test::Deep::RegexpOnly unknown Test::Deep::RegexpRef unknown Test::Deep::RegexpRefOnly unknown Test::Deep::RegexpVersion unknown Test::Deep::ScalarRef unknown Test::Deep::ScalarRefOnly unknown Test::Deep::Set unknown Test::Deep::Shallow unknown Test::Deep::Stack unknown Test::Deep::String unknown Test::Deep::Type 0.008 Test::Differences 0.64 Test::EOL 2.00 Test::Exception 0.43 Test::Expect 0.34 Test::FailWarnings 0.008 Test::Fatal 0.014 Test::File 1.443 Test::File::ShareDir 1.001002 Test::File::ShareDir::Dist 1.001002 Test::File::ShareDir::Module 1.001002 Test::File::ShareDir::Object::Dist 1.001002 Test::File::ShareDir::Object::Inc 1.001002 Test::File::ShareDir::Object::Module 1.001002 Test::File::ShareDir::TempDirObject 1.001002 Test::File::ShareDir::Utils 1.001002 Test::Fork 0.02 Test::Future 0.38 Test::Harness 3.42 Test::Identity 0.01 Test::JSON 0.11 Test::LeakTrace 0.16 Test::LeakTrace::Script unknown Test::LongString 0.17 Test::LWP::UserAgent 0.033 Test::Memory::Cycle 1.06 Test::Mock::HTTP::Request 0.01 Test::Mock::HTTP::Response 0.01 Test::Mock::LWP 0.08 Test::Mock::LWP::UserAgent 0.01 Test::MockModule 0.15 Test::MockObject 1.20180705 Test::MockObject::Extends 1.20180705 Test::MockTime 0.17 Test::Mojo unknown Test::Moose 2.2011 Test::MooseX::Daemonize 0.21 Test::More 1.302138 Test::More::UTF8 0.05 Test::Most 0.35 Test::Most::Exception 0.35 Test::Needs 0.002005 Test::NoWarnings 1.04 Test::NoWarnings::Warning 1.04 Test::Number::Delta 1.06 Test::Object 0.08 Test::Object::Test 0.08 Test::Output 1.031 Test::Perl::Critic::Policy 1.132 Test::Pod 1.52 Test::Pod::Content unknown Test::Pod::Coverage 1.10 Test::RDF::Trine::Store 1.019 Test::Refcount 0.08 Test::Requires 0.10 Test::RequiresInternet 0.05 Test::SharedFork 0.35 Test::SharedFork::Array unknown Test::SharedFork::Scalar unknown Test::SharedFork::Store unknown Test::Simple 1.302138 Test::Spec 0.54 Test::Spec::Context unknown Test::Spec::Example unknown Test::Spec::Mocks unknown Test::Spec::SharedHash unknown Test::Spec::TodoExample unknown Test::Specio 0.42 Test::SQL::Translator 1.59 Test::SubCalls 1.10 Test::TCP 2.19 Test::TCP::CheckPort unknown Test::TempDir::Tiny 0.018 Test::Tester 1.302138 Test::Tester::Capture 1.302138 Test::Tester::CaptureRunner 1.302138 Test::Tester::Delegate 1.302138 Test::Time 0.06 Test::Toolbox 0.4 Test::Trap unknown Test::Trap::Builder unknown Test::Trap::Builder::PerlIO unknown Test::Trap::Builder::SystemSafe unknown Test::Trap::Builder::TempFile unknown Test::TypeTiny 1.002002 Test::Unit::Lite 0.1202 Test::use::ok 1.302138 Test::utf8 1.01 Test::Warn 0.36 Test::Warnings 0.026 Test::Without::Module 0.20 Test::WWW::Mechanize 1.50 Test::WWW::Mechanize::Catalyst 0.60 Test::WWW::Mechanize::PSGI 0.38 Test::WWW::Selenium 1.36 Test::YAML 1.07 Text::Aligner 0.13 Text::Autoformat 1.74 Text::Autoformat::Hang 1.74 Text::Autoformat::NullHang 1.74 Text::CSV 1.95 Text::CSV_PP 1.95 Text::CSV_XS 1.36 Text::Diff 1.45 Text::Diff::Config 1.44 Text::Diff::Table 1.44 Text::Format 0.61 Text::German 0.06 Text::German::Adjektiv unknown Text::German::Ausnahme unknown Text::German::Cache unknown Text::German::Endung unknown Text::German::Regel unknown Text::German::Util unknown Text::German::Verb unknown Text::German::Vorsilbe unknown Text::Glob 0.11 Text::LineFold 2016.00702 Text::Reform 1.20 Text::SimpleTable 2.05 Text::Table 1.133 Text::Template 1.53 Text::Template::Preprocess 1.53 Text::Unidecode 1.30 Text::VisualWidth::PP 0.05 threads::shared::array 0.36 threads::shared::handle 0.36 threads::shared::hash 0.36 threads::shared::scalar 0.36 Throwable 0.200013 Throwable::Error 0.200013 Tie::Handle::Offset 0.004 Tie::Handle::SkipHeader 0.004 Tie::Hash::MultiValue 1.05 Tie::IxHash 1.23 Tie::ToObject 0.03 Tie::Watch 1.302 Time::CTime 2011.0505 Time::DaysInMonth 99.1117 Time::Duration 1.20 Time::Duration::Parse 0.14 Time::HiRes 1.9758 Time::JulianDay 2011.0505 Time::ParseDate 2015.103 Time::Piece 1.3204 Time::Seconds 1.3204 Time::Timezone 2015.0925 Time::Tiny 1.08 Time::Zone 2.24 Tk 804.034 Tk::Adjuster 4.008 Tk::After 4.008 Tk::Animation 4.008 Tk::Balloon 4.012 Tk::Bitmap 4.004 Tk::BrowseEntry 4.015 Tk::Button 4.010 Tk::Canvas 4.013 Tk::Checkbutton 4.006 Tk::Clipboard 4.009 Tk::CmdLine 4.007 Tk::ColorDialog 4.014 Tk::ColorEditor 4.014 Tk::ColorSelect 4.014 Tk::Compound 4.004 Tk::Config 804.034 Tk::Configure 4.009 Tk::Derived 4.011 Tk::Dialog 4.005 Tk::DialogBox 4.016 Tk::Dirlist 4.004 Tk::DirTree 4.022 Tk::DragDrop 4.015 Tk::DragDrop::Common 4.005 Tk::DragDrop::Local 4.004 Tk::DragDrop::Rect 4.012 Tk::DragDrop::SunConst 4.004 Tk::DragDrop::SunDrop 4.006 Tk::DragDrop::SunSite 4.007 Tk::DragDrop::XDNDDrop 4.007 Tk::DragDrop::XDNDSite 4.007 Tk::DropSite 4.008 Tk::DummyEncode 4.007 Tk::English 4.006 Tk::Entry 4.018 Tk::ErrorDialog 4.007 Tk::Event 4.035 Tk::Event::IO 4.009 Tk::FBox 4.018 Tk::FileSelect 4.018 Tk::FloatEntry 4.004 Tk::Font 4.004 Tk::Frame 4.010 Tk::HList 4.015 Tk::IconList 4.007 Tk::Image 4.011 Tk::InputO 4.004 Tk::install 4.004 Tk::IO 4.006 Tk::ItemStyle 4.004 Tk::JPEG 4.003 Tk::Label 4.006 Tk::LabeledEntryLabeledRadiobutton 4.004 Tk::Labelframe 4.003 Tk::LabEntry 4.006 Tk::LabFrame 4.010 Tk::LabRadiobutton 4.004 Tk::Listbox 4.015 Tk::MainWindow 4.015 Tk::MakeDepend 4.015 Tk::Menu 4.023 Tk::Menu::Item 4.005 Tk::Menubar 4.006 Tk::Menubutton 4.005 Tk::Message 4.006 Tk::MMtry 4.009 Tk::MMutil 4.026 Tk::MsgBox 4.002 Tk::Mwm 4.004 Tk::NBFrame 4.004 Tk::NoteBook 4.009 Tk::Optionmenu 4.014 Tk::Pane 4.007 Tk::Panedwindow 4.004 Tk::Photo 4.006 Tk::Pixmap 4.004 Tk::PNG 4.004 Tk::Pretty 4.006 Tk::ProgressBar 4.015 Tk::Radiobutton 4.006 Tk::Region 4.006 Tk::Reindex 4.006 Tk::ReindexedROText 4.004 Tk::ReindexedText 4.004 Tk::ROText 4.010 Tk::Scale 4.004 Tk::Scrollbar 4.010 Tk::Spinbox 4.007 Tk::Stats 4.004 Tk::Submethods 4.005 Tk::Table 4.016 Tk::Text 4.024 Tk::Text::Tag 4.004 Tk::TextEdit 4.004 Tk::TextList 4.006 Tk::TextUndo 4.015 Tk::Tiler 4.012 Tk::TixGrid 4.010 Tk::TList 4.006 Tk::Toplevel 4.006 Tk::Trace 4.009 Tk::Tree 4.72 Tk::Widget 4.036 Tk::widgets 4.005 Tk::WinPhoto 4.005 Tk::Wm 4.015 Tk::X 4.005 Tk::X11Font 4.007 Tk::Xlib 4.004 Tk::Xrm 4.005 Tree::DAG_Node 1.31 Tree::Simple 1.33 Tree::Simple::Visitor 1.33 Tree::Simple::Visitor::BreadthFirstTraversal 0.15 Tree::Simple::Visitor::CreateDirectoryTree 0.15 Tree::Simple::Visitor::FindByNodeValue 0.15 Tree::Simple::Visitor::FindByPath 0.15 Tree::Simple::Visitor::FindByUID 0.15 Tree::Simple::Visitor::FromNestedArray 0.15 Tree::Simple::Visitor::FromNestedHash 0.15 Tree::Simple::Visitor::GetAllDescendents 0.15 Tree::Simple::Visitor::LoadClassHierarchy 0.15 Tree::Simple::Visitor::LoadDirectoryTree 0.15 Tree::Simple::Visitor::PathToRoot 0.15 Tree::Simple::Visitor::PostOrderTraversal 0.15 Tree::Simple::Visitor::PreOrderTraversal 0.15 Tree::Simple::Visitor::Sort 0.15 Tree::Simple::Visitor::ToNestedArray 0.15 Tree::Simple::Visitor::ToNestedHash 0.15 Tree::Simple::Visitor::VariableDepthClone 0.15 Tree::Simple::VisitorFactory 0.15 Try::Tiny 0.30 TryCatch 1.003002 Type::Coercion 1.002002 Type::Coercion::FromMoose 1.002002 Type::Coercion::Union 1.002002 Type::Library 1.002002 Type::Params 1.002002 Type::Parser 1.002002 Type::Registry 1.002002 Type::Tiny 1.002002 Type::Tiny::_HalfOp 1.002002 Type::Tiny::Class 1.002002 Type::Tiny::Duck 1.002002 Type::Tiny::Enum 1.002002 Type::Tiny::Intersection 1.002002 Type::Tiny::Role 1.002002 Type::Tiny::Union 1.002002 Type::Utils 1.002002 Types::Common::Numeric 1.002002 Types::Common::String 1.002002 Types::Serialiser 1.0 Types::Standard 1.002002 Types::Standard::ArrayRef 1.002002 Types::Standard::CycleTuple 1.002002 Types::Standard::Dict 1.002002 Types::Standard::HashRef 1.002002 Types::Standard::Map 1.002002 Types::Standard::ScalarRef 1.002002 Types::Standard::Tuple 1.002002 Types::TypeTiny 1.002002 Unicode::CharName 0.00 Unicode::EastAsianWidth 1.33 Unicode::EastAsianWidth::Detect 0.03 Unicode::GCString 2013.10 Unicode::LineBreak 2018.003 Unicode::LineBreak unknown Unicode::Map 0.112 Unicode::Map8 0.13 Unicode::String 2.10 UNIVERSAL::can 1.20140328 UNIVERSAL::isa 1.20171012 UNIVERSAL::require 0.18 URI 1.74 URI::_foreign 1.74 URI::_generic 1.74 URI::_idna 1.74 URI::_ldap 1.74 URI::_login 1.74 URI::_punycode 1.74 URI::_query 1.74 URI::_segment 1.74 URI::_server 1.74 URI::_userpass 1.74 URI::data 1.74 URI::Escape 3.31 URI::file 4.21 URI::file::Base 1.74 URI::file::FAT 1.74 URI::file::Mac 1.74 URI::file::OS2 1.74 URI::file::QNX 1.74 URI::file::Unix 1.74 URI::file::Win32 1.74 URI::Find 20160806 URI::Find::Schemeless 20160806 URI::ftp 1.74 URI::gopher 1.74 URI::Heuristic 4.20 URI::http 1.74 URI::https 1.74 URI::IRI 1.74 URI::ldap 1.74 URI::ldapi 1.74 URI::ldaps 1.74 URI::mailto 1.74 URI::mms 1.74 URI::news 1.74 URI::nntp 1.74 URI::pop 1.74 URI::QueryParam 1.74 URI::rlogin 1.74 URI::rsync 1.74 URI::rtsp 1.74 URI::rtspu 1.74 URI::sftp 1.74 URI::sip 1.74 URI::sips 1.74 URI::snews 1.74 URI::Split 1.74 URI::ssh 1.74 URI::telnet 1.74 URI::tn3270 1.74 URI::URL 5.04 URI::urn 1.74 URI::isbn 1.74 URI::oid 1.74 URI::WithBase 2.20 URI::ws 0.03 URI::wss 0.03 UUID::Tiny 1.04 Variable::Magic 0.62 Want 0.29 WidgetDemo 4.012 Win32::ShellQuote 0.003001 WWW::Form::UrlEncoded 0.24 WWW::Form::UrlEncoded::PP unknown WWW::Mechanize 1.88 WWW::Mechanize::Image 1.88 WWW::Mechanize::Link 1.88 WWW::Mechanize::TreeBuilder 1.20000 WWW::Pastebin::PastebinCom::Create 1.003 WWW::RobotRules 6.02 WWW::RobotRules::AnyDBM_File 6.00 WWW::Selenium 1.36 WWW::Selenium::Util 1.36 XML::Atom 0.42 XML::Atom::Base unknown XML::Atom::Category unknown XML::Atom::Client unknown XML::Atom::Content unknown XML::Atom::Entry unknown XML::Atom::ErrorHandler unknown XML::Atom::Feed unknown XML::Atom::Link unknown XML::Atom::Person unknown XML::Atom::Server unknown XML::Atom::Thing unknown XML::Atom::Util unknown XML::CommonNS 0.06 XML::Compile 1.60 XML::Compile::Iterator 1.60 XML::Compile::Schema 1.60 XML::Compile::Schema::BuiltInFacets 1.60 XML::Compile::Schema::BuiltInTypes 1.60 XML::Compile::Schema::Instance 1.60 XML::Compile::Schema::NameSpaces 1.60 XML::Compile::Schema::Specs 1.60 XML::Compile::Tester 0.91 XML::Compile::Translate 1.60 XML::Compile::Translate::Reader 1.60 XML::Compile::Translate::Template 1.60 XML::Compile::Translate::Writer 1.60 XML::Compile::Util 1.60 XML::DOM 1.46 XML::DOM::DOMException unknown XML::DOM::NamedNodeMap unknown XML::DOM::NodeList unknown XML::DOM::PerlSAX unknown XML::DOM::XPath 0.14 XML::ESISParser 0.08 XML::Filter::BufferText 1.01 XML::Generator::PerlData 0.95 XML::Handler::BuildDOM unknown XML::Handler::CanonXMLWriter 0.08 XML::Handler::Sample unknown XML::Handler::Subs 0.08 XML::Handler::XMLWriter 0.08 XML::LibXML 2.0132 XML::LibXML::AttributeHash 2.0132 XML::LibXML::Boolean 2.0132 XML::LibXML::Common 2.0132 XML::LibXML::Devel 2.0132 XML::LibXML::ErrNo 2.0132 XML::LibXML::Error 2.0132 XML::LibXML::Literal 2.0132 XML::LibXML::NodeList 2.0132 XML::LibXML::Number 2.0132 XML::LibXML::Reader 2.0132 XML::LibXML::SAX 2.0132 XML::LibXML::SAX::Builder 2.0132 XML::LibXML::SAX::Generator 2.0132 XML::LibXML::SAX::Parser 2.0132 XML::LibXML::Simple 0.99 XML::LibXML::XPathContext 2.0132 XML::LibXSLT 1.96 XML::Namespace 0.02 XML::NamespaceFactory 1.02 XML::NamespaceSupport 1.12 XML::Parser 2.44 XML::Parser::Expat 2.44 XML::Parser::Lite 0.721 XML::Parser::PerlSAX 0.08 XML::Parser::Style::Debug unknown XML::Parser::Style::Objects unknown XML::Parser::Style::Stream unknown XML::Parser::Style::Subs unknown XML::Parser::Style::Tree unknown XML::PatAct::ACTION unknown XML::PatAct::Amsterdam 0.08 XML::PatAct::MatchName 0.08 XML::PatAct::PATTERN unknown XML::PatAct::ToObjects 0.08 XML::Perl2SAX 0.08 XML::RegExp 0.04 XML::RSS 1.60 XML::RSS::Private::Output::Base 1.60 XML::RSS::Private::Output::Roles::ImageDims 1.60 XML::RSS::Private::Output::Roles::ModulesElems 1.60 XML::RSS::Private::Output::V0_9 1.60 XML::RSS::Private::Output::V0_91 1.60 XML::RSS::Private::Output::V1_0 1.60 XML::RSS::Private::Output::V2_0 1.60 XML::SAX 1.00 XML::SAX2Perl 0.08 XML::SAX::Base 1.09 XML::SAX::DocumentLocator unknown XML::SAX::Exception 1.09 XML::SAX::Expat 0.51 XML::SAX::ParserFactory 1.01 XML::SAX::PurePerl 1.00 XML::SAX::PurePerl unknown XML::SAX::PurePerl unknown XML::SAX::PurePerl unknown XML::SAX::PurePerl unknown XML::SAX::PurePerl unknown XML::SAX::PurePerl unknown XML::SAX::PurePerl::DebugHandler unknown XML::SAX::PurePerl::Exception unknown XML::SAX::PurePerl::Productions unknown XML::SAX::PurePerl::Reader unknown XML::SAX::PurePerl::Reader unknown XML::SAX::PurePerl::Reader unknown XML::SAX::PurePerl::Reader::Stream unknown XML::SAX::PurePerl::Reader::String unknown XML::SAX::PurePerl::Reader::URI unknown XML::SAX::Writer 0.57 XML::SAX::Writer::XML 0.57 XML::Simple 2.25 XML::Twig 3.52 XML::Twig::XPath 0.02 XML::Writer 0.625 XML::XPath 1.42 XML::XPath::Boolean 1.42 XML::XPath::Builder 1.42 XML::XPath::Expr 1.42 XML::XPath::Function 1.42 XML::XPath::Literal 1.42 XML::XPath::LocationPath 1.42 XML::XPath::Node 1.42 XML::XPath::Node::Attribute 1.42 XML::XPath::Node::Comment 1.42 XML::XPath::Node::Element 1.42 XML::XPath::Node::Namespace 1.42 XML::XPath::Node::PI 1.42 XML::XPath::Node::Text 1.42 XML::XPath::NodeSet 1.42 XML::XPath::Number 1.42 XML::XPath::Parser 1.42 XML::XPath::PerlSAX 1.42 XML::XPath::Root 1.42 XML::XPath::Step 1.42 XML::XPath::Variable 1.42 XML::XPath::XMLParser 1.42 XML::XPathEngine 0.14 XML::XPathEngine::Boolean unknown XML::XPathEngine::Expr unknown XML::XPathEngine::Function unknown XML::XPathEngine::Literal unknown XML::XPathEngine::LocationPath unknown XML::XPathEngine::NodeSet unknown XML::XPathEngine::Number unknown XML::XPathEngine::Root unknown XML::XPathEngine::Step 1.0 XML::XPathEngine::Variable unknown XS unknown YAML 1.26 YAML::Any 1.26 YAML::Dumper unknown YAML::Dumper::Base unknown YAML::Error unknown YAML::LibYAML 0.72 YAML::Loader unknown YAML::Loader::Base unknown YAML::Marshall unknown YAML::Mo unknown YAML::Node unknown YAML::Tag unknown YAML::Tiny 1.73 YAML::Types unknown YAML::XS 0.72 YAML::XS::LibYAML unknown Include path (INC) directories Searched Number of Modules /sw/comp/perl_modules/5.26.2/rackham/lib/perl5/5.26.2/x86_64-linux-thread-multi yes 0 /sw/comp/perl_modules/5.26.2/rackham/lib/perl5/5.26.2 yes 0 /sw/comp/perl_modules/5.26.2/rackham/lib/perl5/x86_64-linux-thread-multi yes 1055 /sw/comp/perl_modules/5.26.2/rackham/lib/perl5 yes 5989 /sw/comp/perl/5.26.2/rackham/lib/5.26.2/x86_64-linux-thread-multi no unknown /sw/comp/perl/5.26.2/rackham/lib/5.26.2 no unknown /sw/comp/perl/5.26.2/rackham/lib no unknown /sw/comp/perl/5.26.2/rackham/lib/site_perl/5.26.2/x86_64-linux-thread-multi no unknown /sw/comp/perl/5.26.2/rackham/lib/site_perl/5.26.2 no unknown <ul> <li>Total modules : 5989</li> </ul>"},{"location":"software/perl_packages/","title":"How do I install local Perl packages?","text":""},{"location":"software/perl_packages/#how-do-i-install-local-perl-packages","title":"How do I install local Perl packages?","text":""},{"location":"software/perl_packages/#what-is-available-already-in-the-perl-modules","title":"What is available already in the perl modules?","text":"<ul> <li> <p>A number of packages are available by default with all Perl versions.</p> </li> <li> <p>For Perl version 5.18.4 in particular (available through the software module system as <code>perl/5.18.4</code>), we have installed many more Perl packages. These are available by loading the software module <code>perl_modules/5.18.4</code>. We have a complete list of the Perl packages available.</p> </li> <li> <p>If you would like to use BioPerl, <code>module avail BioPerl</code> after loading bioinfo-tools will show the versions available. The latest is <code>BioPerl/1.6.924_Perl5.18.4</code>, which is built against Perl <code>5.18.4</code> so also loads the modules <code>perl/5.18.4</code> and <code>perl_modules/5.18.4</code>.</p> </li> </ul>"},{"location":"software/perl_packages/#install-other-packages","title":"Install other packages","text":"<p>You could email support at <code>support@uppmax.uu.se</code> and suggest we include the package in <code>perl_modules</code>. If that doesn't work, or you decide to install it for yourself, please keep reading.</p> <p>First you have to decide where you want to put your local Perl packages. Save this in a temporary environment variable called MY_PERL, make sure to substitute the path with your own:</p> <pre><code>export MY_PERL=/home/johanhe/slask/perl/\n</code></pre> <p>Then we download and install a more light weight CPAN client called cpanm, which have less confusing settings to configure and also makes it easier to install local packages. We'll then also install the module local::lib to a directory of your choice:</p> <pre><code>wget -O- http://cpanmin.us | perl - -l $MY_PERL App::cpanminus local::lib\n</code></pre> <p>Now we should be ready to set up the correct environment variables and load them for this session:</p> <pre><code>echo \"eval `perl -I $MY_PERL/lib/perl5 -Mlocal::lib=$MY_PERL`\" &gt;&gt; ~/.bash_profile\necho \"export PATH=$MY_PERL/bin/:$PATH\" &gt;&gt; ~/.bash_profile\nsource ~/.bash_profile\n</code></pre> <p>After this is done we can always install local packages easily by using the command:</p> <p><code>bash cpanm [name-of-package-to-install]</code>bash</p>"},{"location":"software/picard/","title":"Picard","text":""},{"location":"software/picard/#picard","title":"Picard","text":"<p>'Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF' (source: the Picard documentation).</p>"},{"location":"software/picard/#usage","title":"Usage","text":"<p>Load the <code>bioinfo-tools</code> module first:</p> <pre><code>module load bioinfo-tools\n</code></pre> <p>Then search for you favorite Picard version:</p> <pre><code>module spider picard\n</code></pre> How does this look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham2 ~]$ module spider picard\n\n----------------------------------------------------------------------------\n  picard:\n----------------------------------------------------------------------------\n     Versions:\n        picard/1.92\n        picard/1.118\n        picard/1.141\n        picard/2.0.1\n        picard/2.10.3\n        picard/2.19.2\n        picard/2.20.4\n        picard/2.23.4\n        picard/2.27.5\n        picard/3.1.1\n\n----------------------------------------------------------------------------\n  For detailed information about a specific \"picard\" package (including how to l\noad the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modu\nles.\n  For example:\n\n     $ module spider picard/3.1.1\n----------------------------------------------------------------------------\n</code></pre> <p>Then load your favorite version:</p> <pre><code>module load picard/3.1.1\n</code></pre> How does this look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham2 ~]$ module load picard/3.1.1\npicard/3.1.1: java -jar $PICARD command ...\n</code></pre> <p>Read up on how to use Picard:</p> <pre><code>module help picard/3.1.1\n</code></pre> How does this look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham2 ~]$ module help picard/3.1.1\n\n----------------------------------------------------------------------- Module Specific Help for \"picard/3.1.1\" -----------------------------------------------------------------------\n picard - use picard/3.1.1\n\n    Version 3.1.1\n\nUsage:\n\n    java -jar $PICARD command ...\n\nor\n\n    java -jar $PICARD_ROOT/picard.jar command ...\n\nwhere 'command' is the desired Picard command, and ... are the desired further arguments.\n</code></pre> <p>Here is an example of using Picard to test if a file is a valid BAM/CRAM/SAM file:</p> <pre><code>java -jar $PICARD ValidateSamFile --INPUT my_file.bam\n</code></pre> How does this look like? <p>First, download an example BAM file from the Picard GitHub repository:</p> <pre><code>[sven@rackham2 ~]$ wget https://github.com/broadinstitute/picard/raw/master/testdata/picard/flow/reads/input/sample_mc.bam\n\n--2024-08-05 09:16:40--  https://github.com/broadinstitute/picard/raw/master/testdata/picard/flow/reads/input/sample_mc.bam\nResolving github.com (github.com)... 140.82.121.3\nConnecting to github.com (github.com)|140.82.121.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/broadinstitute/picard/master/testdata/picard/flow/reads/input/sample_mc.bam [following]\n--2024-08-05 09:16:40--  https://raw.githubusercontent.com/broadinstitute/picard/master/testdata/picard/flow/reads/input/sample_mc.bam\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 117715 (115K) [application/octet-stream]\nSaving to: \u2018sample_mc.bam\u2019\n\n100%[=============================================================================================================================================&gt;] 117,715     --.-K/s   in 0.001s\n\n2024-08-05 09:16:41 (171 MB/s) - \u2018sample_mc.bam\u2019 saved [117715/117715]\n</code></pre> <p>Your output will be similar to this, when using that valid BAM file:</p> <pre><code>[sven@rackham2 ~]$ java -jar $PICARD ValidateSamFile --INPUT sample_mc.bam\nAug 05, 2024 9:16:47 AM com.intel.gkl.NativeLibraryLoader load\nINFO: Loading libgkl_compression.so from jar:file:/sw/bioinfo/picard/3.1.1/rackham/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n[Mon Aug 05 09:16:47 CEST 2024] ValidateSamFile --INPUT sample_mc.bam --MODE VERBOSE --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false\n[Mon Aug 05 09:16:47 CEST 2024] Executing as sven@rackham2.uppmax.uu.se on Linux 3.10.0-1160.119.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 17+35-2724; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:3.1.1\nWARNING 2024-08-05 09:16:47 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.\nNo errors found\n[Mon Aug 05 09:16:48 CEST 2024] picard.sam.ValidateSamFile done. Elapsed time: 0.01 minutes.\nRuntime.totalMemory()=2181038080\n[sven@rackham2 ~]$\n</code></pre> <p>Your output will be similar to this, when using an invalid file, such as an R script file:</p> <pre><code>[sven@rackham2 ~]$ java -jar $PICARD ValidateSamFile --INPUT app.R\nAug 05, 2024 9:13:20 AM com.intel.gkl.NativeLibraryLoader load\nINFO: Loading libgkl_compression.so from jar:file:/sw/bioinfo/picard/3.1.1/rackham/picard.jar!/com/intel/gkl/native/libgkl_compression.so\n[Mon Aug 05 09:13:20 CEST 2024] ValidateSamFile --INPUT app.R --MODE VERBOSE --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 5 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false\n[Mon Aug 05 09:13:21 CEST 2024] Executing as sven@rackham2.uppmax.uu.se on Linux 3.10.0-1160.119.1.el7.x86_64 amd64; OpenJDK 64-Bit Server VM 17+35-2724; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:3.1.1\nWARNING 2024-08-05 09:13:21 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.\nERROR::MISSING_READ_GROUP:Read groups is empty\nSAMFormatException on record 01\nERROR 2024-08-05 09:13:21 ValidateSamFile SAMFormatException on record 01\n[Mon Aug 05 09:13:21 CEST 2024] picard.sam.ValidateSamFile done. Elapsed time: 0.01 minutes.\nRuntime.totalMemory()=2181038080\nTo get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp\n</code></pre>"},{"location":"software/profilers/","title":"Profilers","text":""},{"location":"software/profilers/#profilers","title":"Profilers","text":"<p>There are some profiling tools that are available at UPPMAX.</p> Software Compiler(s) Description Intel VTune Intel Broad set of tools with a focus on performance improvement Intel Advisor Intel Broad set of tools with a focus on performance analysis gprof GCC run-time profiler valgrind GCC and Intel Broad set of tools"},{"location":"software/projplot/","title":"projplot","text":""},{"location":"software/projplot/#projplot","title":"<code>projplot</code>","text":"<p><code>projplot</code> is an UPPMAX tool to plot your core hour usage</p>"},{"location":"software/projplot/#minimal-use","title":"Minimal use","text":"<p><code>projplot</code> needs only the project code:</p> <pre><code>projplot -A [project_code]\n</code></pre> <p>For example:</p> <pre><code>projplot -A uppmax2020-2-2\n</code></pre> <p>Output will look similar to this:</p> <p></p> <p>Example <code>projplot</code> output. The horizontal axis shows the days before today, the vertical axis shows the cores used on that day (hence, the amount of core hours is the area under the curve). For this example project, apparently, the maximum number of cores per day is 800.</p> <p>This graph shows you the projects core usage during the last 30 days. The heights of the peaks in the plot shows you how many cores that were used simultaneously, and the width show you for how long they were used.</p> <p>If we look at the big peak to the left in the diagram, we can see that 15 cores were used for around 24 hours, and somewhere in the middle of that period, another 8 cores was used for a shorter period of time.</p> <p>Since the plots are made using ordinary text, there will sometimes be rounding errors because of the low resolution of the terminal window, which is usually around 80x30 characters. The plot will adapt to your terminal window, so increase the size of your window to increase the resolution of the plot (the data being plotted has a resolution down to single seconds).</p> <p>As time progresses the peaks in the graph will move to the left in the diagram. In the standard plot of the last 30 days, that means that when a peak exits the plot to the left, your get those core hours back to the project.</p>"},{"location":"software/projplot/#if-you-are-over-quota","title":"If you are over quota","text":"<p>If we look at a project that has used more core hours than their projects allocation, the image will look like this:</p> <p></p> <p>There is a message about the core hour limit being reached at the top of the plot. If you look in the diagram at around 10 days ago, you will see the point where the core hour limit is reached (the bar of <code>&gt;</code>s). This point is calculated by summing up all core hour usage to the right of the bar. What this means in reality is that if this project was to stop analyzing right now, they would have to wait until the bar of <code>&gt;</code>s has exited the graph to the left (i.e. ~20 days) before they are below their core hour limit again. Most of the time, projects do not completely stop analyzing, so for each core hour they use the more to the right the <code>&gt;</code> bar will move.</p>"},{"location":"software/projplot/#other-options","title":"Other options","text":"<p><code>projplot</code> has more options, that are shown by using <code>--help</code>:</p> <pre><code>projplot --help\n</code></pre> <p>Below, these options are discussed in detail.</p>"},{"location":"software/projplot/#help","title":"Help","text":"<p>Use <code>--help</code> (or <code>-h</code>) to get a short description of the options and some examples:</p> <pre><code>projplot --help\n</code></pre> How does that look like? <pre><code>Usage: projplot -A &lt;proj-id&gt; [options]\n\nMore details: https://uppmax.uu.se/support/user-guides/plotting-your-core-hour-usage\n\nExample runs:\n\n# Plot the last 30 days of project &lt;proj&gt;\nprojplot -A &lt;proj&gt;\n\n# Plot the last 30 days of project &lt;proj&gt; on cluster &lt;cluster&gt;\nprojplot -A &lt;proj&gt; -c &lt;cluster&gt;\n\n# Plot the last &lt;n&gt; days of project &lt;proj&gt;\nprojplot -A &lt;proj&gt; -d &lt;n&gt;\n\n# Plot the usage for project &lt;proj&gt; since &lt;date&gt;\nprojplot -A &lt;proj&gt; -s &lt;date&gt;\n\n# Plot the usage for project &lt;proj&gt; between &lt;date_1&gt; and &lt;date_2&gt;\nprojplot -A &lt;proj&gt; -s &lt;date_1&gt; -e &lt;date_2&gt;\n\n# Plot the usage for project &lt;proj&gt; between &lt;date_1&gt; and &lt;date_2&gt;, on cluster &lt;cluster&gt;\nprojplot -A &lt;proj&gt; -s &lt;date_1&gt; -e &lt;date_2&gt; -c &lt;cluster&gt;\n\n# Plot the usage for project &lt;proj&gt; between date &lt;date_1&gt; and &lt;days&gt; days later\nprojplot -A &lt;proj&gt; -s &lt;date_1&gt; -d &lt;days&gt;\n\n# Plot the usage for project &lt;proj&gt; between date &lt;date_1&gt; and &lt;days&gt; days earlier\nprojplot -A &lt;proj&gt; -e &lt;date_1&gt; -d &lt;days&gt;\n\n# Plot the last 30 days of project &lt;proj&gt;, but don't check the queue for running jobs\nprojplot -A &lt;proj&gt; -R\n\n\nOptions:\n  -h, --help            show this help message and exit\n  -A ACCOUNT, --account=ACCOUNT\n                        Your UPPMAX project ID\n  -c CLUSTER, --cluster=CLUSTER\n                        The cluster you want to plot (default: current\n                        cluster)\n  -d DAYS, --days=DAYS  The number of days you want to plot (default: none)\n  -s START, --start=START\n                        The starting date you want to plot (format: YYYY-MM-\n                        DD)\n  -e END, --end=END     The ending date you want to plot (format: YYYY-MM-DD)\n  -R, --no-running-jobs\n                        Use to skip including running jobs in the plot\n                        (faster). Useful if you are not running any jobs and\n                        want to save time.\n</code></pre>"},{"location":"software/projplot/#number-of-days","title":"Number of days","text":"<p>Use <code>--days</code> (or <code>-d</code>) the plot a custom number of days, instead of the default of 30 days:</p> <pre><code>projplot -A [project_code] --days [number_of_days]\n</code></pre> <p>For example, this will plot the last 45 days: :</p> <pre><code>projplot -A uppmax2020-2-2 --days 45\n</code></pre>"},{"location":"software/projplot/#starting-date","title":"Starting date","text":"<p>Use <code>--start</code> (or <code>-s</code>) to specify a custom starting date, from when the time in your plot will start:</p> <pre><code>projplot -A [project_code] --start [starting_date_in_yyyy-mm-dd_format]\n</code></pre> <p>For example:</p> <pre><code>projplot -A uppmax2020-2-2 --start 2023-05-03\n</code></pre> <p>will give you a plot starting on the date 2023-05-03 and the default number of days after that date. The command below does exactly the same, yet makes the default number of days explicit:</p> <pre><code>projplot -A uppmax2020-2-2 --start 2023-05-03 --days 30\n</code></pre>"},{"location":"software/projplot/#ending-data","title":"Ending data","text":"<p>Use <code>--end</code> (or <code>-e</code>) to specify a custom ending date, from when the time in your plot will end:</p> <pre><code>projplot -A [project_code] --end [ending_date_in_yyyy-mm-dd_format]\n</code></pre> <p>For example:</p> <pre><code>projplot -A uppmax2020-2-2 --end 2023-05-03\n</code></pre> <p>will give you a plot ending on the date 2023-05-03 and the default number of days before that date. The command below does exactly the same, yet makes the default number of days explicit:</p> <pre><code>projplot -A uppmax2020-2-2 --end 2023-05-03 --days 30\n</code></pre>"},{"location":"software/projplot/#start-and-end-date-combined","title":"Start and end date combined","text":"<p>Use <code>--start</code> and <code>--end</code> combined to specify a custom range of dates for your plot:</p> <pre><code>projplot -A [project_code] --start [starting_date_in_yyyy-mm-dd_format] --end [ending_date_in_yyyy-mm-dd_format]\n</code></pre> <p>For example:</p> <pre><code>projplot -A uppmax2020-2-2 --start 2022-05-03 --end 2023-05-03\n</code></pre>"},{"location":"software/projplot/#cluster","title":"Cluster","text":"<p>Use <code>--cluster</code> (or <code>-c</code>) to determine which UPPMAX cluster to plot. By default, the current cluster is used.</p> <p>Since the different clusters at UPPMAX have separate core hour quotas, it makes sense to being able to plot them separately.</p> <pre><code>projplot -A [project_code] -c [cluster_name]\n</code></pre> <p>For example:</p> <pre><code>projplot -A uppmax2020-2-2 -c snowy\n</code></pre> <p>Valid cluster names are <code>bianca</code>, <code>rackham</code> and <code>snowy</code>.</p> How to get valid cluster names? <p>Use <code>projplot</code> with a nonsense clustername:</p> <pre><code>projplot -A uppmax2020-2-2 --cluster nonsensename\n</code></pre> <p>The error message will display valid cluster names.</p> <p>This option can be combined with all the other options.</p>"},{"location":"software/projplot/#exclude-running-jobs","title":"Exclude running jobs","text":"<p>Use <code>--no-running-jobs</code> (or <code>-R</code>) to skip checking the queue for running jobs.</p> <p>If you don't have any running jobs, asking the queue system to list jobs is just a waste of time (anywhere 1-15 seconds). By giving <code>--no-running-jobs</code> when running <code>projplot</code>, it skips checking the queue and if you do have jobs running, they will not be visible in the plot or in the sum of core hours used.</p> <pre><code>projplot -A [project_code] --no-running-jobs\n</code></pre> <p>For example:</p> <pre><code>projplot -A uppmax2020-2-2 --no-running-jobs\n</code></pre>"},{"location":"software/puttygen/","title":"puttygen","text":"","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#puttygen","title":"<code>puttygen</code>","text":"<p><code>puttygen</code>, also called 'PuTTY Key Generator', is a free program for Linux, Mac and Windows to create SSH keys in multiple formats, among others the <code>.ppk</code> file format, which is needed for FileZilla.</p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#get-puttygen","title":"Get <code>puttygen</code>","text":"<p><code>puttygen</code> can be downloaded from its (advertisement-heavy) website at https://www.puttygen.com.</p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#create-ssh-key-files","title":"Create SSH key files","text":"Would you like a video? <p>This procedure is demonstrated in this YouTube video called 'Create SSH key files in puttygen'.</p> <p>This procedure shows how to generate the following SSH key files:</p> Filename Description <code>my_key</code> Private key, in OpenSSH format <code>my_key.ppk</code> Private key, in Putty private key format <code>my_key.pub</code> Public key <p>Follow the following steps.</p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#1-click-on-generate","title":"1. Click on 'generate'","text":"<p>In <code>puttygen</code>, click on 'Generate'</p> How does that look like? <p></p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#2-move-the-mouse-cursor-over-the-window","title":"2. Move the mouse cursor over the window","text":"<p>Move the mouse cursor over the <code>puttygen</code> window until the progress bar is filled up.</p> How does that look like? <p></p> <p>The keys have now been generated</p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#3-click-on-save-public-key","title":"3. Click on 'Save public key'","text":"<p>In <code>puttygen</code>, click on 'Save public key'.</p> How does that look like? <p></p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#4-save-the-public-key-as-a-pub-file","title":"4. Save the public key as a <code>.pub</code> file","text":"<p>In the 'Save public key as' dialog, save the public key as <code>[description].pub</code>, e.g. <code>demo.pub</code>.</p> How does that look like? <p></p> <p>Now you have your public key file!</p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#5-click-on-save-private-key","title":"5. Click on 'Save private key'","text":"<p>In <code>puttygen</code>, click on 'Save private key'.</p> How does that look like? <p></p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#6-if-needed-ignore-the-warning","title":"6. If needed: ignore the warning","text":"<p>You will get a warning, click on 'Ignore'.</p> Why is there this warning? <p>In this procedure, the private key does not get a password. Some settings, however, do require such a password.</p> How does that look like? <p></p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#7-save-the-private-key-as-a-ppk-file","title":"7. Save the private key as a <code>.ppk</code> file","text":"<p>In the 'Save private key as' dialog, save the private key as <code>[description]</code>, e.g. <code>demo</code>. It will be saved as <code>[description].ppk</code>, e.g. <code>demo.ppk</code>.</p> How does that look like? <p></p> <p>Now you have your private key file in <code>ppk</code> format!</p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#8-click-on-conversions-export-openssh-key","title":"8. Click on 'Conversions | Export OpenSSH key'","text":"<p>In <code>puttygen</code>, click on 'Conversions | Export OpenSSH key'</p> How does that look like? <p></p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#9-if-needed-ignore-the-warning","title":"9. If needed: ignore the warning","text":"<p>You will get a warning, click on 'Ignore'.</p> Why is there this warning? <p>In this procedure, the private key does not get a password. Some settings, however, do require such a password.</p> How does that look like? <p></p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/puttygen/#10-click-on-save-private-key","title":"10. Click on 'Save private key'","text":"<p>In the 'Save private key as' dialog, save the private key as <code>[description]</code>, e.g. <code>demo</code>. It will be saved as <code>[description]</code> (i.e. without a file extension), e.g. <code>demo</code> (again: without a file extension).</p> How does that look like? <p></p> <p>Now you have your private key file in OpenSSH format!</p>","tags":["puttygen","PuTTY Key Generator","software","SSH","key","files","ppk"]},{"location":"software/python/","title":"Python","text":""},{"location":"software/python/#python-user-guide","title":"Python user guide","text":"<p>Welcome to the UPPMAX Python user guide.</p> <p>We describe what Python is and that there are multiple Python versions.</p> <p>Then, we show how to load Python and to load Python packages after which you can run Python.</p> <p>Finally, you can find UPPMAX Python-related courses and these more advanced topics:</p> <ul> <li>Programming in Python</li> <li>Installing Python packages</li> <li>Virtual environments in Python</li> <li>How to run parallel jobs in Python</li> </ul>"},{"location":"software/python/#what-is-python","title":"What is Python?","text":"<p>Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation [Kuhlman, 2009].</p>"},{"location":"software/python/#python-versions","title":"Python versions","text":"<p>Python (or to be precise: the Python interpreter) has different versions. The current major version of Python is Python 3. Python 3 is not backwards compatible with Python 2. This means that you need to use the correct Python version to run a Python script.</p> Could you give me an example of a difference between Python 2 and 3? <p>One example is how Python 2 and Python 3 dividetwo integers. Here is an example that will work on all UPMMAX clusters.</p> <p>Load Python 2.7.15:</p> <pre><code>module load python/2.7.15\n</code></pre> <p>Then</p> <pre><code>python -c \"print(1/2)\"\n</code></pre> <p>will print <code>0</code>, as this is an integer division: two fits zero times in one.</p> <p>Load Python 3.11.4:</p> <pre><code>module load python/3.11.4\n</code></pre> <p>Then</p> <pre><code>python -c \"print(1/2)\"\n</code></pre> <p>will print <code>0.5</code>, as this is turned into a floating point division, equivalent to <code>1.0 / 2.0</code>.</p> Which version of Python is <code>python</code>? <p>To determine which version <code>python</code> is, in a terminal, type:</p> <pre><code>python --version\n</code></pre> <p>to see which Python version you are using now.</p> Which version of Python is <code>python3</code>? <p>To determine which version <code>python3</code> is, in a terminal, type:</p> <pre><code>python3 --version\n</code></pre> <p>to see which Python version you are using now.</p>"},{"location":"software/python/#loading-python","title":"Loading Python","text":"Prefer seeing a video? <p>See the YouTube video how to load the Python modules.</p> <p>The different versions of Python are available via the module system on all UPPMAX clusters. Loading a Python module also makes some Python packages available.</p> Forgot what the module system is? <p>See the UPPMAX pages on the module system.</p> UPPMAX modules or Python modules? <p>At this page, we will use the word 'modules' for UPPMAX modules and 'packages' for Python modules, to be clear in what is meant. The word 'package' is used in multiple other languages, such as R, with a similar definition as a Python module.</p> <p>To find out which Python modules there are, use <code>module spider python</code>.</p> What is the output of that command? <p>The output of <code>module spider python</code> on the day of writing, is:</p> <pre><code>[user@rackham1 ~]$ module spider python\n\n---------------------------------------------------------------------------------------\n  python:\n---------------------------------------------------------------------------------------\n     Versions:\n        python/2.7.6\n        python/2.7.9\n        python/2.7.11\n        python/2.7.15\n        python/3.3\n        python/3.3.1\n        python/3.4.3\n        python/3.5.0\n        python/3.6.0\n        python/3.6.8\n        python/3.7.2\n        python/3.8.7\n        python/3.9.5\n        python/3.10.8\n        python/3.11.4\n     Other possible modules matches:\n        Biopython  Boost.Python  GitPython  IPython  Python  biopython  flatbuffers-python\n ...\n\n---------------------------------------------------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*python.*'\n\n---------------------------------------------------------------------------------------\n  For detailed information about a specific \"python\" package (including how to load the mod\nules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider python/3.11.4\n---------------------------------------------------------------------------------------\n</code></pre> <p>To load a specific version of Python into your environment, type <code>module load python/[version]</code>, where <code>[version]</code> is a Python version, for example, <code>module load python/3.11.4</code></p> Do I really need to load a Python module? <p>It is recommended to load a Python module, but in some case you will not get into trouble.</p> <p>When you do not load a module, the system-installed Python version are used. These are <code>python</code> version 2.7.5, and <code>python3</code> version 3.6.8.</p> <p>If using those older versions give you no trouble, all is well, for example, when running basic Python scripts that have no package imports.</p> <p>However, when any problem occurs, load those newer modules.</p> Why are there both <code>python/3.X.Y</code> and <code>python3/3.X.Y</code> modules? <p>Sometimes existing software might use <code>python2</code> and there\u2019s nothing you can do about that.</p> <p>In pipelines and other toolchains the different tools may together require both <code>python2</code> and <code>python3</code>.</p> How to deal with tools that require both <code>python2</code> and <code>python3</code>? <p>You can run two python modules at the same time if one of the modules is <code>python/2.X.Y</code> and the other module is <code>python3/3.X.Y</code> (i.e. not <code>python/3.X.Y</code>).</p> <p>After loading a Python module, one can start the Python interpreter:</p> <pre><code>python\n</code></pre> How does this look like? <p>The Python interpreter looks like this:</p> <pre><code>Python 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre>"},{"location":"software/python/#loading-python-package-modules","title":"Loading Python package modules","text":"<p>Terminology</p> <p>A Python package consists out of one or more Python modules. in this document, we avoid using this term, to avoid confusion with the UPPMAX modules.</p> <p>For more complex complex Python packages, there exist UPPMAX modules to load these:</p> <ul> <li><code>python_GIS_packages</code>: for geographic information system packages</li> <li><code>python_ML_packages</code>: for machine learning Python packages</li> </ul> How could I find these modules myself? <p>Use:</p> <pre><code>module spider packages\n</code></pre>"},{"location":"software/python/#loading-python-packages","title":"Loading Python packages","text":"<p>Terminology</p> <p>A Python package consists out of one or more Python modules. in this document, we avoid using this term, to avoid confusion with the UPPMAX modules.</p> <p>Many scientific tools are distributed as Python packages, which allows any user to run complex tools from a terminal or script. For example, the following Python code imports the functionality of the <code>pandas</code> library:</p> <pre><code>import pandas\n</code></pre> <p>Some packages/tools are preinstalled on all UPPMAX clusters. To load such a package:</p> <ul> <li>determine if it comes with your Python version</li> <li>determine if it comes as a module</li> </ul>"},{"location":"software/python/#determine-if-a-python-package-comes-with-your-python-module","title":"Determine if a Python package comes with your Python module","text":"<p>To determine if a Python package comes with your Python module, there are multiple ways:</p> <ul> <li>Using <code>pip list</code></li> <li>Using the module help</li> <li>Importing the package</li> </ul>"},{"location":"software/python/#using-pip-list","title":"Using <code>pip list</code>","text":"<p>To determine if a Python package comes with your Python module, <code>pip list</code> is one of the ways to do so.</p> <p>On a terminal, type:</p> <pre><code>pip list\n</code></pre> <p>This shows a list of Python packages that are installed.</p> How does the output of <code>pip list</code> look like? <p>Here is an example:</p> <pre><code>Package                   Version\n------------------------- ---------------\nanndata                   0.10.5.post1\nanyio                     4.2.0\nappdirs                   1.4.4\nargon2-cffi               23.1.0\nargon2-cffi-bindings      21.2.0\n[more Python packages]\nWerkzeug                  3.0.1\nwheel                     0.42.0\nwidgetsnbextension        4.0.9\nzipp                      3.17.0\nzope.interface            6.1\n</code></pre>"},{"location":"software/python/#using-the-module-help","title":"Using the module help","text":"<p>Determine if a Python package comes with your Python module using the module help, in a terminal, type:</p> <pre><code>module help python/[module_version]\n</code></pre> <p>where <code>[module_version]</code> is a version of a Python module, for example:</p> <pre><code>module help python/3.11.4\n</code></pre> What is the output of <code>module help python/3.11.4</code>? <p>Here is part of the output of <code>module help python/3.11.4</code>:</p> <pre><code>------------------------ Module Specific Help for \"python/3.11.4\" -------------------------\n    Python - use Python\n\n    Version 3.11.4\n\n\nThis module provides the executable names 'python' and 'python3'.\n\nSeveral additional python packages are also installed in this module. The complete list of\npackages in this module, produced using 'pip list', is:\n\nPackage                   Version\n------------------------- -----------\nanndata                   0.9.2\nanyio                     3.7.1\nargon2-cffi               21.3.0\n...\nwidgetsnbextension        4.0.8\nzipp                      3.16.2\nzope.interface            6.0\n</code></pre>"},{"location":"software/python/#importing-the-package","title":"Importing the package","text":"<p>Importing a Python package is a way to determine if a Python package comes with your Python module installed. From the terminal do:</p> <pre><code>python -c \"import [your_package]\"\n</code></pre> What does that <code>-c</code> do? <p><code>python -c</code> will run the text after it as Python code. In this way, you can directly run code, i.e. you do not need to create a file to run.</p> <p>where <code>[your_package]</code> is the name of a Python package, for example:</p> <pre><code>python -c \"import pandas\"\n</code></pre> What is the output if the Python package is found? <p>The output if the Python package is found is nothing.</p> What is the output if the Python package is not found? <p>Here an absent package is loaded, with the nonsense name <code>absentpackage</code>:</p> <pre><code>python -c \"import absentpackage\"\n</code></pre> <p>This results in the following error:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'absentpackage'\n</code></pre>"},{"location":"software/python/#determine-if-a-python-package-comes-with-a-module","title":"Determine if a Python package comes with a module","text":"<p>If the Python package is not pre-installed with your version of Python, use the UPPMAX module system to search for it.</p> <p>Not all packages are easy to find, as some are part of super-packages, for example the TensorFlow Python libraries, which are part of the <code>python_ML_packages/[version]-{cpu,gpu}</code>, for example <code>python_ML_packages/3.11.8-cpu</code>.</p> Want to see a list of Python packages in <code>python_ML_packages/3.11.8-cpu</code> that are not in <code>python/3.11.8</code>? <p>Here you go:</p> <ul> <li>absl-py</li> <li>array-record</li> <li>astunparse</li> <li>cachetools</li> <li>cons</li> <li>dill</li> <li>dm-tree</li> <li>ducc0</li> <li>etils</li> <li>etuples</li> <li>flatbuffers</li> <li>gast</li> <li>google-auth</li> <li>google-auth-oauthlib</li> <li>google-pasta</li> <li>googleapis-common-protos</li> <li>grpcio</li> <li>imbalanced-learn</li> <li>importlib_resources</li> <li>keras</li> <li>libclang</li> <li>llvmlite</li> <li>logical-unification</li> <li>miniKanren</li> <li>ml-dtypes</li> <li>multipledispatch</li> <li>nlp</li> <li>numba</li> <li>oauthlib</li> <li>opt-einsum</li> <li>patsy</li> <li>promise</li> <li>protobuf</li> <li>pyasn1</li> <li>pyasn1-modules</li> <li>pytensor</li> <li>requests-oauthlib</li> <li>rsa</li> <li>scikit-learn</li> <li>seaborn</li> <li>statsmodels</li> <li>tensorboard</li> <li>tensorboard-data-server</li> <li>tensorflow-cpu</li> <li>tensorflow-datasets</li> <li>tensorflow-estimator</li> <li>tensorflow-io-gcs-filesyst</li> <li>tensorflow-metadata</li> <li>tensorflow-probability</li> <li>termcolor</li> <li>threadpoolctl</li> <li>toml</li> <li>torch</li> <li>torchaudio</li> <li>torchvision</li> <li>wrapt</li> <li>xxhash</li> </ul> <p>It may not always be easy to find your Python package within the many modules. Do not hesitate to contact support so that you can spend time on your research and we figure this out :-)</p>"},{"location":"software/python/#stand-alone-tools","title":"Stand-alone tools","text":"<p>Some Python packages are working as stand-alone tools, for instance in bioinformatics. The tool may be already installed as a module. Check if it is there by using the module system <code>spider</code> function:</p> <pre><code>module spider [tool_name]\n</code></pre> <p>where <code>[tool_name]</code> is (part of) the name of the tool. <code>module spider</code> is case-insensitive, hence <code>YourTool</code> and <code>yourtool</code> give similar results.</p> What are UPPMAX modules? <p>See the page about the UPPMAX module system</p>"},{"location":"software/python/#running-python","title":"Running Python","text":"<p>You can run Python in multiple ways:</p> <ul> <li>use Python to run a Python script</li> <li>use Python in an interactive session</li> </ul> <p>To program in Python, there are more ways, which are discussed at the UPPMAX page on Python programming</p>"},{"location":"software/python/#use-python-to-run-a-python-script","title":"Use Python to run a Python script","text":"<p>You can run a Python script in the shell by:</p> <pre><code>python example_script.py\n</code></pre> <p>or, if you loaded a <code>python3</code> module:</p> <pre><code>python3 example_script.py\n</code></pre>"},{"location":"software/python/#use-python-in-an-interactive-session","title":"Use Python in an interactive session","text":"<p>You start a python session by typing:</p> <pre><code>python\n</code></pre> <p>or</p> <pre><code>python3\n</code></pre> <p>The python prompt looks like this:</p> <pre><code>&gt;&gt;&gt;\n</code></pre> <p>Exit with <code>&lt;Ctrl-D&gt;</code>, <code>quit()</code> or <code>exit()</code>.</p>"},{"location":"software/python/#programming-in-python","title":"Programming in Python","text":"<p>To program in Python, there are more ways, which are discussed at the UPPMAX page on Python programming</p>"},{"location":"software/python/#uppmax-python-related-courses","title":"UPPMAX Python-related courses","text":"<p>See the UPPMAX courses and workshops to find UPPMAX courses related to Python.</p>"},{"location":"software/python/#installing-python-packages","title":"Installing Python packages","text":"<p>See the UPPMAX page on how to install Python packages.</p>"},{"location":"software/python/#virtual-environments-in-python","title":"Virtual environments in Python","text":"<p>See the UPPMAX page on how to use virtual environments in Python.</p>"},{"location":"software/python/#how-to-run-parallel-jobs-in-python","title":"How to run parallel jobs in Python","text":"<p>See the UPPMAX page on how to run parallel jobs in Python.</p>"},{"location":"software/python/#references","title":"References","text":"<ul> <li>[Kuhlman, 2009] Kuhlman, Dave. A python book: Beginning python, advanced python, and python exercises. Lutz: Dave Kuhlman, 2009.</li> </ul>"},{"location":"software/python/#links","title":"Links","text":"<ul> <li>Official Python documentation</li> <li>Python forum</li> <li>Free online book: 'How to Think Like a Computer Scientist'</li> <li>UPPMAX TensorFlow guide</li> <li>UPPMAX PyTorch guide</li> </ul>"},{"location":"software/python_install_packages/","title":"Installing Python packages","text":"","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#installing-python-packages","title":"Installing Python packages","text":"<p>This page describes how to install Python packages.</p> <p>There are many ways to install a Python package:</p> <ul> <li>Using <code>setup.py</code></li> <li>using a Python package installer<ul> <li>PyPI using <code>pip</code></li> <li>Conda using <code>conda</code></li> </ul> </li> </ul> <p>You may want to check if a package is already installed first :-).</p> <p>The Python package installers are compared after which each is discussed:</p> <ul> <li>PyPI using <code>pip</code></li> <li>Conda using <code>conda</code></li> </ul>","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#check-if-a-package-is-already-installed","title":"Check if a package is already installed","text":"<p>There are multiple ways to check if a Python package is installed:</p>","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#1-pip-list","title":"1. pip list","text":"<p>In the terminal, type:</p> <pre><code>pip list\n</code></pre> <p>You'll see a list of all installed packages.</p>","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#2-import","title":"2. import","text":"<p>Start Python. Then, within the Python interpreter, type:</p> <pre><code>import [package]\n</code></pre> <p>where <code>[package]</code> is the name of the Python package, for example <code>import mhcnuggets</code>.</p> <p>Does it work? Then it is there!</p>","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#comparison-between-conda-and-pypi","title":"Comparison between Conda and PyPI","text":"<ul> <li> <p>PyPI (<code>pip</code>) is traditionally for Python-only packages but it is no problem to also distribute packages written in other languages as long as they provide a Python interface.</p> </li> <li> <p>Conda (<code>conda</code>) is more general and while it contains many Python packages and packages with a Python interface, it is often used to also distribute packages which do not contain any Python (e.g. C or C++ packages).</p> </li> </ul> Parameter <code>conda</code> <code>pip</code> Installs Python packages Yes Yes Installs non-Python software Yes No <p>Many libraries and tools are distributed in both ecosystems.</p>","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#pip","title":"<code>pip</code>","text":"<p><code>pip</code> is a popular Python package installer.</p> <p>To install a Python package using <code>pip</code>, in a terminal or Python shell, do:</p> <pre><code>pip install --user [package name]\n</code></pre> <p>where <code>[package name]</code> is the name of a Python package, for example <code>pip install --user mhcnuggets</code>.</p> Can I also use <code>pip3</code>? <p>Yes, you can. The command then becomes:</p> <pre><code>pip3 install --user [package name]\n</code></pre> <p>For example <code>pip3 install --user mhcnuggets</code>.</p> <p>Most that applies to <code>pip</code> applies to <code>pip3</code>.</p> <p>Due to using <code>--user</code>, the package ends up in a subfolder of the user's home folder, which is <code>~/.local/lib/python[version]/site-packages/</code>, where version is the Python version with only the major and minor version, so for Python version 3.11.8, the folder will be <code>python3.11</code> (i.e. the patch number, <code>8</code> is not included).</p> <p>If you would like to have your packages installed in another folder, do:</p> <pre><code>pip install --prefix=[root_folder] [package name]\n</code></pre> <p>where <code>[root_folder]</code> is the root folder of the package installation, for example <code>--prefix=~/.local</code>. Using this root folder, this option is the same as using <code>--user</code>, as described above.</p> <p>When using a custom root folder, Python cannot find it without help. Setting the environment variable <code>PYTHONPATH</code> to the correct folder allows Python to find packages in a custom folder.</p> <pre><code>export PYTHONPATH=[root_folder]/lib/python[version]/site-packages/:$PYTHONPATH.\n</code></pre> <p>for example, when <code>[root_folder]</code> is <code>~/my_python_packages</code> and for using Python 3.11.8, this will be:</p> <pre><code>export PYTHONPATH=~/my_python_packages/lib/python3.11/site-packages/:$PYTHONPATH.\n</code></pre> <p>Consider adding this line to your <code>.bashrc</code> file, so that it is loaded every time you login.</p>","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#conda","title":"<code>conda</code>","text":"<p>See our Conda user Guide</p>","tags":["pip","pip install","Python","package"]},{"location":"software/python_install_packages/#using-setuppy","title":"Using <code>setup.py</code>","text":"<p>Some Python packages are only available as downloads and need to be installed using a Python script, commonly called <code>setup.py</code>.</p> <p>If that is the case for the package you need, this is how you do it:</p> <ul> <li> <p>Pick a location for your installation   (change below to fit - I am installing under a project storage)</p> <ul> <li><code>mkdir /proj/&lt;project&gt;/&lt;mystorage&gt;/mypythonpackages</code></li> <li><code>cd /proj/&lt;project&gt;/&lt;mystorage&gt;/mypythonpackages</code></li> </ul> </li> <li> <p>Load Python + (on Kebnekaise) site-installed prerequisites (SciPy-bundle, matplotlib, etc.)</p> </li> <li>Install any remaining prerequisites. Remember to activate your Virtualenv if installing with pip!</li> <li>Download Python package, place it in your chosen installation dir, then untar/unzip it</li> <li> <p>cd into the source directory of the Python package</p> <ul> <li>Run <code>python setup.py build</code></li> <li>Then install with: <code>python setup.py install --prefix=&lt;path to install dir&gt;</code></li> </ul> </li> <li> <p>Add the path to $HOME/.bash_profile (note that it will differ by Python version):</p> <ul> <li><code>export PYTHONPATH=$PYTHONPATH:&lt;path to your install directory&gt;/lib/python3.11/site-packages</code></li> </ul> </li> </ul> <p>You can use it as normal inside Python (remember to load dependent modules as well as activate virtual environment if it depends on some packages you installed with pip): <code>import &lt;python-module&gt;</code></p>","tags":["pip","pip install","Python","package"]},{"location":"software/python_parallel_jobs/","title":"How to run parallel jobs in Python","text":"","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#how-to-run-parallel-jobs-in-python","title":"How to run parallel jobs in Python","text":"<p>This page describes how to run parallel jobs in Python.</p> <p>Material here is taken partly from the parallel part of the online course Python for Scientific Computing</p> <p>Parallel computing is when many different tasks are carried out simultaneously. There are three main models:</p> <ul> <li> <p>Embarrassingly parallel: the code does not need to synchronize/communicate with other instances, and you can run multiple instances of the code separately, and combine the results later. If you can do this, great! (array jobs, task queues)</p> </li> <li> <p>Shared memory parallelism: Parallel threads need to communicate and do so via the same memory (variables, state, etc). (OpenMP)</p> </li> <li> <p>Message passing: Different processes manage their own memory segments. They share data by communicating (passing messages) as needed. (Message Passing Interface (MPI)).</p> </li> </ul> <p>There are several packages available for Python that let you run parallel jobs. Some of them are only able to run on one node, while others try to leverage several machines.</p>","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#threading","title":"Threading","text":"<p>Threading divides up your work among a number of cores within a node. The threads shares its memory.</p> <ul> <li>Multi-threading documentation</li> <li>Examples</li> </ul> <p>The designers of the Python language made the choice that only one thread in a process can run actual Python code by using the so-called global interpreter lock (GIL). This means that approaches that may work in other languages (C, C++, Fortran), may not work in Python without being a bit careful. At first glance, this is bad for parallelism. But it\u2019s not all bad!:</p> <p>External libraries (NumPy, SciPy, Pandas, etc), written in C or other languages, can release the lock and run multi-threaded. Also, most input/output releases the GIL, and input/output is slow.</p> <p>If speed is important enough you need things parallel, you usually wouldn\u2019t use pure Python.</p> <p>More on the global interpreter lock</p> <p>Threading python module. This is very low level and you shouldn\u2019t use it unless you really know what you are doing.</p> <p>We recommend you find a UNIX threading tutorial first before embarking on using the threading module.</p>","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#distributed-computing","title":"Distributed computing","text":"<p>As opposed to threading, Python has a reasonable way of doing something similar that uses multiple processes.</p> <p>Distributed processing uses individual processes with individual memory, that communicate with each other. In this case, data movement and communication is explicit. Python supports various forms of distributed computing.</p> <ul> <li>A native master-worker system based on remote procedure calls: multiprocessing.py</li> <li>MPI through mpi4py : a Python wrapper for the MPI protocol, see further down</li> </ul> <p>If choosing between multiprocessing and MPI, distributed is easier to program, whereas MPI may be more suitable for multi-node applications.</p>","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#multiprocessingdistributed","title":"Multiprocessing/distributed","text":"<p>The interface is a lot like threading, but in the background creates new processes to get around the global interpreter lock.</p> <p>There are low-level functions which have a lot of the same risks and difficulties as when using threading.</p> <p>To show an example, the split-apply-combine or map-reduce paradigm is quite useful for many scientific workflows. Consider you have this:</p> <pre><code>def square(x):\n    return x*x\n</code></pre> <p>You can apply the function to every element in a list using the map() function:</p> <pre><code>&gt;&gt;&gt;list(map(square, [1, 2, 3, 4, 5, 6]))\n[1, 4, 9, 16, 25, 36]\n</code></pre> <p>The multiprocessing.pool.Pool class provides an equivalent but parallelized (via multiprocessing) way of doing this. The pool class, by default, creates one new process per CPU and does parallel calculations on the list:</p> <pre><code>&gt;&gt;&gt;from multiprocessing import Pool\n&gt;&gt;&gt;with Pool() as pool:\n...    pool.map(square, [1, 2, 3, 4, 5, 6])\n[1, 4, 9, 16, 25, 36]\n</code></pre> <p>As you can see, you can run distributed computing directly from the python shell.</p> <p>Another example, <code>distributed.py</code>:</p> <pre><code>import random\n\ndef sample(n):\n    \"\"\"Make n trials of points in the square.\n    Return (n, number_in_circle)\n    This is our basic function.\n    By design, it returns everything it needs to compute\n    the final answer: both n (even though it is an input\n    argument) and n_inside_circle.\n    To compute our final answer, all we have to do is\n    sum up the n:s and the n_inside_circle:s and do our\n    computation\"\"\"\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x**2 + y**2 &lt; 1.0:\n            n_inside_circle += 1\n    return n, n_inside_circle\n\nimport multiprocessing.pool\npool = multiprocessing.pool.Pool()\n# The default pool makes one process per CPU\n#%%timeit\n# Do it once to time it\n#results = pool.map(sample, [10**5] * 10)     # \"* 10\" would mean 10 processes\n# Do it again to get the results, since the results of the above\n# cell aren't accessible because of the %%timeit magic.\nresults = pool.map(sample, [10**5] * 10)\npool.close()\nn_sum = sum(x[0] for x in results)\nn_inside_circle_sum = sum(x[1] for x in results)\npi = 4.0 * (n_inside_circle_sum / n_sum)\nprint(pi)\n</code></pre>","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#batch-example","title":"Batch example","text":"<p>If you need to revive your knowledge about the scheduling system, please check Slurm user guide.</p> <p>Batch script job_distributed.slurm:</p> <pre><code>#!/bin/bash\n#SBATCH -A j&lt;proj&gt;\n#SBATCH -p devel\n#SBATCH --job-name=distr_py      # create a short name for your job\n#SBATCH --nodes=1                # node count\n#SBATCH --ntasks=20              # total number of tasks across all nodes\n#SBATCH --cpus-per-task=1        # cpu-cores per task (&gt;1 if multi-threaded tasks)\n#SBATCH --time=00:01:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n#SBATCH --mail-user=&lt;email&gt;\nmodule load python/3.9.5\npython distributed.py\n</code></pre> <p>\u200bPut job in queue:</p> <pre><code>sbatch job_distributed.slurm\n</code></pre>","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#interactive-example","title":"Interactive example","text":"<pre><code>salloc -A &lt;proj&gt; -p node -N 1 -n 10 -t 1:0:0\npython distributed.py\n</code></pre>","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#mpi","title":"MPI","text":"<p>Presently you have to install your own mpi4py. You will need to activate paths to the MPI libraries. Therefore follow these steps.</p> <ol> <li>If you use python 3.10.8:</li> </ol> <pre><code>module load gcc/12.2.0 openmpi/4.1.4\n</code></pre> <pre><code> Otherwise:\n</code></pre> <pre><code>module load gcc/9.3.0 openmpi/3.1.5\n</code></pre> <ol> <li>pip install locally or in an virtual environment</li> </ol> <pre><code>pip install --user mpi4py\n</code></pre> <p>Remember that you will also have to load the the openmpi module before running mpi4py code, so that the MPI header files can be found (e.g. with the command \"module load gcc/X.X.X openmpi/X.X.X\"). Because of how MPI works, we need to explicitly write our code into a file,  pythonMPI.py:</p> <pre><code>import random\nimport time\nfrom mpi4py import MPI\ndef sample(n):\n    \"\"\"Make n trials of points in the square.\n    Return (n, number_in_circle)\n    This is our basic function.\n    By design, it returns everything it needs to compute\n    the final answer: both n (even though it is an input\n    argument) and n_inside_circle.\n    To compute our final answer, all we have to do is\n    sum up the n:s and the n_inside_circle:s and do our\n    computation\"\"\"\n    n_inside_circle = 0\n    for i in range(n):\n        x = random.random()\n        y = random.random()\n        if x ** 2 + y ** 2 &lt; 1.0:\n            n_inside_circle += 1\n    return n, n_inside_circle\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\nn = 10 ** 7\nif size &gt; 1:\n    n_task = int(n / size)\nelse:\n    n_task = n\nt0 = time.perf_counter()\n_, n_inside_circle = sample(n_task)\nt = time.perf_counter() - t0\n\nprint(f\"before gather: rank {rank}, n_inside_circle: {n_inside_circle}\")\nn_inside_circle = comm.gather(n_inside_circle, root=0)\nprint(f\"after gather: rank {rank}, n_inside_circle: {n_inside_circle}\")\nif rank == 0:\n    pi_estimate = 4.0 * sum(n_inside_circle) / n\n    print(f\"\\nnumber of darts: {n}, estimate: {pi_estimate},\n        time spent: {t:.2} seconds\")\n</code></pre> <p>You can execute your code the normal way as</p> <pre><code>mpirun -n 3 python pythonMPI.py\n</code></pre> <p>A batch script, job_MPI.slurm, should include a \"module load gcc/9.3.0 openmpi/3.1.5\"</p> <pre><code>#!/bin/bash\n#SBATCH -A j&lt;proj&gt;\n#SBATCH -p devel\n#SBATCH --job-name=MPI_py        # create a short name for your job\n#SBATCH --nodes=1                # node count\n#SBATCH --ntasks=20              # total number of tasks across all nodes\n#SBATCH --cpus-per-task=1        # cpu-cores per task (&gt;1 if multi-threaded tasks)\n#SBATCH --time=00:05:00          # total run time limit (HH:MM:SS)\n#SBATCH --mail-type=begin        # send email when job begins\n#SBATCH --mail-type=end          # send email when job ends\n#SBATCH --mail-user=&lt;email&gt;\nmodule load python/3.9.5\nmodule load gcc/9.3.0 openmpi/3.1.5\nmpirun -n 20 python pythonMPI.py\n</code></pre>","tags":["Python","parallel"]},{"location":"software/python_parallel_jobs/#using-the-gpu-nodes","title":"Using the GPU nodes","text":"<p>Example with numba. First install numba locally:</p> <pre><code>pip install --user numba\n</code></pre> <p>Test script: add-list.py</p> <pre><code>import numpy as np\nfrom timeit import default_timer as timer\nfrom numba import vectorize\n# This should be a substantially high value.\nNUM_ELEMENTS = 100000000\n# This is the CPU version.\ndef vector_add_cpu(a, b):\n  c = np.zeros(NUM_ELEMENTS, dtype=np.float32)\n  for i in range(NUM_ELEMENTS):\n      c[i] = a[i] + b[i]\n  return c\n# This is the GPU version. Note the @vectorize decorator. This tells\n# numba to turn this into a GPU vectorized function.\n@vectorize([\"float32(float32, float32)\"], target='cuda')\ndef vector_add_gpu(a, b):\n  return a + b;\ndef main():\n  a_source = np.ones(NUM_ELEMENTS, dtype=np.float32)\n  b_source = np.ones(NUM_ELEMENTS, dtype=np.float32)\n  # Time the CPU function\n  start = timer()\n  vector_add_cpu(a_source, b_source)&lt;\n  vector_add_cpu_time = timer() - start\n  # Time the GPU function\n  start = timer()\n  vector_add_gpu(a_source, b_source)\n  vector_add_gpu_time = timer() - start\n  # Report times\n  print(\"CPU function took %f seconds.\" % vector_add_cpu_time)\n  print(\"GPU function took %f seconds.\" % vector_add_gpu_time)\n  return 0\nif __name__ == \"__main__\":\n  main()\n</code></pre> <p>Run in an interactive session with GPU:s on Snowy</p> <pre><code>[bjornc@rackham3 ~]$ interactive -A staff -n 1 -M snowy --gres=gpu:1  -t 1:00:01 --mail-type=BEGIN --mail-user=bjorn.claremar@uppmax.uu.se\nYou receive the high interactive priority.\nPlease, use no more than 8 GB of RAM.\nWaiting for job 6907137 to start...\nStarting job now -- you waited for 90 seconds.\n[bjornc@s160 ~]$ ml python/3.9.5\n[bjornc@s160 ~]$ python add-list.py  #run the script\nCPU function took 36.849201 seconds.\nGPU function took 1.574953 seconds.\n</code></pre>","tags":["Python","parallel"]},{"location":"software/python_programming/","title":"Python programming","text":""},{"location":"software/python_programming/#python-programming","title":"Python programming","text":"<p>This page describes how to program in Python on the UPPMAX clusters.</p> <p>There are multiple ways to program in Python:</p> Description Features Screenshot Use a text editor (see below) Non-interactive, no help Use the Python interpreter (see below) Interactive, terminal-based, some help Use IPython Interactive, terminal-based, more help and features Use Jupyter Interactive, web-based Use Visual Studio Code Interactive, install on local computer, use locally installed Python and Python packages"},{"location":"software/python_programming/#use-a-text-editor","title":"Use a text editor","text":"<p>Using a text editor to program in Python is a simple way to write code: it is the same as writing any text file.</p> <p>Here we use the text editor GNU <code>nano</code> to write a Python script:</p> <pre><code>nano example_script.py\n</code></pre> <p>Within nano, write:</p> <pre><code>print('Hello, world!')\n</code></pre> <ul> <li>To save, press <code>CTRL + O</code> (i.e. the letter), then enter to keep the same filename</li> <li>To quite, press <code>CTRL + Q</code></li> </ul> <p>You can run this Python script in the shell by:</p> <pre><code>python example_script.py\n</code></pre> <p>or, if you want to be explicitly use Python 3:</p> <pre><code>python3 example_script.py\n</code></pre> <p>Some features of this approach are:</p> <ul> <li>this is a simple way to write code: it is the same as writing any text file.</li> <li>you get no help while writing code</li> <li>you can only run the script from start to finish, i.e. you cannot   partially run the script</li> </ul> How to run a Python script line-by-line? <p>You can run a Python script line-by-line using a Python debugger, such as <code>pdb</code>.</p> <p>On the terminal, for <code>python</code>, do:</p> <pre><code>pdb example_script.py\n</code></pre> <p>or for <code>python3</code>:</p> <pre><code>pdb3 example_script.py\n</code></pre> <p>See the official Python documentation of <code>pdb</code>.</p>"},{"location":"software/python_programming/#use-the-python-interpreter","title":"Use the Python interpreter","text":"<p>After loading a Python module, you have the Python interpreter available.</p> Forgot how to load a Python module? <p>See the UPPMAX page about Python.</p> What is a Python interpreter? <p>In computing, an interpreter is a program that reads text and runs it directly, without any additional steps.</p> <p>The Python interpreter runs the Python commands you type directly, without any additional steps.</p> <p>Start the Python interpreter by typing:</p> <pre><code>python\n</code></pre> <p>or (for explicit Python 3):</p> <pre><code>python3\n</code></pre> <p>The Python prompt looks like this:</p> <pre><code>&gt;&gt;&gt;\n</code></pre> <p>Type, for example:</p> <pre><code>print('Hello, world!')\n</code></pre> <p>and the interpreter will run the statement.</p> <p>Exit the Python interpreter with <code>CTRL + D</code>, <code>quit()</code> or <code>exit()</code>.</p> <p>The Python interpreter gives limited auto-complete while writing code</p> How do I get auto-complete? <p>As an example, writing this line of code in the Python interpreter ...</p> <pre><code>s = 'Hello, world!'\n</code></pre> <p>... and press enter. Now a variable called <code>s</code> will hold some text.</p> <p>Now type ...</p> <pre><code>s.\n</code></pre> <p>and press Tab twice. You will see a list of things you can do with that string.</p> <p>The Python interpreter can show graphics.</p> How do I get the Python interpreter to show graphics? <p>In the Python interpreter, run this code line-by-line:</p> <pre><code>import matplotlib.pyplot as plt\nplt.plot([1, 4, 9, 16])\nplt.show()\n</code></pre> <p>(or as a one-liner: <code>import matplotlib.pyplot as plt; plt.plot([1, 4, 9, 16]); plt.show()</code>)</p> <p>You will see a window appear:</p> <p></p> <p>You will only see a window appear, if you've logged in to Rackham with SSH with X forwarding enabled.</p> <p>Spoiler: <code>ssh -X sven@rackham.uppmax.uu.se</code>.</p> <p>The Python interpreter cannot directly run scripts.</p>"},{"location":"software/python_programming/#links","title":"Links","text":"<ul> <li>Official Python documentation</li> <li>Python forum</li> <li>Free online book: 'How to Think Like a Computer Scientist'</li> <li>UPPMAX TensorFlow guide</li> <li>UPPMAX PyTorch guide</li> </ul>"},{"location":"software/python_pyenv/","title":"Python pyenv","text":"","tags":["Python","pyenv"]},{"location":"software/python_pyenv/#python-pyenv","title":"Python <code>pyenv</code>","text":"<p><code>pyenv</code> is one of multiple Python virtual environment managers.</p> <p>This approach is more advanced and should be, in our opinion, used only if the above are not enough for the purpose. Probably Conda will work well for you. The approach below allows you to install your own python version and much more\u2026</p> <p>Confer the official pyenv documentation.</p>","tags":["Python","pyenv"]},{"location":"software/python_pyenv/#first-time-at-uppmax","title":"First time at UPPMAX","text":"<ol> <li> <p>Download pyenv:</p> <pre><code>git clone git://github.com/yyuu/pyenv.git ~/.pyenv\n</code></pre> </li> <li> <p>Make pyenv start when you login each time</p> </li> </ol> <pre><code>echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bash_profile\necho 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bash_profile\necho 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bash_profile\n</code></pre> <p>To make sure everything gets loaded correctly, log out and back in to uppmax.</p>","tags":["Python","pyenv"]},{"location":"software/python_pyenv/#installing-own-python-version-not-already-available-as-an-uppmax-module","title":"Installing own python version (not already available as an UPPMAX module)","text":"<ol> <li> <p>Get pyenv to install the python version of your liking.</p> <pre><code>pyenv install 3.10.6\n</code></pre> </li> <li> <p>Make the version you just installed to the standard version for every time you run python.</p> <pre><code>pyenv global 3.10.6\n</code></pre> </li> </ol> <p>Now you should be all set. If you change your mind about which version of Python to use, just redo this section and choose a different version. You can also have multiple versions installed at the same time and just switch between them usuing 'pyenv global' as shown above, if you have a script that requires Python 3.3 or any other version.</p>","tags":["Python","pyenv"]},{"location":"software/python_pyenv/#install-packages-in-your-selected-python-version","title":"Install packages in your selected python version","text":"<ol> <li> <p>Set python version with</p> <pre><code>pyenv global &lt;version&gt;\n</code></pre> </li> <li> <p>Install packages in your python, use <code>pip</code></p> <pre><code>pip install [package name]\n</code></pre> </li> </ol> <p>Example:</p> <pre><code>pip install mechanize\n</code></pre>","tags":["Python","pyenv"]},{"location":"software/python_pyenv/#links","title":"Links","text":"<ul> <li>CodeRefinery's course: Python for Scientific Computing.</li> </ul>","tags":["Python","pyenv"]},{"location":"software/python_venv/","title":"Python venv","text":""},{"location":"software/python_venv/#python-venv","title":"Python <code>venv</code>","text":"<p><code>venv</code> is one of multiple Python virtual environment managers.</p> <p><code>venv</code> is a Python-only environment manager and is an official Python library, with its own official Python tutorial.</p> <pre><code>flowchart TD\n  create[Create]\n  activate[Activate]\n  use[Use]\n  deactivate[Deactivate]\n\n  create --&gt; activate\n  activate --&gt; use\n  use --&gt; deactivate\n  deactivate --&gt; activate</code></pre> <p>The <code>venv</code> workflow</p> <p>First, the common workflow for using a <code>venv</code> is described:</p> <ul> <li>how to create a virtual environment</li> <li>how to activate a virtual environment</li> <li>how to deactivate a virtual environment</li> </ul> <p>Then:</p> <ul> <li>how to export and import a virtual environment</li> </ul>"},{"location":"software/python_venv/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>A virtual environment can be created in multiple ways, for example, from scratch, which is not recommended.</p> <p>Here we discuss the recommended way to create a virtual environment, which has these steps:</p> <ol> <li>Load a Python module or a modules with Python packages</li> <li>Create the virtual environment</li> </ol>"},{"location":"software/python_venv/#1-load-a-python-module-or-a-modules-with-python-packages","title":"1. Load a Python module or a modules with Python packages","text":"<p>The first step is described at 'Loading Python' and 'Loading Python package modules'.</p> Just show me how to do this <p>Sure, here is how to load a Python module:</p> <pre><code>module load python/3.11.8\n</code></pre> <p>Here is how to load a Python package module:</p> <pre><code>module load python_ML_packages/3.11.8-cpu\n</code></pre> <p>Because you can load Python modules of different Python versions, you can create <code>venv</code> virtual environments with different Python versions. Consider adding this in the <code>venv</code> name, e.g. <code>my_python2_venv</code> or <code>my_python3_venv</code>.</p>"},{"location":"software/python_venv/#2-create-the-virtual-environment","title":"2. Create the virtual environment","text":"<p>After loading the needed Python modules, one can create a virtual environment most efficiently using:</p> <pre><code>python -m venv --system-site-packages [path]/[venv_name]\n</code></pre> <p>where <code>[path]</code> is the path where you want to create your <code>venv</code> virtual environment and <code>[venv_name]</code> is the name of the <code>venv</code> virtual environment. For example <code>python -m venv --system-site-packages ~/my_venvs/example_venv</code>.</p> <p>Create virtual environments in your project storage</p> <p>Virtual environments can take up a lot of disc space.</p> <p>If you use either (1) many <code>venv</code> virtual environments, or (2) install many Python packages to a <code>venv</code> virtual environment, we strongly recommend that you create the <code>venv</code> virtual environments in your project (<code>/proj/[your_uppmax_project]</code>) folder.</p> <p>The <code>-m</code> flag makes sure that you use the libraries from the Python version you are using. The <code>--system-site-packages</code> flags ensure you use the packages already installed in the loaded Python module.</p> How long does this step take? <p>This depends.</p> <p>This takes around 10 seconds:</p> <pre><code>module load python/3.11.8\npython -m venv --system-site-packages ~/my_venvs/example_venv\n</code></pre> <p>This takes around 10 seconds:</p> <pre><code>module load python_ML_packages/3.11.8-cpu\npython -m venv --system-site-packages ~/my_venvs/example_ml_venv\n</code></pre>"},{"location":"software/python_venv/#activate-a-virtual-environment","title":"Activate a virtual environment","text":"<p>To activate your newly created virtual environment locate the script called <code>activate</code> and execute it:</p> <pre><code>source [path]/[venv_name]/bin/activate\n</code></pre> <p>where <code>[path]</code> is the path where you want to create your <code>venv</code> virtual environment and <code>[venv_name]</code> is the name of the <code>venv</code> virtual environment. For example <code>source ~/my_venvs/example_venv/bin/activate</code>.</p> <p>When a <code>venv</code> virtual environment is active, the prompt is changed to start with the name of your <code>venv</code>.</p> How does that look like? <p>This is how your changed prompt looks like:</p> <pre><code>[sven@rackham1 ~]$ module load python_ML_packages/3.11.8-cpu\n[sven@rackham1 ~]$ python -m venv --system-site-packages ~/my_venvs/example_venv\n[sven@rackham1 ~]$ source ~/my_venvs/example_venv/bin/activate\n(example_venv) [sven@rackham1 ~]$\n</code></pre> <p>With the <code>venv</code> virtual environment active, you can now install and update Python packages in an isolated way.</p>"},{"location":"software/python_venv/#deactivate-a-virtual-environment","title":"Deactivate a virtual environment","text":"<p>To deactivate a <code>venv</code> virtual environment:</p> <pre><code>deactivate\n</code></pre> <p>As the <code>venv</code> virtual environment you just used is now inactive, the prompt will not show the name of your <code>venv</code> anymore.</p> <p>You will need to activate a virtual environment to work with it again.</p>"},{"location":"software/python_venv/#export-and-import-a-virtual-environment","title":"Export and import a virtual environment","text":""},{"location":"software/python_venv/#export","title":"Export","text":"<p>To export the Python packages used in your virtual environment, do:</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>This will create a file with all the Python packages and their versions, using the conventional name for such a file.</p> How does that file look like? <p>This is how a <code>requirements.txt</code> file may look like:</p> <pre><code>anndata==0.10.5.post1\nanyio==4.2.0\nappdirs==1.4.4\nargon2-cffi==23.1.0\nargon2-cffi-bindings==21.2.0\n[more Python packages]\nwebsocket-client==1.7.0\nWerkzeug==3.0.1\nwidgetsnbextension==4.0.9\nzipp==3.17.0\nzope.interface==6.1\n</code></pre> <p>Note that <code>[more Python packages]</code> is a placeholder for many more Python packages.</p>"},{"location":"software/python_venv/#import","title":"Import","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"software/python_virtual_environments/","title":"Virtual environments in Python","text":""},{"location":"software/python_virtual_environments/#virtual-environments-in-python","title":"Virtual environments in Python","text":"<p>This page described how to use virtual environments in Python.</p>"},{"location":"software/python_virtual_environments/#why-use-virtual-environments","title":"Why use virtual environments?","text":"<p>Virtual environments allows one to have independent Python environments.</p> <p>This allows one to have multiple projects</p> <ul> <li>You can install specific, also older, versions into them</li> <li>You can create one for each project and no problem if the two projects   require different versions</li> <li>If you make some mistake and install something you did not want or need, you   can remove the environment and create a new one</li> </ul>"},{"location":"software/python_virtual_environments/#environment-managers","title":"Environment managers","text":"<p>Here is an incomplete overview of virtual environment managers that work with Python:</p> Virtual environment manager Description <code>venv</code> Works on Rackham <code>virtualenv</code> <code>venv</code> for older Python versions <code>conda</code> Works on Rackham, recommended on Bianca <code>pyenv</code> More advanced than <code>venv</code>"},{"location":"software/python_virtual_environments/#general-virtual-environment-manager-workflow","title":"General virtual environment manager workflow","text":"<pre><code>flowchart TD\n  create[Create]\n  activate[Activate]\n  use[Use]\n  deactivate[Deactivate]\n\n  create --&gt; activate\n  activate --&gt; use\n  use --&gt; deactivate\n  deactivate --&gt; activate</code></pre> <p>Whatever virtual environment manager you use, this is the workflow:</p> <ul> <li>You create the isolated environment</li> <li>You activate the environment</li> <li>You work in the isolated environment.   Here you install (or update) the environment with the packages you need</li> <li>You deactivate the environment after use</li> </ul> <p>A virtual environment can be created in multiple ways, for example, from scratch. However, there are more efficient ways, such as by re-using already installed Python packages. How to do so, can be found on the page about your specific virtual environment manager.</p>"},{"location":"software/python_virtualenv/","title":"Python virtualenv","text":""},{"location":"software/python_virtualenv/#python-virtualenv","title":"Python <code>virtualenv</code>","text":"<p><code>virtualenv</code> is one of multiple Python virtual environment managers.</p> <p>Here we show the differences between <code>venv</code> and <code>virtualenv</code></p> Parameter <code>venv</code> <code>virtualenv</code> Supports which Python versions? Newer Older Is standard library? Yes No <p>Also, <code>virtualenv</code> has a few more minor unique features.</p> <p>Because these two are so similar, most information is documented at <code>venv</code>.</p>"},{"location":"software/pytorch/","title":"PyTorch","text":""},{"location":"software/qiime2/","title":"qiime2","text":""},{"location":"software/qiime2/#qiime2","title":"qiime2","text":"<p>qiime2 is a tool.</p> <p>qiime2 can be found among the UPPMAX modules.</p> <pre><code>module spider qiime2\n</code></pre> How does that look like? <p>You output will look similar to this:</p> <pre><code>[sven@rackham3 ~]$ module spider qiime2\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  qiime2:\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        qiime2/2018.11.0\n        qiime2/2024.2\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  For detailed information about a specific \"qiime2\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider qiime2/2024.2\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>To find out how to load a specific version:</p> <pre><code>module spider qiime2/1.22.2\n</code></pre> How does that look like? <p>Output will look similar to:</p> <pre><code>[sven@rackham3 ~]$ module spider qiime2/2024.2\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nqiime2: qiime2/2024.2\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"qiime2/2024.2\" module is available to load.\n\n      bioinfo-tools\n\n    Help:\n      qiime2 - use qiime2\n\n      Description\n\n      Version 2024.2\n\n      https://qiime2.org\n\n      The version installed is 2024.2 amplicon, slightly modified from the publicly available docker image.\n\n\n         qiime ...\n\n\n      You may see a message like\n\n          Matplotlib created a temporary config/cache directory at /scratch/matplotlib-a10b2an0 because the default path (/home/qiime2/matplotlib) is not a writable directory...\n\n      This is because qiime2 is running within an Apptainer container. This message can be ignored.\n</code></pre> <p>After reading that documentation, we know how to load it:</p> <pre><code>module load bioinfo-tools\nmodule load qiime2/2024.2\n</code></pre> How does that look like? <p>Your output will look similar to this:</p> <pre><code>[sven@rackham3 ~]$ module load bioinfo-tools\n[sven@rackham3 ~]$ module load qiime2/2024.2\n[sven@rackham3 ~]$\n</code></pre>"},{"location":"software/qiime2/#singularity-script","title":"Singularity script","text":"<p>If you want to put qiime2 in a Singularity container, here is an example script:</p> <pre><code>BootStrap: library\nFrom: centos:7\n\n%runscript\n  . /miniconda/etc/profile.d/conda.sh\n  PATH=$PATH:/miniconda/bin\n  conda activate qiime2-2019.7\n  qiime \"$@\"\n\n%post\n  yum clean all\n  yum -y update\n  yum -y install wget python-devel\n  cd /tmp\n  wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh\n  bash ./Miniconda2-latest-Linux-x86_64.sh -b -p /miniconda\n  /miniconda/bin/conda update -y conda\n  wget https://data.qiime2.org/distro/core/qiime2-2019.7-py36-linux-conda.yml\n  /miniconda/bin/conda env create -n qiime2-2019.7 --file qiime2-2019.7-py36-linux-conda.yml\n  # OPTIONAL CLEANUP\n  rm qiime2-2019.7-py36-linux-conda.yml\n  /miniconda/bin/conda clean -a\n</code></pre> <p>See the documentation on Singularity how to do so.</p>"},{"location":"software/r/","title":"R","text":"","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r","title":"R","text":"<p>R is a programming language for statistical computing and data visualization (from Wikipedia).</p> <p>Here we discuss:</p> <ul> <li>the R programming language</li> <li>the R interpreter</li> <li>R packages</li> <li>R software development</li> <li>How to install personal packages</li> <li>How to create a Singularity container for an R package</li> </ul> <pre><code>flowchart TD\n\n    subgraph r[R]\n      r_interpreter[the R interpreter]\n      r_packages[R packages]\n      r_language[the R programming language]\n      r_dev[R software development]\n      rstudio[RStudio]\n\n      interpreted_language[Interpreted]\n      cran[CRAN]\n    end\n\n    subgraph uppmax_modules[UPPMAX modules]\n      r_module[R]\n      r_packages_module[R_packages]\n      rstudio_module[RStudio]\n    end\n\n\n    r_language --&gt; |has| r_dev\n    r_language --&gt; |is| interpreted_language\n    r_language --&gt; |uses| r_packages\n    interpreted_language --&gt; |done by| r_interpreter\n    r_packages --&gt; |maintained by| cran\n    r_dev --&gt; |commonly done in| rstudio\n\n    r_interpreter --&gt; r_module\n    r_packages --&gt; r_packages_module\n    rstudio --&gt; rstudio_module\n\n    rstudio_module --&gt; |automatically loads latest| r_packages_module\n    r_packages_module --&gt; |automatically loads corresponding version of| r_module</code></pre>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#the-r-programming-language","title":"the R programming language","text":"<p>R is 'a programming language for statistical computing and data visualization') and is of the most commonly used programming languages in data mining, analysis and visualization.</p> <p>R is an interpreted language; users can access it through the R interpreter.</p> <p>R is a dynamically typed programming language with basic built-in data structures are (among others): vectors, arrays, lists, and data frames. and its supports both procedural programming and object-oriented programming.</p> <p>R has many user-created R packages to augment the functions of the R language, most commonly hosted on CRAN. These packages offer statistical techniques, graphical devices, import/export, reporting (RMarkdown, knitr, Sweave), etc.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#the-r-interpreter","title":"the R interpreter","text":"<p>The R interpreter is the program that reads R code and runs it. Commonly, 'the programming language R' and 'the R interpreter' are use as synonyms.</p> <p>To load the latest version of the R interpreter, load the <code>R</code> module version 4.3.1 like this:</p> <pre><code>module load R/4.3.1\n</code></pre> Do I really need to load an R module? <p>We strongly recommend loading an R module.</p> <p>If you do not load an R module, you will be using the version of R used by the UPPMAX systems.</p> <p>Sometimes that may work.</p> <p>If not, load an R module.</p> Need a different version? <p>If you need a different R version, use the following command to see which versions of the R interpreter are installed on UPPMAX:</p> <pre><code>module spider R\n</code></pre> <p>Then start the R interpreter with:</p> <pre><code>R\n</code></pre>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r-packages","title":"R packages","text":"<p>R packages extend what R can do. The most common repository for R packages is CRAN. As these packages are so common, UPPMAX provides most of CRAN packages in one module, called <code>R_packages</code></p> <p>To load the latest version of the pre-installed R packages, do:</p> <pre><code>module load R_packages/4.3.1\n</code></pre> <p>This will automatically load the corresponding version of the R interpreter.</p> Do I really need to load the <code>R_packages</code> module? <p>We strongly recommend loading the <code>R_packages</code> module.</p> <p>If you do not load the <code>R_packages</code> module (nor the <code>R</code> module), you will be using the version of R used by the UPPMAX systems.</p> <p>Sometimes that may work.</p> <p>If not, load the <code>R_packages</code> module.</p> Need a different version? <p>If you need a different package version, use the following command to see which versions of the R packages are installed on UPPMAX:</p> <pre><code>module spider R_packages\n</code></pre>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r-software-development","title":"R software development","text":"<p>RStudio in action on Bianca using the remote desktop environment</p> <p>Software development is commonly done in a so-called Integrated Development Environment, abbreviated 'IDE.</p> <p>RStudio is the most commonly used IDE for R software development. See the UPPMAX page about RStudio on how to use.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#how-to-install-personal-packages","title":"How to install personal packages","text":"<p>Installing R packages on Bianca</p> <p>If a package is unavailable on Bianca, one can:</p> <ul> <li>create a Singularity container for R packages</li> <li>install R packages following this procedure</li> </ul> <p>First load <code>R_packages</code> to make sure that the package is not already installed!</p> <p>To install personal packages in your own home directory you type</p> <pre><code>install.packages(\"package_name\")\n</code></pre> <p>as usual. That will install all your packages under the path <code>~/R/[arch]/[version of R]/</code>. Then you can load it by just doing <code>library(package_name)</code> or <code>require(package_name)</code> in the R environment.</p> <p>You can also specify a specific folder for where to put your packages, with</p> <pre><code>install.packages(\"package_name\", lib=\"~/some/path/under/your/home/directory/\")\n</code></pre> <p>But to then be able to find the package inside the R environment you need to either export the <code>R_LIBS_USER</code> environment variable, or specify the flag <code>lib.loc</code> when calling <code>require</code>/<code>library</code>, e.g.</p> <pre><code>library(package_name, lib.loc='~/some/path/under/your/home/directory')\n</code></pre> <p>Notice that if you are planning on running R on different clusters then it is probably wisest to manually specify the installation directory, and to have separate directories for each cluster. This is because some of the clusters have different architectures, and this will render some packages unusable if you compile them on one system but try to run them on the other.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#technicalities","title":"Technicalities","text":"<p>As of this writing, our most recent installations are</p> <ul> <li><code>R/4.3.1</code></li> <li><code>R_packages/4.3.1</code></li> <li><code>RStudio/2023.06.2-561</code></li> </ul> <p>If you need an older version, do module avail R or R_packages or RStudio to see older versions as well.</p> <p>Note that <code>R_packages/4.3.1</code> contains 23475 packages, nearly all packages available on CRAN and BioConductor, as well as several custom packages installed from Github and other repositories. See module help R_packages/4.3.1 and R_packages for more information.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#what-r-packages-are-in-the-omnibus-r_packages-modules","title":"What R packages are in the omnibus <code>R_packages</code> modules?","text":"","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages411","title":"R_PACKAGES/4.1.1","text":"<p>As of 2021-11-11 there are a total of 21659 R packages installed. A total of 21740 packages are available in CRAN and BioConductor. 18022 CRAN packages are installed, out of 18348 available. 3382 BioConductor-specific packages are installed, out of 3392 available. 255 other R packages are installed. These are not in CRAN/BioConductor, and instead are hosted on github or elsewhere.</p> <p>These R packages are available as part of the R_packages/4.1.1 module as installed on rackham, bianca and snowy, which requires and loads the R/4.1.1 module.  When the R_packages/4.1.1 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p> <ul> <li>To use some R packages from this module, other modules may need to be loaded. For example, to use the Rmpi package, the openmpi/3.1.5 module must be loaded after loading R_packages/4.0.4.</li> <li>See module help R_packages/4.1.1 for more information.</li> </ul>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages404","title":"R_PACKAGES/4.0.4","text":"<p>As of 2021-04-16 there are a total of 20663 CRAN and BioConductor packages installed, out of 20751 packages available. 17354 CRAN packages are installed, out of 17428 available. 3309 BioConductor-specific packages are installed, out of 3323 available.</p> <p>These R packages are available as part of the R_packages/4.0.4 module as installed on rackham, bianca and snowy, which requires and loads the R/4.0.4 module.  When the R_packages/4.0.4 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p> <ul> <li>To use some R packages from this module, other modules may need to be loaded. For example, to use the Rmpi package, the openmpi/3.1.5 module must be loaded after loading R_packages/4.0.4.</li> <li>See module help R_packages/4.0.4 for more information.</li> </ul>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages400","title":"R_PACKAGES/4.0.0","text":"<p>As of 2021-02-24 there are a total of 18652 CRAN and BioConductor packages installed, out of 20422 packages available. 14839 CRAN packages are installed, out of 17165 available. 3217 BioConductor-specific packages are installed, out of 3257 available.</p> <p>These R packages are available as part of the R_packages/4.0.0 module as installed on rackham, bianca and snowy, which requires and loads the R/4.0.0 module.  When the R_packages/4.0.0 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p> <p>See module help R_packages/4.0.0 for more information.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages361","title":"R_PACKAGES/3.6.1","text":"<p>As of 2019-09-18 there are a total of 17657 packages available in this module. This includes 14579 CRAN packages installed, out of 14913 available; and 3054 BioConductor-specific packages installed, out of 3079 available. These R packages are available as part of the R_packages/3.6.1 module as installed on rackham, bianca and snowy, which requires and loads the R/3.6.1 module.  When the R_packages/3.6.1 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p> <p>See module help R_packages/3.6.1 for more information.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages360","title":"R_PACKAGES/3.6.0","text":"<p>As of 2019-05-14 there are a total of 17257 packages available. This includes 13769 CRAN packages installed, out of 14178 available; and 3031 BioConductor-specific packages installed, out of 3079 available. These R packages are available as part of the R_packages/3.6.0 module as installed on rackham, bianca and snowy, which requires and loads the R/3.6.0 module.  When the R_packages/3.6.0 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p> <p>See module help R_packages/3.6.0 for more information.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages352","title":"R_PACKAGES/3.5.2","text":"<p>As of 2019-02-08 there are a total of 16642 packages available. This includes 13355 CRAN packages installed, out of 13683 available; and 2933 BioConductor-specific packages installed, out of 2959 available. These R packages are available as part of the R_packages/3.5.2 module as installed on rackham, bianca and snowy, which requires and loads the R/3.5.2 module.  When the R_packages/3.5.2 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p> <p>See module help R_packages/3.5.2 for more information.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages350","title":"R_PACKAGES/3.5.0","text":"<p>With its 3.5.0 version, R_packages now attempts to install all available R packages from both CRAN and BioConductor.</p> <p>As of 2018-06-26 there are a total of 14532 packages available. This includes 11734 CRAN packages installed, out of 12867 available; and 2798 BioConductor-specific packages installed, out of 2843 available. These R packages are available as part of the R_packages/3.5.0 module as installed on rackham, bianca and snowy, which requires and loads the R/3.5.0 module.  When the R_packages/3.5.0 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p> <p>See module help R_packages/3.5.0 for more information.</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages343","title":"R_packages/3.4.3","text":"<p>A large number of R packages are available as part of the R_packages/3.4.3 module as installed on rackham and bianca, which requires and loads the R/3.4.3 module.  When the R_packages/3.4.3 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages340","title":"R_packages/3.4.0","text":"<p>A large number of R packages are available as part of the R_packages/3.4.0 module, which requires and loads the R/3.4.0 module.  When the R_packages/3.4.0 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages332","title":"R_packages/3.3.2","text":"<p>A large number of R packages are available as part of the R_packages/3.3.2 module, which requires and loads the R/3.3.2 module.  When the R_packages/3.3.2 module is loaded, it adds a directory to the R_LIBS_SITE environment variable.  Within R, these packages will be available via library(package-name).</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages331","title":"R_packages/3.3.1","text":"<p>A large number of R packages are available as part of the R_packages/3.3.1 module, which requires and loads the R/3.3.1 module.  When the R_packages/3.3.1 module is loaded, it adds a directory to the R_LIBS_SITE environment variable. Within R, these should be available via library(package-name).</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#r_packages330","title":"R_packages/3.3.0","text":"<p>A large number of R packages are available as part of the R_packages/3.3.0 module, which requires and loads the R/3.3.0 module.  When the R_packages/3.3.0 module is loaded, it adds a directory to the R_LIBS_SITE environment variable. Within R, these should be available via library(package-name).</p>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#learning-r","title":"Learning R","text":"","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#starter-r-courses","title":"Starter R courses","text":"<p>The Carpentries teaches basic lab skills for research computing, such as:</p> <ul> <li>Programming with R</li> <li>R for reproducible scientific analysis</li> </ul>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#experienced-r-courses","title":"Experienced R courses","text":"<p>CodeRefinery develops and maintains training material on software best practices for researchers that already write code. Their material addresses all academic disciplines and tries to be as programming language-independent as possible:</p> <ul> <li>CodeRefinery lessons</li> </ul> <p>Aalto Scientific Computing:</p> <ul> <li>Data analysis workflows with R and Python</li> </ul>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#overview-of-naiss-centers-and-their-documentation-about-r","title":"Overview of NAISS centers and their documentation about R","text":"<ul> <li>C3SE</li> <li>HPC2N</li> <li>LUNARC</li> <li>NSC</li> <li>PDC</li> <li>UPPMAX</li> </ul>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r/#links","title":"Links","text":"<ul> <li>The home page of \"The R Project for Statistical Computing\"</li> <li>Official R documentation</li> <li>CRAN homepage</li> <li>CRAN mirrors</li> </ul>","tags":["R","software","language","interpreter","programming language"]},{"location":"software/r_packages_bianca/","title":"Installing R packages on Bianca","text":""},{"location":"software/r_packages_bianca/#installing-r-packages-on-bianca","title":"Installing R packages on Bianca","text":"Read through the content below Try to do the exercise"},{"location":"software/r_packages_bianca/#first-check-if-package-is-already-in-r_packagesxyz","title":"First check if package is already in R_packages/x.y.z","text":"<ul> <li>On UPPMAX the module <code>R_packages</code> is an omnibus package library containing almost all packages in the CRAN and BioConductor repositories.<ul> <li>As of 2023-11-21, there were a total of 23478  R packages installed in <code>R_packages/4.3.1</code>.<ul> <li>A total of 23603 packages are available in CRAN and BioConductor</li> <li>19586 CRAN packages are installed, out of 20044 available</li> <li>3544 BioConductor-specific packages are installed, out of 3559 available</li> <li>346 other R packages are installed. These are not in CRAN/BioConductor, are only available in the CRAN/BioConductor archives, or are hosted on github, gitlab or elsewhere</li> </ul> </li> </ul> </li> </ul> <p>Chances are good the R packages you need are already available once you load this module.  You can quickly check by loading it:</p> <p><code>$ ml R_packages/4.3.1</code></p> <p>Then within R, try loading the package you want:</p> <p><code>library(glmnet)</code></p> <p>Alternatively, and this is both a longer solution and not our recommended one, you can <code>grep</code> for the package after this module is loaded using the environment variable <code>$R_LIBS_SITE</code>, which contains the locations of all R packages installed within the module.</p> <pre><code>$ ls -l $R_LIBS_SITE | grep glmnet\ndrwxrwsr-x  9 douglas sw  4096 May 28 16:59 EBglmnet\ndrwxrwsr-x 11 douglas sw  4096 May 25 01:22 glmnet\ndrwxrwsr-x  6 douglas sw  4096 May 25 04:03 glmnetSE\ndrwxrwsr-x  7 douglas sw  4096 May 25 04:04 glmnetUtils\ndrwxrwsr-x  8 douglas sw  4096 May 25 04:04 glmnetcr\ndrwxrwsr-x  7 douglas sw  4096 May 25 10:46 glmnetr\n</code></pre>"},{"location":"software/r_packages_bianca/#install-steps","title":"Install steps","text":""},{"location":"software/r_packages_bianca/#install-on-rackham","title":"Install on Rackham","text":"<ul> <li>R on UPPMAX course</li> <li>note First decide on which R version it should be based on and load that R_packages module.</li> <li>If not stated otherwise, your installation will end up in the <code>~/R</code> directory within your home directory</li> </ul>"},{"location":"software/r_packages_bianca/#methods","title":"Methods","text":"<ul> <li> <p>automatic download and install from CRAN</p> <ul> <li>https://uppmax.github.io/bianca_workshops/extra/rpackages</li> </ul> </li> <li> <p>automatic download and install from GitHub</p> <ul> <li>https://uppmax.github.io/bianca_workshops/extra/rpackages</li> </ul> </li> <li> <p>manual download and install</p> <ul> <li>https://uppmax.github.io/bianca_workshops/extra/rpackages</li> <li>NOTE that if you install a package this way, you need to handle any dependencies yourself.<ul> <li>For instance you might get use of our modules</li> </ul> </li> </ul> </li> </ul>"},{"location":"software/r_packages_bianca/#transfer-to-wharf","title":"Transfer to wharf","text":"<ul> <li>You may transfer the whole R library (in you home folder)<ul> <li>this is usually the easiest way</li> </ul> </li> <li>or select the directory(-ies) related to you new installation<ul> <li>note there may be more than one directory</li> </ul> </li> </ul>"},{"location":"software/r_packages_bianca/#move-package-to-local-bianca-r-package-path","title":"Move package to local Bianca R package path","text":"<ul> <li>Sync or move the R directory or the specific folders to your <code>~/R</code> directory on bianca</li> </ul>"},{"location":"software/r_packages_bianca/#test-your-installation","title":"Test your installation","text":"<ul> <li>Start an R session on bianca and load the new package</li> </ul>"},{"location":"software/r_packages_bianca/#example-update-dowser","title":"Example: Update dowser","text":"<p>dowser on ReadTheDocs</p> <p>Info</p> <ul> <li>Dowser is part of the Immcantation analysis framework for Adaptive Immune Receptor Repertoire sequencing (AIRR-seq).</li> <li>Dowser provides a set of tools for performing phylogenetic analysis on B cell receptor repertoires.</li> <li>It supports building and visualizing trees using multiple methods, and implements statistical tests for discrete trait analysis of B cell migration, differentiation, and isotype switching.</li> </ul> <p>The version of dowser in <code>R_packages/4.2.1</code> is 1.1.0. It was updated to version 1.2.0 on 2023-05-30.</p>"},{"location":"software/r_packages_bianca/#install-dowser-rackham","title":"Install dowser Rackham","text":"<p>You can update this for yourself by beginning on rackham. Do</p> <pre><code>module load R_packages/4.2.1\n</code></pre> <p>and then, within R, do</p> <pre><code>install.packages('dowser')\n</code></pre> <p>The <code>install.packages()</code> command that you use to install new packages is also used to update already installed packages.</p> <p>As the update begins, you will see two questions, answer yes to both:</p> <pre><code>Warning in install.packages(\"dowser\") :\n      'lib = \"/sw/apps/R_packages/4.2.1/rackham\"' is not writable\n    Would you like to use a personal library instead? (yes/No/cancel) yes\n</code></pre> <p>and</p> <pre><code>Would you like to create a personal library\n    '~/R/x86_64-pc-linux-gnu-library/4.2'\n    to install packages into? (yes/No/cancel) yes\n</code></pre> <p>If you have already installed or updated an R package with R_packages/4.2.1 loaded that resulted in creating a personal library, you may not see one or both of these questions.</p> <p>This will then lead to a brief installation process.  This creates the directory <code>~/R/x86_64-pc-linux-gnu-library/4.2</code> that it refers to in the question.  This directory contains your personal installations and updates of R packages.</p> <p>The complete installation output for this update on rackham was:</p> <pre><code>&gt; packageVersion('dowser')\n[1] '1.1.0'\n&gt; install.packages('dowser')\nInstalling package into '/sw/apps/R_packages/4.2.1/rackham'\n(as 'lib' is unspecified)\nWarning in install.packages(\"dowser\") :\n  'lib = \"/sw/apps/R_packages/4.2.1/rackham\"' is not writable\nWould you like to use a personal library instead? (yes/No/cancel) yes\nWould you like to create a personal library\n'/domus/h1/douglas/R/x86_64-pc-linux-gnu-library/4.2'\nto install packages into? (yes/No/cancel) yes\n--- Please select a CRAN mirror for use in this session ---\ntrying URL 'https://ftp.acc.umu.se/mirror/CRAN/src/contrib/dowser_1.2.0.tar.gz'\nContent type 'application/x-gzip' length 1722229 bytes (1.6 MB)\n==================================================\ndownloaded 1.6 MB\n\n* installing *source* package 'dowser' ...\n** package 'dowser' successfully unpacked and MD5 sums checked\n** using staged installation\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (dowser)\n\nThe downloaded source packages are in\n    '/scratch/RtmpRo0Gz5/downloaded_packages'\n&gt;\n&gt; packageVersion('dowser')\n[1] '1.2.0'\n</code></pre>"},{"location":"software/r_packages_bianca/#transfer-to-the-wharf","title":"Transfer to the Wharf","text":"<p>After installation, the next step is to copy the contents of this directory over to bianca so that it is the same directory within your bianca home directory.</p> <p>Make sure you are in your home directory. Then connect to the bianca wharf.  Replace the name and project with your bianca user name and project.</p> <pre><code>sftp douglas-sens2017625@bianca-sftp\n</code></pre> <p>You log in here like you log into bianca: the first password is your password followed by the 6-digit authenticator code, the second password (if required for you) is only your password.</p> <p>Once sftp has connected, the contents of the current directory can be listed with</p> <pre><code>dir\n</code></pre> <p>It should look like this:</p> <pre><code>sftp&gt; dir\ndouglas-sens2017625\n</code></pre> <p>Now <code>cd</code> to this directory, which is your wharf directory within your project.</p> <pre><code>sftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> <p>If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <p>Now, upload your entire personal <code>R</code> directory from rackham here.</p> <pre><code>sftp&gt; put -r R\n</code></pre> <p>This will take a while to upload all the files. When it has completed, quit.</p> <pre><code>sftp&gt; quit\n</code></pre> <ul> <li>Now, log into bianca using the shell, or using the web interface and start a terminal.</li> <li>Once you have a bianca shell, change to your wharf directory within your project.  Replace my user and project with yours.</li> </ul> <pre><code>cd /proj/sens2017625/nobackup/wharf/douglas/douglas-sens2017625\n</code></pre> <p>Within this directory should be your R directory.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ ls -l\ntotal 1892\ndrwxrwxr-x  3 douglas douglas    4096 Mar  2 14:27 R\n</code></pre>"},{"location":"software/r_packages_bianca/#sync-from-wharf-to-home-directory","title":"Sync from Wharf to Home directory","text":"<ul> <li>Now sync this to your home directory:</li> </ul> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ rsync -Pa R ~/\n</code></pre>"},{"location":"software/r_packages_bianca/#start-an-r-session-and-load-the-new-package","title":"Start an R session and load the new package","text":"<p>Because R_packages/4.2.1 was loaded when you installed/updated the packages in your personal R library, you need to have it loaded when you use these packages as well.</p> <p>Simply change to the directory you want to work in, load the R_packages/4.2.1 module, and get to work.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ cd /proj/sens2017625/nobackup/douglas/\n    [douglas@sens2017625-bianca douglas]$ module load R_packages/4.2.1\n</code></pre> <p>Then start R, and load the new package.</p> <pre><code>[douglas@sens2017625-bianca douglas]$ R\n</code></pre> <pre><code>    &gt; packageVersion('dowser')\n    [1] '1.2.0'\n    &gt; library(dowser)\n    &gt;\n</code></pre>"},{"location":"software/r_packages_bianca2/","title":"Installing R packages on Bianca","text":""},{"location":"software/r_packages_bianca2/#installing-r-packages-on-bianca","title":"Installing R packages on Bianca","text":"<p>R in NAISS course</p>"},{"location":"software/r_packages_bianca2/#what-is-a-package-really","title":"What is a package, really?","text":"<ul> <li> <p>An R package is essentially a contained folder and file structure containing R code (and possibly C/C++ or other code) and other files relevant for the package e.g. documentation(vignettes), licensing and configuration files.</p> </li> <li> <p>Let us look at a very simple example</p> </li> </ul> <pre><code>   $ git clone git@github.com:MatPiq/R_example.git\n\n   $ cd R_example\n\n   $ tree\n   .\n   \u251c\u2500\u2500 DESCRIPTION\n   \u251c\u2500\u2500 NAMESPACE\n   \u251c\u2500\u2500 R\n   \u2502   \u2514\u2500\u2500 hello.R\n   \u251c\u2500\u2500 man\n   \u2502   \u2514\u2500\u2500 hello.Rd\n   \u2514\u2500\u2500 r_example.Rproj\n</code></pre>"},{"location":"software/r_packages_bianca2/#installing-your-own-packages","title":"Installing your own packages","text":"<p>Sometimes you will need R packages that are not already installed. The solution to this is to install your own packages.</p> <ul> <li> <p>These packages will usually come from CRAN - the Comprehensive R Archive Network, or</p> </li> <li> <p>sometimes from other places, like GitHub or R-Forge</p> </li> </ul> <p>Here we will look at installing R packages with automatic download and with manual download. It is also possible to install from inside RStudio.</p>"},{"location":"software/r_packages_bianca2/#methods","title":"Methods","text":"<ul> <li>setup (first time)</li> <li>automatic download and install from CRAN</li> <li>automatic download and install from GitHub</li> <li>manual download and install</li> </ul>"},{"location":"software/r_packages_bianca2/#setup-first-time","title":"setup (first time)","text":"<p>https://uppmax.github.io/bianca_workshops/extra/rpackages/#setup</p> <ul> <li> <p>We need to create a place for the own-installed packages to be and to tell R where to find them. The initial setup only needs to be done once, but separate package directories need to be created for each R version used.</p> </li> <li> <p>R reads the <code>$HOME/.Renviron</code> file to setup its environment. It should be created by R on first run, or you can create it with the command: touch $HOME/.Renviron</p> </li> </ul> <p>NOTE: In this example we are going to assume you have chosen to place the R packages in a directory under your home directory. As mentioned, you will need separate ones for each R version.</p> <p>If you have not yet installed any packages to R yourself, the environment file should be empty and you can update it like this:</p> <pre><code>    echo R_LIBS_USER=\\\"$HOME/R-packages-%V\\\" &gt; ~/.Renviron\n</code></pre> <p>If it is not empty, you can edit <code>$HOME/.Renviron</code> with your favorite editor so that <code>R_LIBS_USER</code> contain the path to your chosen directory for own-installed R packages. It should look something like this when you are done:</p> <pre><code>    R_LIBS_USER=\"/home/u/user/R-packages-%V\"\n</code></pre> <p>| NOTE: Replace <code>/home/u/user</code> with the value of <code>$HOME</code>. Run <code>echo $HOME</code> to see its value. | NOTE: The <code>%V</code> should be written as-is, it's substituted at runtime with the active R version.</p> <p>For each version of R you are using, create a directory matching the pattern used in <code>.Renviron</code> to store your packages in. This example is shown for R version 4.0.4:</p> <pre><code>    mkdir -p $HOME/R-packages-4.0.4\n</code></pre>"},{"location":"software/r_packages_bianca2/#automatic-download-and-install-from-cran","title":"Automatic download and install from CRAN","text":"<p>https://uppmax.github.io/bianca_workshops/extra/rpackages/#automatic-download-and-install-from-cran</p> <p>Note</p> <p>You find a list of packages in CRAN and a list of repos here: https://cran.r-project.org/mirrors.html</p> <ul> <li>Please choose a location close to you when picking a repo.</li> </ul> From command lineFrom inside R <pre><code>R --quiet --no-save --no-restore -e \"install.packages('&lt;r-package&gt;', repos='&lt;repo&gt;')\"\n</code></pre> <pre><code>install.packages('&lt;r-package&gt;', repos='&lt;repo&gt;')\n</code></pre> <p>In either case, the dependencies of the package will be downloaded and installed as well.</p>"},{"location":"software/r_packages_bianca2/#automatic-download-and-install-from-github","title":"Automatic download and install from GitHub","text":"<p>https://uppmax.github.io/bianca_workshops/extra/rpackages/#automatic-download-and-install-from-github</p> <p>If you want to install a package that is not on CRAN, but which do have a GitHub page, then there is an automatic way of installing, but you need to handle prerequisites yourself by installing those first.</p> <ul> <li>It can also be that the package is not in as finished a state as those on CRAN, so be careful.</li> </ul> <p>Note</p> <p>To install packages from GitHub directly, from inside R, you first need to install the devtools package. Note that you only need to install this once.</p> <p>This is how you install a package from GitHub, inside R:</p> <pre><code>    install.packages(\"devtools\")   # ONLY ONCE\n    devtools::install_github(\"DeveloperName/package\")\n</code></pre>"},{"location":"software/r_packages_bianca2/#manual-download-and-install","title":"Manual download and install","text":"<p>https://uppmax.github.io/bianca_workshops/extra/rpackages/#manual-download-and-install</p> <p>If the package is not on CRAN or you want the development version, or you for other reason want to install a package you downloaded, then this is how to install from the command line:</p> <pre><code>    R CMD INSTALL -l &lt;path-to-R-package&gt;/R-package.tar.gz\n</code></pre> <p>NOTE that if you install a package this way, you need to handle any dependencies yourself.</p> <p>Note</p> <p>Places to look for R packages</p> <ul> <li>CRAN</li> <li>R-Forge</li> <li>Project's own GitHub page</li> <li>etc.</li> </ul>"},{"location":"software/r_packages_bianca2/#example-install-tidycmprsk","title":"Example \u2014 Install Tidycmprsk","text":"<p>tidycmprsk on GitHub</p> <p>Info</p> <p>The tidycmprsk package provides an intuitive interface for working with the competing risk endpoints. The package wraps the cmprsk package, and exports functions for univariate cumulative incidence estimates with cuminc() and competing risk regression with crr().</p>"},{"location":"software/r_packages_bianca2/#install-on-rackham","title":"Install on Rackham","text":"<p>You can install this for yourself by beginning on rackham. Do</p> <pre><code>module load R_packages/4.1.1\n</code></pre> <p>and then, within R, do</p> <pre><code>install.packages('tidycmprsk')\n</code></pre> <p>You will see two questions to answer yes to:</p> <pre><code>Warning in install.packages(\"tidycmprsk\") :\n      'lib = \"/sw/apps/R_packages/4.1.1/rackham\"' is not writable\n    Would you like to use a personal library instead? (yes/No/cancel) yes\n</code></pre> <p>and</p> <pre><code>Would you like to create a personal library\n    '~/R/x86_64-pc-linux-gnu-library/4.1'\n    to install packages into? (yes/No/cancel) yes\n</code></pre> <p>This will then to an extended installation process that also does some updates.  This creates a directory ~/R that contains the installations and updates of R packages.</p>"},{"location":"software/r_packages_bianca2/#transfer-to-the-wharf","title":"Transfer to the Wharf","text":"<p>After installation, the next step is to copy the contents of this directory over to bianca so that it is the same directory within your bianca home directory.</p> <p>Make sure you are in your home directory. Then connect to the bianca wharf.  Replace the name and project with your bianca user name and project.</p> <pre><code>sftp douglas-sens2017625@bianca-sftp\n</code></pre> <p>You log in here like you log into bianca: the first password is your password followed by the 6-digit authenticator code, the second password (if required for you) is only your password.</p> <p>Once sftp has connected, the contents of the current directory can be listed with</p> <pre><code>dir\n</code></pre> <p>It should look like this:</p> <pre><code>sftp&gt; dir\ndouglas-sens2017625\n</code></pre> <p>Now <code>cd</code> to this directory, which is your wharf directory within your project.</p> <pre><code>sftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> <p>If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <p>Now, upload your (whole) <code>R</code> directory here.</p> <pre><code>sftp&gt; put -r R\n</code></pre> <p>This will take a while to upload all the files. When it has completed, quit.</p> <pre><code>sftp&gt; quit\n</code></pre> <ul> <li>Now, log into bianca using the shell, or using the web interface and start a terminal.</li> <li>Once you have a bianca shell, change to your wharf directory within your project.  Replace my user and project with yours.</li> </ul> <pre><code>cd /proj/sens2017625/nobackup/wharf/douglas/douglas-sens2017625\n</code></pre> <p>Within this directory should be your R directory.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ ls -l\ntotal 1892\ndrwxrwxr-x  3 douglas douglas    4096 Mar  2 14:27 R\n</code></pre>"},{"location":"software/r_packages_bianca2/#sync-from-wharf-to-home-directory","title":"Sync from Wharf to Home directory","text":"<ul> <li>Now sync this to your home directory:</li> </ul> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ rsync -Pa R ~/\n</code></pre>"},{"location":"software/r_packages_bianca2/#start-an-r-session-and-load-the-new-package","title":"Start an R session and load the new package","text":"<p>To use R_packages/4.1.1 with these new installations/updates, change to the directory you want to work in, load the R_packages/4.1.1 module.  Substitute your directory for my example directory.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ cd /proj/sens2017625/nobackup/douglas/\n    [douglas@sens2017625-bianca douglas]$ module load R_packages/4.1.1\n</code></pre> <p>Then start R, and load the new package.</p> <pre><code>[douglas@sens2017625-bianca douglas]$ R\n</code></pre> <pre><code>    R version 4.1.1 (2021-08-10) -- \"Kick Things\"\n    Copyright (C) 2021 The R Foundation for Statistical Computing\n    ....\n    Type 'demo()' for some demos, 'help()' for on-line help, or\n    'help.start()' for an HTML browser interface to help.\n    Type 'q()' to quit R.\n\n    &gt; library(tidycmprsk)\n    &gt;\n</code></pre>"},{"location":"software/rackham_file_transfer_using_filezilla/","title":"File transfer to/from Rackham using FileZilla","text":"","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#file-transfer-tofrom-rackham-using-filezilla","title":"File transfer to/from Rackham using FileZilla","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>Here, we show how to transfer files using a graphical tool called FileZilla.</p> <p></p> <p>FileZilla connected to Rackham</p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"Would you like a video? <p>See the YouTube video file transfer from/to Rackham using FileZilla</p> <p>FileZilla is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from Rackham using FileZilla, do the following steps:</p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#1-start-filezilla","title":"1. Start FileZilla","text":"<p>Start FileZilla.</p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#2-start-filezillas-site-manager","title":"2. Start FileZilla's site manager","text":"<p>From the menu, select 'File | Site manager'</p> Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#3-add-a-new-site-in-filezillas-site-manager","title":"3. Add a new site in FileZilla's site manager","text":"<p>In FileZilla's site manager, click 'New site'</p> Where is that? <p>It is here:</p> <p></p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#4-setup-the-site","title":"4. Setup the site","text":"<p>In FileZilla's site manager:</p> <ul> <li>create a name for the site, e.g. <code>rackham</code>.</li> <li>for that site, use all standards, except:<ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>rackham.uppmax.uu.se</code></li> <li>Set user to <code>[username]</code>, e.g. <code>sven</code></li> </ul> </li> </ul> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#5-connect-to-the-site","title":"5. Connect to the site","text":"<p>Click 'Connect'.</p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#6-fill-in-your-password","title":"6. Fill in your password","text":"<p>You will be asked for your password, hence type <code>[your password]</code>, e.g. <code>VerySecret</code>. You can save the password.</p> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_filezilla/#7-ready-to-transfer-files","title":"7. Ready to transfer files","text":"<p>Now you can transfer files between your local computer and Rackham.</p> How does that look like? <p>It looks like this:</p> <p></p>","tags":["FileZilla","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/","title":"Data transfer to/from Rackham using rsync","text":"","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#data-transfer-tofrom-rackham-using-rsync","title":"Data transfer to/from Rackham using rsync","text":"<p>There are multiple ways to transfer files to or from Rackham.</p> <p>Here it is described how to do file transfer to/from Rackham using rsync. <code>rsync</code> can be used in scripts for regular file transfer. However, <code>rsync</code> shines by providing a so-called 'delta' file transfer: when you transfer files twice, <code>rsync</code> will only transfer the files that have changed. This is ideal for backups.</p>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#procedure","title":"Procedure","text":"Prefer a video? <p>See this procedure as a video at YouTube.</p>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#2-transfer-files-to-rackham","title":"2. Transfer files to Rackham","text":"<p>You can transfer files to Rackham by:</p> <ul> <li>2a. Transfer individual files to Rackham</li> <li>2b. Transfer all files in a folder to Rackham</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#2a-transfer-individual-files-to-rackham","title":"2a. Transfer individual files to Rackham","text":"<p>On local computer, do:</p> <pre><code>rsync [my_local_file] [username]@rackham.uppmax.uu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[my_local_file]</code> is the path to your local file</li> <li><code>[target_folder]</code> is the path of the folder you want to copy your file to</li> <li><code>[username]</code> is your UPPMAX username</li> </ul> <p>for example:</p> <pre><code>rsync my_local_file.txt sven@rackham.uppmax.uu.se:/home/sven/\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#2b-transfer-all-files-in-a-folder-to-rackham","title":"2b. Transfer all files in a folder to Rackham","text":"<p>On local computer, do:</p> <pre><code>rsync --recursive my_folder [username]@rackham.uppmax.uu.se:[target_folder]\n</code></pre> <p>where</p> <ul> <li><code>[target_folder]</code> is the target folder</li> <li><code>[username]</code> is your UPPMAX username</li> </ul> <p>for example:</p> <pre><code>rsync --recursive my_folder sven@rackham.uppmax.uu.se:/home/sven/\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p> <p>Note that in <code>rsync</code>, a slash (<code>/</code>) matters:</p> Command Effect <code>rsync --recursive my_folder sven@rackham.uppmax.uu.se:/home/sven</code> Will put the files in <code>my_folder</code> in the Rackham home folder <code>rsync --recursive my_folder sven@rackham.uppmax.uu.se:/home/sven/</code> Will put the folder <code>my_folder</code> in the Rackham home folder","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#3-transfer-files-from-rackham-to-you-local-computer","title":"3. Transfer files from Rackham to you local computer","text":"<p>You can transfer files from Rackham to your local computer by:</p> <ul> <li>3a. Transfer individual files from Rackham to your local computer</li> <li>3b. Transfer all folders from Rackham to you local computer</li> </ul>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#3a-transfer-individual-files-from-rackham-to-your-local-computer","title":"3a. Transfer individual files from Rackham to your local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync [username]@rackham.uppmax.uu.se:[path_to_file] .\n</code></pre> <p>where</p> <ul> <li><code>[path_to_file]</code> is the path to the file you want to download</li> <li><code>[username]</code> is your UPPMAX username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync sven@rackham.uppmax.uu.se:/home/sven/my_file.txt .\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_rsync/#3b-transfer-all-folders-from-rackham-to-you-local-computer","title":"3b. Transfer all folders from Rackham to you local computer","text":"<p>On your local computer, do:</p> <pre><code>rsync --recursive [username]@rackham.uppmax.uu.se:/[source_folder] .\n</code></pre> <p>where</p> <ul> <li><code>[source_folder]</code> is the path to the folder you want to download</li> <li><code>[username]</code> is your UPPMAX username</li> <li><code>.</code> means 'in the current folder of my local computer' or 'here'</li> </ul> <p>for example:</p> <pre><code>rsync --recursive sven@rackham.uppmax.uu.se:/home/sven/my_folder .\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you are using an SSH key pair.</p>","tags":["transfer","data transfer","file transfer","rsync","Rackham"]},{"location":"software/rackham_file_transfer_using_scp/","title":"Data transfer to/from Rackham using SCP","text":"","tags":["transfer","data transfer","file transfer","scp","SCP","Rackham"]},{"location":"software/rackham_file_transfer_using_scp/#data-transfer-tofrom-rackham-using-scp","title":"Data transfer to/from Rackham using SCP","text":"<p>There are multiple ways to transfer files to or from Rackham.</p> <p>Here it is described how to do so using <code>scp</code>.</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Rackham"]},{"location":"software/rackham_file_transfer_using_scp/#procedure","title":"Procedure","text":"Prefer a video? <p>See this procedure as a video at YouTube</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Rackham"]},{"location":"software/rackham_file_transfer_using_scp/#1-start-a-terminal-on-your-local-computer","title":"1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Rackham"]},{"location":"software/rackham_file_transfer_using_scp/#2-copy-files-using-scp","title":"2. Copy files using <code>scp</code>","text":"<p>In the terminal, copy files using <code>scp</code> to connect to Rackham:</p> <pre><code>scp [from] [to]\n</code></pre> <p>Where <code>[from]</code> is the file(s) you want to copy, and <code>[to]</code> is the destination. This is quite a shorthand notation!</p> <p>This is how you copy a file from your local computer to Rackham:</p> <pre><code>scp [local_filename] [username]@rackham.uppmax.uu.se:/home/[username]\n</code></pre> <p>where <code>[local_filename]</code> is the path to a local filename, and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_file.txt sven@rackham.uppmax.uu.se:/home/sven\n</code></pre> <p>To copy a file from Rackham to your local computer, do the command above in reverse order:</p> <pre><code>scp [username]@rackham.uppmax.uu.se:/home/[username]/[remote_filename] [local_folder]\n</code></pre> <p>where <code>[remote_filename]</code> is the path to a remote filename, <code>[username]</code> is your UPPMAX username, and <code>[local_folder]</code> is your local folder, for example:</p> <pre><code>scp sven@rackham.uppmax.uu.se:/home/sven/my_remote_file.txt /home/sven\n</code></pre>","tags":["transfer","data transfer","file transfer","scp","SCP","Rackham"]},{"location":"software/rackham_file_transfer_using_scp/#3-if-asked-give-your-uppmax-password","title":"3. If asked, give your UPPMAX password","text":"<p>If asked, give your UPPMAX password. You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Rackham"]},{"location":"software/rackham_file_transfer_using_sftp/","title":"Data transfer to/from Rackham using SFTP","text":""},{"location":"software/rackham_file_transfer_using_sftp/#data-transfer-tofrom-rackham-using-sftp","title":"Data transfer to/from Rackham using SFTP","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>Data transfer to/from Rackham using SFTP is one of the ways ways to transfer files to/from Rackham</p> What are the other ways? <p>See the other ways to transfer data to/from Rackham</p> <p>One can transfer files to/from Rackham using SFTP. SFTP is an abbreviation of 'SSH File Transfer Protocol', where 'SSH' is an abbreviation of 'Secure Shell protocol' The program <code>sftp</code> allows you to transfer files to/from Rackham using SFTP.</p> <p>The process is described here:</p>"},{"location":"software/rackham_file_transfer_using_sftp/#step-1-start-a-terminal-on-your-local-computer","title":"Step 1. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer.</p>"},{"location":"software/rackham_file_transfer_using_sftp/#step-2-run-sftp-to-connect-to-rackham","title":"Step 2. Run <code>sftp</code> to connect to Rackham","text":"<p>In the terminal, run <code>sftp</code> to connect to Rackham by doing:</p> <pre><code>sftp [username]@rackham.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>sftp sven@rackham.uppmax.uu.se\n</code></pre>"},{"location":"software/rackham_file_transfer_using_sftp/#step-3-if-asked-give-your-uppmax-password","title":"Step 3. If asked, give your UPPMAX password","text":"<p>If asked, give your UPPMAX password. You can get rid of this prompt if you have setup SSH keys</p>"},{"location":"software/rackham_file_transfer_using_sftp/#step-4-uploaddownload-files-tofrom-rackham","title":"Step 4. Upload/download files to/from Rackham","text":"<p>In <code>sftp</code>, upload/download files to/from Rackham.</p> <pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n\n    user(User)\n      user_local_files(Files on user computer):::file_node\n\n    subgraph sub_inside[SUNET]\n      subgraph sub_rackham_shared_env[Rackham]\n          login_node(login/calculation/interactive session):::calculation_node\n          files_in_rackham_home(Files in Rackham home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#fcc,color:#000,stroke:#fcc\n    style sub_rackham_shared_env fill:#ffc,color:#000,stroke:#ffc\n\n    user --&gt; |logs in |login_node\n    user --&gt; |uses| user_local_files\n\n    login_node --&gt; |can use|files_in_rackham_home\n    %% user_local_files &lt;--&gt; |graphical tool|files_in_rackham_home\n    %% user_local_files &lt;--&gt; |SCP|files_in_rackham_home\n    user_local_files &lt;==&gt; |SFTP|files_in_rackham_home\n\n    %% Aligns nodes prettier\n    user_local_files ~~~ login_node</code></pre> <p>Overview of file transfer on Rackham The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>"},{"location":"software/rackham_file_transfer_using_transit_scp/","title":"Data transfer to/from Rackham using Transit using SCP","text":"","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp/#data-transfer-tofrom-rackham-using-transit-using-scp","title":"Data transfer to/from Rackham using Transit using SCP","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>One can use SCP to copy files between Rackham and Transit, from either Rackham or Transit.</p> <p>Both ways are shown step-by-step below.</p> <ul> <li>Using SCP from Rackham</li> <li>Using SCP from transit</li> </ul>","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_rackham/","title":"Data transfer to/from Rackham using Transit using SCP from Rackham","text":"","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Rackham"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_rackham/#data-transfer-tofrom-rackham-using-transit-using-scp-from-rackham","title":"Data transfer to/from Rackham using Transit using SCP from Rackham","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>One can transfer files to/from Rackham using the UPPMAX Transit server, using <code>scp</code>.</p> <p>The process is:</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Rackham"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_rackham/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Rackham"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_rackham/#2-use-the-terminal-to-login-to-rackham","title":"2. Use the terminal to login to Rackham","text":"<p>Use a terminal to login to Rackham.</p> Forgot how to login to Rackham? <p>See our step-by-step guide how to login to Rackham.</p> <p>Spoiler: <code>ssh [username]@rackham.uppmax.uu.se</code></p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Rackham"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_rackham/#3a-run-scp-to-copy-files-from-rackham-to-transit","title":"3a. Run <code>scp</code> to copy files from Rackham to Transit","text":"<p>This is how you would copy a file from Rackham to Transit: in the terminal, run <code>scp</code> to copy files from Rackham to Transit by doing:</p> <pre><code>scp [file_on_rackham] [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[file_on_rackham]</code> is the name of a file on Rackham and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_rackham_file.txt [username]@transit.uppmax.uu.se\n</code></pre> <p>However, Transit is a service, not a file server. The <code>scp</code> command will complete successfully, yet the file will not be found on Transit.</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Rackham"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_rackham/#3b-run-scp-to-copy-files-from-transit-to-rackham","title":"3b. Run <code>scp</code> to copy files from Transit to Rackham","text":"<p>In the terminal, run <code>scp</code> to copy files from Transit to Rackham by doing:</p> <pre><code>scp [file_on_rackham] [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[file_on_rackham]</code> is the name of a file on Rackham and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_rackham_file.txt [username]@transit.uppmax.uu.se\n</code></pre>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Rackham"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_rackham/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","SCP","scp","Transit","transit","Rackham"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_transit/","title":"Data transfer to/from Rackham using Transit using SCP from Transit","text":"","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_transit/#data-transfer-tofrom-rackham-using-transit-using-scp-from-transit","title":"Data transfer to/from Rackham using Transit using SCP from Transit","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>One can transfer files to/from Rackham using the UPPMAX Transit server, using <code>scp</code>.</p> <p>The process is:</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_transit/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_transit/#2-use-the-terminal-to-login-to-transit","title":"2. Use the terminal to login to Transit","text":"<p>Use a terminal to login to Transit.</p> Forgot how to login to Transit? <p>A step-by-step guide how to login to Transit See our step-by-step guide how to login to Transit.</p> <p>Spoiler: <code>ssh [username]@transit.uppmax.uu.se</code></p>","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_transit/#3a-run-scp-to-copy-files-from-transit-to-rackham","title":"3a. Run <code>scp</code> to copy files from Transit to Rackham","text":"<p>In the terminal, run <code>scp</code> to copy files from Transit to Rackham by doing:</p> <pre><code>scp [username]@rackham.uppmax.uu.se:/home/[username]/[file_on_rackham] [path_on_transit]\n</code></pre> <p>where <code>[file_on_rackham]</code> is the name of a file on Rackham, <code>[username]</code> is your UPPMAX username, and <code>[path_on_transit]</code> is the target path on Transit, for example:</p> <pre><code>scp sven@rackham.uppmax.uu.se:/home/sven/my_rackham_file.txt .\n</code></pre> <p>Where <code>.</code> means 'the directory where I am now on Transit'.</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_transit/#3b-run-scp-to-copy-files-from-rackham-to-transit","title":"3b.  Run <code>scp</code> to copy files from Rackham to Transit","text":"<p>This is how you would copy a file from Rackham to Transit: in the terminal, run <code>scp</code> to copy files from Rackham to Transit by doing:</p> <pre><code>scp [file_on_rackham] [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[file_on_transit]</code> is the name of a file on Transit and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_local_rackham_file.txt [username]@transit.uppmax.uu.se\n</code></pre> <p>However, Transit is a service, not a file server. The <code>scp</code> command will complete successfully, yet the file will not be found on Transit.</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_scp_from_transit/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","Transit","transit","Rackham","SCP","scp"]},{"location":"software/rackham_file_transfer_using_transit_sftp/","title":"Data transfer to/from Rackham using Transit and SFTP","text":""},{"location":"software/rackham_file_transfer_using_transit_sftp/#data-transfer-tofrom-rackham-using-transit-and-sftp","title":"Data transfer to/from Rackham using Transit and SFTP","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>Data transfer to/from Rackham using Transit is one of the ways ways to transfer files to/from Rackham</p> <p>One can use SFTP to copy files between Rackham and Transit, from either Rackham or Transit.</p> <p>Both ways are shown step-by-step below.</p> <ul> <li>Using SFTP from Rackham</li> <li>Using SFTP from transit</li> </ul>"},{"location":"software/rackham_file_transfer_using_transit_sftp/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n    classDef transit_node fill:#fff,color:#000,stroke:#fff\n\n    subgraph sub_inside[SUNET]\n      direction LR\n      user(User)\n      subgraph sub_transit_env[Transit]\n        transit_login(Transit login):::calculation_node\n        files_on_transit(Files posted to Transit):::transit_node\n      end\n      subgraph sub_rackham_shared_env[Rackham]\n          files_in_rackham_home(Files in Rackham home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#ccc,color:#000,stroke:#000\n    style sub_transit_env fill:#cfc,color:#000,stroke:#000\n    style sub_rackham_shared_env fill:#fcc,color:#000,stroke:#000\n\n    user --&gt; |logs in |transit_login\n\n    transit_login --&gt; |can use|files_on_transit\n    %% user_local_files &lt;--&gt; |graphical tool|files_in_rackham_home\n    %% user_local_files &lt;--&gt; |SCP|files_in_rackham_home\n    files_on_transit &lt;==&gt; |transfer|files_in_rackham_home</code></pre> <p>Overview of file transfer on Rackham The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_rackham/","title":"Data transfer to/from Rackham using Transit and SFTP from Rackham","text":""},{"location":"software/rackham_file_transfer_using_transit_sftp_from_rackham/#data-transfer-tofrom-rackham-using-transit-and-sftp-from-rackham","title":"Data transfer to/from Rackham using Transit and SFTP from Rackham","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>Data transfer to/from Rackham using Transit is one of the ways ways to transfer files to/from Rackham</p> <p>One can transfer files to/from Rackham using the UPPMAX Transit server. Transit is an abbreviation of 'SSH File Transfer Protocol', where 'SSH' is an abbreviation of 'Secure Shell protocol' The program <code>sftp</code> allows you to transfer files to/from Rackham using Transit.</p> <p>The process is:</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_rackham/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_rackham/#2-use-the-terminal-to-login-to-rackham","title":"2. Use the terminal to login to Rackham","text":"<p>Use a terminal to login to Rackham.</p> Forgot how to login to Rackham? <p>See this step-by-step guide how to login to Rackham.</p> <p>Spoiler: <code>ssh [username]@rackham.uppmax.uu.se</code></p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_rackham/#3-run-sftp-to-connect-to-transit","title":"3. Run <code>sftp</code> to connect to Transit","text":"<p>In the terminal, run <code>sftp</code> to connect to Transit by doing:</p> <pre><code>sftp [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>sftp sven@transit.uppmax.uu.se\n</code></pre>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_rackham/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_rackham/#5-in-sftp-uploaddownload-files-tofrom-transit","title":"5. In <code>sftp</code>, upload/download files to/from Transit","text":"<p>Transit is a service, not a file server. This means that if you upload files to Transit using SFTP, they will remain there as long a the connection is active. These files need to be forwarded to more permanent storage.</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_transit/","title":"Data transfer to/from Rackham using Transit and SFTP from Transit","text":""},{"location":"software/rackham_file_transfer_using_transit_sftp_from_transit/#data-transfer-tofrom-rackham-using-transit-and-sftp-from-transit","title":"Data transfer to/from Rackham using Transit and SFTP from Transit","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>Data transfer to/from Rackham using Transit is one of the ways ways to transfer files to/from Rackham</p> <p>One can transfer files to/from Rackham using the UPPMAX Transit server. Transit is an abbreviation of 'SSH File Transfer Protocol', where 'SSH' is an abbreviation of 'Secure Shell protocol' The program <code>sftp</code> allows you to transfer files to/from Rackham using Transit.</p> <p>The process is:</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_transit/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_transit/#2-use-the-terminal-to-login-to-transit","title":"2. Use the terminal to login to Transit","text":"<p>Use a terminal to login to Transit</p> Forgot how to login to Transit? <p>See this step-by-step guide how to login to Transit.</p> <p>Spoiler: <code>ssh [username]@transit.uppmax.uu.se</code></p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_transit/#3-run-sftp-to-connect-to-rackham","title":"3. Run <code>sftp</code> to connect to Rackham","text":"<p>In the terminal, run <code>sftp</code> to connect to Rackham by doing:</p> <pre><code>sftp [username]@rackham.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>sftp sven@rackham.uppmax.uu.se\n</code></pre>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_transit/#4-if-asked-give-your-uppmax-password","title":"4. If asked, give your UPPMAX password","text":"<p>You can get rid of this prompt if you have setup SSH keys</p>"},{"location":"software/rackham_file_transfer_using_transit_sftp_from_transit/#5-in-sftp-uploaddownload-files-tofrom-rackham","title":"5. In <code>sftp</code>, upload/download files to/from Rackham","text":"<p>Transit is a service, not a file server. This means that if you upload files to Transit using SFTP, they will remain there as long a the connection is active. These files need to be forwarded to more permanent storage.</p>"},{"location":"software/rackham_file_transfer_using_winscp/","title":"File transfer to/from Rackham using WinSCP","text":"","tags":["transfer","data transfer","file transfer","Rackham","WinSCP"]},{"location":"software/rackham_file_transfer_using_winscp/#file-transfer-tofrom-rackham-using-winscp","title":"File transfer to/from Rackham using WinSCP","text":"<p>There are multiple ways to transfer data to/from Rackham.</p> <p>Here, we show how to transfer files using a graphical tool called WinSCP.</p> <p>To transfer files to/from Rackham using WinSCP, do:</p> <ul> <li>Start WinSCP</li> </ul> <p></p> <ul> <li>Create a new site</li> <li>For that site, use all standards, except:<ul> <li>Set file protocol to 'SFTP'</li> <li>Set host name to <code>rackham.uppmax.uu.se</code></li> <li>Set user name to <code>[username]</code>, e.g. <code>sven</code></li> <li>Save</li> </ul> </li> <li>Double-click on the saved session to the left OR Press the \"Login\" button</li> <li>Enter password</li> </ul>","tags":["transfer","data transfer","file transfer","Rackham","WinSCP"]},{"location":"software/rclone/","title":"Rclone","text":""},{"location":"software/rclone/#rclone","title":"Rclone","text":"<p>Rclone is a command-line program to manage files on cloud storage.</p> <p>There is an Rclone module called <code>rclone</code>.</p>"},{"location":"software/rclone/#finding-an-rclone-version","title":"Finding an Rclone version","text":"<pre><code>module spider rclone\n</code></pre> What is the output? <p>Here is some example output:</p> <pre><code>---------------------------------------------------------------------------------------\n  rclone: rclone/1.56.2\n---------------------------------------------------------------------------------------\n\n    This module can be loaded directly: module load rclone/1.56.2\n\n    Help:\n      rclone - use rclone\n\n      Description\n\n      a command line program to manage files on cloud storage, supporting over 40 cloud sto\nrage products\n\n      Version 1.56.2\n\n      https://rclone.org\n\n      Run 'rclone config' to set up rclone for your own use.\n</code></pre>"},{"location":"software/rclone/#loading-an-rclone-module","title":"Loading an Rclone module","text":"<p>Here the Rclone module for version 1.56.2 is loaded:</p> <pre><code>module load rclone/1.56.2\n</code></pre> What is the output? <p>Here is some example output:</p> <pre><code>rclone/1.56.2 : run 'rclone config' to set up rclone for your own use.  'man rclone' is available for further documentation, and see https://rclone.org/ for more\n</code></pre>"},{"location":"software/rclone/#finding-the-rclone-config-file","title":"Finding the Rclone config file","text":"<p>After having loaded an Rclone mode, one can find the path to the Rclone config file by:</p> <pre><code>rclone config file\n</code></pre> What is the output? <p>Here is some example output:</p> <pre><code>Configuration file doesn't exist, but rclone will use this path:\n/home/sven/.config/rclone/rclone.conf\n</code></pre>"},{"location":"software/rclone/#using-the-rclone-web-interface","title":"Using the Rclone web interface","text":"<p>With SSH X forwarding enabled, one can use <code>rclone</code> from a web interface:</p> <pre><code>rclone rcd --rc-web-gui\n</code></pre> <p>?Do not run this on the login node?</p> What is the output? <p>Here is some example output:</p> <pre><code>2024/04/02 08:31:59 ERROR : Error reading tag file at /home/sven/.cache/rclone/webgui/tag\n2024/04/02 08:31:59 NOTICE: A new release for gui (v2.0.5) is present at https://github.com/rclone/rclone-webui-react/releases/download/v2.0.5/currentbuild.zip\n2024/04/02 08:31:59 NOTICE: Downloading webgui binary. Please wait. [Size: 4763452, Path :  /home/sven/.cache/rclone/webgui/v2.0.5.zip]\n2024/04/02 08:32:00 NOTICE: Unzipping webgui binary\n2024/04/02 08:32:01 NOTICE: Serving Web GUI\n2024/04/02 08:32:01 NOTICE: Serving remote control on http://localhost:5572/\n</code></pre>"},{"location":"software/rclone/#connect-to-swestore","title":"Connect to Swestore","text":"<p>Rclone is one of the recommended ways to connect to Swestore.</p> <ul> <li>The Swestore documentation on Rclone</li> <li>YouTube video: Connect to Swestore using Rclone</li> </ul> URL invalid? <p>When setting the URL to the correct <code>https://webdav.swestore.se</code>, Rclone will flag this as an error:</p> <p></p> <p>Rclone flags an error, that may be a false error</p> <p>However, this may be a false error. To determine this: click on 'Explorer' and explore Swestore.</p> <p></p> <p>An example Swestore folder structure</p> <p>If you see the Swestore folder structure above, Rclone works fine.</p>"},{"location":"software/rclone/#links","title":"Links","text":"<ul> <li>The Rclone homepage</li> <li>YouTube video: A Beginner's Guide To Rclone</li> <li>YouTube video: Connect to Swestore using Rclone</li> </ul>"},{"location":"software/rstudio/","title":"RStudio","text":"","tags":["RStudio"]},{"location":"software/rstudio/#rstudio","title":"RStudio","text":"<p>RStudio is an IDE specialized for the R programming language.</p> What is an IDE? <p>See the page on IDEs.</p> <p>Using RStudio differs per UPPMAX cluster:</p> <ul> <li>RStudio on Bianca</li> <li>RStudio on Rackham</li> </ul>","tags":["RStudio"]},{"location":"software/rstudio/#rstudio-versions","title":"RStudio versions","text":"Which versions of RStudio are available? <p>Use <code>module spider Rstudio</code> to see all versions:</p> <pre><code>[sven@r210 sven]$ module spider Rstudio\n\n----------------------------------------------------------------------------\n  RStudio:\n----------------------------------------------------------------------------\n     Versions:\n        RStudio/1.0.136\n        RStudio/1.0.143\n        RStudio/1.0.153\n        RStudio/1.1.423\n        RStudio/1.1.463\n        RStudio/1.4.1106\n        RStudio/2022.02.0-443\n        RStudio/2022.02.3-492\n        RStudio/2022.07.1-554\n        RStudio/2023.06.0-421\n        RStudio/2023.06.2-561\n        RStudio/2023.12.1-402 (may not always work)\n</code></pre> <p>Some links between version and official documentation:</p> RStudio module RStudio Builds documentation <code>RStudio/2023.06.2-561</code> Documentation","tags":["RStudio"]},{"location":"software/rstudio/#troubleshooting","title":"Troubleshooting","text":"","tags":["RStudio"]},{"location":"software/rstudio/#rstudio-runs-partially","title":"RStudio runs partially","text":"<p>RStudio runs partially:</p> <ul> <li>File content is displayed just fine</li> <li>The R interpreter does not respond</li> <li>The files pane at the bottom-right is loading forever</li> </ul> <p></p> <p>In one case (see ticket for details), the problem was caused by a process called <code>-bash</code> (yes, the first character is a dash/minus). Killing it with <code>kill -s 1 [PID]</code> (for example, <code>kill -s 1 11723</code>) and then restarting RStudio solved the problem.</p>","tags":["RStudio"]},{"location":"software/rstudio/#r-encountered-a-fatal-error","title":"R encountered a fatal error","text":"<p>Full error message:</p> <pre><code>R encountered a fatal error. The session was terminated.\n</code></pre> <p></p> <p>This is because the home folder is full.</p> <p>Check this by using uquota.md.</p> How does that look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham3 ~]$ uquota\nYour project     Your File Area       Unit        Usage  Quota Limit  Over Quota\n---------------  -------------------  -------  --------  -----------  ----------\nhome             /home/sven           GiB          24.7           32\nhome             /home/sven           files       79180       300000\nnaiss2024-22-49  /proj/worldpeace     GiB           5.1          128\nnaiss2024-22-49  /proj/worldpeace     files       20276       100000\n</code></pre> <p>Candidates for files that are too big, that are hidden files:</p> <ul> <li><code>.RData</code></li> <li><code>.Renviron</code></li> <li><code>.Rhistory</code></li> </ul> <p>One can use <code>ls -all</code> to see all files, including hidden files:</p> <pre><code>ls --all\n</code></pre> How does that look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham2 ~]$ ls --all\n.                      .gtkrc               .nextflow.log.8\n..                     .ICEauthority        .nextflow.log.9\n.allinea               .ipython             .nv\n.bash_history          .java                .oracle_jre_usage\n.bash_logout           .jupyter             .pki\n.bash_profile          .kde                 private\n.bashrc                .keras               .profile\n.bashrc.save           .lesshst             .python_history\n.beast                 lib                  .r\nbin                    .lmod.d              R\n.cache                 .local               .RData\n.conda                 .login               .Rhistory\n.config                .MathWorks           .rstudio-desktop\n.cshrc                 .matlab              .ssh\n.dbus                  .mozilla             .subversion\nDNABERT_2              my_little_turtle.py  ticket_297538\n.emacs                 .nextflow            users\n.esd_auth              .nextflow.log        .viminfo\n.gitconfig             .nextflow.log.1      .vscode-oss\n.git-credential-cache  .nextflow.log.2      .vscode-server\nglob                   .nextflow.log.3      .wget-hsts\n.gnupg                 .nextflow.log.4      .Xauthority\n.gracetimefile         .nextflow.log.5      .xfce4-session.verbose-log\n.gradle                .nextflow.log.6      .xfce4-session.verbose-log.last\n.gstreamer-0.10        .nextflow.log.7      .zshrc\n</code></pre> <p>You can delete these hidden files, by:</p> <pre><code>rm .RData\nrm .Renviron\nrm .Rhistory\n</code></pre> For staff <p>Full report can be found at RT ticket 298623</p>","tags":["RStudio"]},{"location":"software/rstudio_on_bianca/","title":"RStudio on Bianca","text":"","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#rstudio-on-bianca","title":"RStudio on Bianca","text":"","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#introduction","title":"Introduction","text":"<p>RStudio is an IDE specialized for the R programming language.</p> What is an IDE? <p>See the page on IDEs.</p> <p>In this session, we show how to use RStudio on Bianca, using Bianca's remote desktop environment.</p> Forgot how to login to a remote desktop environment? <p>See the 'Logging in to Bianca' page.</p> <p>Spoiler: go to https://bianca.uppmax.uu.se/</p> <p>As RStudio is a resource-heavy program, it must be run on an interactive session.</p> Forgot how to start an interactive session? <p>See the 'Starting an interactive session' page.</p>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#procedure-to-start-rstudio","title":"Procedure to start RStudio","text":"<p>Below is a step-by-step procedure to start RStudio on Bianca.</p> Prefer a video? <p>This procedure is also demonstrated in this YouTube video.</p>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#1-get-within-sunet","title":"1. Get within SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#2-start-the-bianca-remote-desktop-environment","title":"2. Start the Bianca remote desktop environment","text":"Forgot how to start Bianca's remote desktop environment? <p>See the 'Logging in to Bianca' page.</p>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#3-start-an-interactive-session","title":"3. Start an interactive session","text":"<p>Within the Bianca remote desktop environment, start a terminal. Within that terminal, start an interactive session with 2 cores:</p> <p>Why two cores?</p> <p>RStudio is a resource-heavy program. Due to this, we recommend using at least two cores for a more pleasant user experience.</p> <pre><code>interactive -A [project_number] -n 2 -t 8:00:00\n</code></pre> <p>Where <code>[project_number]</code> is your UPPMAX project, for example:</p> <pre><code>interactive -A sens2016001 -n 2 -t 8:00:00\n</code></pre> What is my UPPMAX project number? <p>Easy answers that is probably true:</p> <p>The one you used to login, which is part of your prompt. For example, in the prompt below, the project is <code>sens2016001</code>.</p> <pre><code>[sven@sens2016001-bianca sven]$\n</code></pre> <p>Do not start RStudio from the menus</p> <p>You can start a version of RStudio from the menus. However, this will not have access to loaded modules.</p> <p>Instead, load RStudio from the module system instead.</p>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#4-load-the-modules-needed","title":"4. Load the modules needed","text":"<p>In the terminal of the interactive session, do:</p> <pre><code>module load R_packages/4.3.1 RStudio/2023.12.1-402\n</code></pre> Do all combinations of <code>R_packages</code> and <code>RStudio</code> work? <p>No.</p> <p>Not all combination of <code>R_packages</code> and <code>RStudio</code> work equally well, but this one is known to work (as it was used in this solved ticket).</p> <p>There have been issues using <code>RStudio/2023.06.2-561</code> together with <code>R/4.3.1</code></p> Shouldn't I load <code>R</code> first? <p>No.</p> <p>Loading <code>R_packages</code> will load the corresponding <code>R</code> module.</p> What happens if I do not load <code>R_packages</code>? <p>Then you will have RStudio running without any R packages installed</p>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#5-start-rstudio","title":"5. Start RStudio","text":"<p>With the modules loaded, start RStudio from the terminal (on the interactive session):</p> <pre><code>rstudio\n</code></pre> <p>RStudio can be slow to startup, as R has thousands (!) of packages. Additionally, at startup and if enabled, your saved RStudio workspace (with potentially a lot of data!) is read.</p> How does RStudio look on Bianca? <p>RStudio when starting up:</p> <p></p> <p>RStudio when started up:</p> <p></p> <p>RStudio in action:</p> <p></p> <p>The RStudio debugger, at the error message level:</p> <p></p> <p>The RStudio debugger, at the function-that-caused-the-error level:</p> <p></p> <p>The RStudio debugger, at the program level:</p> <p></p>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#troubleshooting","title":"Troubleshooting","text":"","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#rstudio-freezes-when-i-start-it-where-yesterday-it-still-worked","title":"RStudio freezes when I start it, where yesterday it still worked","text":"","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_bianca/#hypothesis-your-home-folder-is-full","title":"Hypothesis: Your home folder is full","text":"<p>Your home folder is full. That explains why it still worked yesterday: at that day, your home folder was not full yet.</p> <p>RStudio uses your home folder to store the things it needs, so when it is full, it cannot do its things.</p> <p>To confirm, from a terminal do:</p> <pre><code>du -h -d 1 .\n</code></pre> <p>This will show how much space the folders in your home folder take:</p> <p></p> <p>In this example, there is a folder called <code>wharf_backup</code> that is 4.5 gigabyte. Moving it to a project folder solved the problem:</p> <pre><code>mv wharf_backup/ /proj/nobackup/[your_project_folder]\n</code></pre> <p>For example:</p> <pre><code>mv wharf_backup/ /proj/nobackup/sens2016001\n</code></pre>","tags":["RStudio","Bianca"]},{"location":"software/rstudio_on_rackham/","title":"RStudio on Rackham","text":"","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#rstudio-on-rackham","title":"RStudio on Rackham","text":"","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#introduction","title":"Introduction","text":"<p>RStudio is an IDE specialized for the R programming language.</p> What is an IDE? <p>See the page on IDEs.</p> <p>In this session, we show how to use RStudio on Rackham, using Rackham's remote desktop environment.</p> Forgot how to login to a remote desktop environment? <p>See the 'Logging in to Rackham' page.</p> <p>Spoiler: go to https://rackham.uppmax.uu.se/</p> <p>As RStudio is a resource-heavy program, it must be run on an interactive session.</p> Forgot how to start an interactive session? <p>See the 'Starting an interactive session' page.</p>","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#procedure-to-start-rstudio","title":"Procedure to start RStudio","text":"<p>Below is a step-by-step procedure to start RStudio on Rackham.</p> Prefer a video? <p>This procedure is also demonstrated in this YouTube video.</p>","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#1-get-within-sunet","title":"1. Get within SUNET","text":"<p>This step is only needed when outside of Sweden.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#2-start-a-rackham-remote-desktop-environment","title":"2. Start a Rackham remote desktop environment","text":"<p>This can be either:</p> <ul> <li>Login to the Rackham remote desktop environment using the website</li> <li>Login to the Rackham remote desktop environment using a local ThinLinc client</li> </ul>","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#3-start-an-interactive-session","title":"3. Start an interactive session","text":"<p>Within the Rackham remote desktop environment, start a terminal. Within that terminal, start an interactive session with 2 cores:</p> <pre><code>interactive -A [naiss_project_id] -n 2 -t [duration]\n</code></pre> <p>Where:</p> <ul> <li><code>[naiss_project_id]</code> is your UPPMAX project code</li> <li><code>[duration]</code> is the duration of the interactive session</li> </ul> <p>Resulting in, For example:</p> <pre><code>interactive -A naiss2024-22-310 -n 2 -t 8:00:00\n</code></pre> <p>Why two cores?</p> <p>RStudio is a resource-heavy program. Due to this, we recommend using at least two cores for a more pleasant user experience.</p> What is an interactive session? <p>See start an interactive session</p> <p>Do not start RStudio from the menus</p> <p>You can start a version of RStudio from the menus. However, this will not have access to loaded modules.</p> <p>Instead, load RStudio from the module system instead.</p>","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#4-load-the-modules-needed","title":"4. Load the modules needed","text":"<p>In the terminal of the interactive session, do:</p> <pre><code>module load R/4.3.1 R_packages/4.3.1 RStudio/2023.12.1-402\n</code></pre> How does that look like? <p>Your output will be similar to:</p> <pre><code>[sven@r210 sven]$ module load R/4.3.1 R_packages/4.3.1 RStudio/2023.06.2-561\nR/4.3.1: Nearly all CRAN and BioConductor packages are installed and available by loading\nthe module R_packages/4.3.1\nR_packages/4.3.1: Note that loading some spatial analysis packages, especially geo-related packages, might\nR_packages/4.3.1: require you to load additional modules prior to use. monocle3 is such a package. See\nR_packages/4.3.1: 'module help R_packages/4.3.1'\n\nR_packages/4.3.1: The RStudio packages pane is disabled when loading this module, due to RStudio slowdowns\nR_packages/4.3.1: because there are &gt;20000 available packages. *All packages are still available.*  For\nR_packages/4.3.1: more information and instructions to re-enable the packages pane (not recommended) see\nR_packages/4.3.1: 'module help R_packages/4.3.1'\n\nRStudio/2023.12.1-402: Sandboxing is not enabled for RStudio at UPPMAX. See 'module help RStudio/2023.12.1-402' for more information\n</code></pre> What happens if I do not load <code>R</code> or <code>R_packages</code>? <p>Then you will have the sytem-wide R version 3.6.0 without any packages installed.</p> What does 'Sandboxing is not enabled for RStudio at UPPMAX' mean? <p>Nothing.</p> <p>Here is how it looks like:</p> <pre><code>[sven@r482 sven]$ module load RStudio/2023.06.2-561\nRStudio/2023.06.2-561: Sandboxing is not enabled for RStudio at UPPMAX. See 'module help RStudio/2023.06.2-561' for more information\n[sven@r482 sven]$ module help RStudio/2023.06.2-561\n\n--------------------------------------------------- Module Specific Help for \"RStudio/2023.06.2-561\" ----------------------------------------------------\n RStudio - use RStudio 2023.06.2-561\n\n Version 2023.06.2-561\n\nWith the Linux distribution used on most UPPMAX clusters (CentOS 7), RStudio/2023.06.2-561\nprefers to use a 'suid sandbox'. We do not enable this at UPPMAX. Instead, we disable sandboxing\nduring startup of RStudio by defining a shell alias for the 'rstudio' command. You may notice\nadditional errors in the terminal window from which you ran the 'rstudio' command. This is\nexpected and does not affect RStudio operation.\n\nFor performance reasons, UPPMAX disables checks for updates.\n\nUPPMAX also disables the 'Packages' pane of RStudio if an R_packages module is loaded.\n</code></pre>","tags":["RStudio","Rackham"]},{"location":"software/rstudio_on_rackham/#5-start-rstudio","title":"5. Start RStudio","text":"<p>With the modules loaded, start RStudio from the terminal (on the interactive session):</p> <pre><code>rstudio\n</code></pre> <p>RStudio can be slow to startup, as R has thousands (!) of packages. Additionally, at startup and if enabled, your saved RStudio workspace (with potentially a lot of data!) is read.</p> How does RStudio look on Rackham? <p>RStudio when starting up:</p> <p></p> <p>RStudio when started up:</p> <p></p> <p>RStudio when ready:</p> <p></p> <p>RStudio in action:</p> <p></p> <p>The RStudio debugger, at the error message level:</p> <p></p> <p>The RStudio debugger, at the function-that-caused-the-error level:</p> <p></p> <p>The RStudio debugger, at the program level:</p> <p></p>","tags":["RStudio","Rackham"]},{"location":"software/rsync/","title":"rsync","text":""},{"location":"software/rsync/#rsync","title":"<code>rsync</code>","text":"<p><code>rsync</code> is a command-line tool for file transfer, with the goal of ensuring integrity of the data, as well as a minimal amount of data transfer.</p> <p><code>rsync</code> can be used for copying, but also synchronizing files, such as is ideal for making a backup. At this page, we use the word 'copy', although <code>rsync</code> by default does a one-way synchronize: if the data is already there, it will do nothing.</p> <ul> <li>Using <code>rsync</code> on Bianca</li> <li>Using <code>rsync</code> on Rackham</li> </ul>"},{"location":"software/rsync/#installing-rsync","title":"Installing <code>rsync</code>","text":"<p>To installing <code>rsync</code>, see the official <code>rsync</code> download page.</p> Tip for Ubuntu users <p>Use <code>apt</code> like usual:</p> <pre><code>sudo apt install rsync\n</code></pre> Tip for Windows users <p>When looking to download an executable of <code>rsycn</code>, look for the words 'binary' (all executables are binary) and Cygwin (the environment in which the <code>rsync</code> executable was built on Windows).</p>"},{"location":"software/rsync/#copy-a-folder-from-local-to-rackham","title":"Copy a folder from local to Rackham","text":"<p>Copy a folder from a local computer to a Rackham home folder.</p> <p>On your local computer, do:</p> <pre><code>rsync --recursive [folder_name] [user_name]@rackham.uppmax.uu.se:/home/[user_name]/\n</code></pre> <p>For example:</p> <pre><code>rsync --recursive my_folder sven@rackham.uppmax.uu.se:/home/sven/\n</code></pre> <p>The <code>--recursive</code> flag is used to copy a folder and all of its subfolders.</p> Want to preserve timestamps? <p>To preserve the files' timestamps, use the <code>--archive</code> flag, e.g.</p> <pre><code>rsync --recursive --archive my_folder sven@rackham.uppmax.uu.se:/home/sven/\n</code></pre>"},{"location":"software/rsync/#copy-a-folder-from-rackham-to-local","title":"Copy a folder from Rackham to local","text":"<p>Copy a folder from Rackham to your local computer.</p> <p>On your local computer, do:</p> <pre><code>rsync --recursive [user_name]@rackham.uppmax.uu.se:/home/[user_name]/[folder_name] [local_folder_destination]\n</code></pre> <p>For example:</p> <pre><code>rsync --recursive sven@rackham.uppmax.uu.se:/home/sven/my_folder .\n</code></pre> <p>Where <code>.</code> means 'the folder where I am now'.</p> Want to preserve timestamps? <p>To preserve the files' timestamps, use the <code>--archive</code> flag, e.g.</p> <pre><code>rsync --recursive --archive my_folder sven@rackham.uppmax.uu.se:/home/sven/\n</code></pre>"},{"location":"software/rsync_on_bianca/","title":"rsync on Bianca","text":""},{"location":"software/rsync_on_bianca/#rsync-on-bianca","title":"<code>rsync</code> on Bianca","text":"<p><code>rsync</code> is a command-line tool for file transfer.</p> <p>This page describes how to use <code>rsync</code> on Bianca.</p> <p>Using <code>rsync</code> for direct file transfer from a local computer to wharf fails, as cannot <code>rsync</code> directly to wharf.</p> <p>It can be made to work (by using transit), as described in the UPPMAX Bianca file transfer using rsync.</p> How does it look like if I try to <code>rsync</code> directly to <code>wharf</code> anyways? <p>One cannot <code>rsync</code> directly to wharf.</p> <p>However, this is how it looks like:</p> <pre><code>sven@sven-N141CU:~$ rsync my_local_file.txt sven-sens2016001@bianca-sftp.uppmax.uu.se:/sven-sens2016001\n\nHi!\n\nYou are connected to the bianca wharf (sftp service) at\nbianca-sftp.uppmax.uu.se.\n\nNote that we only support SFTP, which is not exactly the\nsame as SSH (rsync and scp will not work).\n\nPlease see our homepage and the Bianca User Guide\nfor more information:\n\nhttps://www.uppmax.uu.se/support/user-guides/bianca-user-guide/\n\nIf you have any questions not covered by the User Guide, you are\nwelcome to contact us at support@uppmax.uu.se.\n\nBest regards,\nUPPMAX\n\nsven-sens2016001@bianca-sftp.uppmax.uu.se's password:\nprotocol version mismatch -- is your shell clean?\n(see the rsync manpage for an explanation)\nrsync error: protocol incompatibility (code 2) at compat.c(622) [sender=3.2.7]\n</code></pre> <p>If you want to do file transfer to/from Bianca, read the UPPMAX page on Bianca file transfer using rsync.</p>"},{"location":"software/rsync_on_bianca/#links","title":"Links","text":"<ul> <li><code>rsync</code> homepage</li> </ul>"},{"location":"software/rsync_on_rackham/","title":"rsync on Rackham","text":""},{"location":"software/rsync_on_rackham/#rsync-on-rackham","title":"<code>rsync</code> on Rackham","text":"<p><code>rsync</code> is a command-line tool for file transfer.</p> <p>This page describes how to use <code>rsync</code> on Rackham.</p>"},{"location":"software/rsync_on_rackham/#copy-a-folder-from-local-to-rackham","title":"Copy a folder from local to Rackham","text":"<pre><code>flowchart LR\n  local_computer[Your local computer. Run rsync from here]\n  rackham[Rackham]\n\n  local_computer --&gt; |rsync| rackham</code></pre> <p>Copy a folder from a local computer to a Rackham home folder.</p> <p>On your local computer, do:</p> <pre><code>rsync --recursive [folder_name] [user_name]@rackham.uppmax.uu.se:/home/[user_name]/\n</code></pre> <p>For example:</p> <pre><code>rsync --recursive my_folder sven@rackham.uppmax.uu.se:/home/sven/\n</code></pre> <p>The <code>--recursive</code> flag is used to copy a folder and all of its subfolders.</p>"},{"location":"software/rsync_on_rackham/#copy-a-folder-from-rackham-to-local","title":"Copy a folder from Rackham to local","text":"<pre><code>flowchart LR\n  local_computer[Your local computer. Run rsync from here]\n  rackham[Rackham]\n\n  rackham --&gt; |rsync| local_computer</code></pre> <p>Copy a folder from Rackham to your local computer.</p> <p>On your local computer, do:</p> <pre><code>rsync --recursive [user_name]@rackham.uppmax.uu.se:/home/[user_name]/[folder_name] [local_folder_destination]\n</code></pre> <p>For example:</p> <pre><code>rsync --recursive sven@rackham.uppmax.uu.se:/home/sven/my_folder .\n</code></pre> <p>Where <code>.</code> means 'the folder where I am now'.</p>"},{"location":"software/rsync_on_rackham/#links","title":"Links","text":"<ul> <li><code>rsync</code> homepage</li> </ul>"},{"location":"software/sbatch/","title":"sbatch","text":"","tags":["Slurm","scheduler","submit","sbatch","software"]},{"location":"software/sbatch/#sbatch","title":"<code>sbatch</code>","text":"<p>The job scheduler consists of many programs to manage jobs. <code>sbatch</code> is the program to submit a job to the scheduler.</p> <pre><code>flowchart TD\n  sbatch[sbatch: submit a job]\n  scancel[scancel: cancel a running job]\n  squeue[squeue: view the job queue]\n  sbatch --&gt; |Oops| scancel\n  sbatch --&gt; |Verify| squeue</code></pre> <p>After submitting a job, one can use <code>squeue</code> to verify the job is in the job queue. If there is an error in the <code>sbatch</code> command, one can cancel a job using <code>scancel</code>.</p>","tags":["Slurm","scheduler","submit","sbatch","software"]},{"location":"software/sbatch/#minimal-examples","title":"Minimal examples","text":"<p>There are two ways to demonstrate minimal use of <code>sbatch</code>:</p> <ul> <li>with command-line Slurm arguments:   easier to experiment with</li> <li>with Slurm parameters in the script:   easier when you know what you need</li> </ul> <p>These minimal examples use a run-time of a short, default time.</p>","tags":["Slurm","scheduler","submit","sbatch","software"]},{"location":"software/sbatch/#with-command-line-slurm-arguments","title":"with command-line Slurm arguments","text":"<p>To let Slurm schedule a job, one uses <code>sbatch</code>.</p> <p>For Bianca and Rackham, one uses <code>sbatch</code> like this:</p> <pre><code>sbatch -A [project_code] [script_filename]\nsbatch -M snowy -A [project_code] [script_filename]\n</code></pre> <p>For Snowy, one uses <code>sbatch</code> like this:</p> <pre><code>sbatch -M snowy -A [project_code] [script_filename]\n</code></pre> <p>Where:</p> <ul> <li><code>-A [project_code]</code>: the project to use,   for example <code>sens2017625</code></li> <li><code>[script_filename]</code>: the name of a file that is a bash script,   for example, <code>my_script.sh</code></li> <li><code>-M snowy</code>: if you use the Snowy computational resources</li> </ul> <p>Filling this all in, for Bianca and Rackham:</p> <pre><code>sbatch -A sens2017625 my_script.sh\n</code></pre> <p>Filling this all in, for Snowy:</p> <pre><code>sbatch -M snowy -A sens2017625 my_script.sh\n</code></pre> What is my project? <p>See the UPPMAX documentation on projects.</p> How do I convert my project name to the project code I need to use here? <p>See the UPPMAX documentation on projects.</p> What is in the script file? <p>The script file <code>my_script.sh</code> is a minimal example script. Such a minimal example script could be:</p> <pre><code>#!/bin/bash\necho \"Hello\"\n</code></pre>","tags":["Slurm","scheduler","submit","sbatch","software"]},{"location":"software/sbatch/#with-slurm-parameters-in-the-script","title":"with Slurm parameters in the script","text":"<p>The minimal command to use <code>sbatch</code> with Slurm parameters in the script:</p> <pre><code>sbatch [script_filename]\n</code></pre> <p>where <code>[script_filename]</code> the name of a bash script, for example:</p> <pre><code>sbatch my_script.sh\n</code></pre> <p>For Bianca and Rackham, the script must contain at least the following lines:</p> <pre><code>#SBATCH -A [project_code]\n</code></pre> <p>For Snowy, the script must contain at least the following lines:</p> <pre><code>#SBATCH -A [project_code]\n#SBATCH -M snowy\n</code></pre> <p>With:</p> <ul> <li><code>[project_code]</code>: the project code, for example <code>uppmax2023-2-25</code></li> </ul> What is in the script file, for Bianca and Rackham? <p>A full example script would be:</p> <pre><code>#!/bin/bash\n#SBATCH -A uppmax2023-2-25\necho \"Hello\"\n</code></pre> What is in the script file, for Snowy? <p>A full example script would be:</p> <pre><code>#!/bin/bash\n#SBATCH -A uppmax2023-2-25\n#SBATCH -M snowy\necho \"Hello\"\n</code></pre>","tags":["Slurm","scheduler","submit","sbatch","software"]},{"location":"software/sbatch/#more-parameters","title":"More parameters","text":"<p>See the Slurm documentation on sbatch</p>","tags":["Slurm","scheduler","submit","sbatch","software"]},{"location":"software/sbatch/#troubleshooting","title":"Troubleshooting","text":"<p>See Slurm troubleshooting</p>","tags":["Slurm","scheduler","submit","sbatch","software"]},{"location":"software/scancel/","title":"scancel","text":"","tags":["scancel"]},{"location":"software/scancel/#scancel","title":"<code>scancel</code>","text":"<p>The job scheduler consists of many programs to manage jobs. <code>scancel</code> is a tool to cancel jobs that are in the job queue or are running.</p> <p>Usage:</p> <pre><code>scancel [job_number]\n</code></pre> <p>Where the <code>[job_number]</code> is the number of the job. You can see the job number when submitting a job using <code>sbatch</code> and you can find it in the job queue (when doing <code>squeue</code>).</p> <p>For example:</p> <pre><code>[sven@rackham3 ~]$ sbatch -A my_project my_script.sh\nSubmitted batch job 49311056\n[sven@rackham3 ~]$ scancel 49311056\n[sven@rackham3 ~]$\n</code></pre>","tags":["scancel"]},{"location":"software/scp/","title":"scp","text":"","tags":["scp","secure copy protocol","program","software","tool","copy"]},{"location":"software/scp/#scp","title":"<code>scp</code>","text":"<p>The program <code>scp</code> allows you to transfer files to/from the UPPMAX HPC clusters from a md</p> <p>Although <code>scp</code> is an abbreviation of 'Secure copy protocol', it is not considered 'secure' anymore: instead, it is considered an outdated protocol: <code>rsync</code> is a similar tool that is considered secure.</p> <p>In general, using <code>scp</code> to copy files from a certain location to another location, looks like this:</p> <pre><code>scp [from] [to]\n</code></pre> <p>UPPMAX guides can be found here:</p> Between UPPMAX guide Local and Bianca Does not work, see Bianca file transfer Local and Pelle File transfer to/from Pelle using <code>scp</code> Local and Rackham File transfer to/from Rackham using <code>scp</code>","tags":["scp","secure copy protocol","program","software","tool","copy"]},{"location":"software/screen/","title":"Screen","text":""},{"location":"software/screen/#running-a-detachable-screen-process-in-a-job","title":"Running a detachable screen process in a job","text":"<p>When you run the <code>interactive</code> command, you get a command prompt in the screen program.</p> <p>Warning</p> <p>When running the screen program in other environments, you can detach from your screen and later reattach to it. Within the environment of the <code>interactive</code> command, you lose this ability: Your job is terminated when you detach. (This is a design decision and not a bug.)</p> <p>In case you want the best of both worlds, i.e. to be able to detach and reattach to your screen program within a job, you need to start a job in some other way and start your screen session from a separate ssh login. Here is an example of how you can do this:</p> <pre><code>$ salloc -A project_ID -t 15:00  -n 1 --qos=short --bell --no-shell\nsalloc: Pending job allocation 46964140\nsalloc: job 46964140 queued and waiting for resources\nsalloc: job 46964140 has been allocated resources\nsalloc: Granted job allocation 46964140\nsalloc: Waiting for resource configuration\nsalloc: Nodes r174 are ready for job\n</code></pre> <p>Check the queue manager for the allocated node. In the example bellow, one core was allocated on <code>r174</code> compute node.</p> <pre><code>$ squeue -j 46964140\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          46964140      core no-shell     user  R       0:44      1 r174\n</code></pre> <p>You can start <code>xterm</code> terminal in this allocated session like this:</p> <pre><code>xterm -e ssh -AX r174 &amp;\n</code></pre> <p><code>salloc</code> command gives you a job allocation of one node for 15 minutes (the \"--no-shell\" option is important here). Instead you can log in to any node of any of your running jobs, started with e.g. the <code>sbatch</code> command.</p> <p>You get a job number and from that you can find out the node name, in this example r174.</p> <p>When you log in to the node with the <code>ssh</code> command, start the screen program:</p> <pre><code>screen\n</code></pre> <p>When you detach from the screen program, with e.g. the \"d\" command, you can later in the same ssh session or in another ssh session reattach to your screen session:</p> <pre><code>screen -r\n</code></pre> <p>When your job has terminated, you can neither reattach to your screen session nor log in to the node.</p> <p>The screen session of the <code>interactive</code> command is integrated into your job, so e.g. all environment variables for the job is correctly assigned. For a separate ssh session, as in this example, that is not the case.</p> <p>Please note that it is the job allocation that determines your core hour usage and not your ssh or screen sessions.</p>"},{"location":"software/screen/#tips","title":"Tips","text":"<ul> <li> <p>Start a new screen session with a command:</p> <pre><code>screen -dm your_command\n</code></pre> <p>This will start a new screen session, run the command, and then detach from the session.</p> </li> <li> <p>If you want to run multiple commands, you can do so like this:</p> <pre><code>screen -dm bash -c \"command1; command2\"\n</code></pre> <p>This will run <code>command1</code> and <code>command2</code> in order.</p> </li> <li> <p>To reattach to the screen session, use:</p> <pre><code>screen -r\n</code></pre> <p>If you have multiple sessions, you'll need to specify the session ID.</p> </li> <li> <p>To list your current screen sessions, use:</p> <pre><code>screen -ls\n</code></pre> </li> </ul> <p>Please note that when a program terminates, <code>screen</code> (by default) kills the window that contained it. If you don't want your session to get killed after the script is finished, add <code>exec sh</code> at the end. For example:</p> <pre><code>screen -dm bash -c 'your_command; exec sh'\n</code></pre> <p>This will keep the screen session alive after <code>your_command</code> has finished executing.</p> <p>YouTube : How to use GNU SCREEN - the Terminal Multiplexer</p>"},{"location":"software/seff/","title":"seff","text":"","tags":["Slurm","scheduler","efficiency","tool","software"]},{"location":"software/seff/#seff","title":"<code>seff</code>","text":"<p><code>seff</code> is a tool to determine the efficiency of Slurm jobs. On UPPMAX, use jobstats instead.</p> Why is this on the UPPMAX pages? <p>As there may be plans to have it on UPPMAX too.</p> How does its output look like? <p>Output will be similar to this, as run on Dardel:</p> <pre><code>svensv@login1:~&gt; seff 123456\nJob ID: 123456\nCluster: dardel\nUser/Group: bobek/bobek\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 256\nCPU Utilized: 1-07:31:54\nCPU Efficiency: 21.56% of 6-02:16:32 core-walltime\nJob Wall-clock time: 00:34:17\nMemory Utilized: 7.26 GB\nMemory Efficiency: 3.27% of 222.00 GB\n</code></pre> HPC cluster Has <code>seff</code> installed? Dardel Yes Rackham No, use jobstats instead","tags":["Slurm","scheduler","efficiency","tool","software"]},{"location":"software/sftp/","title":"SFTP","text":"","tags":["sftp","SFTP"]},{"location":"software/sftp/#sftp","title":"<code>sftp</code>","text":"<p><code>sftp</code> is a tool to transfer data.</p>","tags":["sftp","SFTP"]},{"location":"software/sftp/#1-getting-help","title":"1. Getting Help","text":"<p>Once, you in the <code>sftp</code> prompt, check the available commands by typing <code>?</code> or <code>help</code> at command prompt. This will print out a list of the available commands and give a short description of them. We'll cover the most common ones in this guide.</p> <pre><code>sftp&gt; ?\nAvailable commands:\ncd path                       Change remote directory to 'path'\n...\n...\n...\n</code></pre>","tags":["sftp","SFTP"]},{"location":"software/sftp/#2-check-present-working-directory","title":"2. Check Present Working Directory","text":"<p>The command <code>lpwd</code> is used to check the Local present working directory, whereas <code>pwd</code> command is used to check Remote working directory.</p> <pre><code>sftp&gt; lpwd\nLocal working directory: /\nsftp&gt; pwd\nRemote working directory: /tecmint/\nlpwd \u2013 print the current directory on your system\npwd \u2013 print the current directory on the ftp server\n</code></pre>","tags":["sftp","SFTP"]},{"location":"software/sftp/#3-listing-files","title":"3. Listing Files","text":"<p>Listing files and directories in local as well as remote system.</p> <p>On Remote</p> <pre><code>sftp&gt; ls\n</code></pre> <p>On Local</p> <pre><code>sftp&gt; lls\n</code></pre>","tags":["sftp","SFTP"]},{"location":"software/sftp/#4-upload-file","title":"4. Upload File","text":"<p>Put single or multiple files in remote system.</p> <pre><code>sftp&gt; put local.profile\n</code></pre> <p>Uploading local.profile to /tecmint/local.profile</p>","tags":["sftp","SFTP"]},{"location":"software/sftp/#5-upload-multiple-files","title":"5. Upload Multiple Files","text":"<p>Putting multiple files on remote system.</p> <pre><code>sftp&gt; mput *.xls\n</code></pre> <p>Another alternative to uploading many files is to tar and/or compress the files to a single file before uploading. The file transfer will stop in between every file, so the more file you have to upload the more stops it will make. This can have a dramatic impact on transfer speed if there are 1000s of files that you want to transfer. Running tar and/or zip on the files before transferring them will package all the files into a single file, so there will be no stops at all during the transfer.</p>","tags":["sftp","SFTP"]},{"location":"software/sftp/#6-download-files","title":"6. Download Files","text":"<p>Getting single or multiple files in local system.</p> <pre><code>sftp&gt; get SettlementReport_1-10th.xls\n</code></pre> <p>Fetching /tecmint/SettlementReport_1-10th.xls to SettlementReport_1-10th.xls Get multiple files on a local system.</p> <pre><code>sftp&gt; mget *.xls\n</code></pre> <p>Note: As we can see by default the get command downloads the file to the local system with the same name. We can download remote file and store it with a different name by specifying the name at the end. (This applies only while downloading single file).</p>","tags":["sftp","SFTP"]},{"location":"software/sftp/#7-switching-directories","title":"7. Switching Directories","text":"<p>Switching from one directory to another directory in local and remote locations.</p> <p>On Remote</p> <pre><code>sftp&gt; cd test\n</code></pre> <p>On Local</p> <pre><code>sftp&gt; lcd Documents\n</code></pre>","tags":["sftp","SFTP"]},{"location":"software/sftp/#8-create-directories","title":"8. Create Directories","text":"<p>Creating new directories on remote and local locations.</p> <pre><code>sftp&gt; mkdir test\nsftp&gt; lmkdir Documents\n</code></pre>","tags":["sftp","SFTP"]},{"location":"software/sftp/#9-remove-directory-or-file","title":"9. Remove Directory or File","text":"<p>Remove directory or file in remote system.</p> <pre><code>sftp&gt; rm Report.xls\nsftp&gt; rmdir sub1\n</code></pre> <p>Note: To remove/delete any directory from remote location, the directory must be empty.</p>","tags":["sftp","SFTP"]},{"location":"software/sftp/#10-exit-sftp-shell","title":"10. Exit sFTP Shell","text":"<p>The <code>!</code> (exclamation mark) command drops us in local shell from where we can execute Linux commands. Type <code>exit</code> command where we can see sftp&gt; prompt return.</p> <pre><code>sftp&gt; !\n[root@sftp ~]# exit\nShell exited with status 1\nsftp&gt;\n</code></pre>","tags":["sftp","SFTP"]},{"location":"software/sinfo/","title":"sinfo","text":""},{"location":"software/sinfo/#sinfo","title":"sinfo","text":"<p><code>sinfo</code> is a tool to view information about Slurm nodes and partitions.</p> How does that look like on Bianca? <pre><code>[sven@sens2016001-bianca ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nall        down 10-00:00:0    204 drain* sens2016001-b[1-8,10-204,1178]\nall        down 10-00:00:0     89   unk* sens2016001-b[205-210,301-312,1073-1084,1119-1177]\nall        down 10-00:00:0      1   idle sens2016001-b9\nnode         up 10-00:00:0    204 drain* sens2016001-b[1-8,10-204,1178]\nnode         up 10-00:00:0     89   unk* sens2016001-b[205-210,301-312,1073-1084,1119-1177]\nnode         up 10-00:00:0      1   idle sens2016001-b9\ncore*        up 10-00:00:0    204 drain* sens2016001-b[1-8,10-204,1178]\ncore*        up 10-00:00:0     89   unk* sens2016001-b[205-210,301-312,1073-1084,1119-1177]\ncore*        up 10-00:00:0      1   idle sens2016001-b9\ndevel        up    1:00:00    192 drain* sens2016001-b[10-200,1178]\ndevel        up    1:00:00     71   unk* sens2016001-b[1073-1084,1119-1177]\ndevel        up    1:00:00      1   idle sens2016001-b9\ndevcore      up    1:00:00    192 drain* sens2016001-b[10-200,1178]\ndevcore      up    1:00:00     71   unk* sens2016001-b[1073-1084,1119-1177]\ndevcore      up    1:00:00      1   idle sens2016001-b9\n</code></pre> <p>Although it may seem unexpected that only 1 node is idle, this is the expected behavior from a virtual cluster: most physical nodes are not allocated to this project and hence unavailable.</p> How does that look like on Rackham? <pre><code>[sven@rackham3 ~]$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nall        down 10-00:00:0     22   comp r[2,36,66,68,94,110,112,132,139,163,185,200,206,216,247,281,288,293,319,326,418,481]\nall        down 10-00:00:0     10   plnd r[49-50,58-60,63,283-285,287]\nall        down 10-00:00:0     72 drain$ r[1001-1072]\nall        down 10-00:00:0     18 drain* r[167,175,186,252,258,318,431,437-438,440,455-462]\nall        down 10-00:00:0     45  down* r[13,23,57,99,108-109,122,165,177-184,187,218,254,331,423,432-436,439,441,452,463-470,479,483-484,1189-1190,1199,1212,1240]\nall        down 10-00:00:0      8  drain r[29,35,78,154,212,226,335,485]\nall        down 10-00:00:0    115    mix r[37-41,43,45-46,65,70-72,76-77,79,85,98,102,106,116,120,127-128,135-136,142,146,152-153,161,169,171-172,174,189,210-211,222,227,230-231,234,237,243,250,260,264,266,273,275-276,280,289,292,302,311,313-314,316-317,332-333,344,360-361,363-365,368,373,376,382,386-388,391,393-395,398,402-403,410,417,422,425,430,449,453,472-473,475-477,480,482,486,1180-1181,1203,1208,1210-1211,1217,1223,1227,1231,1235,1237,1239,1242-1246]\nall        down 10-00:00:0    317  alloc r[1,3,6,9,19,25-28,30,32-34,42,44,47-48,51-56,62,64,67,69,73-75,80-84,86-93,95-97,100-101,103-105,107,111,113-115,117,119,121,123-126,129-131,133-134,137-138,140-141,143,147-151,155-160,162,164,166,168,170,173,176,188,190-199,201-205,207-209,213-215,217,220-221,223-225,228-229,232-233,235-236,238-242,244-246,248-249,251,253,255-257,259,261-263,265,267-272,274,277,279,282,286,290-291,294-301,303-310,312,315,320-325,327-330,334,336-343,345-359,362,366-367,369-372,374-375,377-381,383-385,389-390,392,396-397,399-401,404-409,411-416,419-421,424,426-429,442-448,450-451,454,471,474,478,1179,1182-1188,1191-1198,1200-1202,1204-1207,1209,1213-1216,1218-1222,1224-1226,1228-1230,1232-1234,1236,1238,1241,1247-1250]\nall        down 10-00:00:0     13   idle r[8,10-12,14-18,20-22,24]\nall        down 10-00:00:0     10   down r[4-5,7,31,61,118,144-145,219,278]\ncore*        up 10-00:00:0     21   comp r[36,66,68,94,110,112,132,139,163,185,200,206,216,247,281,288,293,319,326,418,481]\ncore*        up 10-00:00:0     10   plnd r[49-50,58-60,63,283-285,287]\ncore*        up 10-00:00:0     72 drain$ r[1001-1072]\ncore*        up 10-00:00:0     18 drain* r[167,175,186,252,258,318,431,437-438,440,455-462]\ncore*        up 10-00:00:0     41  down* r[57,99,108-109,122,165,177-184,187,218,254,331,423,432-436,439,441,452,463-470,479,1189-1190,1199,1212,1240]\ncore*        up 10-00:00:0      5  drain r[35,78,154,212,226]\ncore*        up 10-00:00:0    114    mix r[37-41,43,45-46,65,70-72,76-77,79,85,98,102,106,116,120,127-128,135-136,142,146,152-153,161,169,171-172,174,189,210-211,222,227,230-231,234,237,243,250,260,264,266,273,275-276,280,289,292,302,311,313-314,316-317,332-333,344,360-361,363-365,368,373,376,382,386-388,391,393-395,398,402-403,410,417,422,425,430,449,453,472-473,475-477,480,482,1180-1181,1203,1208,1210-1211,1217,1223,1227,1231,1235,1237,1239,1242-1246]\ncore*        up 10-00:00:0    301  alloc r[33-34,42,44,47-48,51-56,62,64,67,69,73-75,80-84,86-93,95-97,100-101,103-105,107,111,113-115,117,119,121,123-126,129-131,133-134,137-138,140-141,143,147-151,155-160,162,164,166,168,170,173,176,188,190-199,201-205,207-209,213-215,217,220-221,223-225,228-229,232-233,235-236,238-242,244-246,248-249,251,253,255-257,259,261-263,265,267-272,274,277,279,282,286,290-291,294-301,303-310,312,315,320-325,327-330,334,340,342-343,345-359,362,366-367,369-372,374-375,377-381,383-385,389-390,392,396-397,399-401,404-409,411-416,419-421,424,426-429,442-448,450-451,454,471,474,478,1179,1182-1188,1191-1198,1200-1202,1204-1207,1209,1213-1216,1218-1222,1224-1226,1228-1230,1232-1234,1236,1238,1241,1247-1250]\ncore*        up 10-00:00:0      6   down r[61,118,144-145,219,278]\nnode         up 10-00:00:0     22   comp r[2,36,66,68,94,110,112,132,139,163,185,200,206,216,247,281,288,293,319,326,418,481]\nnode         up 10-00:00:0     10   plnd r[49-50,58-60,63,283-285,287]\nnode         up 10-00:00:0     18 drain* r[167,175,186,252,258,318,431,437-438,440,455-462]\nnode         up 10-00:00:0     38  down* r[13,23,57,99,108-109,122,165,177-184,187,218,254,331,423,432-436,439,441,452,463-470,479]\nnode         up 10-00:00:0      7  drain r[29,35,78,154,212,226,335]\nnode         up 10-00:00:0     96    mix r[37-41,43,45-46,65,70-72,76-77,79,85,98,102,106,116,120,127-128,135-136,142,146,152-153,161,169,171-172,174,189,210-211,222,227,230-231,234,237,243,250,260,264,266,273,275-276,280,289,292,302,311,313-314,316-317,332-333,344,360-361,363-365,368,373,376,382,386-388,391,393-395,398,402-403,410,417,422,425,430,449,453,472-473,475-477,480,482]\nnode         up 10-00:00:0    268  alloc r[1,3,6,9,19,25-28,30,32-34,42,44,47-48,51-56,62,64,67,69,73-75,80-84,86-93,95-97,100-101,103-105,107,111,113-115,117,119,121,123-126,129-131,133-134,137-138,140-141,143,147-151,155-160,162,164,166,168,170,173,176,188,190-199,201-205,207-209,213-215,217,220-221,223-225,228-229,232-233,235-236,238-242,244-246,248-249,251,253,255-257,259,261-263,265,267-272,274,277,279,282,286,290-291,294-301,303-310,312,315,320-325,327-330,334,336-343,345-359,362,366-367,369-372,374-375,377-381,383-385,389-390,392,396-397,399-401,404-409,411-416,419-421,424,426-429,442-448,450-451,454,471,474,478]\nnode         up 10-00:00:0     13   idle r[8,10-12,14-18,20-22,24]\nnode         up 10-00:00:0     10   down r[4-5,7,31,61,118,144-145,219,278]\ndevel        up    1:00:00      2  down* r[483-484]\ndevel        up    1:00:00      1  drain r485\ndevel        up    1:00:00      1    mix r486\ndevcore      up    1:00:00      2  down* r[483-484]\ndevcore      up    1:00:00      1  drain r485\ndevcore      up    1:00:00      1    mix r486\n</code></pre>"},{"location":"software/singularity/","title":"Singularity/Apptainer","text":""},{"location":"software/singularity/#singularity-user-guide","title":"Singularity User Guide","text":"<p>Singularity www.sylabs.io/docs provide tools for running containers that are more suitable to traditional HPC environments when some other tools such as Docker or lxc. These containers can be portable and could be run both on your desktop machine and our clusters.</p> <p>One of the ways in which Singularity is more suitable for HPC is that it very actively restricts permissions so that you do not gain access to additional resources while inside the container. One consequence of this is that some common tools like ping or sudo do not work when run within a container (as a regular user).</p> <p>Singularity is installed and usable to run custom container images on the clusters bianca and rackham.</p>"},{"location":"software/singularity/#pulling-an-existing-singularity-image","title":"Pulling an existing Singularity image","text":"<p>It's possible to download and run pre-built images from the Singularity hub https://singularity-hub.org and the Singularity library (https://cloud.sylabs.io) using the singularity pull sub command such as:</p> <pre><code>singularity pull library://ubuntu\n</code></pre> <p>Which will download the requested image and place it in the current directory. You can also upload and run the image directly yourself.</p>"},{"location":"software/singularity/#creating-a-singularity-container","title":"Creating a Singularity container","text":"<p>See creating a Singularity container for the multiple ways how to build a Singularity container.</p>"},{"location":"software/singularity/#examples","title":"Examples","text":"<ul> <li>Create a Singularity container from conda</li> <li>Create a Singularity container for an R package</li> <li>Create a Singularity container from Docker Hub</li> <li>Create a Singularity container from a <code>docker pull</code></li> </ul>"},{"location":"software/singularity/#running-an-existing-image","title":"Running an existing image","text":"<p>Once you have an image, you can \"run\" it with a command such as</p> <pre><code>singularity run singularityhub-ubuntu-14.04.img\n</code></pre> <p>which will try to execute a \"run\" target in the container. There are also the <code>shell</code> and <code>exec</code> subcommands for starting a shell and running a specific command respectively.</p>"},{"location":"software/singularity/#access-to-uppmax-file-systems","title":"Access to UPPMAX file systems","text":"<p>By default, singularity will try to help and map the UPPMAX file systems from the current cluster so that they can be accessed from within the container. For CentOS7 based clusters (snowy, rackham, bianca), this works as expected.</p> <p>Singularity is installed on the system (on each separate node) and does not require any module load to be available.</p> <p>It's possible to run Docker containers. You can try to run</p> <pre><code>singularity shell docker://debian:stretch\n</code></pre> <p>but note that Docker containers are typically designed to run with more privileges than are allowed with Singularity, so it's quite possible things do not work as expected.</p>"},{"location":"software/singularity/#not-all-images-may-work-everywhere","title":"Not all images may work everywhere","text":"<p>Images run with the same linux kernel as the rest of the system. For HPC we systems, the kernel used tend to be quite old for stability reasons. This is not normally a problem, but can cause issues if the libraries of the images you try to run expects functionality added in newer kernels. How and what works is difficult to know without trying, but we have successfully started a shell in an image for the currently most recent Ubuntu release (17.04).</p>"},{"location":"software/spack/","title":"Spack","text":""},{"location":"software/spack/#spack-on-uppmax","title":"Spack on UPPMAX","text":""},{"location":"software/spack/#introduction","title":"Introduction","text":"<p>Spack is a simple package management tool or installer that also installs dependencies automatically to the main software. Installing a new software version does not break existing installations, so many configurations can coexist on the same system.</p> <p>It offers a simple spec syntax so that users can specify versions and configuration options concisely. Spack is also simple for package authors: package files are written in pure Python, and specs allow package authors to maintain a single file for many different builds of the same package.</p> <p>Spack documentation</p> <p>The UPPMAX staff has already other ways to install most software applications. Please use Spack only if other ways to install your tool is not possible or very difficult, e.g. requiring very many dependencies and it is not available through, e.g. EasyBuild (that the staff can manage centrally). One or the reasons is that SPACK produces very many small files and that having two parallel build systems centrally may make things a little complex.</p> <p>This guide may change with time. Please come back and see updates.</p> <p>This version assumes no available SPACK module, which may come in the near future. You have your own instance of Spack but can get a configuration file provided by UPPMAX.</p>"},{"location":"software/spack/#first-steps-installing-your-own-instance-of-spack","title":"First steps: Installing your own instance of SPACK","text":"<p>You may want to use your project folder if you want your colleagues to be able to run the application. Then change directory to a good place before installing Spack.</p> <pre><code>cd &lt;good place&gt;\n</code></pre>"},{"location":"software/spack/#step-1-clone-spack","title":"Step 1: clone spack","text":"<pre><code>module load git\ngit clone -c feature.manyFiles=true https://github.com/spack/spack.git\ncd spack\n</code></pre> <p>To get version v0.18:</p> <pre><code>git checkout releases/v0.18\n</code></pre> <p>Next, add Spack to your path. Spack has some nice command-line integration tools, so instead of simply appending to your PATH variable, source the Spack setup script.</p> <pre><code>source &lt;root dir of spack&gt;/spack/share/spack/setup-env.sh\n</code></pre> <p>Adding this line to your <code>~/.bashrc</code> as well will activate the \"spack commands\" each time you start a new terminal session.</p>"},{"location":"software/spack/#orientation-of-the-spack-files","title":"Orientation of the SPACK files","text":"<p>The Spack oriented files are stored in two places:</p> <ul> <li>Spack directory<ul> <li>the cloned git repository</li> <li>directories (important in bold)<ul> <li>bin        spack executables</li> <li>etc        configuration files</li> <li>lib         libraries</li> <li>share       documentation, scripts etc...</li> <li>var        other settings</li> <li>opt        produced after first installation, contains all packages (tools, dependencies and libraries)<ul> <li>tools are found in a tree: .<code>..opt/spack/linux-&lt;arch&gt;/&lt;compiler&gt;/tool/</code></li> </ul> </li> </ul> </li> </ul> </li> <li>.spack<ul> <li>local config and packages files</li> <li>directories (important in bold)<ul> <li>bootstrap</li> <li>cache</li> <li>reports</li> <li>linux<ul> <li>\u200bcompilers.yaml</li> <li>packages.yaml</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>The .yaml files in the .spack/linux directory contains information which tolls you want to include from the UPPMAX system.</p> <ul> <li>The compilers.yaml file lists the compilers (intel or gcc) modules available to build your software tool.</li> <li>The packages.yaml file lists tools available already as modules.</li> </ul> <p>By default, these files are empty but you can copy working \"central\" files that can be extended for your needs. The content of the files can be larger than the needed packages/compilers, i.e. only the packages /dependencies needed for your installation will be \"taken\" from these files and the rest will be ignored. Therefore, the UPPMAX staff may update these central files once in a while.</p>"},{"location":"software/spack/#get-templates","title":"Get templates","text":"<p>Do the following to get these templates (be sure to not overwrite old versions of these .yaml files that you configured yourself and might need).</p> <pre><code>cp /sw/build/spack/0.17.1/src/spack/share/spack/templates/compilers.yaml ~/.spack/linux/\ncp /sw/build/spack/0.17.1/src/spack/share/spack/templates/packages.yaml ~/.spack/linux/\n</code></pre>"},{"location":"software/spack/#install-your-program","title":"Install your program","text":"<p>Check available software applications via Spack:</p> <pre><code>spack list\nspack list &lt;search string&gt;\n</code></pre> <p>Check already installed software applications with spack</p> <pre><code>spack find\nspack find &lt;search string&gt;\n</code></pre> <p>Some installations won't need any compilers or \"large dependencies\". The installation is straightforward:</p> <pre><code>spack install &lt;tool&gt;\n</code></pre> <p>Example:</p> <pre><code>spack install zlib\n</code></pre> <p>In other cases, for larger applications tools that require larger dependencies (that we might already have as modules), watch the installation documentation to see what is needed. Any recommended compiler? You can also check with a \"dry run\" before installing, to see what Spack \"thinks\" its needs to install. Use the spec command:</p> <pre><code>spack spec -I &lt;tool&gt;\n</code></pre> <p>To check the presently, for Spack, available compilers, type:</p> <pre><code>spack compilers\n</code></pre> <p>If your desired compiler is not there you can add it by first loading the module and then integrate it into the compilers.yaml file with a spack command:</p> <p>Example:</p> <pre><code>module load intel/20.4\nspack compiler add\n</code></pre> <p>You can check if the compiler was added, either in the .spack/linux/compilers.yaml file or directly by:</p> <pre><code>spack compilers\n</code></pre> <p>To install a tool with a certain compiler version, if there are several compilers added for Spack, use \"%\". For specific version of the software tool or package, use \"@\".</p> <pre><code>spack install &lt;tool&gt;%&lt;compiler&gt;@&lt;compiler-version&gt;\n</code></pre> <p>Example:</p> <pre><code>spack install zlib%gcc@5.3.0\n</code></pre> <p>Large application tools may take a couple of hours so might be good to run in an interactive session (4 cores, -n 4).</p> <pre><code>spack install -j 4 &lt;tool&gt;\n</code></pre> <p>Use dependencies already available from our environment module system) ('module load').</p> <pre><code>cat .spack/linux/packages.yaml\n</code></pre> <p>Fill it with text,defining the spack name and lmod module names (be careful with indentations) Then install you tool, as above. To install a specific version of a dependency with Spack, use the command \"^\":</p> <pre><code>spack install &lt;tool&gt;%&lt;compiler&gt;@&lt;compiler-version&gt;^&lt;dependency&gt;@&lt;version&gt;\n</code></pre> <p>Here is a summarizing table</p> Command Option @ Which version % which compiler ^ which dependency"},{"location":"software/spack/#use-your-tool","title":"Use your tool","text":"<pre><code>$ spack load &lt;tool&gt;\n# module load of the install dependencies will not be needed here, since their paths are integrated in spack\n$ &lt;tool&gt; [&lt;arguments&gt;]\n</code></pre>"},{"location":"software/spack/#develop","title":"Develop","text":"<p>More to come... Meanwhile:</p> <p>Developer guide</p> <p>Developer workflows tutorial</p> <p>The builds are by default located here: <code>&lt;spack-root&gt;/opt/spack/linux-centos7-broadwell/&lt;compiler-version&gt;/</code></p>"},{"location":"software/spack/#packages-and-environments","title":"Packages and environments","text":"<p>More to come... Meanwhile:</p> <p>Packaging guide</p> <p>Environments guide</p> <p>Environments tutorial</p>"},{"location":"software/spack/#garbage-collection","title":"Garbage collection","text":"<p>Installing and uninstalling software will in the end use up your disk space so it is good practice to do some garbage collection</p> <pre><code>spack gc\n</code></pre>"},{"location":"software/squeue/","title":"squeue","text":"","tags":["squeue"]},{"location":"software/squeue/#squeue","title":"<code>squeue</code>","text":"<p>The job scheduler consists of many programs to manage jobs. <code>squeue</code> is a tool to view information about the job queues.</p>","tags":["squeue"]},{"location":"software/squeue/#view-all-jobs","title":"View all jobs","text":"","tags":["squeue"]},{"location":"software/squeue/#view-all-jobs-in-the-bianca-or-rackham-queue","title":"View all jobs in the Bianca or Rackham queue","text":"<p>View all jobs in the Bianca or Rackham queue:</p> <pre><code>squeue\n</code></pre> How does that look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham1 ~]$ squeue | head -n 1; squeue | shuf | head\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n          49086999      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49086465      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49085829      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49086067      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49086600      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49087075      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49080199      node /proj/sn torsteng PD       0:00      1 (Priority)\n          49088741      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49086825      core sbatch_l matca755 PD       0:00      1 (Priority)\n          49087385      core sbatch_l matca755 PD       0:00      1 (Priority)\n</code></pre>","tags":["squeue"]},{"location":"software/squeue/#view-all-jobs-in-snowy-queue","title":"View all jobs in Snowy queue","text":"<p>View all jobs in the Snowy queue:</p> <pre><code>squeue -M snowy\n</code></pre> How does that look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham1 ~]$ squeue -M snowy\nCLUSTER: snowy\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           9642748      core blast2un qiuzh610 PD       0:00      1 (Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions)\n           9642749      core blast2un qiuzh610 PD       0:00      1 (Priority)\n           9642750      core blast2un qiuzh610 PD       0:00      1 (Priority)\n           9642751      core blast2un qiuzh610 PD       0:00      1 (Priority)\n           9640955      core interact    teitu  R 1-00:09:18      1 s201\n           9642778      core snakejob yildirim  R       9:18      1 s25\n           9641765      core Ridge_al yildirim  R   17:28:32      1 s201\n           9642747      core blast2un qiuzh610  R      31:48      1 s33\n           6968659      core  bpe_nmt moamagda RD       0:00      1 (Reservation uppmax2022-2-18_4 was deleted)\n           6968658      core  bpe_nmt moamagda RD       0:00      1 (Reservation uppmax2022-2-18_4 was deleted)\n           6968656      core word_nmt moamagda RD       0:00      1 (Reservation uppmax2022-2-18_4 was deleted)\n           6968644      core word_nmt  matsten RD       0:00      1 (Reservation uppmax2022-2-18_4 was deleted)\n           9642777      node P20608_5    teitu PD       0:00      1 (Resources)\n           9642764      node     flye   octpa7  R    8:14:14      1 s9\n           9641505      node Fed_3_10  koussai  R   21:48:40      1 s73\n           9639430      node hmm_alig   ninaza  R 8-16:57:07      1 s149\n           9642775      node rhd0_st3    ariah  R      31:58      8 s[123-124,126-129,131,133]\n           9642763      node rhd1_st3    ariah  R   13:57:58      8 s[121,139,141,143-145,147-148]\n           9639541   veryfat interact  nikolay PD       0:00      1 (ReqNodeNotAvail, UnavailableNodes:s230)\n           9545835   veryfat     BAND    baldo PD       0:00      1 (AssocMaxCpuMinutesPerJobLimit)\n           9639540   veryfat interact  nikolay  R 7-21:34:31      1 s229\n</code></pre>","tags":["squeue"]},{"location":"software/squeue/#view-your-jobs-in-the-queue","title":"View your jobs in the queue","text":"","tags":["squeue"]},{"location":"software/squeue/#view-your-jobs-in-the-bianca-or-rackham-queue","title":"View your jobs in the Bianca or Rackham queue","text":"<p>View your jobs in the in the Bianca or Rackham queue:</p> <pre><code>squeue --me\n</code></pre> How does that look like? <p>Your output will be similar to this, when you have no jobs in the queue:</p> <pre><code>[sven@rackham1 ~]$ squeue -u $USER\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n</code></pre> <p>Or alternatively:</p> <pre><code>squeue -u $USER\n</code></pre>","tags":["squeue"]},{"location":"software/squeue/#view-your-jobs-in-the-snowy-queue","title":"View your jobs in the Snowy queue","text":"<p>View your jobs in the in the Snowy queue:</p> <pre><code>squeue -M snowy --me\n</code></pre> How does that look like? <p>Your output will be similar to this, when you have no jobs in the queue:</p> <pre><code>[sven@rackham1 ~]$ squeue -u $USER -M snowy\nCLUSTER: snowy\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n</code></pre>","tags":["squeue"]},{"location":"software/ssh/","title":"SSH","text":"","tags":["ssh","SSH"]},{"location":"software/ssh/#ssh","title":"ssh","text":"<p>From Wikipedia:</p> <p>The Secure Shell Protocol (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.</p> <p>At UPPMAX we allow users to login via SSH, using the program <code>ssh</code>.</p> <ul> <li>to use graphical applications, use SSH X forwarding,   i.e. <code>ssh -X</code> when logging in</li> <li>to login via SSH, see how to create and use an SSH key for the different HPC clusters</li> </ul>","tags":["ssh","SSH"]},{"location":"software/ssh/#ssh-key-management","title":"SSH key management","text":"<p>For WSL2 under Windows10 or Windows11, here as a neat way to get persistent key-manager in WSL2 (credits: original source).</p> <pre><code>sudo apt-get install keychain\n</code></pre> <p>Replace <code>XXXX</code> with the output of <code>hostname</code> command on the command line.</p> <pre><code>/usr/bin/keychain -q --nogui $HOME/.ssh/id_ed25519_key\nsource $HOME/.keychain/XXXX-sh\n</code></pre> <p>Remove <code>-q</code> to get some information if you want</p> <pre><code>* keychain 2.8.5 ~ http://www.funtoo.org\n* Found existing ssh-agent: 4487\n* Known ssh key: /home/user/.ssh/id_ed25519_key\n</code></pre> <p>First time you login, you will be asked for the password and the key will be handled by the key-manager. Check with</p> <pre><code>ssh-add -l\n256 SHA256:wLJvQOM....   ....cTTtiU MyNewKey (ED25519)\n</code></pre>","tags":["ssh","SSH"]},{"location":"software/ssh/#mobaxterm","title":"MobaXterm","text":"<p>In MobaXterm you can use the internal <code>MobAgent</code> or/and the <code>Peagent</code> from the <code>PuTTy</code> tools.</p> <p></p>","tags":["ssh","SSH"]},{"location":"software/ssh/#optional-ssh-config","title":"OPTIONAL: SSH config","text":"<p>Example <code>$HOME/.ssh/config</code> file to make your work easier.</p> <pre><code>Host rackham\nUser username\nHostName rackham.uppmax.uu.se\nServerAliveInterval 240\nServerAliveCountMax 2\n\n# Default settings\n#=======================================\nHost *\nForwardAgent no\nForwardX11 yes\nForwardX11Trusted yes\nServerAliveInterval 120\n#=======================================\n</code></pre> <p>Now</p> <pre><code># without config\nssh -X username@rackham.uppmax.uu.se\n# with config\nssh rackham\n\n# without config\nscp local_file username@rackham.uppmax.uu.se:remote_folder/\n# with config\nscp local_file rackham:remote_folder/\n\nrsync ...\nsftp ...\n</code></pre>","tags":["ssh","SSH"]},{"location":"software/ssh/#links","title":"Links","text":"<ul> <li>SSH Tips by Pavlin Mitev</li> </ul>","tags":["ssh","SSH"]},{"location":"software/ssh_client/","title":"SSH client","text":"","tags":["ssh","SSH","client","clients","ssh client","SSH client","ssh clients","SSH clients"]},{"location":"software/ssh_client/#ssh-client","title":"SSH client","text":"<p>An SSH client is a program that allows on to use SSH.</p>","tags":["ssh","SSH","client","clients","ssh client","SSH client","ssh clients","SSH clients"]},{"location":"software/ssh_client/#overview-of-ssh-clients","title":"Overview of SSH clients","text":"Operating system SSH Client Recommended? Allows graphics? [1] Description Linux <code>ssh</code> Yes Yes Start from a terminal MacOS <code>ssh</code> Yes Yes [2] Start from a terminal, needs install for graphics [2] Windows MobaXterm Yes Yes Easiest for Windows users [5] Windows PuTTY Neutral Yes [3] Needs install for graphics [3] Windows <code>ssh</code> Neutral Unknown Start from <code>CMD</code>, later Windows versions [4] Windows <code>ssh</code> Neutral Unknown Start from PowerShell [4] <ul> <li>[1] The technical question is 'Allows X forwarding',   as this is the way graphical displays are allowed</li> <li>[2] After installing XQuartz</li> <li>[3] After installing Xming</li> <li>[4] Untested</li> <li>[5] MobaXterm has a built-in X server</li> </ul>","tags":["ssh","SSH","client","clients","ssh client","SSH client","ssh clients","SSH clients"]},{"location":"software/ssh_client/#using-ssh-with-different-terminals-that-do-not-allow-for-graphics","title":"Using <code>ssh</code> with different terminals that do not allow for graphics","text":"MacWindows <ul> <li>Start <code>terminal</code> (e.g. from Launchpad) or iTerm2   to run <code>ssh</code></li> </ul> <pre><code>ssh [username]@rackham.uppmax.uu.se\n</code></pre> <ul> <li>where <code>[username]</code> is your UPPMAX username, for example <code>ssh sven@rackham.uppmax.uu.se</code></li> </ul> <ul> <li> <p>iTerm2 goodies:</p> <ul> <li>You can save hosts for later.</li> <li>Drag and drop scp</li> </ul> </li> </ul> <ul> <li>Start a terminal (see below) to run <code>ssh</code>:</li> </ul> <pre><code>$ ssh [username]@rackham.uppmax.uu.se\n</code></pre> <ul> <li>where <code>[username]</code> is your UPPMAX username, for example <code>ssh sven@rackham.uppmax.uu.se</code></li> </ul> <p></p> <ul> <li> <p>The ssh (secure shell) client putty</p> <ul> <li>You can save hosts for later.</li> <li>No graphics.</li> </ul> </li> <li> <p>Windows Powershell terminal can also work</p> <ul> <li>Cannot save hosts</li> <li>no graphics</li> <li>PowerShell</li> </ul> </li> <li> <p>Windows command prompt can also work</p> <ul> <li>Cannot save hosts</li> <li>no graphics</li> <li>Command Prompt</li> </ul> </li> <li> <p>Git bash</p> </li> </ul>","tags":["ssh","SSH","client","clients","ssh client","SSH client","ssh clients","SSH clients"]},{"location":"software/ssh_client/#using-ssh-with-different-terminals-that-allow-for-graphics","title":"Using <code>ssh</code> with different terminals that allow for graphics","text":"MacWindows <ul> <li>Download XQuartz or other X11 server for Mac OS from https://www.xquartz.org/</li> </ul> How do I know XQuartz has been installed? <p>As far as we know: you cannot check this directly: you will have to find out by running an application of Rackham that uses this. See below :-)</p> <ul> <li>Start <code>terminal</code> (e.g. from Launchpad) or iTerm2   to run <code>ssh</code>:</li> </ul> <pre><code>$ ssh -X [username]@rackham.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username and <code>-X</code> enables X forwarding. For example, if your UPPMAX username is <code>sven</code>, this would be <code>ssh -X sven@rackham.uppmax.uu.se</code></p> How do I know XQuartz has been installed? <p>See SSH X forwarding.</p> <p>Spoiler: use <code>xeyes</code></p> <ul> <li> <p>Download and install ONE of the X-servers below (to enable graphics)</p> <ul> <li>GWSL (recommended because of hardware integration)</li> <li>X-ming</li> <li>VCXSRV</li> </ul> </li> <li> <p>or...</p> </li> <li> <p>Install a ssh (secure shell) program with built-in X11 and sftp file manager</p> <ul> <li>MobaXterm</li> <li>sftp frame makes it easy to move, upload and download files.</li> <li>... though downloading from remote host to local is usually easier.</li> <li>tabs for several sessions</li> </ul> </li> </ul> <ul> <li>Start local terminal and an SSH session by:</li> </ul> <pre><code>$ ssh -X [username]@rackham.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username and <code>-X</code> enables X forwarding. For example, if your UPPMAX username is <code>sven</code>, this would be <code>ssh -X sven@rackham.uppmax.uu.se</code></p> <p></p> <ul> <li>Or even better, create and save a SSH session, as shown in image below.<ul> <li>This allows you to use MobaXterm as a file manager and  to use the built-in graphical texteditor.</li> <li>You can rename the session in the Bookmark settings tab.</li> </ul> </li> </ul> <p></p>","tags":["ssh","SSH","client","clients","ssh client","SSH client","ssh clients","SSH clients"]},{"location":"software/ssh_key_use/","title":"Create and use an SSH key pair","text":"","tags":["ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair"]},{"location":"software/ssh_key_use/#create-and-use-an-ssh-key-pair","title":"Create and use an SSH key pair","text":"<p>Here we show how to create and use an SSH key pair for use with our clusters:</p> <ul> <li>Create and use an SSH key pair for Bianca</li> <li>Create and use an SSH key pair for Dardel</li> <li>Create and use an SSH key pair for Pelle</li> <li>Create and use an SSH key pair for Rackham</li> </ul>","tags":["ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair"]},{"location":"software/ssh_key_use_bianca/","title":"Create an SSH key pair for use with Bianca","text":"","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#create-an-ssh-key-pair-for-use-with-bianca","title":"Create an SSH key pair for use with Bianca","text":"<p>This page describes how to create and use an SSH key for the Bianca cluster.</p>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#procedure","title":"Procedure","text":"<p>This procedure will fail if:</p> <ul> <li>You are outside of the university networks,   see how to get inside the university networks.   This video shows it will fail when being   outside of the university networks</li> </ul> <p>Here is the procedure.</p>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#1-create-an-ssh-key-pair","title":"1. Create an SSH key pair","text":"<p>On your local computer, create an SSH key pair with the following command:</p> Can I also do this from Rackham? <p>Yes.</p> <p>In that case, read 'Rackham' instead of 'local computer'</p> <pre><code>ssh-keygen -a 100 -t ed25519 -f ~/.ssh/id_ed25519_uppmax_login -C \"My comment\"\n</code></pre> <p>Here is a description of the flags:</p> <ul> <li><code>-a 100</code>:  100 rounds of key derivations,   making your key's password harder to brute-force,   as is recommended   by this StackExchange post</li> <li><code>-t ed25519</code>: type of encryption scheme</li> <li><code>-f ~/.ssh/id_ed25519_uppmax_login</code>: specify filename,   following the naming scheme as suggested   in this Superuser post</li> <li><code>-C \"My comment\"</code>: a comment that will be stored in the key, so you can find out what it was for</li> </ul>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#2-add-the-content-of-your-public-key-to-biancas-authorized-keys","title":"2. Add the content of your public key to Bianca's authorized keys","text":"<p>Add the content of the public key <code>id_ed25519_uppmax_login.pub</code> on your local computer to the Bianca's <code>$HOME/.ssh/authorized_keys</code>.</p> <p>There are multiple ways to do so.</p> Can I use <code>ssh-copy</code>? <p>No.</p> <p>You can not use <code>ssh-copy</code>.</p> <p>One way is to, on your local computer, view the content of the file:</p> <pre><code>cat $HOME/.ssh/id_ed25519_uppmax_login.pub\n</code></pre> <p>Then copy that line to your clipboard.</p> How does that look like? <pre><code>$ cat $HOME/.ssh/id_ed25519_uppmax_login.pub\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFGXV8fRK+cazt8qHX+fGS+w6WPOuE82Q19A12345678 Sven's key to UPPMAX\n</code></pre> <p>On Bianca, to edit the authorized keys file, do:</p> <pre><code>nano $HOME/.ssh/authorized_keys\n</code></pre> <p>In <code>nano</code>, paste the line in your clipboard. Save the file and close <code>nano</code>.</p> <p>The public key must be one line</p> <p>The public key you've just copy-pasted must be one line. It must not be wrapped/split over multiple lines.</p> How can I check? <p>On Bianca, do:</p> <pre><code>cat .ssh/authorized_keys\n</code></pre> <p>You should find your public key there. It looks similar to this:</p> <pre><code>[sven@sens2017625-bianca ~]$ cat .ssh/authorized_keys\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFGXV8fRK+cazt8qHX+fGS+w6WPOuE82Q19A12345678 Sven's key to UPPMAX\n</code></pre>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#3-set-the-right-permissions","title":"3. Set the right permissions","text":"<p>On Bianca, do:</p> <pre><code>chmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre> How can I check? <p>You can check by doing the following and observing similar output:</p> <pre><code>ls -ld .ssh\n</code></pre> <p>Output should be:</p> <pre><code>drwx--S--- 2 sven sven 4096 Jan  8 10:26 .ssh\n</code></pre> <p>Second checkL</p> <pre><code>[richel@sens2017625-bianca ~]$ ls -ld .ssh/authorized_keys\n</code></pre> <p>Output should be similar to:</p> <pre><code>-rwx------ 1 sven sven 104 Jan  8 10:26 .ssh/authorized_keys\n</code></pre> <p>Third check:</p> <pre><code>ls -l .ssh\n</code></pre> <p>Output should be similar to:</p> <pre><code>total 1\n-rw-r----- 1 user user 743 May  7  2019 authorized_keys\n</code></pre> <p>or</p> <pre><code>total 1\n-rwx------ 1 sven sven 104 Jan  8 10:26 authorized_keys\n</code></pre>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#4-log-in-to-bianca-via-the-console-using-an-ssh-key","title":"4. Log in to Bianca via the console using an SSH key","text":"<p>Log in to Bianca via the console using an SSH key, using <code>ssh -A</code>:</p> <pre><code>ssh -A [username]@bianca.uppmax.uu.se\n</code></pre> <p>For example:</p> <pre><code>ssh -A sven@bianca.uppmax.uu.se\n</code></pre> <p>You will still get a login, asking for (1) your UPPMAX password, (2) your UPPMAX 2FA, and (3) your UPPMAX project.</p> <p>If all worked, there will be no need anymore to again type the UPPMAX password.</p>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#troubleshooting","title":"Troubleshooting","text":"<p>To debug, run SSH commands with the <code>-vv</code> flag.</p> How does that look like? <pre><code>...\ndebug1: Requesting authentication agent forwarding.\ndebug2: channel 1: request auth-agent-req@openssh.com confirm 0\n...\n\ndebug1: client_input_channel_open: ctype auth-agent@openssh.com rchan 2 win 65536 max 16384\ndebug1: client_request_agent: bound agent to hostkey\ndebug2: fd 8 setting O_NONBLOCK\ndebug1: channel 2: new [authentication agent connection]\ndebug1: confirm auth-agent@openssh.com\nLast login: Tue Jul 11 18:44:21 2023 from 172.18.144.254\n _   _ ____  ____  __  __    _    __  __\n| | | |  _ \\|  _ \\|  \\/  |  / \\   \\ \\/ /   | System:    sens2017625-bianca\n| | | | |_) | |_) | |\\/| | / _ \\   \\  /    | User:      user\n| |_| |  __/|  __/| |  | |/ ___ \\  /  \\    |\n \\___/|_|   |_|   |_|  |_/_/   \\_\\/_/\\_\\   |\n\n  ###############################################################################\n</code></pre>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#on-linux-it-still-asks-for-a-password","title":"On Linux, it still asks for a password","text":"<p>From this post and its answer:</p> <p>On Bianca, do:</p> <pre><code>chmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre> <p>On your local computer, do:</p> <pre><code>chmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_bianca/#links","title":"Links","text":"<ul> <li>Notes from Pavlin Mitev</li> </ul>","tags":["Bianca","ssh","SSH","ssh key","SSH key","ssh keys","SSH keys","ssh key pair","SSH key pair","create"]},{"location":"software/ssh_key_use_dardel/","title":"Create and use an SSH key pair for Dardel","text":"","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#create-and-use-an-ssh-key-pair-for-dardel","title":"Create and use an SSH key pair for Dardel","text":"<p>This page describes how to create and use an SSH key for the Dardel cluster.</p> <p>This guide will show you:</p> <ul> <li>1. How to create SSH keys</li> <li>2. How to add an SSH key to the PDC Login Portal</li> </ul> <p>This makes it possible for you to login to Dardel.</p> <p>PDC has a more comprehensive guide on how to do this on various operating systems if you want a more in-depth guide.</p> <p>Warning</p> <ul> <li>To be able to transfer from Rackham you have to do the following steps on Rackham.</li> <li>You can also do the steps for you local computer to be able to log in   directly from your terminal and not via Rackham.</li> </ul>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#1-how-to-create-ssh-keys","title":"1. How to create SSH keys","text":"<p>To create an SSH key, one needs to</p> <ul> <li>start generating the key</li> <li>specify the filename</li> <li>specify the password</li> </ul>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#11-start-generating-the-key","title":"1.1 Start generating the key","text":"<ul> <li>Add the content of your public key <code>id_ed25519.pub.</code> To create a SSH key, run the following command:</li> </ul> <pre><code>ssh-keygen -t ed25519\n</code></pre> <p>This will start the creating of a SSH key using the <code>ed25519</code> algorithm.</p>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#12-specify-where-to-save-the-file","title":"1.2 Specify where to save the file","text":"<p>The program will ask you where to save the file,</p> <pre><code>user@rackham ~ $ ssh-keygen -t ed25519\nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/home/user/.ssh/id_ed25519):\n</code></pre> <p>If you press enter it will save the new key using the suggested name, <code>/home/user/.ssh/id_ed25519</code></p> <p>If it asks you if you want to overwrite, you probably want to press <code>n</code> since you already have one created and might want to use that one instead. If you overwrite it you will lose access to wherever the old key file is used, so just run the <code>ssh-keygen</code> command above again and type in a new name for the file.</p> <pre><code>/home/user/.ssh/id_ed25519 already exists.\nOverwrite (y/n)?\n</code></pre>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#13-specify-the-password","title":"1.3 Specify the password","text":"<p>The next step is to add a password to your key file. This makes sure that even if someone manages to copy your key they will not be able to use it without the password you set here. Type in a password you will remember, press enter, type it in again and press enter.</p> <pre><code>Enter passphrase (empty for no passphrase):\nEnter same passphrase again:\n</code></pre> <p>The key will now be created and you can add it to the PDC Login Portal.</p> How does this look like? <p>This is output similar to what you will see:</p> <pre><code>Your identification has been saved in /home/user/.ssh/id_ed25519\nYour public key has been saved in /home/user/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:g+rvY4HoDNlim+Bj43L3pxr56hrlwC4hzPa/yE/2YqE user@rackham\nThe keys randomart image is:\n+--[ED25519 256]--+\n|.o               |\n|o   .            |\n| . = .           |\n|    B ..         |\n| + * B..S        |\n|= + o =          |\n|*+.oo=..         |\n|+=oE+ B          |\n| o +*X o         |\n+----[SHA256]-----+\n</code></pre>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#2-how-to-add-an-ssh-key-to-the-pdc-login-portal","title":"2. How to add an SSH key to the PDC Login Portal","text":"<p>To add an SSH key to the PDC login portal, one needs to:</p> <ul> <li>Open the PDC login portal</li> <li>Start adding a new key</li> <li>Actually adding that public key</li> <li>Allow the key to be used from UPPMAX</li> </ul>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#21-open-the-pdc-login-portal","title":"2.1. Open the PDC login portal","text":"<p>Go to the PDC Login Portal</p> How does that look like? <p>That will look like this:</p> <p></p> <p>Example PDC login portal without any SSH keys yet. We will need to add an SSH key that allows access from UPPMAX to PDC</p>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#22-start-adding-a-new-key","title":"2.2. Start adding a new key","text":"<p>Click the <code>Add new key</code> link:</p> How does adding an SSH key pair look like? <p>That will look like this:</p> <p></p> <p>Example of the first step of adding an SSH key pair to the PDC portal. The 'SSH public key' is copy-pasted from <code>cat ~/id_ed25519_pdc.pub</code> on Rackham. The 'Key name' can be chosen freely. Note that this SSH key cannot be used yet for UPPMAX, as it only allows one IP address.</p> How does it look like when the key is added? <p>That will look like this:</p> <p></p> <p>Example PDC login portal with one key. Note that the second column only has one IP address and is still missing <code>*.uppmax.uu.se</code>.</p>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#23-actually-adding-the-public-key","title":"2.3. Actually adding the public key","text":"<p>Here you can either upload the public part of the key file you created before, or you can enter the information manually.</p> Forgot where the key was? <p>Here is how to the display the SSH public key content at the default location:</p> <pre><code>cat ~/.ssh/id_ed25519.pub\n</code></pre> <p>Else, the SSH keys are where you created them in step 1.2 :-)</p> How does the content of a public SSH key look like? <p>When displaying the content of a public SSH key, it will show text like this:</p> <pre><code>ssh-ed25519 AAAA69Nz1C1lZkI1NdE5ABAAIA7RHe4jVBRTEvHVbEYxV8lnOQl22N+4QcUK+rDv1gPS user@rackham2.uppmax.uu.se\n</code></pre> <p>Copy the content of the SSH public key. Paste it into the field <code>SSH public key</code>, make up a name for the key so you know which computer it is on and fill it into the field <code>Key name</code>.</p>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_dardel/#24-allow-the-key-to-be-used-from-uppmax","title":"2.4. Allow the key to be used from UPPMAX","text":"<p>Once you have added you key you have to add UPPMAX as allowed to use the key. Click on <code>Add address</code> for it and add <code>*.uppmax.uu.se</code>.</p> <p><code>Address</code> specifies which IP address(es) are allowed to use this key and the field is prefilled with the IP of the computer you are on at the moment.</p> How does it look like to edit an SSH key so that can be used for UPPMAX? <p>That will look like this:</p> <p></p> <p>Example of the second step of adding an SSH key pair to the PDC portal. Here the custom address <code>*.uppmax.uu.se</code> is added, so that this SSH key can be used for UPPMAX.</p> How does it look like to have a key that can be used for UPPMAX? <p>That will look like this:</p> <p></p> <p>Example PDC login portal with one key. Note the <code>*.uppmax.uu.se</code> at the bottom of the second column.</p>","tags":["ssh","SSH","key","ssh key","SSH key","Dardel"]},{"location":"software/ssh_key_use_pelle/","title":"Create an SSH key pair for Pelle","text":"","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_pelle/#create-an-ssh-key-pair-for-pelle","title":"Create an SSH key pair for Pelle","text":"<p>This page describes how to create and use an SSH key so that you can login to the Pelle console environment with an SSH key</p>","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_pelle/#procedure","title":"Procedure","text":"Prefer a video? <p>Watch the YouTube video Create an SSH key pair for Pelle</p> <p>This figure shows the procedure:</p> <pre><code>flowchart TD\n  subgraph ip_inside_sunet[IP inside SUNET]\n    create[1.Create an SSH key pair]\n    add[2.Add your keys to an SSH agent]\n    copy[3.Copy the public key to Pelle]\n  end\n  create --&gt; add\n  add --&gt; copy</code></pre> <p>This procedure fails if:</p> <ul> <li>You use Ubuntu 24.04 Noble, even when   inside the university networks.   as demonstrated in the end of the YouTube video   Create an SSH key pair for Pelle</li> </ul>","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_pelle/#1-create-an-ssh-key-pair","title":"1. Create an SSH key pair","text":"<p>On your local computer, in a terminal, create an SSH key pair with the following command:</p> <pre><code>ssh-keygen -a 100 -t ed25519 -f ~/.ssh/id_ed25519_uppmax_login -C \"My comment\"\n</code></pre> <ul> <li><code>-a 100</code>:  100 rounds of key derivations,   making your key's password harder to brute-force,   as is recommended by   this StackExchange post</li> <li><code>-t ed25519</code>: type of encryption scheme</li> <li><code>-f ~/.ssh/id_ed25519_uppmax_login</code>: specify filename,   following the naming scheme as suggested   in this Superuser post</li> <li><code>-C \"My comment\"</code>: a comment that will be stored in the key, so you can find out what it was for</li> </ul>","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_pelle/#2-add-your-keys-to-an-ssh-agent","title":"2. Add your keys to an SSH agent","text":"<p>On your local computer, in a terminal, add your newly generated <code>ed25519</code> key to an SSH agent:</p> <pre><code>ssh-add ~/.ssh/id_ed25519_uppmax_login\n</code></pre>","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_pelle/#3-copy-the-public-key-to-pelle","title":"3. Copy the public key to Pelle","text":"<p>On your local computer, in a terminal, copy the public key to Pelle:</p> <pre><code>ssh-copy-id -i .ssh/id_ed25519_uppmax_login.pub [username]@pelle.uppmax.uu.se\n</code></pre> <ul> <li><code>-i .ssh/id_ed25519_uppmax_login.pub</code>: the identity file, the public key's filename</li> <li><code>[username]@pelle.uppmax.uu.se</code>: your UPPMAX username, for example <code>sven@pelle.uppmax.uu.se</code></li> </ul> <p>After this, you can login to Pelle without specifying a password.</p>","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_pelle/#troubleshooting","title":"Troubleshooting","text":"","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_pelle/#on-linux-it-still-asks-for-a-password","title":"On Linux, it still asks for a password","text":"<p>From this post and its answer:</p> <p>On Pelle, do:</p> <pre><code>chmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre> <p>On your local computer, do:</p> <pre><code>chmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre>","tags":["create","ssh","SSH","key","ssh key","SSH key","ssh key pair","SSH key pair","Pelle"]},{"location":"software/ssh_key_use_rackham/","title":"Create and use an SSH key pair for Rackham","text":"","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_key_use_rackham/#create-and-use-an-ssh-key-pair-for-rackham","title":"Create and use an SSH key pair for Rackham","text":"<p>This page describes how to create and use an SSH key for the Rackham cluster.</p>","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_key_use_rackham/#procedure","title":"Procedure","text":"Prefer a video? <ul> <li>Create and use an SSH key pair for Rackham when outside of SUNET (fails!)</li> <li>Create and use an SSH key pair for Rackham on Ubuntu 24.04 Noble (fails!)</li> </ul> <p>This figure shows the procedure:</p> <pre><code>flowchart TD\n  subgraph ip_inside_sunet[IP inside SUNET]\n    create[1.Create an SSH key pair]\n    add[2.Add your keys to an SSH agent]\n    copy[3.Copy the public key to Rackham]\n  end\n  create --&gt; add\n  add --&gt; copy</code></pre> <p>This procedure will fail if:</p> <ul> <li>You are outside of the university networks,   see how to get inside the university networks.   This video shows it will fail when being   outside of the university networks</li> <li>You use Ubuntu 24.04 Noble, as demonstrated by this video,   where a password is still requested after doing this procedure</li> </ul>","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_key_use_rackham/#1-create-an-ssh-key-pair","title":"1. Create an SSH key pair","text":"<p>Create an SSH key pair with the following command:</p> <pre><code>ssh-keygen -a 100 -t ed25519 -f ~/.ssh/id_ed25519_uppmax_login -C \"My comment\"\n</code></pre> <ul> <li><code>-a 100</code>:  100 rounds of key derivations,   making your key's password harder to brute-force,   as is recommended by this StackExchange post</li> <li><code>-t ed25519</code>: type of encryption scheme</li> <li><code>-f ~/.ssh/id_ed25519_uppmax_login</code>: specify filename,   following the naming scheme,   as suggested by this Superuser post</li> <li><code>-C \"My comment\"</code>: a comment that will be stored in the key, so you can find out what it was for</li> </ul>","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_key_use_rackham/#2-add-your-keys-to-an-ssh-agent","title":"2. Add your keys to an SSH agent","text":"<p>Add your newly generated <code>ed25519</code> key to an SSH agent:</p> <pre><code>ssh-add ~/.ssh/id_ed25519_uppmax_login\n</code></pre>","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_key_use_rackham/#3-copy-the-public-key-to-rackham","title":"3. Copy the public key to Rackham","text":"<p>Copy the public key to Rackham or other server.</p> <pre><code>ssh-copy-id -i .ssh/id_ed25519_uppmax_login.pub [username]@rackham.uppmax.uu.se\n</code></pre> <ul> <li><code>-i .ssh/id_ed25519_uppmax_login.pub</code>: the identity file, the public key's filename</li> <li><code>[username]@rackham.uppmax.uu.se</code>: your UPPMAX username, for example <code>sven@rackham.uppmax.uu.se</code></li> </ul> <p>After this, you can login to Rackham without specifying a password.</p>","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_key_use_rackham/#troubleshooting","title":"Troubleshooting","text":"","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_key_use_rackham/#on-linux-it-still-asks-for-a-password","title":"On Linux, it still asks for a password","text":"<p>From this post and its answer:</p> <p>On Rackham, do:</p> <pre><code>chmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre> <p>On your local computer, do:</p> <pre><code>chmod 700 .ssh/authorized_keys\nchmod 700 .ssh\nchmod 700 ~\n</code></pre>","tags":["ssh","SSH","key","ssh key","SSH key","Rackham"]},{"location":"software/ssh_x_forwarding/","title":"SSH X forwarding","text":"","tags":["ssh","SSH","console","terminal","x-forwarding"]},{"location":"software/ssh_x_forwarding/#ssh-x-forwarding","title":"SSH X forwarding","text":"<p>SSH X forwarding (or simply 'X forwarding') allows one to use graphics when using an SSH client.</p> <p>For example, this is how UPPMAX user <code>sven</code> would login to Rackham using <code>ssh</code> with X forwarding enabled:</p> <pre><code>ssh -X sven@rackham.uppmax.uu.se\n</code></pre> <p>It is the <code>-X</code> that allows ssh to show graphics.</p> What is X? <p>In this context, the X window system.</p> How can I verify I allow X forwarding? <p>Using <code>xyes</code>.</p>","tags":["ssh","SSH","console","terminal","x-forwarding"]},{"location":"software/ssh_x_forwarding/#uppmax-clusters-that-allow-ssh-with-x-forwarding","title":"UPPMAX clusters that allow SSH with X forwarding","text":"Cluster Allows SSH with X forwarding Bianca No Pelle Yes Rackham Yes Snowy Yes","tags":["ssh","SSH","console","terminal","x-forwarding"]},{"location":"software/ssh_x_forwarding/#ssh-clients","title":"SSH clients","text":"<p>See SSH clients.</p>","tags":["ssh","SSH","console","terminal","x-forwarding"]},{"location":"software/ssh_x_forwarding/#difference-between-ssh-x-and-ssh-y","title":"Difference between <code>ssh -X</code> and <code>ssh -Y</code>","text":"<p>Adapted from this AskUbuntu answer:</p> <p>If you need graphics, <code>ssh -X</code> is more secure. However, it may be too secure for your software to run. In that case, run <code>ssh -Y</code>.</p> <pre><code>flowchart TD\n  need_graphics[Need graphics?]\n  ssh[Using 'ssh' works]\n  try_ssh_x[Try to use 'ssh -X'. Does it work?]\n  ssh_x[Use 'ssh -X']\n  ssh_y[Use 'ssh -Y']\n\n  need_graphics --&gt; |no| ssh\n  need_graphics --&gt; |yes| try_ssh_x\n  try_ssh_x --&gt; |yes| ssh_x\n  try_ssh_x --&gt; |no| ssh_y</code></pre> <p>Flowchart to determine to use <code>ssh</code> or <code>ssh -X</code> or <code>ssh -Y</code>.</p> Using <code>ssh -Y</code>? Let us know! <p>If you -a user- use <code>ssh -Y</code> when <code>ssh -X</code> does not work, let us know (see the UPPMAX support page). It helps us choose which option to show at these documentation websites.</p>","tags":["ssh","SSH","console","terminal","x-forwarding"]},{"location":"software/tabix/","title":"tabix","text":""},{"location":"software/tabix/#tabix","title":"tabix","text":"<p>tabix is a tool.</p>"},{"location":"software/tabix/#finding-tabix","title":"Finding tabix","text":"<p>To find the versions of tabix installed, use:</p> <pre><code>module spider tabix\n</code></pre> How does that look like? <p>The output may look like this:</p> <pre><code>[sven@rackham1 sven]$ module spider tabix\n\n----------------------------------------------------------------------------\n  tabix: tabix/0.2.6\n----------------------------------------------------------------------------\n\n     Other possible modules matches:\n        tabixpp\n\n    You will need to load all module(s) on any one of the lines below before the\n \"tabix/0.2.6\" module is available to load.\n\n      bioinfo-tools\n\n    Help:\n       tabix - use tabix 0.2.6\n\n       Version 0.2.6\n\n\n\n\n----------------------------------------------------------------------------\n  To find other possible module matches execute:\n</code></pre>"},{"location":"software/tensorflow/","title":"TensorFlow","text":""},{"location":"software/tensorflow/#tensorflow","title":"TensorFlow","text":"<p>TensorFlow is a library for machine learning and artificial intelligence.</p> <p>TensorFlow is available in multiple variants:</p> <ul> <li>TensorFlow as a Python package for CPU:   works on Bianca and Rackham</li> <li>TensorFlow as a Python package for GPU   works on Bianca and Snowy</li> </ul>"},{"location":"software/tensorflow/#tensorflow-as-a-python-package-for-cpu","title":"TensorFlow as a Python package for CPU","text":"<p>It is part of the <code>python_ML_packages/[version]-cpu</code> modules, where <code>[version]</code> is a version, for example, <code>python_ML_packages/3.11.8-cpu</code>.</p> <p>Only <code>3.9.5-cpu</code> and <code>3.9.5-gpu</code> are available on Bianca.</p> How to test TensorFlow as a Python package for CPU? <p>On Rackham, load the module to get access to the library:</p> <pre><code>module load python_ML_packages/3.11.8-cpu\n</code></pre> <p>Start Python:</p> <pre><code>python\n</code></pre> <p>In Python, type:</p> <pre><code>import tensorflow as tf\nprint(tf.test.is_gpu_available())\n</code></pre> <p>This should print:</p> <pre><code>False\n</code></pre> <p>The output is correct: this is the CPU version.</p>"},{"location":"software/tensorflow/#tensorflow-as-a-python-package-for-gpu","title":"TensorFlow as a Python package for GPU","text":"<p>It is part of the <code>python_ML_packages/[version]-gpu</code> modules, where <code>[version]</code> is a version, for example, <code>python_ML_packages/3.9.5-gpu</code></p> <p> You can load this package on nodes without GPU but python will not find TensorFlow!</p> <p>If you want to work interactively and test things, first allocate resources as seen below:</p>"},{"location":"software/tensorflow/#on-snowy","title":"On Snowy","text":"<pre><code>interactive -A &lt;proj&gt; -n 2 -M snowy --gres=gpu:1  -t 1:00:01\n</code></pre>"},{"location":"software/tensorflow/#on-bianca","title":"On Bianca","text":"<pre><code>interactive -A &lt;proj&gt; -n 1 -C gpu --gres=gpu:1 -t 01:10:00\n</code></pre> How to test TensorFlow as a Python package for GPU? <p>Load the module to get access to the library:</p> <pre><code>module load python_ML_packages/3.9.5-gpu\n</code></pre> <p>Start Python:</p> <pre><code>python\n</code></pre> <p>In Python, type:</p> <pre><code>import tensorflow as tf\nprint(tf.test.is_gpu_available())\n</code></pre> <p>This should print something like:</p> <pre><code>2024-03-15 14:13:02.038401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 13614 MB memory:  -&gt; device: 0, name: Tesla T4, pci bus id: 0000:08:00.0, compute capability: 7.5\nTrue\n</code></pre> <p>The output is correct: this is the GPU version.</p>"},{"location":"software/terminal/","title":"Terminal","text":"","tags":["terminal"]},{"location":"software/terminal/#terminal","title":"Terminal","text":"<p>A terminal.</p> <p>A terminal is a program that allows you to run commands.</p> How to copy-paste to/from a terminal? <p>This depends on the terminal you use, however, this is the most common options:</p> <p>Press <code>CTRL + SHIFT + C</code> for copt, <code>CTRL + SHIFT + V</code> for pasting.</p> What does all the stuff on the line I can type on mean? <p>The text at the start of the line you can type on, is called the command prompt.</p> What is the command prompt? <p>The command prompt indicates that the terminal is waiting for user input.</p> <p>Here is an example prompt:</p> <pre><code>[sven@rackham2 my_folder]$\n</code></pre> <ul> <li><code>[</code> and <code>]</code>: indicates the beginning and end of information</li> <li><code>sven</code>: the username</li> <li><code>@</code>: at which cluster</li> <li><code>rackham2</code>: the remote node's name,   in this case Rackham's second login node</li> <li><code>my_folder</code>: (part of) the path of the user,   in this case, a folder called <code>my_folder</code>.   The indication <code>~</code> means that the user in the home folder</li> <li><code>$</code>: indicate to be ready for user input</li> </ul> <p>The node's name is useful to find out where you are:</p> Name Location <code>rackham1</code> to <code>rackham4</code> A Rackham login node <code>r1</code> and higher A Rackham compute node node <code>bianca</code> A Bianca login node <code>b1</code> and higher A Bianca compute node","tags":["terminal"]},{"location":"software/test/","title":"Test","text":""},{"location":"software/test/#test","title":"Test","text":""},{"location":"software/test/#level-2","title":"Level 2","text":""},{"location":"software/test/#level-3","title":"Level 3","text":""},{"location":"software/text_editors/","title":"Text editors","text":"","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/text_editors/#text-editors","title":"Text editors","text":"<p>There are many editors that can be used on the UPPMAX clusters:</p> Editor type Features Simple terminal editors Used in terminal, easy to learn, limited features Advanced terminal editors Used in terminal, harder to learn, powerful features Simple graphical editors Graphical, needs X forwarding, easy to learn, limited features Advanced graphical editors Graphical, needs X forwarding, harder to learn, powerful features <p>Try them out and pick one favorite editor!</p> <p>Tip</p> <p>These commands are useful in the command line when something is stuck or a program is limiting you to do further work.</p> <ul> <li><code>ctrl-C</code> interrupts a program or a command that is \"stuck\"</li> <li><code>ctrl-D</code> quits some programs from the program environment in the terminal</li> <li><code>ctrl-Z</code> pauses a program, can be continued in background (<code>bg</code>) or  foreground (<code>fg</code>)</li> </ul>","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/text_editors/#simple-terminal-editors","title":"Simple terminal editors","text":"<ul> <li>nano: used in terminal, easy to learn, limited features</li> </ul>","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/text_editors/#advanced-terminal-editors","title":"Advanced terminal editors","text":"<p>Warning</p> <ul> <li>we suggest that you learn this tools before trying to work with them on UPPMAX</li> <li>If you start one of these editors you may have difficulties to exit!</li> </ul> <ul> <li>emacs</li> <li>vim</li> </ul>","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/text_editors/#simple-graphical-editors","title":"Simple graphical editors","text":"<p>To use a graphical editors you will need to:</p> <ul> <li>work on an UPPMAX cluster that allows SSH X forwarding</li> <li>login with SSH X forwarding enabled</li> </ul> <p>See the SSH X forwarding page how to do so.</p> And what about Bianca? <p>Bianca is an UPPMAX cluster that does not allow X forwarding.</p> <p>See the 'How to login to Bianca' page for more details.</p>","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/text_editors/#gedit","title":"gedit","text":"<p>See gedit</p>","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/text_editors/#advanced-graphical-editors","title":"Advanced graphical editors","text":"","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/text_editors/#gvim","title":"gvim","text":"<ul> <li><code>vim</code> with a GUI, lots of features, very fast</li> </ul>","tags":["text","editor","editors","text editor","text editors"]},{"location":"software/thinlinc/","title":"ThinLinc","text":"","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#thinlinc","title":"ThinLinc","text":"<p>Remote desktop environment for Rackham, using the webbrowser login.</p> <p>ThinLinc provides for a remote desktop environment for the UPPMAX clusters.</p>","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#local-thinlinc-client-versus-web-browser","title":"Local ThinLinc client versus web browser","text":"<p>There are two ways of connecting to the clusters using ThinLinc, using a local ThinLinc client or login using a webbrowser. Here are the differences:</p> Parameter Local ThinLinc client Web browser login Bianca use Impossible Possible Rackham use Recommended Possible Pelle use Recommended  Not yet possible Install ThinLinc client Nothing [1] Simplicity Easy Trivial Performance Higher Lower Recommended for Most use cases Small tasks, when other approach fails <ul> <li>[1] You already have a webbrowser installed :-)</li> </ul> <p>The first is by using the web client and connect from the browser. This can be useful for smaller tasks or if you are unable to install software on the computer you are currently using. Please see below for more information.</p> <p>The second option is to download the ThinLinc client, which offers higher performance and is recommended for most users. The client can be downloaded from the official download page.</p> <ul> <li>ThinLinc on Bianca</li> <li>ThinLinc on Rackham</li> <li>ThinLinc on Snowy: same as ThinLinc on Rackham</li> </ul>","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#installing-thinlinc","title":"Installing ThinLinc","text":"<p>The ThinLinc client can be downloaded from the official Cendio download page.</p>","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#thinlinc-usage","title":"ThinLinc usage","text":"","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#how-do-i-copypaste-within-a-thinlinc-session","title":"How do I copy/paste within a ThinLinc session?\"","text":"<ul> <li>Windows/Mac: Right-click and choose, or</li> <li>Windows:<ul> <li>paste: <code>shift+insert</code></li> <li>copy: <code>ctrl+insert</code></li> </ul> </li> </ul>","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#how-do-i-copypaste-between-thinlinc-and-locally","title":"How do I copy/paste between ThinLinc and locally?","text":"<p>ThinLinc has a clipboard where one can shuttle text via copy-pasting inside/outside the ThinLinc remote desktop environment.</p> <ul> <li> <p>Copy in ThinLinc by the ThinLinc command (see above) and it ends up here in the ThinLinc clipboard</p> <ul> <li>Mark and copy with Windows/Mac command</li> <li>Paste locally with Windows/Mac command</li> </ul> </li> <li> <p>Copy from locally</p> <ul> <li>paste in the ThinLinc clipboard with Windows/Mac command</li> <li>paste to ThinLinc place by the ThinLinc command (see above)</li> </ul> </li> </ul>","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#settings","title":"Settings","text":"<p>Under the \"Screen\" tab, you can set the starting size of the session and choose to enable/disable Full screen mode. Typically, users prefer to turn off full screen mode.</p> <p>Normally you don't have to change anything else here, and we have also disabled all \"local devices\" (USB-sticks, sound and printers) on server side. So no point to fiddle with these specific options.</p>","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>ThinLinc error: no agent server was available</li> </ul>","tags":["ThinLinc","local client","install","usage","copy-paste"]},{"location":"software/thinlinc_error_no_server_agent_was_available/","title":"ThinLinc error: no agent server was available","text":"","tags":["ThinLinc","error","no agent server was available","agent server"]},{"location":"software/thinlinc_error_no_server_agent_was_available/#thinlinc-error-no-agent-server-was-available","title":"ThinLinc error: no agent server was available","text":"<p>A ThinLinc error</p> <p></p> <p>ThinLinc error: no agent server available</p> <p>This sometimes happens.</p> Need a more detailed answer? <p>When one has reached this point, it tends to be a matter of the login node of the individual project. We from UPPMAX might be able to verify. The action to take by UPPMAX might be to reboot the node, but it's naturally possible that something else has happened to our setup (of this UPPMAX node) somehow.</p> <p>All you can do, is try again.</p> <p>If this fails multiple times, contact support.</p>","tags":["ThinLinc","error","no agent server was available","agent server"]},{"location":"software/thinlinc_on_bianca/","title":"ThinLinc on Bianca","text":"","tags":["ThinLinc","Bianca"]},{"location":"software/thinlinc_on_bianca/#thinlinc-on-bianca","title":"ThinLinc on Bianca","text":"<p>Bianca's remote desktop, using a webbrowser</p> <p>ThinLinc provides for a remote desktop environment for the UPPMAX clusters. This page describes how to use ThinLinc on Bianca.</p> <p>For Bianca, there is only one way to access Bianca's remote desktop using a website: see the UPPMAX page 'Login to the Bianca remote desktop environment website'.</p>","tags":["ThinLinc","Bianca"]},{"location":"software/thinlinc_on_rackham/","title":"ThinLinc on Rackham","text":"","tags":["ThinLinc","Rackham"]},{"location":"software/thinlinc_on_rackham/#thinlinc-on-rackham","title":"ThinLinc on Rackham","text":"<p>Rackham's remote desktop environment accessed via a webbrowser</p> <p>ThinLinc provides for a remote desktop environment for the UPPMAX clusters. This page describes how to use ThinLinc on Rackham.</p> <p>There are two ways of connecting to the clusters using ThinLinc: using a local ThinLinc client or login using a webbrowser. See ThinLinc for a comparison.</p>","tags":["ThinLinc","Rackham"]},{"location":"software/thinlinc_on_rackham/#local-thinlinc-client","title":"Local ThinLinc client","text":"<p>Rackham's remote desktop environment accessed via a local ThinLinc client</p> <p>See the UPPMAX page 'Login to the Rackham remote desktop environment using a local ThinLinc client'.</p>","tags":["ThinLinc","Rackham"]},{"location":"software/thinlinc_on_rackham/#web-browser-login","title":"Web browser login","text":"<p>Rackham's remote desktop environment accessed via a webbrowser</p> <p>See the UPPMAX page 'Login to the Rackham remote desktop environment website'.</p>","tags":["ThinLinc","Rackham"]},{"location":"software/tkinter/","title":"Tkinter","text":""},{"location":"software/tkinter/#tkinter","title":"Tkinter","text":"<p>Tkinter is a package built with (every!) Python executable.</p>"},{"location":"software/tkinter/#use-tkinter","title":"Use Tkinter","text":"<p>Load a Python module:</p> <pre><code>module load python/3.12.1\n</code></pre> <p>Start Python:</p> <pre><code>python\n</code></pre> <p>Import <code>thkinter</code> in Python:</p> <pre><code>import tkinter\n</code></pre>"},{"location":"software/tkinter/#history","title":"History","text":"<p>In January 2024, there was a Tkinter UPPMAX ticket. and documentation how to load <code>tkinter</code>.</p> <p>At that time, doing:</p> <pre><code>module load python/3.11.4\n</code></pre> <p>and then in Python:</p> <pre><code>import turtle\n</code></pre> <p>results in:</p> <pre><code>Traceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n  File \"/sw/comp/python3/3.11.4/rackham/lib/python3.11/turtle.py\", line 107, in &lt;module&gt;\n    import tkinter as TK\n  File \"/sw/comp/python3/3.11.4/rackham/lib/python3.11/tkinter/__init__.py\", line 38, in &lt;module&gt;\n    import _tkinter # If this fails your Python may not be configured for Tk\n    ^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named '_tkinter'\n</code></pre> <p>With the application experts, we found out that <code>python</code> version <code>3.11.4</code> did not have <code>tkinter</code> built in. That Python version was rebuilt. Now all that is needed is to load a Python version and do a regular <code>pip install</code>. That is, this solution should work:</p>"},{"location":"software/tkinter/#links","title":"Links","text":"<ul> <li>Wikipedia page on Tkinter</li> </ul>"},{"location":"software/tracer/","title":"Tracer","text":""},{"location":"software/tracer/#tracer","title":"Tracer","text":"<p>Tracer is a tool to analyse the results of a BEAST or BEAST2 run.</p> <p>Tracer is not an UPPMAX module.</p> <p>Instead, it needs to be download and run:</p>"},{"location":"software/tracer/#1-download","title":"1. Download","text":"<p>Pick a Tracer release, such as Tracer v1.7.2 and download the Linux/UNIX version.</p> How does that look like? <p>Here is how the release page of Tracer v1.7.2 looks like:</p> <p></p> <p>Tracer</p> <p>Download the file <code>Tracer_v1.7.2.tgz</code>.</p> How to download from the command-line? <p>Use <code>wget</code> on the URL to download from, for example:</p> <pre><code>wget https://github.com/beast-dev/tracer/releases/download/v1.7.2/Tracer_v1.7.2.tgz\n</code></pre>"},{"location":"software/tracer/#2-extract","title":"2. Extract","text":"<p>Extract the downloaded file.</p> How to do so, using the remote desktop environment? <p>Right-click the file and click 'Extract here'.</p> <p></p> How to do so, using the console environment? <p>Use <code>tar</code> on the file to extract:</p> <pre><code>tar zxvf  Tracer_v1.7.2.tgz\n</code></pre>"},{"location":"software/tracer/#3-run","title":"3. Run","text":"<p>Use <code>java</code> to run the Tracer <code>jar</code> file:</p> <pre><code>java -jar lib/tracer.jar\n</code></pre> How does that look like? <p>Here is how Tracer looks like in a console environment:</p> <p></p> <p>Tracer in a console environment</p> <p>For this to work, one needs to login using SSH with X forwarding enabled.</p> <p>Spoiler: use <code>ssh -X</code></p>"},{"location":"software/tracer/#links","title":"Links","text":"<ul> <li>Tracer GitHub repository</li> </ul>"},{"location":"software/transit_file_transfer_using_filezilla/","title":"File transfer to/from Transit using FileZilla","text":"","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#file-transfer-tofrom-transit-using-filezilla","title":"File transfer to/from Transit using FileZilla","text":"<p>There are multiple ways to transfer files to/from Transit using a graphical tool</p> <p>Here it is shown how to transfer files using a graphical tool called FileZilla.</p> What is Transit? <p>See the page about the UPPMAX Transit server.</p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#procedure","title":"Procedure","text":"<p>FileZilla connected to Transit</p> Would you like a video? <p>See the YouTube video file transfer from/to Transit using FileZilla</p> <p>FileZilla is a secure file transfer tool that works under Linux, Mac and Windows.</p> <p>To transfer files to/from Transit using FileZilla, do:</p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#2-start-filezilla","title":"2. Start FileZilla","text":"","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#3-from-the-menu-select-file-site-manager","title":"3. From the menu, select 'File | Site manager'","text":"Where is that? <p>It is here:</p> <p></p> <p>The FileZilla 'File' menu contains the item 'Site manager'</p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#4-click-new-site","title":"4. Click 'New site'","text":"Where is that? <p>It is here:</p> <p></p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#5-create-a-name-for-the-site","title":"5. Create a name for the site","text":"<p>Create a name for the site, e.g. Transit.</p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#6-setup-the-site","title":"6. Setup the site","text":"<p>For that site, use all standards, except:</p> <ul> <li>Set protocol to 'SFTP - SSH File Transfer Protocol'</li> <li>Set host to <code>transit.uppmax.uu.se</code></li> <li>Set user to <code>[username]</code>, e.g. <code>sven</code></li> </ul> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#7-click-connect","title":"7. Click 'Connect'","text":"","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#8-you-will-be-asked-for-your-password","title":"8. You will be asked for your password","text":"<p>You will be asked for your password, hence type <code>[your password]</code>, e.g. <code>VerySecret</code>. You can save the password.</p> How does that look like? <p>It looks similar to this:</p> <p></p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#9-transfer-files-between-local-and-transit","title":"9. Transfer files between local and Transit","text":"<p>Now you can transfer files between your local computer and Transit.</p> How does that look like? <p>It looks like this:</p> <p></p>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#where-do-my-files-end-up","title":"Where do my files end up?","text":"<p>They seem to end up in your Transit home folder.</p> <p>Its location is at <code>/home/[user_name]</code>, for example, at <code>/home/sven</code>.</p> <p>However, this is not the case: upon closing FileZilla, the files you've uploaded are gone.</p> <p>You do need to transfer these files to other HPC clusters before closing FileZilla. For detailed instructions, see the guides at the respective cluster, among others:</p> <ul> <li>Bianca file transfer using Transit</li> <li>Rackham file transfer using Transit</li> </ul>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#extra-material","title":"Extra material","text":"","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_filezilla/#winscp","title":"WinSCP","text":"<p>WinSCP is a secure file transfer tool that works under Windows.</p> <p>To transfer files to/from Transit using WinSCP, do:</p> <ul> <li>Start WinSCP</li> <li>Create a new site</li> <li>For that site, use all standards, except:<ul> <li>Set file protocol to 'SFTP'</li> <li>Set host name to <code>transit.uppmax.uu.se</code></li> <li>Set user name to <code>[username]</code>, e.g. <code>sven</code></li> </ul> </li> </ul>","tags":["transfer","FileZilla","Transit"]},{"location":"software/transit_file_transfer_using_scp/","title":"Data transfer to/from Transit using SCP","text":"","tags":["transfer","data transfer","file transfer","scp","SCP","Transit","transit"]},{"location":"software/transit_file_transfer_using_scp/#data-transfer-tofrom-transit-using-scp","title":"Data transfer to/from Transit using SCP","text":"<p>Data transfer to/from Transit using <code>scp</code> is one of the ways ways to transfer files to/from Transit.</p> What is Transit? <p>Transit is an UPPMAX service to send files around. It is not a file server.</p> <p>See the page about Transit for more detailed information.</p> What are the other ways to transfer files from/to Transit? <p>See the other ways to transfer data to/from Transit</p> <p>One cannot transfer files to/from Transit using <code>scp</code>: <code>scp</code> is not considered 'secure' anymore: instead it is considered an outdated protocol.</p> <p>The program <code>scp</code> allows you to transfer files to/from Transit using SCP, by coping them between your local computer and Transit.</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Transit","transit"]},{"location":"software/transit_file_transfer_using_scp/#how-to-transfer-files-between-a-local-computer-and-transit","title":"How to transfer files between a local computer and Transit","text":"<p>The process is:</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Transit","transit"]},{"location":"software/transit_file_transfer_using_scp/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Transit","transit"]},{"location":"software/transit_file_transfer_using_scp/#2-start-a-terminal-on-your-local-computer","title":"2. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Transit","transit"]},{"location":"software/transit_file_transfer_using_scp/#3a-using-scp-to-download-from-transit","title":"3a. Using <code>scp</code> to download from Transit","text":"<p>In the terminal, copy files using <code>scp</code> to download files from Transit:</p> <pre><code>scp [username]@transit.uppmax.uu.se:/home/[username]/[remote_filename] [local_folder]\n</code></pre> <p>where <code>[remote_filename]</code> is the path to a remote filename, <code>[username]</code> is your UPPMAX username, and <code>[local_folder]</code> is your local folder, for example:</p> <pre><code>scp sven@transit.uppmax.uu.se:/home/sven/my_remote_file.txt /home/sven\n</code></pre> <p>If asked, give your UPPMAX password.</p> <p>You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Transit","transit"]},{"location":"software/transit_file_transfer_using_scp/#3b-using-scp-to-upload-to-transit","title":"3b. Using <code>scp</code> to upload to Transit","text":"<p>This is how you would copy a file from your local computer to Transit:</p> <pre><code>scp [local_filename] [username]@transit.uppmax.uu.se:/home/[username]\n</code></pre> <p>where <code>[local_filename]</code> is the path to a local filename, and <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>scp my_file.txt sven@transit.uppmax.uu.se:/home/sven\n</code></pre> <p>However, Transit is not a file server. The <code>scp</code> command will complete successfully, yet the file will not be found on Transit.</p> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you have setup SSH keys</p>","tags":["transfer","data transfer","file transfer","scp","SCP","Transit","transit"]},{"location":"software/transit_file_transfer_using_sftp/","title":"Data transfer to/from Transit using SFTP","text":"","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#data-transfer-tofrom-transit-using-sftp","title":"Data transfer to/from Transit using SFTP","text":"<p>Data transfer to/from Transit using SFTP is one of the ways ways to transfer files to/from Transit.</p> What is Transit? <p>See the page about the UPPMAX Transit server.</p> What are the other ways? <p>See the other ways to transfer data to/from Transit</p> <p>One can transfer files to/from Transit using SFTP. SFTP is an abbreviation of 'SSH File Transfer Protocol', where 'SSH' is an abbreviation of 'Secure Shell protocol' The program <code>sftp</code> allows you to transfer files to/from Transit using SFTP.</p>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#using-sftp","title":"Using SFTP","text":"<p>The procedure is described in the following steps.</p>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#1-get-inside-sunet","title":"1. Get inside SUNET","text":"<p>Get inside SUNET.</p> Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#2-start-a-terminal-on-your-local-computer","title":"2. Start a terminal on your local computer","text":"<p>Start a terminal on your local computer.</p>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#3-connect-sftp-to-transit","title":"3. Connect <code>sftp</code> to Transit","text":"<p>In the terminal, connect <code>sftp</code> to Transit by doing:</p> <pre><code>sftp [username]@transit.uppmax.uu.se\n</code></pre> <p>where <code>[username]</code> is your UPPMAX username, for example:</p> <pre><code>sftp sven@transit.uppmax.uu.se\n</code></pre> <p>If asked, give your UPPMAX password. You can get rid of this prompt if you have setup SSH keys.</p>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#5-in-sftp-uploaddownload-files-tofrom-transit","title":"5. In <code>sftp</code>, upload/download files to/from Transit","text":"<p>In <code>sftp</code>, upload/download files to/from Transit.</p> <p>For example, to upload a file to Transit:</p> <pre><code>put my_file.txt\n</code></pre>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#where-do-my-files-end-up","title":"Where do my files end up?","text":"<p>They seem to end up in your Transit home folder.</p> <p>Its location is at <code>/home/[user_name]</code>, for example, at <code>/home/sven</code>.</p> <p>However, this is not the case: upon closing <code>sftp</code>, the files you've uploaded are gone.</p> <p>You do need to transfer these files to other HPC clusters before closing <code>sftp</code>. For detailed instructions, see the guides at the respective cluster, among others:</p> <ul> <li>Rackham file transfer using SFTP</li> </ul>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_sftp/#overview","title":"Overview","text":"<pre><code>flowchart TD\n\n    %% Give a white background to all nodes, instead of a transparent one\n    classDef node fill:#fff,color:#000,stroke:#000\n\n    %% Graph nodes for files and calculations\n    classDef file_node fill:#fcf,color:#000,stroke:#f0f\n    classDef calculation_node fill:#ccf,color:#000,stroke:#00f\n\n    user(User)\n      user_local_files(Files on user computer):::file_node\n\n    subgraph sub_inside[SUNET]\n      subgraph sub_transit_shared_env[Transit]\n          login_node(login/calculation/interactive session):::calculation_node\n          files_in_transit_home(Files in Transit home folder):::file_node\n      end\n    end\n\n    %% Shared subgraph color scheme\n    %% style sub_outside fill:#ccc,color:#000,stroke:#ccc\n    style sub_inside fill:#fcc,color:#000,stroke:#fcc\n    style sub_transit_shared_env fill:#ffc,color:#000,stroke:#ffc\n\n    user --&gt; |logs in |login_node\n    user --&gt; |uses| user_local_files\n\n    login_node --&gt; |can use|files_in_transit_home\n    %% user_local_files &lt;--&gt; |graphical tool|files_in_transit_home\n    %% user_local_files &lt;--&gt; |SCP|files_in_transit_home\n    user_local_files &lt;==&gt; |SFTP|files_in_transit_home\n\n    %% Aligns nodes prettier\n    user_local_files ~~~ login_node</code></pre> <p>Overview of file transfer on Transit The purple nodes are about file transfer, the blue nodes are about 'doing other things'. The user can be either inside or outside SUNET.</p>","tags":["transfer","SFTP","sftp","Transit"]},{"location":"software/transit_file_transfer_using_winscp/","title":"File transfer to/from Transit using WinSCP","text":"","tags":["transfer","data transfer","file transfer","Transit","transit","WinSCP"]},{"location":"software/transit_file_transfer_using_winscp/#file-transfer-tofrom-transit-using-winscp","title":"File transfer to/from Transit using WinSCP","text":"<p>There are multiple ways to transfer files to/from Transit using a graphical tool</p> <p>Here it is shown how to transfer files using a graphical tool called WinSCP.</p> What is Transit? <p>See the page about the UPPMAX Transit server.</p> What are the other ways? <p>See the other ways to transfer data to/from Transit</p>","tags":["transfer","data transfer","file transfer","Transit","transit","WinSCP"]},{"location":"software/transit_file_transfer_using_winscp/#procedure","title":"Procedure","text":"<p>WinSCP is a secure file transfer tool that works under Windows.</p> <p>To transfer files to/from Transit using WinSCP, do:</p> <ul> <li>Start WinSCP</li> <li>Create a new site</li> <li>For that site, use all standards, except:<ul> <li>Set file protocol to 'SFTP'</li> <li>Set host name to <code>transit.uppmax.uu.se</code></li> <li>Set user name to <code>[username]</code>, e.g. <code>sven</code></li> </ul> </li> </ul>","tags":["transfer","data transfer","file transfer","Transit","transit","WinSCP"]},{"location":"software/uquota/","title":"uquota","text":""},{"location":"software/uquota/#uquota","title":"<code>uquota</code>","text":"<p><code>uquota</code> is an UPPMAX tool to determine how much storage space is left in all projects.</p> <p>See the help file:</p> <pre><code>uquota --help\n</code></pre> How does that look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham1 ~]$ uquota --help\nusage: uquota [-h] [-q] [-d] [-u USER] [-p PROJECTS_FILE] [--include-expired]\n              [--random-usage] [--only-expired] [--sort-by-col SORT_BY_COL]\n              [-s] [-f]\n\noptional arguments:\n  -h, --help            Ask for help\n  -q, --quiet           Quiet, abbreviated output\n  -d, --debug           Include debug output\n  -u USER, --user USER\n  -p PROJECTS_FILE, --projects-file PROJECTS_FILE\n  --include-expired     Include expired projects\n  --random-usage        removed option, don't use\n  --only-expired        Only show expired projects\n  --sort-by-col SORT_BY_COL\n                        Index (0-4) of column to sort by. Default is 0.\n  -s, --slow            Deprecated. Previously ran 'du' command\n  -f, --files           Reports on number of files. Only for home directories\n</code></pre> <p>Usage:</p> <pre><code>uquota\n</code></pre> How does that look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham3 ~]$ uquota\nYour project     Your File Area       Unit        Usage  Quota Limit  Over Quota\n---------------  -------------------  -------  --------  -----------  ----------\nhome             /home/sven           GiB          24.7           32\nhome             /home/sven           files       79180       300000\nnaiss2024-22-49  /proj/worldpeace     GiB           5.1          128\nnaiss2024-22-49  /proj/worldpeace     files       20276       100000\n</code></pre> <p>If you find out that your home folder is full, but do not know which folder takes up most space, use the command below to find it:</p> <pre><code>du --human --max-depth 1 .\n</code></pre> How does that look like? <p>Your output will be similar to this:</p> <pre><code>[sven@rackham2 ~]$ du --human --max-depth 1 .\n28K ./bin\n52M ./.config\n8.0K ./glob\n1.5G ./users\n484K ./.ssh\n9.7M ./.lmod.d\n514M ./.gradle\n4.0K ./.oracle_jre_usage\n84K ./.pki\n3.2G ./.singularity\n4.0K ./.git-credential-cache\n8.0K ./.keras\n6.1G ./.cache\n344M ./R\n740K ./.local\n8.0K ./.nv\n32M ./.nextflow\n88K ./.r\n140K ./.dbus\n48K ./.subversion\n8.0K ./.gnupg\n480K ./.java\n8.0K ./.vscode-oss\n29M ./.mozilla\n41M ./private\n64K ./.ipython\n8.0K ./.rstudio-desktop\n4.0K ./.allinea\n8.8M ./.beast\n688K ./.gstreamer-0.10\n8.4G ./.apptainer\n4.0K ./my_best_folder\n3.7G ./GitHubs\n260K ./.kde\n24K ./.jupyter\n849M ./.conda\n4.7M ./lib\n176M ./.vscode-server\n16K ./.MathWorks\n8.2M ./.matlab\n25G .\n</code></pre>"},{"location":"software/valgrind/","title":"Valgrind","text":""},{"location":"software/valgrind/#valgrind","title":"Valgrind","text":"<p>There are multiple profilers available on UPPMAX. This page describes Valgrind.</p> <p>Valgrind is a suite of simulation-based debugging and profiling tools for programs.</p> <p>Valgrind contains several tools:</p> <ul> <li><code>memcheck</code>, for detecting memory-management problems in your program</li> <li><code>cachegrind</code>, for cache profiling</li> <li><code>helgrind</code>, finds data races in multithreaded programs</li> <li><code>callgrind</code>, a call graph profiler</li> <li><code>drd</code>, a thread error detector</li> <li><code>massif</code>, a heap profiler</li> <li><code>ptrcheck</code>, a pointer checking tool</li> <li><code>lackey</code>, a simple profiler and memory tracer</li> </ul> <p>Valgrind works best with the GCC and Intel compilers.</p> <p>There is a system <code>valgrind-3.15.0</code> from 2020.</p> <p>First load compiler:</p> <pre><code>module load gcc\n</code></pre> <p>or</p> <pre><code>module load intel\n</code></pre> <p>then you can use <code>valgrind</code> by:</p> <pre><code>valgrind [options] ./your-program [your programs options]\n</code></pre>"},{"location":"software/valgrind/#how-to-use-valgrind-with-mpi-programs","title":"How to use valgrind with MPI programs","text":"<p>Load your compiler, <code>openmpi</code> and the <code>valgrind</code> module as before:</p> <pre><code>module load gcc/10.3.0 openmpi/3.1.6\n</code></pre> <p>or</p> <pre><code>module load intel/20.4 openmpi/3.1.6\n</code></pre> <p>As of now, Valgrind seems not compatible with <code>openmpi/4.X.X</code>.</p> <p>Then run:</p> <pre><code>LD_PRELOAD=$VALGRIND_MPI_WRAPPER\nmpirun -np 2 valgrind ./your-program\n</code></pre>"},{"location":"software/vartrix/","title":"VarTrix","text":""},{"location":"software/vartrix/#vartrix","title":"VarTrix","text":"<p>VarTrix is 'a software tool for extracting single cell variant information from 10x Genomics single cell data' (as quoted from the VarTrix repository).</p> <p>To use VarTrix on an UPPMAX cluster, do</p> <pre><code>module load bioinfo-tools\n</code></pre> <p>After this, search for the module of your favorite Vartrix version, using:</p> <pre><code>module spider vartrix\n</code></pre> How does that look like? <p>The output will look similar to:</p> <pre><code>[sven@rackham3 vartrix]$ module spider vartrix\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  vartrix: vartrix/1.1.22\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"vartrix/1.1.22\" module is available to load.\n\n      bioinfo-tools\n\n    Help:\n      vartrix - use vartrix\n\n      Description\n\n      Single-Cell Genotyping Tool\n\n      Version 1.1.22\n\n      https://github.com/10XGenomics/vartrix\n\n      Usage:\n\n          Example:\n\n          vartrix --bam $VARTRIX_TEST/test_dna.bam \\\n                  --cell-barcodes $VARTRIX_TEST/dna_barcodes.tsv \\\n                  --fasta $VARTRIX_TEST/test_dna.fa  \\\n                  --vcf $VARTRIX_TEST/test_dna.vcf\n</code></pre> <p>Then load your favorite version, for example:</p> <pre><code>module load vartrix/1.1.22\n</code></pre>"},{"location":"software/vartrix/#links","title":"Links","text":"<ul> <li>vartrix repository</li> </ul>"},{"location":"software/venv_on_rackham/","title":"venv on Rackham","text":""},{"location":"software/venv_on_rackham/#venv-on-rackham","title":"<code>venv</code> on Rackham","text":"Want to see a video? <p>See the YouTube video   How to use a Python venv on the Rackham UPPMAX cluster</p>"},{"location":"software/vim/","title":"vim","text":""},{"location":"software/vim/#vim","title":"vim","text":"<p>UPPMAX has multiple text editors available. This page describes the <code>vim</code> text editor.</p> <p><code>vim</code> is an advanced terminal editor that is fast fast and powerful, once you learn it.</p> <p>Start <code>vim</code> on a terminal with:</p> <pre><code>vi\n</code></pre> <p>Then:</p> <ul> <li>Insert mode: type like normal text editor. Press <code>i</code> to enter insert mode</li> <li>Command mode: give commands to the editor.   Press <code>Escape</code> to enter command mode)</li> <li>Cheat sheet: https://coderwall.com/p/adv71w/basic-vim-commands-for-getting-started</li> </ul>"},{"location":"software/vscode/","title":"VSCode","text":""},{"location":"software/vscode/#vscode","title":"VSCode","text":"<p>Visual Studio Code ('VSCode') is an IDE that can be used for software development in many languages.</p> <p></p> <p>VSCode from a local computer working on Rackham.</p> <p>If you can use VSCode, depends on the HPC cluster:</p> Cluster Works/fails Documentation page Bianca Fails [1] VSCode on Bianca Rackham Works VSCode on Rackham <ul> <li>[1] Use VSCodium on Bianca instead</li> </ul>"},{"location":"software/vscode_on_bianca/","title":"Using Visual Studio Code on Bianca","text":"","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_bianca/#using-visual-studio-code-on-bianca","title":"Using Visual Studio Code on Bianca","text":"<p>VSCode fails, use VSCodium instead</p> <p>The approach below will fail (note that using VSCode on Rackham does work).</p> <p>Instead, go to the page Using VSCodium on Bianca</p>","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_bianca/#introduction","title":"Introduction","text":"<p>There are multiple IDEs on Bianca, among other VSCodium. Here we discuss that running VSCode on Bianca will fail.</p> <p>Visual Studio Code ('VSCode') is an IDE that can be used for software development in many languages.</p> What is an IDE? <p>See the page on IDEs.</p> <p>In this session, we show how to use VSCode on Bianca.</p>","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_bianca/#procedure-to-start-vscode","title":"Procedure to start VSCode","text":"","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_bianca/#1-install-vscode-on-your-local-computer","title":"1. Install VSCode on your local computer","text":"","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_bianca/#2-start-vscode-on-your-local-computer","title":"2. Start VSCode on your local computer","text":"","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_bianca/#3-in-vscode-install-the-vscode-remote-tunnels-plugin","title":"3. In VSCode, install the VSCode 'Remote Tunnels' plugin","text":"","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_bianca/#4-in-vscode-connect-to-bianca","title":"4. In VSCode, connect to Bianca","text":"<p>In VSCode, at the 'Remote Explorer' tab, click on 'SSH', then on 'New Remote'.</p> <p></p> <p>This is the step that fails</p>","tags":["VSCode","Bianca"]},{"location":"software/vscode_on_rackham/","title":"Connecting Visual Studio Code to Rackham","text":"","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#connecting-visual-studio-code-to-rackham","title":"Connecting Visual Studio Code to Rackham","text":"<p>Latest VSCode does not work</p> <p>VSCode versions 1.99 and newer no longer support the operating system used on Rackham.</p> <p>Downgrade VSCode to version 1.98 or earlier to use with Rackham.</p> <p>This will not be needed with the upcoming Rackham replacement cluster Pelle.</p> <p></p> <p>VSCode from a local computer working on Rackham.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#introduction","title":"Introduction","text":"<p>Visual Studio Code ('VSCode') is an IDE that can be used for software development in many languages.</p> What is an IDE? <p>See the page on IDEs.</p> <p>In this session, we show how to connect VSCode on your local computer to work with your files on Rackham.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#procedure","title":"Procedure","text":"<p>Below is a step-by-step procedure to start VSCode.</p> Prefer a video? <p>See this YouTube video.</p> <p>An older version of this procedure, where the 'Remote Tunnel' extension is used, can be seen in this YouTube video.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#1-install-vscode-on-your-local-computer","title":"1. Install VSCode on your local computer","text":"<p>Install VSCode on your local computer.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#2-start-vscode-on-your-local-computer","title":"2. Start VSCode on your local computer","text":"How does that look like?","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#3-in-vscode-install-the-vscode-remote-ssh-plugin","title":"3. In VSCode, install the VSCode 'Remote-SSH' plugin","text":"<p>In VSCode, install the VSCode 'Remote-SSH' plugin.</p> How does that look like? <p></p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#4-in-the-remote-explorer-tab-at-ssh-click-the-plus","title":"4. In the 'Remote Explorer' tab, at SSH, click the plus","text":"<p>In VSCode, go to the 'Remote Explorer' tab. At the SSH section, click on the '+' (with tooltip 'New remote').</p> How does that look like? <p></p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#5-give-the-ssh-command-to-connect-to-rackham","title":"5. Give the SSH command to connect to Rackham","text":"<p>In the main edit bar, give the SSH command to connect to Rackham, e.g. <code>ssh sven@rackham.uppmax.uu.se</code></p> How does that look like? <p></p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#6-pick-the-a-location-for-the-ssh-config-file","title":"6. Pick the a location for the SSH config file","text":"<p>In the dropdown menu, pick the a location for the SSH config file, e.g. the first, which is similar to <code>/home/sven/.ssh/config</code>.</p> How does that look like? <p></p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#7-click-connect","title":"7. Click 'Connect'","text":"<p>In the bottom left of VSCode, click on the popup window 'Connect'.</p> How does that look like? <p></p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#8-done","title":"8. Done","text":"<p>You are now connected: there is a new window with VSCode connected to Rackham.</p> How does that look like? <p>The window that is connected to a Rackham home folder:</p> <p></p> <p>Going to <code>/proj/staff</code>:</p> <p></p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#setting-up-vscode-for-rackham-and-snowy","title":"Setting up VSCode for Rackham and Snowy","text":"<p>Info</p> <ul> <li>You can run VSCode on your local and still be able to work with modules loaded or environment created on Rackham.</li> <li>Similarly it is possible to take advantage of Snowy GPUs meanwhile developing on your local computer.</li> </ul>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#1-connect-your-local-vscode-to-vscode-server-running-on-rackham","title":"1. Connect your local VSCode to VSCode server running on Rackham","text":"<p>Perform steps mentioned under the section Procedure to start VSCode.</p> <p>When you first establish the ssh connection to Rackham, your VSCode server directory <code>.vscode-server</code> will be created in your home folder <code>/home/[username]</code>.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#2-install-and-manage-extensions-on-remote-vscode-server","title":"2. Install and manage Extensions on remote VSCode server","text":"<p>By default all the VSCode extensions will get installed on your home folder <code>/home/[username]</code>. Due to less storage quota on home folder <code>32 GB, 300k files</code>, can quickly fill up with extensions and other file operations. The default installation path for VSCode extensions can however be changed to your project folder which have way more storage space and file count capacity, <code>1TB, 1M files</code>.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#21-manage-extensions","title":"2.1. Manage Extensions","text":"<p>Go to Command Palette <code>Ctrl+Shift+P</code> or <code>F1</code>. Search for <code>Remote-SSH: Settings</code> and then go to <code>Remote.SSH: Server Install Path</code>. Add Item as remote host <code>rackham.uppmax.uu.se</code> and Value as folder in which you want to install all your data and extensions <code>/proj/uppmax202x-x-xx/nobackup</code> (without a trailing slash <code>/</code>).</p> <p>If you already had your <code>vscode-server</code> running and storing extensions in home directory. Make sure to kill the server by selecting <code>Remote-SSH: KIll VS Code Server on Host</code> on Command Palette and deleting the <code>.vscode-server</code> directory in your home folder.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#22-install-extensions","title":"2.2. Install Extensions","text":"<p>You can sync all your local VSCode extensions to the remote server after you are connected with VSCode server on Rackham by searching for <code>Remote: Install Local Extensions in 'SSH: rackham.uppmax.uu.se'</code> in Command Palette. You can alternatively, go to Extensions tab and select each individually.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#23-selecting-kernels","title":"2.3. Selecting Kernels","text":"<p>Request allocation in either Rackham or Snowy compute node depending on your need, for that use <code>interactive</code> slurm command. Load the correct module on Rackham/Snowy that you contains the interpret you want on your VSCode. For example in case you need ML packages and python interpreter, do <code>module load python_ML_packages</code>. Check the file path for python interpreter by checking <code>which python</code> and copy this path. Go to Command Palette <code>Ctrl+Shift+P</code> or <code>F1</code> on your local VSCode. Search for \"interpreter\" for python, then paste the path of your interpreter/kernel.</p> <p><code>venv</code> or <code>conda</code> environments are also visible on VSCode when you select interpreter/kernel for python or jupyter server. For jupyter, you need to start the server first, check Point 3.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#3-working-with-jupyter-server-on-rackham-and-snowy","title":"3. Working with jupyter server on Rackham and snowy","text":"","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#rackham","title":"Rackham","text":"<p>Module load jupyter packages either from <code>module load python</code> or <code>module load python_ML_packages</code> as per your needs. For heavy compute and longer running jupyter server, allocate a Rackham compute node instead of using login node. Either request for rackham compute node by using, for example, <code>interactive -A uppmax202x-x-xx -p node -N 1 -t 2:00:00</code> or move to the next step to run jupyter on login node itself. Start the jupyter server <code>jupyter notebook --ip 0.0.0.0 --no-browser</code>. Copy the jupyter server URL which goes something like <code>http://r52.uppmax.uu.se:8888/tree?token=xxx</code>, click on Select Kernel on VSCode and select Existing Jupyter Server. Past the URL here and confirm your choice.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscode_on_rackham/#snowy","title":"Snowy","text":"<p>Start an interactive session with GPU allocation on Snowy <code>interactive -A uppmax202x-x-xx -p node -N 1 -t 02:00:00 --gres=gpu:1 -M snowy</code>. Module load the jupyter packages <code>module load python_ML_packages</code> and start the jupyter server <code>jupyter notebook --ip 0.0.0.0 --no-browser</code>. This should start a jupyter server on Snowy compute node with one T4 GPU. Copy the URL of the running jupyter server which goes something like <code>http://s193.uppmax.uu.se:8888/tree?token=xxx</code> and paste it in the jupyter kernel path on your local VSCode. The application will automatically perform port forwarding to Rackham, which already is listening to Snowy compute nodes over certain ports.</p>","tags":["VSCode","Rackham","connect"]},{"location":"software/vscodium/","title":"VSCodium","text":"","tags":["VSCodium"]},{"location":"software/vscodium/#vscodium","title":"VSCodium","text":"<p>VSCodium is the community edition of Visual Studio Code and is an IDE that can be used for software development in many languages.</p> <p></p> <p>VSCodium running on Bianca</p> <p>If you can use VSCodium, depends on the HPC cluster:</p> Cluster Works/fails Documentation page Bianca Works VSCodium on Bianca Rackham Fails [1] VSCodium on Rackham <ul> <li>[1] Use VSCode on Rackham instead</li> </ul>","tags":["VSCodium"]},{"location":"software/vscodium_on_bianca/","title":"Using VSCodium on Bianca","text":"","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#using-vscodium-on-bianca","title":"Using VSCodium on Bianca","text":"<p>VSCodium running on Bianca</p>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#introduction","title":"Introduction","text":"<p>There are multiple IDEs on Bianca, among other VSCodium. Here we discuss how to run VSCodium on Bianca.</p> <p>VSCodium is the community edition of Visual Studio Code and can be used for software development in many languages.</p> What is an IDE? <p>See the page on IDEs.</p> <p>In this session, we show how to use VSCodium on Bianca, using Bianca's remote desktop environment.</p> Forgot how to login to a remote desktop environment? <p>See the 'Logging in to Bianca' page.</p> <p>As VSCodium is a resource-heavy program, it must be run on an interactive session.</p>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#procedure-to-start-vscodium","title":"Procedure to start VSCodium","text":"<p>Below is a step-by-step procedure to start RStudio. This procedure is also demonstrated in this YouTube video.</p>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#1-get-within-sunet","title":"1. Get within SUNET","text":"Forgot how to get within SUNET? <p>See the 'get inside the university networks' page</p>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#2-start-the-bianca-remote-desktop-environment","title":"2. Start the Bianca remote desktop environment","text":"Forgot how to start Bianca's remote desktop environment? <p>See the 'Logging in to Bianca' page.</p>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#3-start-an-interactive-session","title":"3. Start an interactive session","text":"<p>Within the Bianca remote desktop environment, start a terminal. Within that terminal, start an interactive session with 1 core.</p> Forgot how to start an interactive session? <p>See the 'Starting an interactive session' page.</p> <p>Spoiler: use:</p> <pre><code>interactive -A sens2023598 -n 1 -t 8:00:00\n</code></pre>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#4-load-the-modules-needed","title":"4. Load the modules needed","text":"<p>VSCodium needs the <code>VSCodium/latest</code> module.</p> <p>In the terminal of the interactive session, do:</p> <pre><code>module load VSCodium/latest\n</code></pre>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_bianca/#5-start-vscodium","title":"5. Start VSCodium","text":"<p>With the modules loaded, in that same terminal, start VSCodium:</p> <pre><code>code\n</code></pre> <p>VSCodium starts up quickly.</p> How does VSCodium look on Bianca? <p></p>","tags":["VSCodium","Bianca"]},{"location":"software/vscodium_on_rackham/","title":"Using VSCodium on Rackham","text":"","tags":["VSCodium","Rackham"]},{"location":"software/vscodium_on_rackham/#using-vscodium-on-rackham","title":"Using VSCodium on Rackham","text":"<p>VSCodium on another cluster, as VSCodium on Rackham fails</p> <p>VSCodium fails, use VSCode instead</p> <p>The approach below will fail (note that using VSCodium on Bianca does work).</p> <p>Instead, go to the page Using VSCode on Rackham</p>","tags":["VSCodium","Rackham"]},{"location":"software/vscodium_on_rackham/#introduction","title":"Introduction","text":"<p>VSCodium is the community edition of Visual Studio Code and can be used for software development in many languages.</p> What is an IDE? <p>See the page on IDEs.</p> <p>In this session, we show how to use VSCodium on Rackham, using Rackham's remote desktop environment.</p> Forgot how to login to a remote desktop environment? <p>See the 'Logging in to Rackham' page.</p> <p>As VSCodium is a resource-heavy program, it must be run on an interactive session.</p>","tags":["VSCodium","Rackham"]},{"location":"software/vscodium_on_rackham/#procedure-to-start-vscodium","title":"Procedure to start VSCodium","text":"","tags":["VSCodium","Rackham"]},{"location":"software/vscodium_on_rackham/#1-start-the-rackham-remote-desktop-environment","title":"1. Start the Rackham remote desktop environment","text":"Forgot how to start Rackham's remote desktop environment? <p>See the 'Logging in to Rackham' page.</p>","tags":["VSCodium","Rackham"]},{"location":"software/vscodium_on_rackham/#2-start-an-interactive-session","title":"2. Start an interactive session","text":"<p>Within the Rackham remote desktop environment, start a terminal. Within that terminal, start an interactive session with 1 core.</p> Forgot how to start an interactive session? <p>See the 'Starting an interactive session' page.</p> <p>Spoiler: use:</p> <pre><code>interactive -A uppmax2023-2-25\n</code></pre>","tags":["VSCodium","Rackham"]},{"location":"software/vscodium_on_rackham/#3-load-the-modules-needed","title":"3. Load the modules needed","text":"<p>VSCodium needs the <code>VSCodium/latest</code> module.</p> <p>In the terminal of the interactive session, do:</p> <pre><code>module load VSCodium/latest\n</code></pre>","tags":["VSCodium","Rackham"]},{"location":"software/vscodium_on_rackham/#4-start-vscodium","title":"4. Start VSCodium","text":"<p>With the modules loaded, in that same terminal, start VSCodium:</p> <pre><code>code\n</code></pre> <p>VSCodium will give an error?</p> How does the VSCodium error look on Rackham? <p></p>","tags":["VSCodium","Rackham"]},{"location":"software/whisper/","title":"Whisper","text":"","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#whisper","title":"Whisper","text":"","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#introduction","title":"Introduction","text":"<p>This guide provides instructions for loading and using OpenAI's Whisper, an automatic speech recognition system. Whisper app is available on Bianca or can also be loaded as a module.</p> <p>AI tool caution</p> <p>Like all other AI models, Whisper too hallucinates while transcribing or translating. ie, \"make-up\" words or even sentences, resulting in misinterpretation or misrepresentation of the speaker.</p> Quality of transcriptions/ translations and audio formats <p>Transcriptions (error rate):</p> <ul> <li>Swedish: ~10%</li> <li>English: ~5%</li> <li>English with heavy accent: ~ 20%</li> </ul> <p>Translations:</p> <ul> <li>Any to English: \"DeepL\" level performance. Slightly better than google translate.</li> </ul> <p>Supported file types: mp3, mp4, mpeg, mpga, m4a, wav, webm and wma.</p> <p>Quality as a factor of duration of recordings:</p> <ul> <li>A few minutes: Excellent</li> <li>A few minutes to an hour: Excellent at the beginning, then detoriates.</li> <li>An hour or more: Excellent at the beginning, then detoriates.</li> </ul> <p>Quality as a factor of noise and count of speakers:</p> <ul> <li>2 speakers: Excellent</li> <li>Background noise: Good</li> <li>2+ speakers: Very Good</li> <li>Conversational overlap: Average. Difficulty disambiguating speakers.</li> <li>Long silences: Good. Might repeat sentences and get stuck in loop.</li> </ul> <p>Whisper also tries to give separate sentences for different speakers. But it is not guaranteed.</p> <p>Recordings from Dictaphone </p> <p>If you record using dictaphone such as Olympus DS-9000, it would by default record in <code>.DS</code> or <code>.DS2</code> file formats which are NOT supported by Whisper. Make sure to change the settings on the dictaphone to <code>.mp3</code> format before you start recording. Follow this guide to convert your <code>DS</code> or <code>.DS2</code> recording to <code>.mp3</code> using the software that comes with your dictaphone. Else, you can also download the sofware and then follow the same guide.</p> Glossary <ul> <li>SUPR account : Gives access to project management account for submitting project proposals on SUPR.</li> <li>UPPMAX account : Gives access to UPPMAX servers, like Bianca.</li> <li>GUI : Graphical User Interface for taking transcription/translation inputs.</li> <li>WinSCP / FileZilla: user interface to send data from your computer to Bianca and vice-versa.</li> <li>SUNET: Swedish university network</li> <li>Terminal : Black text-based environment that is used for performing jobs.</li> <li>Wharf: private folder in Bianca that is used to transfer data to and from your computer.</li> <li>Proj: project folder in Bianca that is shared among all project members.</li> <li>Job: A request for transcribing/translating one or many recordings.</li> <li>Slurm: \"job\" handler.</li> </ul> <p>Checklist for new project</p> <ul> <li> SUPR account</li> <li> Submit project proposal</li> <li> UPPMAX username and password</li> <li> UPPMAX two factor authentication.</li> </ul>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#accessing-your-project","title":"Accessing your project","text":"<p>Following steps are derived from Project application for Bianca:</p> <ol> <li> <p>Register an account on SUPR.</p> </li> <li> <p>Apply for a project for sensitive data at Bianca. Give adequate information while creating your proposal by following this template.</p> </li> <li> <p>Register an account for UPPMAX at SUPR by clicking \"Request Account at UPPMAX\" button. You will receive an UPPMAX username and password via email.</p> </li> <li> <p>Setup two factor authentication for this newly created UPPMAX account. (Video)</p> </li> <li> <p>Check access to your project on Bianca. (Video)</p> </li> </ol>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#whisper-app","title":"Whisper App","text":"","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#step-1-data-transfer-from-local-computer-to-bianca","title":"Step 1: Data transfer from local computer to Bianca","text":"<ol> <li> <p>Transfer your data from your local computer to Wharf using WinSCP app (for Windows only) or FileZilla app (Mac, Windows or Linux). Instruction on how to do it is in their respective links or watch FileZilla Video.</p> <p></p> </li> </ol>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#step-2-transcribingtranslating","title":"Step 2: Transcribing/Translating","text":"<ol> <li> <p>Login to Bianca. It requires your UPPMAX username (visible in SUPR), project name and two factor authentication code. Make sure you are inside SUNET for the link to work.</p> </li> <li> <p>Click on the Terminal icon on the bottom of the Desktop and enter the following command in it to load Whisper app. If you cannot find the Terminal icon, you can alternatively right-click on the Desktop and select 'Open Terminal'.</p> <pre><code>module load Whisper-gui\n</code></pre> <p></p> Video Demo <p></p> </li> <li> <p>You shall now see <code>proj</code> and <code>wharf</code> folders on your Desktop along with a Whisper application icon. <code>wharf</code> contains the data that was transferred in Step 1. (Next time you start transcribing/translating by logging in again to Bianca, you can start from this step and skip the previous one, since <code>wharf</code> and <code>proj</code> folder are already created.)</p> <p></p> </li> <li> <p>Open <code>wharf</code> and <code>proj</code> folder.  Select all the data that you transferred in <code>wharf</code>, drag and drop it into the  <code>proj</code> folder. NOTE: if you drag and drop, it will cut-paste your data instead of copy-paste. Do not keep files in <code>wharf</code> for a long period, as this folder is connected to the outside world and hence is a security risk. <code>proj</code>, on the other hand, is safe to keep data in as it is cut-off from the internet, so move your data there.</p> </li> <li> <p>Click on Whisper application on Desktop. It would look like this:</p> <p></p> <p>Select appropriate options, or use the following for the best results:</p> <ul> <li> <p>Total audio length in hours: [give a rough average if transcribing files in bulk, rounding up to nearest hour]</p> </li> <li> <p>Language used in recordings (leave blank for autodetection): If you have multiple languages in the selected recordings or you are unsure about the spoken language, leave it blank. If your language of choice is unavailable in the drop down, check the \"Languages available\" list for its availability and contact support.</p> </li> <li> <p>Select whether to transcribe or translate (english only): 'Transcribe' [for language X -&gt; language X]. 'Translate' [for language X -&gt; English].</p> </li> <li> <p>Model: large-v2</p> </li> <li> <p>Initial Prompt: [leave blank]</p> </li> </ul> <p>Select your files that need to be transcribed/translated. Then, click 'Okay'. Select your folder where you would like to save your transcriptions/translations. Then, click 'Okay'. After this, your job will be submitted, and you will have to wait for the files to be processed.</p> Video Demo <p></p> </li> </ol>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#step-3-monitoring-jobs","title":"Step 3: Monitoring jobs","text":"<ol> <li> <p>Your job will first wait in a queue and then start executing. To first check if your job is waiting in the queue, type <code>squeue --me -o \"%.30j\"</code> on terminal. If you see your job name <code>Whisper_xxx</code> it means it is in the queue, where <code>xxx</code> is the date and time of job submission, example: Whisper_2024-10-25_11-10-30.</p> </li> <li> <p>To check if your job has started executing, locate a file named <code>[Whisper_xxx_yyy].out</code> that will get created in <code>Whisper_logs</code> folder inside <code>proj</code> folder, where <code>xxx</code> is date and time of job submission and <code>yyy</code> is your username followed by a \"job id\", example: Whisper_2024-10-25_11-10-30_jayan_234.out. This contains a progress bar for each recording that you sent for transcribing/translating.</p> </li> <li> <p>If neither job name <code>Whisper_xxx</code> was found in queue, nor a <code>[Whisper_xxx_yyy].out</code> was created in <code>Whisper_logs</code>, contact support.</p> </li> </ol>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#step-4-data-transfer-from-project-to-local-computer","title":"Step 4: Data transfer from project to local computer","text":"<ol> <li> <p>Drag and drop your transcriptions/translations from <code>proj</code> folder to <code>wharf</code>.</p> </li> <li> <p>Use WinSCP/FileZilla like you did in Step 1 and transfer your data from <code>wharf</code> to your local computer.</p> </li> </ol>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#output-files","title":"Output files","text":"<p>By default you receive 5 types of output files for each file you transcribe/translate:</p> <ul> <li>With timestamps: <code>.srt</code>, <code>.vtt</code>, <code>.tsv</code></li> <li>Without timestamps: <code>.txt</code></li> <li>With detailed model metadata: <code>.json</code>.</li> <li>The most popular ones are <code>.srt</code> and <code>.txt</code> formats.</li> </ul> <p>On Mac, <code>.txt</code>, <code>.srt</code> and <code>.vtt</code> can be opened in Word by:</p> <ul> <li>Tap with two fingers</li> <li>Select Encoding as \"Unicode (UTF-8)\"</li> <li>Change the name of the file like <code>some_name.docx</code> and change type of file to <code>.docx</code></li> <li>Open the file and then Save As a new file.</li> </ul> <p></p> Advance settings <p>Use below features only if transcriptions/translations are not satisfactory and for less spoken languages or languages that are not having good resources online for understanding :</p> <ol> <li> <p>When asked for Initial Prompt, provide a list of comma separated words or sentences (less than 80 words) that describe what the recording is about or the words used by the speaker in the recording. It should be in written in same language as the language in spoken in the recordings.</p> </li> <li> <p>Try switching to Model: large-v3.</p> </li> <li> <p>Use combination of both 1 and 2.</p> </li> <li> <p>If you are sure about the language used in the recording, use the dropdown menu and select the appropriate language.</p> </li> </ol> Languages available <p>Following languages are available for transcribing. If your language of choice does not appear in Whisper application but is listed here, contact support:</p> <ul> <li><code>en</code>: \"english\",</li> <li><code>zh</code>: \"chinese\",</li> <li><code>de</code>: \"german\",</li> <li><code>es</code>: \"spanish\",</li> <li><code>ru</code>: \"russian\",</li> <li><code>ko</code>: \"korean\",</li> <li><code>fr</code>: \"french\",</li> <li><code>ja</code>: \"japanese\",</li> <li><code>pt</code>: \"portuguese\",</li> <li><code>tr</code>: \"turkish\",</li> <li><code>pl</code>: \"polish\",</li> <li><code>ca</code>: \"catalan\",</li> <li><code>nl</code>: \"dutch\",</li> <li><code>ar</code>: \"arabic\",</li> <li><code>sv</code>: \"swedish\",</li> <li><code>it</code>: \"italian\",</li> <li><code>id</code>: \"indonesian\",</li> <li><code>hi</code>: \"hindi\",</li> <li><code>fi</code>: \"finnish\",</li> <li><code>vi</code>: \"vietnamese\",</li> <li><code>he</code>: \"hebrew\",</li> <li><code>uk</code>: \"ukrainian\",</li> <li><code>el</code>: \"greek\",</li> <li><code>ms</code>: \"malay\",</li> <li><code>cs</code>: \"czech\",</li> <li><code>ro</code>: \"romanian\",</li> <li><code>da</code>: \"danish\",</li> <li><code>hu</code>: \"hungarian\",</li> <li><code>ta</code>: \"tamil\",</li> <li><code>no</code>: \"norwegian\",</li> <li><code>th</code>: \"thai\",</li> <li><code>ur</code>: \"urdu\",</li> <li><code>hr</code>: \"croatian\",</li> <li><code>bg</code>: \"bulgarian\",</li> <li><code>lt</code>: \"lithuanian\",</li> <li><code>la</code>: \"latin\",</li> <li><code>mi</code>: \"maori\",</li> <li><code>ml</code>: \"malayalam\",</li> <li><code>cy</code>: \"welsh\",</li> <li><code>sk</code>: \"slovak\",</li> <li><code>te</code>: \"telugu\",</li> <li><code>fa</code>: \"persian\",</li> <li><code>lv</code>: \"latvian\",</li> <li><code>bn</code>: \"bengali\",</li> <li><code>sr</code>: \"serbian\",</li> <li><code>az</code>: \"azerbaijani\",</li> <li><code>sl</code>: \"slovenian\",</li> <li><code>kn</code>: \"kannada\",</li> <li><code>et</code>: \"estonian\",</li> <li><code>mk</code>: \"macedonian\",</li> <li><code>br</code>: \"breton\",</li> <li><code>eu</code>: \"basque\",</li> <li><code>is</code>: \"icelandic\",</li> <li><code>hy</code>: \"armenian\",</li> <li><code>ne</code>: \"nepali\",</li> <li><code>mn</code>: \"mongolian\",</li> <li><code>bs</code>: \"bosnian\",</li> <li><code>kk</code>: \"kazakh\",</li> <li><code>sq</code>: \"albanian\",</li> <li><code>sw</code>: \"swahili\",</li> <li><code>gl</code>: \"galician\",</li> <li><code>mr</code>: \"marathi\",</li> <li><code>pa</code>: \"punjabi\",</li> <li><code>si</code>: \"sinhala\",</li> <li><code>km</code>: \"khmer\",</li> <li><code>sn</code>: \"shona\",</li> <li><code>yo</code>: \"yoruba\",</li> <li><code>so</code>: \"somali\",</li> <li><code>af</code>: \"afrikaans\",</li> <li><code>oc</code>: \"occitan\",</li> <li><code>ka</code>: \"georgian\",</li> <li><code>be</code>: \"belarusian\",</li> <li><code>tg</code>: \"tajik\",</li> <li><code>sd</code>: \"sindhi\",</li> <li><code>gu</code>: \"gujarati\",</li> <li><code>am</code>: \"amharic\",</li> <li><code>yi</code>: \"yiddish\",</li> <li><code>lo</code>: \"lao\",</li> <li><code>uz</code>: \"uzbek\",</li> <li><code>fo</code>: \"faroese\",</li> <li><code>ht</code>: \"haitian creole\",</li> <li><code>ps</code>: \"pashto\",</li> <li><code>tk</code>: \"turkmen\",</li> <li><code>nn</code>: \"nynorsk\",</li> <li><code>mt</code>: \"maltese\",</li> <li><code>sa</code>: \"sanskrit\",</li> <li><code>lb</code>: \"luxembourgish\",</li> <li><code>my</code>: \"myanmar\",</li> <li><code>bo</code>: \"tibetan\",</li> <li><code>tl</code>: \"tagalog\",</li> <li><code>mg</code>: \"malagasy\",</li> <li><code>as</code>: \"assamese\",</li> <li><code>tt</code>: \"tatar\",</li> <li><code>haw</code>: \"hawaiian\",</li> <li><code>ln</code>: \"lingala\",</li> <li><code>ha</code>: \"hausa\",</li> <li><code>ba</code>: \"bashkir\",</li> <li><code>jw</code>: \"javanese\",</li> <li><code>su</code>: \"sundanese\",</li> <li><code>yue</code>: \"cantonese\"</li> </ul>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#proposal-template","title":"Proposal template","text":"<p>Under the Basic Information section on NAISS SUPR, provide the following compulsory details pertaining to your project in the following fashion:</p> <ul> <li> <p>Project Title : Whisper service for [Name of the project]</p> </li> <li> <p>Abstract: [What is the project about, give links, funding info, duration etc.]</p> </li> <li> <p>Resource Usage:  [Explain where the recordings are derived from, like interview recordings on device/ zoom or other forms of audio/video recordings from offline/online sources. Give the average and maximum number of recordings to be transcribed/translated. Give the average and maximum size of recordings in mins/hours. Mention if it is a transcribing or translation requirement. Mention the language spoken in the recordings, if known, and a rough estimate of number of recordings for each of these languages. Ignore the \"core-hours\" and \"hours required to analyse one sample\" requirement.]</p> </li> <li> <p>Abridged Data Management Plan:  [Address all points. Mention the recording file types example: .mp3, .mp4, .wav etc.]</p> </li> <li> <p>Primary Classification: [Either follow the  Standard f\u00f6r svensk indelning av forsknings\u00e4mnen link given or search by entering the field of research such as 'Social Work', 'Human Geography' etc. ]</p> </li> <li> <p>Requested Duration: [Mention the duration for which Whisper service is strictly required. Mentioning more duration than actually required might reflect negatively when a new allocation is requested for the same or new project next time. It is possible to request for a shorter duration of 1 month at first and then ask for a new one once the need arises again in the future.]</p> </li> </ul> Module Loading <p>To load the Whisper module, run the following command:</p> <pre><code>[jayan@sens2024544-bianca jayan]$ module load Whisper\n</code></pre> <p>This will also load the necessary dependencies, including <code>python</code> and <code>ffmpeg</code>.</p> <pre><code>[jayan@sens2024544-bianca jayan]$ module list\nCurrently Loaded Modules:\n1) uppmax   2) python/3.11.4   3) FFmpeg/5.1.2   4) Whisper/20240930\n</code></pre>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#command-line","title":"Command-line","text":"<p>The <code>whisper</code> command can be used to transcribe audio files. For example:</p> <pre><code>[jayan@sens2024544-bianca jayan]$ whisper audio.flac audio.mp3 audio.wav --model medium\n</code></pre> <p>For more ways to run whisper, for example on cpu node or do translations, check the correct flags by doing : <code>whisper --help</code> You can also check the source code with arguments here on the official GitHub repository.</p>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#python","title":"Python","text":"example.py<pre><code>import whisper\n\n# Load the model\nmodel = whisper.load_model(\"base\")\n\n# Transcribe an audio file\nresult = model.transcribe(\"/path/to/audiofile.mp3\")\n\n# Output the transcription\nprint(result[\"text\"])\n</code></pre>","tags":["Whisper","transcriptions","AI"]},{"location":"software/whisper/#available-models","title":"Available Models","text":"<p>For making offline usage of Whisper more convenient, we provide pre-trained models as part of the Whisper module. You can list all the available models by:</p> <pre><code>[jayan@sens2024544-bianca jayan]$ ll /sw/apps/Whisper/0.5.1/rackham/models\ntotal 13457440\n-rw-rw-r-- 1 sw  145261783 Nov 10 14:22 base.en.pt\n-rw-rw-r-- 1 sw  145262807 Nov 10 14:23 base.pt\n-rw-rw-r-- 1 sw 3086999982 Nov 10 14:39 large-v1.pt\n-rw-rw-r-- 1 sw 3086999982 Nov 10 14:40 large-v2.pt\n-rw-rw-r-- 1 sw 3087371615 Nov 10 14:27 large-v3.pt\n-rw-rw-r-- 1 sw 1528006491 Nov 10 14:24 medium.en.pt\n-rw-rw-r-- 1 sw 1528008539 Nov 10 14:25 medium.pt\n-rw-rw-r-- 1 sw  483615683 Nov 10 14:23 small.en.pt\n-rw-rw-r-- 1 sw  483617219 Nov 10 14:23 small.pt\n-rw-rw-r-- 1 sw   75571315 Nov 10 14:22 tiny.en.pt\n-rw-rw-r-- 1 sw   75572083 Nov 10 14:22 tiny.pt\n</code></pre>","tags":["Whisper","transcriptions","AI"]},{"location":"software/winscp/","title":"WinSCP","text":"","tags":["winscp","WinSCP","Windows","windows"]},{"location":"software/winscp/#winscp","title":"WinSCP","text":"<p>WinSCP is a secure file transfer tool that works under Windows.</p> <p></p> <ul> <li>Transfer file to/from Bianca using WinSCP</li> <li>Transfer file to/from Rackham using WinSCP</li> <li>Transfer file to/from Transit using WinSCP</li> </ul>","tags":["winscp","WinSCP","Windows","windows"]},{"location":"software/wrf/","title":"WRF","text":""},{"location":"software/wrf/#wrf-user-guide","title":"WRF user guide","text":""},{"location":"software/wrf/#introduction","title":"Introduction","text":"<ul> <li> <p>The Weather Research and Forecasting (WRF) Model is a next-generation mesoscale numerical weather prediction system designed to serve both operational forecasting and atmospheric research needs.</p> </li> <li> <p>Model home page</p> </li> <li> <p>ARW branch page</p> </li> <li> <p>WRF Preprocessing System (WPS). The Weather Research and Forecasting (WRF) Model is a next-generation mesoscale numerical weather prediction system designed to serve both operational forecasting and atmospheric research needs.</p> </li> <li> <p>WRF is installed as modules for version 4.1.3 and compiled with INTEL and parallelized for distributed memory (dmpar) or hybrid shared and distributed memory (sm+dm). These are available as:</p> <ul> <li>WRF/4.1.3-dmpar     default as WRF/4.1.3</li> <li>WRF/4.1.3-dm+sm</li> <li> <p>WPS is installed as version 4.1 and available as:</p> </li> <li> <p>WPS/4.1</p> </li> </ul> </li> <li> <p>There are WPS_GEOG data available.</p> </li> <li>Set the path in <code>namelist.wps</code> to:</li> </ul> <pre><code>geog_data_path = '/sw/data/WPS-geog/4/rackham/WPS_GEOG'\n</code></pre> <ul> <li>Corine and metria data are included in the WPS_GEOG directory.</li> <li>In /sw/data/WPS-geog/4/rackham you'll find GEOGRID.TBL.ARW.corine_metria that hopefully works. Copy to your WPS/GEOGRID directory and then link to GEOGRID.TBL file.</li> <li> <p>It may not work for a large domain. If so, either modify TBL file or use in inner domains only.</p> </li> <li> <p>To analyse the WRF output on the cluster you can use Vapor, NCL (module called as NCL-graphics) or wrf-python (module called as wrf-python). For details on how, please confer the web pages below:</p> <ul> <li>wrf-python,</li> <li>Vapor or</li> <li>NCL<ul> <li>is not updated anymore and the developers recommend GeoCAT which serves as an umbrella over wrf-python, among others.</li> </ul> </li> </ul> </li> </ul>"},{"location":"software/wrf/#get-started","title":"Get started","text":"<ul> <li> <p>This section assumes that you are already familiar in running WRF. If not, please check the tutorial, where you can at least omit the first 5 buttons and go directly to the last button, or depending on your needs, also check the \u201cStatic geography data\u201d and \u201cReal-time data\u201d.</p> </li> <li> <p>When running WRF/WPS you would like your own settings for the model to run and not to interfere with other users. Therefore, you need to set up a local or project directory (e.g. 'WRF') and work from there like for a local installation. You also need some of the content from the central installation. Follow these steps:</p> </li> <li> <p>Create a directory where you plan to have your input and result files.</p> </li> <li> <p>Standing in this directory copy the all or some of the following directories from the central installation.</p> <ol> <li> <p>Run directory                           for real runs</p> <ul> <li><code>cp -r /sw/EasyBuild/rackham/software/WRF/4.1.3-intel-2019b-dmpar/WRF-4.1.3/run .</code></li> <li>You can remove *.exe files in this run directory because the module files shall be used.</li> </ul> </li> <li> <p>WPS directory                          if input data has to be prepared</p> <ul> <li><code>cp -r /sw/EasyBuild/rackham/software/WPS/4.1-intel-2019b-dmpar/WPS-4.1 .</code></li> <li>You can remove *.exe files in the new directory because the module files shall be used.</li> </ul> </li> <li> <p>Test directory                          for ideal runs</p> <ul> <li><code>cp -r /sw/EasyBuild/rackham/software/WRF/4.1.3-intel-2019b-dmpar/WRF-4.1.3/test .</code></li> <li>You can remove *.exe files because the module files shall be used.</li> </ul> </li> </ol> </li> <li> <p>When WRF or WPS modules are loaded you can run with \u201cungrib.exe\u201d or for instance \u201cwrf.exe\u201d, i.e. without the \u201c./\u201d.</p> </li> <li>Normally you can run ungrib.exe, geogrid.exe and real.exe and, if not too long period, metgrid.exe, in the command line or in interactive mode.</li> <li>wrf.exe has to be run on the compute nodes. Make a batch script, see template below:</li> </ul> <pre><code>#!/bin/bash\n#SBATCH -J\n#SBATCH --mail-user\n#SBATCH --mail-type=ALL\n#SBATCH -t 0-01:00:0\n#set wall time c. 50% higher than expected\n#SBATCH -A\n#\n#SBATCH -n 40 -p node\n#this gives 40 cores on 2 nodes\nmodule load WRF/4.1.3-dmpar\n# With PMI jobs on very many nodes starts more efficiently.\nexport I_MPI_PMI_LIBRARY=/usr/lib64/libpmi2.so\nexport I_MPI_PMI2=yes\nsrun -n 40 --mpi=pmi2 wrf.exe\n</code></pre>"},{"location":"software/wrf/#running-smpardmpar","title":"Running smpar+dmpar","text":"<p>WRF compiled for Hybrid Shared + Distributed memory (OpenMP+MPI) can be more efficient than dmpar only. With good settings it runs approximately 30% faster and similarly less resources.</p> <p>To load this module type:</p> <pre><code>module load WRF/4.1.3-dm+sm\n</code></pre> <p>The submit script can look like this:</p> <pre><code>#!/bin/bash -l\n#SBATCH -J &lt;jobname&gt;\n#SBATCH --mail-user &lt;email address&gt;\n#SBATCH --mail-type=ALL\n#SBATCH -t 0-01:00:0    #set wall time c. 50% higher than expected\n#SBATCH -A &lt;project name&gt;\n#\n#SBATCH -N 2  ## case with 2 nodes = 40 cores on Rackham\n#SBATCH -n 8  ## make sure that n x c = (cores per node) x N\n#SBATCH -c 5\n#SBATCH --exclusive\n# We want to run OpenMP on one unit (the cores that share a memory channel, 10 on Rackham) or a part of it.\n# So, for Rackham, choose -c to be either 10, 5 or 2.\n# c = 5 seems to be the most efficient!\n# Set flags below!\nnt=1\nif [ -n \"$SLURM_CPUS_PER_TASK\" ]; then\n  nt=$SLURM_CPUS_PER_TASK\nfi\nml purge &gt; /dev/null 2&gt;&amp;1 # Clean the environment\nml WRF/4.1.3-dm+sm\nexport OMP_NUM_THREADS=$nt\nexport I_MPI_PIN_DOMAIN=omp\nexport I_MPI_PMI_LIBRARY=/usr/lib64/libpmi2.so\nexport I_MPI_PMI2=yes\nsrun -n 8 --mpi=pmi2 wrf.exe\n</code></pre>"},{"location":"software/wrf/#local-installation-with-module-dependencies","title":"Local installation with module dependencies","text":"<p>If you would like to change in the FORTRAN code for physics or just want the latest version you can install locally but with the dependencies from the modules</p>"},{"location":"software/wrf/#step-1-wrf-source-code-registration-and-download","title":"Step 1: WRF Source Code Registration and Download","text":"<ol> <li>Register and download</li> <li> <p>Identify download URLs you need (on Github for v4 and higher)</p> <ol> <li>WRF</li> <li>WPS</li> <li>Other?</li> </ol> </li> <li> <p>In folder of your choice at UPPMAX:</p> <ol> <li><code>wget &lt;download url&gt;</code></li> </ol> </li> <li><code>tar zxvf &lt;file&gt;</code></li> </ol>"},{"location":"software/wrf/#step-2-configure-and-compile","title":"Step 2: Configure and compile","text":"<ul> <li>Create and set the environment in a <code>SOURCEME</code>  file, see example below for a intel-dmpar build.</li> <li>Loading module WRF sets most of the environment but some variables have different names in configure file.</li> <li>Examples below assumes dmpar, but can be interchanged to dm+sm for hybrid build.</li> </ul> <pre><code>#!/bin/bash\n\nmodule load WRF/4.1.3-dmpar\n\nmodule list\n\nexport WRF_EM_CORE=1\n\nexport WRFIO_NCD_LARGE_FILE_SUPPORT=1\n\nexport NETCDFPATH=$NETCDF\n\nexport HDF5PATH=$HDF5_DIR\n\nexport HDF5=$HDF5_DIR\n</code></pre> <ul> <li>Then</li> </ul> <pre><code>source SOURCEME\n./configure\n</code></pre> <ul> <li>Choose intel and dmpar (15) or other, depending on WRF version and parallelization.</li> <li> <p>When finished it may complain about not finding netcdf.inc file. This is solved below as you have to modify the configure.wrf file.</p> </li> <li> <p>Intelmpi settings (for dmpar)</p> </li> </ul> <pre><code>DM_FC           =        mpiifort\n\nDM_CC           =        mpiicc -DMPI2_SUPPORT\n\n#DM_FC           =       mpif90 -f90=$(SFC)\n\n#DM_CC           =       mpicc -cc=$(SCC)\n</code></pre> <ul> <li>NetCDF-fortran paths</li> </ul> <pre><code>LIB_EXTERNAL    = add  flags \"-$(NETCDFFPATH)/lib -lnetcdff -lnetcdf\"  (let line end with \"\\\")\nINCLUDE_MODULES =    add flag \"-I$(NETCDFFPATH)/include\" (let line end with \"\\\")\nAdd the line below close to  NETCDFPATH:\nNETCDFFPATH     =    $(NETCDFF)\n</code></pre> <p>Then:</p> <pre><code>./compile em_real\n</code></pre> <p>When you have made modification of the code and once configure.wrf is created, just</p> <pre><code>source SOURCEME\n</code></pre> <p>and run:</p> <pre><code>./compile em_real\n</code></pre>"},{"location":"software/wrf/#running","title":"Running","text":"<p>Batch script should include:</p> <pre><code>module load WRF/4.1.3-dmpar\n\nexport I_MPI_PMI_LIBRARY=/usr/lib64/libpmi2.so\n\nexport I_MPI_PMI2=yes\n\nsrun -n 40 --mpi=pmi2 ./wrf.exe     #Note \u201d./\u201d, otherwise \u201dmodule version of wrf.exe\u201d is used\n</code></pre>"},{"location":"software/xeyes/","title":"xeyes","text":"","tags":["xeyes","eyes","console","terminal","x-forwarding"]},{"location":"software/xeyes/#xeyes","title":"<code>xeyes</code>","text":"<p><code>xeyes</code> is a program that shows two eyes. The <code>x</code> in its name refers to the X11 display server, which is one of many ways to display graphics on screen.</p> <p><code>xeyes</code> is used mostly diagnostically, i.e. to find out if one used SSH with X-forwarding. When <code>xeyes</code> is run, but does not show the eyes, it means that SSH with X-forwarding does not work.</p>","tags":["xeyes","eyes","console","terminal","x-forwarding"]},{"location":"software/xeyes/#how-to-run-xeyes","title":"How to run <code>xeyes</code>","text":"<p>In a terminal, type:</p> <pre><code>xeyes\n</code></pre> <p>If you've logged in via SSH with X-forwarding and it works correctly, you will see this:</p> <p></p> <p>If you've logged in without SSH with X-forwarding or the SSH client is not setup correctly, you will see:</p> <p></p> <p>The line that indicates the error is:</p> <pre><code>Error: Can't open display:\n</code></pre>","tags":["xeyes","eyes","console","terminal","x-forwarding"]},{"location":"software/directly-from-IG/img/","title":"Put related images here","text":""},{"location":"staff/","title":"UPPMAX staff info","text":"","tags":["staff"]},{"location":"staff/#uppmax-staff-info","title":"UPPMAX staff info","text":"<p>The UPPMAX staff uses this documentation too. There are some things that are staff-only.</p> <ul> <li>Sysadmin responsibility</li> <li>UPPMAX Matrix</li> </ul>","tags":["staff"]},{"location":"staff/#links","title":"Links","text":"<ul> <li>UPPMAX Jira</li> <li>NAISS Nextcloud<ul> <li>NAISS templates</li> </ul> </li> <li>NAISS training newsletter</li> <li>NAISS training overview</li> </ul>","tags":["staff"]},{"location":"staff/sysadmin_responsibility/","title":"Sysadmin responsibility","text":"","tags":["staff","Ansvarsforbindelse","Sysadmin","responsibility"]},{"location":"staff/sysadmin_responsibility/#sysadmin-responsibility","title":"Sysadmin responsibility","text":"","tags":["staff","Ansvarsforbindelse","Sysadmin","responsibility"]},{"location":"staff/sysadmin_responsibility/#sysadmin-responsibility-document","title":"Sysadmin responsibility document","text":"<ul> <li>Sysadmin responsibility document as PDF</li> </ul> <pre><code>Erinran om sekretess\n\nK\u00c4NSLIG INFORMATION\n\nVid UPPMAX som \u00e4r en del av Uppsala universitet hanteras forskningsdata\ninom bland annat livsvetenskap p\u00e5 uppdrag av forskare vid Uppsala universitet och andra l\u00e4ros\u00e4ten.\n\nSom verksam vid UPPMAX kommer jag i kontakt med k\u00e4nslig information,\ns\u00e5 som t.ex. DNA-sekvenser. Informationen kan vara i skriftlig, elektronisk\neller muntlig form.\n\nJag \u00e4r medveten om att informationen kan utg\u00f6ra k\u00e4nsliga personuppgifter\neller annan k\u00e4nslig information och vikten av att informationen inte otillb\u00f6rligen sprids. Jag \u00e4r \u00e4ven medveten om att informationen kan omfattas av\nsekretess enligt Offentlighets- och sekretesslagens (2009:400) best\u00e4mmelser\noch att spridande av informationen kan utg\u00f6ra brott mot tystnadsplikten\nenligt brottsbalken 20 kap 3 \u00a7.\n\nJag har tagit del av Uppsala universitets riktlinjer \u201dSystemadministrat\u00f6rers\nbefogenheter och skyldigheter\u201d och f\u00f6rbinder mig att f\u00f6lja dessa.\n\nOvanst\u00e5ende ska inte tolkas som en begr\u00e4nsning av min grundlagsskyddade\nmeddelarfrihet (tryckfrihetsf\u00f6rordningen 1 kap 1\u00a7 tredje stycket eller yttrandefrihetsgrundlagen 1 kap 2 \u00a7).\nGenom nedanst\u00e5ende signatur bekr\u00e4ftar jag ovanst\u00e5ende.\n\n___________ _____\nUnderskrift Datum\n\n_________________\nNamnf\u00f6rtydligande\n</code></pre>","tags":["staff","Ansvarsforbindelse","Sysadmin","responsibility"]},{"location":"staff/uppmax_matrix/","title":"UPPMAX Matrix","text":"","tags":["staff","matrix"]},{"location":"staff/uppmax_matrix/#uppmax-matrix","title":"UPPMAX Matrix","text":"<p>UPPMAX uses Matrix for real-time communication.</p> <ul> <li>Home server: <code>https://matrix.uppmax.uu.se</code></li> <li>Login name: you UPPMAX username, e.g. <code>sven</code></li> <li>Username: <code>@[username]:matrix.uppmax.uu.se</code>,   where <code>[username]</code> your UPPMAX username,   e.g. <code>@sven:matrix.uppmax.uu.se</code></li> </ul>","tags":["staff","matrix"]},{"location":"storage/compress_fastQ/","title":"Compress FastQ","text":""},{"location":"storage/compress_fastQ/#compress-fastq","title":"Compress FastQ","text":"<p>Page under construction</p>"},{"location":"storage/compress_format/","title":"Compress format","text":""},{"location":"storage/compress_format/#compress-format","title":"Compress format","text":"<p>Page under construction</p>"},{"location":"storage/compress_guide/","title":"Compress guide","text":""},{"location":"storage/compress_guide/#compress-guide","title":"Compress guide","text":"Modality Format Examples Compression Methods Linux Tools / Libraries Text .txt, .csv, .json, .xml Gzip, Bzip2, Zstandard, Parquet gzip, bzip2, zstd, pigz, xz, parquet-tools, jq Image .jpg, .png, .tiff, .webp JPEG, PNG, WebP, AVIF, LZW imagemagick, jpegoptim, optipng, cwebp, vips Audio .wav, .mp3, .flac, .ogg MP3, FLAC, OGG, AAC ffmpeg, lame, flac, sox, opusenc, audacity Video .mp4, .avi, .mkv, .mov H.264, H.265 (HEVC), VP9, AV1, frame sampling ffmpeg, HandBrakeCLI, avconv, mkvmerge Tabular .csv, .tsv, .parquet, .orc Parquet, ORC, Snappy, Zstd, Delta Encoding parquet-tools, csvkit, orc-tools, duckdb, spark-shell Time-Series .csv, .hdf5, .parquet Delta + Zlib, Gorilla, Run-Length, HDF5 h5py, influx-tools, zstd, lz4, h5repack Graph .graphml, .gml, .json, .adj Adjacency list compression, Sparse matrices (CSR, CSC) networkx, igraph, snap.py, zlib, np.savez_compressed Multimodal Mixed formats (JSONL + JPG + WAV) TFRecord, WebDataset, ZIP with chunked reads tar, zip, tfrecord, webdataset, shardwriter, lz4 Geospatial .shp, .geojson, .tif, .gpkg GeoTIFF + LZW/DEFLATE, ECW, JPEG2000, TopoJSON GDAL, ogr2ogr, gdal_translate, mapshaper, qgis, tippecanoe Genomic .fasta, .fastq, .bam, .vcf Gzip, BGZF (Blocked Gzip), CRAM (lossy BAM), BCF samtools, bcftools, htslib, bgzip, tabix, seqtk"},{"location":"storage/disk_storage_guide/","title":"Disk storage guide","text":""},{"location":"storage/disk_storage_guide/#disk-storage-guide","title":"Disk storage guide","text":"<p>Page under construction</p>"},{"location":"uppmax/gitlab/","title":"UPPMAX GitLab","text":"","tags":["UPPMAX","GitLab"]},{"location":"uppmax/gitlab/#uppmax-gitlab","title":"UPPMAX GitLab","text":"<p>For the UPPMAX staff, there is a GitLab page at https://gitlab.uppmax.uu.se/.</p>","tags":["UPPMAX","GitLab"]}]}